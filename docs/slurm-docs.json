[
    {
        "url": "https://slurm.schedmd.com/big_sys.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Large Cluster Administration Guide",
                "content": "This document contains Slurm administrator information specifically\nfor clusters containing 1,024 nodes or more.\nSome examples of large systems currently managed by Slurm are:\n\nFrontier at Oak Ridge National Laboratory (ORNL) with 8,699,904 cores.\nTianhe-2 at the National University of Defense Technology in China with\n4,981,760 cores.\nPerlmutter at National Energy Research Scientific Computing (NERSC) with\n761,856 cores.\n\nSlurm operation on systems orders of magnitude larger has been validated\nusing emulation.\nGetting optimal performance at that scale does require some tuning and\nthis document should help get you off to a good start.\nA working knowledge of Slurm should be considered a prerequisite\nfor this material.PerformanceTimes below are for execution of an MPI program printing \"Hello world\" and\nexiting and includes the time for processing output. Your performance may\nvary due to differences in hardware, software, and configuration.\n1,966,080 tasks on 122,880 compute nodes of a BlueGene/Q: 322 seconds\n30,000 tasks on 15,000 compute nodes of a Linux cluster: 30 seconds\nSystem Configuration\n\nThree system configuration parameters must be set to support a large number\nof open files and TCP connections with large bursts of messages. Changes can\nbe made using the /etc/rc.d/rc.local or /etc/sysctl.conf \nscript to preserve changes after reboot. In either case, you can write values\ndirectly into these files\n(e.g. \"echo 388067 > /proc/sys/fs/file-max\").\n/proc/sys/fs/file-max:\nThe maximum number of concurrently open files. The appropriate amount is highly\ndependent on system specs and workload. We recommend starting with a minimum of\n388067 or the default for your OS, whichever is greater. This may need to be\nadjusted upwards, depending on your needs.\n/proc/sys/net/ipv4/tcp_max_syn_backlog:\nMaximum number of remembered connection requests, which still have not\nreceived an acknowledgment from the connecting client.\nThe default value is 1024 for systems with more than 128Mb of memory, and 128\nfor low memory machines. If server suffers of overload, try to increase this\nnumber.\n/proc/sys/net/core/somaxconn:\nLimit of socket listen() backlog, known in userspace as SOMAXCONN. Defaults to\n128. The value should be raised substantially to support bursts of request.\nFor example, to support a burst of 1024 requests, set somaxconn to 1024.\nThe transmit queue length (txqueuelen) may also need to be modified\nusing the ifconfig command. A value of 4096 has been found to work well for one\nsite with a very large cluster\n(e.g. \"ifconfig  txqueuelen 4096\").Thread/Process Limit\n\nThere is a newly introduced limit in SLES 12 SP2 (used on Cray systems\nwith CLE 6.0UP04, to be released mid-2017).\nThe version of systemd shipped with SLES 12 SP2 contains support for the\n\nPIDs cgroup controller.\nUnder the new systemd version, each init script or systemd service is limited\nto 512 threads/processes by default.\nThis could cause issues for the slurmctld and slurmd daemons on large clusters\nor systems with a high job throughput rate.\nTo increase the limit beyond the default:\nIf using a systemd service file: Add TasksMax=N to the [Service]\n section. N can be a specific number, or special value infinity.\nIf using an init script: Create the file\n/etc/systemd/system/<init script name>.service.d/override.conf\nwith these contents:\n\n  [Service]\n  TasksMax=N\n\nNote: Earlier versions of systemd that don't support the PIDs cgroup\ncontroller simply ignore the TasksMax setting.User LimitsThe ulimit values in effect for the slurmctld daemon should\nbe set quite high for memory size, open file count and stack size.Node Selection Plugin (SelectType)\n\nWhile allocating individual processors within a node is great\nfor smaller clusters, the overhead of keeping track of the individual\nprocessors and memory within each node adds significant overhead.\nFor best scalability, allocate whole nodes using select/linear\nand avoid select/cons_tres.Job Accounting Gather Plugin (JobAcctGatherType)\n\nJob accounting relies upon the slurmstepd daemon on each compute\nnode periodically sampling data.\nThis data collection will take compute cycles away from the application\ninducing what is known as system noise.\nFor large parallel applications, this system noise can detract from\napplication scalability.\nFor optimal application performance, disabling job accounting\nis best (jobacct_gather/none).\nConsider use of job completion records (JobCompType) for accounting\npurposes as this entails far less overhead.\nIf job accounting is required, configure the sampling interval\nto a relatively large size (e.g. JobAcctGatherFrequency=300).\nSome experimentation may be required to deal with collisions\non data transmission.Node Configuration\n\nWhile Slurm can track the amount of memory and disk space actually found\non each compute node and use it for scheduling purposes, this entails\nextra overhead.\nOptimize performance by specifying the expected configuration using\nthe available parameters (RealMemory, CPUs, and\nTmpDisk).\nIf the node is found to contain less resources than configured,\nit will be marked DOWN and not used.\nWhile Slurm can easily handle a heterogeneous cluster, configuring\nthe nodes using the minimal number of lines in slurm.conf\nwill both make for easier administration and better performance.TimersThe EioTimeout configuration parameter controls how long the srun\ncommand will wait for the slurmstepd to close the TCP/IP connection used to\nrelay data between the user application and srun when the user application\nterminates. The default value is 60 seconds. Larger systems and/or slower\nnetworks may need a higher value.If a high throughput of jobs is anticipated (i.e. large numbers of jobs\nwith brief execution times) then configure MinJobAge to the smallest\ninterval practical for your environment. MinJobAge specifies the\nminimum number of seconds that a terminated job will be retained by Slurm's\ncontrol daemon before purging. After this time, information about terminated\njobs will only be available through accounting records.The configuration parameter SlurmdTimeout determines the interval\nat which slurmctld routinely communicates with slurmd.\nCommunications occur at half the SlurmdTimeout value.\nThe purpose of this is to determine when a compute node fails\nand thus should not be allocated work.\nLonger intervals decrease system noise on compute nodes (we do\nsynchronize these requests across the cluster, but there will\nbe some impact upon applications).\nFor really large clusters, SlurmdTimeout values of\n120 seconds or more are reasonable.If MPICH-2 is used, the srun command will manage the key-pairs\nused to bootstrap the application.\nDepending upon the processor speed and architecture, the communication\nof key-pair information may require extra time.\nThis can be done by setting an environment variable PMI_TIME before\nexecuting srun to launch the tasks.\nThe default value of PMI_TIME is 500 and this is the number of\nmicroseconds allotted to transmit each key-pair.\nWe have executed up to 16,000 tasks with a value of PMI_TIME=4000.The individual slurmd daemons on compute nodes will initiate messages\nto the slurmctld daemon only when they start up or when the epilog\ncompletes for a job. When a job allocated a large number of nodes\ncompletes, it can cause a very large number of messages to be sent\nby the slurmd daemons on these nodes to the slurmctld daemon all at\nthe same time. In order to spread this message traffic out over time\nand avoid message loss, The EpilogMsgTime parameter may be\nused. Note that even if messages are lost, they will be retransmitted,\nbut this will result in a delay for reallocating resources to new jobs.OtherSlurm uses hierarchical communications between the slurmd daemons\nin order to increase parallelism and improve performance. The\nTreeWidth configuration parameter controls the fanout of messages.\nThe default value is 16, meaning each slurmd daemon can communicate\nwith up to 16 other slurmd daemons and 4368 nodes can be contacted\nwith three message hops.\nThe default value will work well for most clusters.\nOptimal system performance can typically be achieved if TreeWidth\nis set to the cube root of the number of nodes in the cluster.The srun command automatically increases its open file limit to\nthe hard limit in order to process all of the standard input and output\nconnections to the launched tasks. It is recommended that you set the\nopen file hard limit to 8192 across the cluster.Last modified 22 April 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/cpu_management.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": " CPU Management User and Administrator Guide",
                "content": "OverviewThe purpose of this guide is to assist Slurm users and administrators in selecting configuration options\nand composing command lines to manage the use of CPU resources by jobs, steps and tasks. The document\nis divided into the following sections:\nOverview\nCPU Management Steps performed by Slurm\nGetting Information about CPU usage by Jobs/Steps/Tasks\nCPU Management and Slurm Accounting\nCPU Management Examples\nCPU Management through user commands is constrained by the configuration parameters\nchosen by the Slurm administrator. The interactions between different CPU management options are complex\nand often difficult to predict. Some experimentation may be required to discover the exact combination\nof options  needed to produce a desired outcome. Users and administrators should refer to the man pages\nfor slurm.conf, cgroup.conf,\nsalloc,\nsbatch and srun for detailed explanations of each\noption. The following html documents may also be useful:\nConsumable Resources in Slurm\nSharing Consumable Resources\nSupport for Multi-core/Multi-thread\nArchitectures\nPlane distributionCPU Management Steps performed by Slurm\n\nSlurm uses four basic steps to manage CPU resources for a job/step:\nStep 1: Selection of Nodes\nStep 2: Allocation of CPUs from the selected Nodes\nStep 3: Distribution of Tasks to the selected Nodes\nStep 4: Optional Distribution and Binding of Tasks to CPUs within a Node\n\nStep 1: Selection of Nodes\n\nIn Step 1, Slurm selects the set of nodes from which CPU resources are to be allocated to a job or\njob step.  Node selection is therefore influenced by many of the configuration and command line options\nthat control the allocation of CPUs (Step 2 below).\nIf \nSelectType=select/linear is configured, all resources on the selected nodes will be allocated\nto the job/step. If SelectType is configured to be\nselect/cons_tres,\nindividual sockets, cores and threads may be allocated from the selected nodes as\nconsumable resources. The consumable resource type is defined by\nSelectTypeParameters.\n\n\nStep 1 is performed by slurmctld and the select plugin.\n\n\n\nslurm.conf options that control Step 1\n\n\n\n\n\nslurm.conf\n\t\t\t\tparameter\n\n\nPossible values\n\n\nDescription\n\n\n\n\n\nNodeName\n\n\n<name of the node>\n\t\t\t\tPlus additional parameters. See man page for details.\n\n\nDefines\n\t\t\t\ta node. This includes the number and layout of boards, sockets, cores,\n\t\t\t\tthreads and processors (logical CPUs) on the node.\n\n\n\n\n\nPartitionName\n\n\n<name of the partition>\n\t\t\t\tPlus additional parameters. See man page for details.\n\n\nDefines\n\t\t\t\ta partition. Several parameters of the partition definition\n\t\t\t\taffect the selection of nodes (e.g., Nodes,\n\t\t\t\tOverSubscribe, MaxNodes)\n\n\n\n\n\nSlurmdParameters\n\n\nconfig_overrides\n\n\nControls\n\t\t\t\thow the information in a node definition is used.\n\n\n\n\n\nSelectType\n\n\n\nselect/linear | select/cons_tres\n\n\nControls\n\t\t\t\twhether CPU resources are allocated to jobs and job steps in\n\t\t\t\tunits of whole nodes or as consumable resources (sockets, cores\n\t\t\t\tor threads).\n\n\n\n\n\nSelectTypeParameters\n\n\nCR_CPU | CR_CPU_Memory | CR_Core |\nCR_Core_Memory | CR_Socket | CR_Socket_MemoryPlus additional options.  See man page for details.\n\n\nDefines\n\t\t\t\tthe consumable resource type and controls other aspects of CPU\n\t\t\t\tresource allocation by the select plugin.\n\n\n\n\n\nsrun/salloc/sbatch command line options that control Step 1\n\n\n\n\n\n\nCommand\n\t\t\t\tline option\n\n\n\nPossible values\n\n\nDescription\n\n\n\n\n\n-B, --extra-node-info\n\n\n\n\n\t\t\t\t<sockets[:cores[:threads]]>\n\n\nRestricts\n\t\t\t\tnode selection to nodes with a specified layout of sockets, cores\n\t\t\t\tand threads.\n\n\n\n\n\n-C, --constraint\n\n\n<list>\n\t\t\t\t\n\n\nRestricts\n\t\t\t\tnode selection to nodes with specified attributes\n\n\n\n\n\n--contiguous\n\n\nN/A\n\n\nRestricts\n\t\t\t\tnode selection to contiguous nodes\n\n\n\n\n\n--cores-per-socket\n\n\n<cores>\n\n\nRestricts\n\t\t\t\tnode selection to nodes with at least the specified number of cores per socket\n\n\n\n\n\n-c, --cpus-per-task\n\n\n<ncpus>\n\n\nControls\n\t\t\t\tthe number of CPUs allocated per task\n\n\n\n\n\n--exclusive\n\n\nN/A\n\n\nPrevents\n\t\t\t\tsharing of allocated nodes with other jobs. Suballocates CPUs to job steps.\n\n\n\n\n\n-F, --nodefile\n\n\n<node file>\n\n\nFile\n\t\t\t\tcontaining a list of specific nodes to be selected for the job (salloc and sbatch only)\n\n\n\n\n\n--hint\n\n\ncompute_bound |\n\t\t\t\tmemory_bound | [no]multithread\n\n\nAdditional\n\t\t\t\tcontrols on allocation of CPU resources\n\n\n\n\n\n--mincpus\n\n\n<n>\n\n\nControls\n\t\t\t\tthe minimum number of CPUs allocated per node\n\n\n\n\n\n-N, --nodes\n\n\n\n\t\t\t\t<minnodes[-maxnodes]>\n\n\nControls\n\t\t\t\tthe minimum/maximum number of nodes allocated to the job\n\n\n\n\n\n-n, --ntasks\n\n\n<number>\n\n\nControls\n\t\t\t\tthe number of tasks to be created for the job\n\n\n\n\n\n--ntasks-per-core\n\n\n<number>\n\n\nControls\n\t\t\t\tthe maximum number of tasks per allocated core\n\n\n\n\n\n--ntasks-per-socket\n\n\n<number>\n\n\nControls\n\t\t\t\tthe maximum number of tasks per allocated socket\n\n\n\n\n\n--ntasks-per-node\n\n\n<number>\n\n\nControls\n\t\t\t\tthe maximum number of tasks per allocated node\n\n\n\n\n\n-O, --overcommit\n\n\nN/A\n\n\nAllows\n\t\t\t\tfewer CPUs to be allocated than the number of tasks\n\n\n\n\n\n-p, --partition\n\n\n\n\t\t\t\t<partition_names>\n\n\nControls\n\t\t\t\twhich partition is used for the job\n\n\n\n\n\n-s, --oversubscribe\n\n\nN/A\n\n\nAllows\n\t\t\t\tsharing of allocated nodes with other jobs\n\n\n\n\n\n--sockets-per-node\n\n\n<sockets>\n\n\nRestricts\n\t\t\t\tnode selection to nodes with at least the specified number of sockets\n\n\n\n\n\n--threads-per-core\n\n\n<threads>\n\n\nRestricts\n\t\t\t\tnode selection to nodes with at least the specified number of threads per core\n\n\n\n\n\n-w, --nodelist\n\n\n\n\t\t\t\t<host1,host2,... or filename>\n\n\nList\n\t\t\t\tof specific nodes to be allocated to the job\n\n\n\n\n\n-x, --exclude\n\n\n\n\t\t\t\t<host1,host2,... or filename>\n\n\nList\n\t\t\t\tof specific nodes to be excluded from allocation to the job\n\n\n\n\n\n-Z, --no-allocate\n\n\nN/A\n\n\nBypass\n\t\t\t\tnormal allocation (privileged option available to users\n\t\t\t\t\u201cSlurmUser\u201d and \u201croot\u201d only)\n\n\n\nStep 2: Allocation of CPUs from the selected Nodes\n\nIn Step 2, Slurm allocates CPU resources to a job/step from the set of nodes selected\nin Step 1. CPU allocation is therefore influenced by the configuration and command line options\nthat relate to node selection.\nIf \nSelectType=select/linear is configured, all resources on the selected nodes will be allocated\nto the job/step. If SelectType is configured to be\nselect/cons_tres,\nindividual sockets, cores and threads may be allocated from the selected nodes as\nconsumable resources. The consumable resource type is defined by\nSelectTypeParameters.\n\nWhen using a SelectType of\nselect/cons_tres,\nthe default allocation method across nodes is block allocation (allocate all available CPUs in\na node before using another node). The default allocation method within a node is cyclic\nallocation (allocate available CPUs in a round-robin fashion across the sockets within a node).\nUsers may override the default behavior using the appropriate command\nline options described below.  The choice of allocation methods may influence which specific\nCPUs are allocated to the job/step.\n\nStep 2 is performed by slurmctld and the select plugin.\n\n\n\nslurm.conf options that control Step 2\n\n\n\n\n\nslurm.conf\n\t\t\t\tparameter\n\n\nPossible values\n\n\nDescription\n\n\n\n\n\nNodeName\n\n\n<name of the node>\n\t\t\t\tPlus additional parameters. See man page for details.\n\n\nDefines\n\t\t\t\ta node. This includes the number and layout of boards, sockets, cores,\n\t\t\t\tthreads and processors (logical CPUs) on the node.\n\n\n\n\n\nPartitionName\n\n\n<name of the partition>\n\t\t\t\tPlus additional parameters. See man page for details.\n\n\nDefines\n\t\t\t\ta partition. Several parameters of the partition definition\n\t\t\t\taffect the allocation of CPU resources to jobs (e.g., Nodes,\n\t\t\t\tOverSubscribe, MaxNodes)\n\n\n\n\n\nSlurmdParameters\n\n\nconfig_overrides\n\n\nControls\n\t\t\t\thow the information in a node definition is used.\n\n\n\n\n\nSelectType\n\n\n\nselect/linear | select/cons_tres\n\n\nControls\n\t\t\t\twhether CPU resources are allocated to jobs and job steps in\n\t\t\t\tunits of whole nodes or as consumable resources (sockets, cores\n\t\t\t\tor threads).\n\n\n\n\n\nSelectTypeParameters\n\n\nCR_CPU | CR_CPU_Memory | CR_Core |\nCR_Core_Memory | CR_Socket | CR_Socket_MemoryPlus additional options.  See man page for details.\n\n\nDefines\n\t\t\t\tthe consumable resource type and controls other aspects of CPU\n\t\t\t\tresource allocation by the select plugin.\n\n\n\n\n\nsrun/salloc/sbatch command line options that control Step 2\n\n\n\n\n\n\nCommand\n\t\t\t\tline option\n\n\n\nPossible values\n\n\nDescription\n\n\n\n\n\n-B, --extra-node-info\n\n\n\n\n\t\t\t\t<sockets[:cores[:threads]]>\n\n\nRestricts\n\t\t\t\tnode selection to nodes with a specified layout of sockets, cores\n\t\t\t\tand threads.\n\n\n\n\n\n-C, --constraint\n\n\n<list>\n\t\t\t\t\n\n\nRestricts\n\t\t\t\tnode selection to nodes with specified attributes\n\n\n\n\n\n--contiguous\n\n\nN/A\n\n\nRestricts\n\t\t\t\tnode selection to contiguous nodes\n\n\n\n\n\n--cores-per-socket\n\n\n<cores>\n\n\nRestricts\n\t\t\t\tnode selection to nodes with at least the specified number of cores per socket\n\n\n\n\n\n-c, --cpus-per-task\n\n\n<ncpus>\n\n\nControls\n\t\t\t\tthe number of CPUs allocated per task\n\n\n\n\n\n--distribution, -m\n\n\n\n\t\t\t\tblock|cyclic|arbitrary|plane=<options>[:block|cyclic]\n\n\nThe second specified distribution (after the \":\")\n\t\t\t\tcan be used to override the default allocation method within nodes\n\n\n\n\n\n--exclusive\n\n\nN/A\n\n\nPrevents\n\t\t\t\tsharing of allocated nodes with other jobs\n\n\n\n\n\n-F, --nodefile\n\n\n<node file>\n\n\nFile\n\t\t\t\tcontaining a list of specific nodes to be selected for the job (salloc and sbatch only)\n\n\n\n\n\n--hint\n\n\ncompute_bound |\n\t\t\t\tmemory_bound | [no]multithread\n\n\nAdditional\n\t\t\t\tcontrols on allocation of CPU resources\n\n\n\n\n\n--mincpus\n\n\n<n>\n\n\nControls\n\t\t\t\tthe minimum number of CPUs allocated per node\n\n\n\n\n\n-N, --nodes\n\n\n\n\t\t\t\t<minnodes[-maxnodes]>\n\n\nControls\n\t\t\t\tthe minimum/maximum number of nodes allocated to the job\n\n\n\n\n\n-n, --ntasks\n\n\n<number>\n\n\nControls\n\t\t\t\tthe number of tasks to be created for the job\n\n\n\n\n\n--ntasks-per-core\n\n\n<number>\n\n\nControls\n\t\t\t\tthe maximum number of tasks per allocated core\n\n\n\n\n\n--ntasks-per-socket\n\n\n<number>\n\n\nControls\n\t\t\t\tthe maximum number of tasks per allocated socket\n\n\n\n\n\n--ntasks-per-node\n\n\n<number>\n\n\nControls\n\t\t\t\tthe maximum number of tasks per allocated node\n\n\n\n\n\n-O, --overcommit\n\n\nN/A\n\n\nAllows\n\t\t\t\tfewer CPUs to be allocated than the number of tasks\n\n\n\n\n\n-p, --partition\n\n\n\n\t\t\t\t<partition_names>\n\n\nControls\n\t\t\t\twhich partition is used for the job\n\n\n\n\n\n-s, --oversubscribe\n\n\nN/A\n\n\nAllows\n\t\t\t\tsharing of allocated nodes with other jobs\n\n\n\n\n\n--sockets-per-node\n\n\n<sockets>\n\n\nRestricts\n\t\t\t\tnode selection to nodes with at least the specified number of sockets\n\n\n\n\n\n--threads-per-core\n\n\n<threads>\n\n\nRestricts\n\t\t\t\tnode selection to nodes with at least the specified number of threads per core\n\n\n\n\n\n-w, --nodelist\n\n\n\n\t\t\t\t<host1,host2,... or filename>\n\n\nList\n\t\t\t\tof specific nodes to be allocated to the job\n\n\n\n\n\n-x, --exclude\n\n\n\n\t\t\t\t<host1,host2,... or filename>\n\n\nList\n\t\t\t\tof specific nodes to be excluded from allocation to the job\n\n\n\n\n\n-Z, --no-allocate\n\n\nN/A\n\n\nBypass\n\t\t\t\tnormal allocation (privileged option available to users\n\t\t\t\t\u201cSlurmUser\u201d and \u201croot\u201d only)\n\n\n\nStep 3: Distribution of Tasks to the selected Nodes\n\nIn Step 3, Slurm distributes tasks to the nodes that were selected for\nthe job/step in Step 1. Each task is distributed to only one node, but more than one\ntask may be distributed to each node.  Unless overcommitment of CPUs to tasks is\nspecified for the job, the number of tasks distributed to a node is\nconstrained by the number of CPUs allocated on the node and the number of CPUs per\ntask. If consumable resources is configured, or resource sharing is allowed, tasks from\nmore than one job/step may run on the same node concurrently.\n\nStep 3 is performed by slurmctld.\n\n\n\nslurm.conf options that control Step 3\n\n\n\n\n\nslurm.conf\n\t\t\t\tparameter\n\n\nPossible values\n\n\nDescription\n\n\n\n\n\nMaxTasksPerNode\n\n\n<number>\n\n\n\n\t\t\t\tControls the maximum number of tasks that a job step can spawn on a single node\n\t\t\t\t\n\n\n\n\n\nsrun/salloc/sbatch command line options that control Step 3\n\n\n\n\n\n\nCommand\n\t\t\t\tline option\n\n\n\nPossible values\n\n\nDescription\n\n\n\n\n\n--distribution, -m\n\n\n\n\t\t\t\tblock|cyclic|arbitrary|plane=<options>[:block|cyclic]\n\n\nThe first specified distribution (before the \":\")\n\t\t\t\tcontrols the sequence in which tasks are distributed to each of the selected nodes. Note that\n\t\t\t\tthis option does not affect the number of tasks distributed to each node, but only the sequence of\n\t\t\t\tdistribution.\n\n\n\n\n\n--ntasks-per-core\n\n\n<number>\n\t\t\t\t\n\n\n\n\t\t\t\tControls the maximum number of tasks per allocated core\n\n\n\n\n\n--ntasks-per-socket\n\n\n<number>\n\n\n\n\t\t\t\tControls the maximum number of tasks per allocated socket\n\n\n\n\n\n--ntasks-per-node\n\n\n<number>\n\n\n\n\t\t\t\tControls the maximum number of tasks per allocated node\n\n\n\n\n\n-r, --relative\n\n\nN/A\n\n\nControls\n\t\t\t\twhich node is used for a job step\n\n\n\n\nStep 4: Optional Distribution and Binding of Tasks to CPUs within a Node\n\nIn optional Step 4, Slurm distributes and binds each task to a specified subset of\nthe allocated CPUs on the node to which the task was distributed in Step 3. Different\ntasks distributed to the same node may be bound to the same subset of CPUs or to\ndifferent subsets. This step is known as task affinity or task/CPU binding.\n\nStep 4 is performed by slurmd and the task plugin.\n\n\n\nslurm.conf options that control Step 4\n\n\n\n\n\nslurm.conf\n\t\t\t\tparameter\n\n\nPossible values\n\n\nDescription\n\n\n\n\n\nTaskPlugin\n\n\n\n\t\t\t\ttask/none | task/affinity | task/cgroup\n\n\n\n\t\t\t\tControls whether this step is enabled and which task plugin to use\n\t\t\t\t\n\n\n\n\n\ncgroup.conf options that control Step 4 (task/cgroup plugin only)\n\n\n\n\n\ncgroup.conf\n\t\t\t\tparameter\n\n\nPossible values\n\n\nDescription\n\n\n\n\n\nConstrainCores\n\n\n\n\t\t\t\tyes|no\n\n\n\n\t\t\t\tControls whether jobs are constrained to their allocated CPUs\n\t\t\t\t\n\n\n\n\n\nsrun/salloc/sbatch command line options that control Step 4\n\n\n\n\n\n\nCommand\n\t\t\t\tline option\n\n\n\nPossible values\n\n\nDescription\n\n\n\n\n\n--cpu-bind\n\n\n\n\t\t\t\tSee man page\n\n\nControls binding of tasks to CPUs\n\t\t\t\t(srun only)\n\n\n\n\n\n--ntasks-per-core\n\n\n<number>\n\t\t\t\t\n\n\n\n\t\t\t\tControls the maximum number of tasks per allocated core\n\n\n\n\n\n--distribution, -m\n\n\n\n\t\t\t\tblock|cyclic|arbitrary|plane=<options>[:block|cyclic]\n\n\n\n\t\t\t\tThe second specified distribution (after the \":\") controls the sequence in which tasks are\n\t\t\t\tdistributed to allocated CPUs within a node for binding of tasks to CPUs\n\n\n\nAdditional Notes on CPU Management Steps\n\nFor consumable resources, it is important for users to understand the difference between\ncpu allocation (Step 2) and task affinity/binding (Step 4).  Exclusive (unshared) allocation\nof CPUs as consumable resources limits the number of jobs/steps/tasks that\ncan use a node concurrently.  But it does not limit the set of CPUs on the node that each\ntask distributed to the node can use.  Unless some form of CPU/task binding is used\n(e.g., a task or spank plugin), all tasks distributed to a node can use all of\nthe CPUs on the node, including CPUs not allocated to their job/step.  This may have\nunexpected adverse effects on performance, since it allows one job to use CPUs allocated\nexclusively to another job.  For this reason, it may not be advisable to configure\nconsumable resources without also configuring task affinity.  Note that task affinity\ncan also be useful when select/linear (whole node allocation) is configured, to improve\nperformance by restricting each task to a particular socket or other subset of CPU\nresources on a node.Getting Information about CPU usage by Jobs/Steps/Tasks\n\nThere is no easy way to generate a comprehensive set of CPU management information\nfor a job/step (allocation, distribution and binding). However, several\ncommands/options provide limited information about CPU usage.\n\n\n\n\nCommand/Option\n\n\nInformation\n\n\n\n\n\nscontrol show job option:\n--details\n\n\n\nThis option provides a list of the nodes selected for the job and the CPU ids allocated to the job on each\nnode. Note that the CPU ids reported by this command are Slurm abstract CPU ids, not Linux/hardware CPU ids\n(as reported by, for example, /proc/cpuinfo).\n\n\n\n\n\n\nLinux command: env\n\n\n\nMan. Slurm environment variables provide information related to node and CPU usage:\n\n\nSLURM_JOB_CPUS_PER_NODE\nSLURM_CPUS_PER_TASK\nSLURM_CPU_BIND\nSLURM_DISTRIBUTION\nSLURM_JOB_NODELIST\nSLURM_TASKS_PER_NODE\nSLURM_STEP_NODELIST\nSLURM_STEP_NUM_NODES\nSLURM_STEP_NUM_TASKS\nSLURM_STEP_TASKS_PER_NODE\nSLURM_JOB_NUM_NODES\nSLURM_NTASKS\nSLURM_NPROCS\nSLURM_CPUS_ON_NODE\nSLURM_NODEID\nSLURMD_NODENAME\n\n\n\n\n\n\n\nsrun option:\n--cpu-bind=verbose\n\n\n\nThis option provides a list of the CPU masks used by task affinity to bind tasks to CPUs.\nNote that the CPU ids represented by these masks are Linux/hardware CPU ids, not Slurm\nabstract CPU ids as reported by scontrol, etc.\n\n\n\n\n\n\nsrun/salloc/sbatch option:\n-l\n\n\n\nThis option adds the task id as a prefix to each line of output from a task sent to stdout/stderr.\nThis can be useful for distinguishing node-related and CPU-related information by task id\nfor multi-task jobs/steps.\n\n\n\n\n\n\nLinux command:\ncat /proc/<pid>/status | grep Cpus_allowed_list\n\n\n\nGiven a task's pid (or \"self\" if the command is executed by the task itself), this command\nproduces a list of the CPU ids bound to the task. This is the same information that is\nprovided by --cpu-bind=verbose, but in a more readable format.\n\n\n\n\nA Note on CPU Numbering\n\nThe number and layout of logical CPUs known to Slurm is described in the node definitions in slurm.conf. This may\ndiffer from the physical CPU layout on the actual hardware.  For this reason, Slurm generates its own internal, or\n\"abstract\", CPU numbers.  These numbers may not match the physical, or \"machine\", CPU numbers known to Linux.CPU Management and Slurm Accounting\n\nCPU management by Slurm users is subject to limits imposed by Slurm Accounting. Accounting limits may be applied on CPU\nusage at the level of users, groups and clusters. For details, see the sacctmgr man page.CPU Management Examples\n\nThe following examples illustrate some scenarios for managing CPU\nresources using Slurm. Many additional scenarios are possible. In\neach example, it is assumed that all CPUs on each node are available\nfor allocation.\nExample Node and Partition Configuration\nExample 1: Allocation of whole nodes\nExample 2: Simple allocation of cores as consumable resources\nExample 3: Consumable resources with balanced allocation across nodes\nExample 4: Consumable resources with minimization of resource fragmentation\nExample 5: Consumable resources with cyclic distribution of tasks to nodes\nExample 6: Consumable resources with default allocation and plane distribution of tasks to nodes\nExample 7: Consumable resources with overcommitment of CPUs to tasks\nExample 8: Consumable resources with resource sharing between jobs\nExample 9: Consumable resources on multithreaded node, allocating only one thread per core\nExample 10: Consumable resources with task affinity and core binding\nExample 11: Consumable resources with task affinity and socket binding, Case 1\nExample 12: Consumable resources with task affinity and socket binding, Case 2\nExample 13: Consumable resources with task affinity and socket binding, Case 3\nExample 14: Consumable resources with task affinity and customized allocation and distribution\nExample 15: Consumable resources with task affinity to optimize the performance of a multi-task,\nmulti-thread job\nExample 16: Consumable resources with task cgroup\nExample Node and Partition Configuration\n\nFor these examples, the Slurm cluster contains the following nodes:\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\nn1\n\n\nn2\n\n\nn3\n\n\n\n\nNumber\n\t\t\t\tof Sockets\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n\n\nNumber\n\t\t\t\tof Cores per Socket\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n\n\nTotal\n\t\t\t\tNumber of Cores\n\n\n8\n\n\n8\n\n\n8\n\n\n8\n\n\n\n\nNumber\n\t\t\t\tof Threads (CPUs) per Core\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n\n\nTotal\n\t\t\t\tNumber of CPUs\n\n\n8\n\n\n8\n\n\n8\n\n\n16\n\n\n\nAnd the following partitions:\n\n\n\n\n\n\nPartitionName\n\n\nregnodes\n\n\nhypernode\n\n\n\n\nNodes\n\n\nn0\n\t\t\t\t n1  n2\n\n\nn3\n\n\n\n\nDefault\n\n\nYES\n\n\n-\n\n\n\nThese entities are defined in slurm.conf as follows:Nodename=n0 NodeAddr=node0 Sockets=2 CoresPerSocket=4 ThreadsPerCore=1 Procs=8\nNodename=n1 NodeAddr=node1 Sockets=2 CoresPerSocket=4 ThreadsPerCore=1 Procs=8 State=IDLE\nNodename=n2 NodeAddr=node2 Sockets=2 CoresPerSocket=4 ThreadsPerCore=1 Procs=8 State=IDLE\nNodename=n3 NodeAddr=node3 Sockets=2 CoresPerSocket=4 ThreadsPerCore=2 Procs=16 State=IDLE\nPartitionName=regnodes Nodes=n0,n1,n2 OverSubscribe=YES Default=YES State=UP\nPartitionName=hypernode Nodes=n3 State=UP\nThese examples show the use of the\ncons_tres plugin.Example 1: Allocation of whole nodes\n\nAllocate a minimum of two whole nodes to a job.slurm.conf options:SelectType=select/linear\nCommand line:srun --nodes=2 ...\nComments:SelectType=select/linear --nodes=2Example 2: Simple allocation of cores as consumable resources\n\nA job requires 6 CPUs (2 tasks and 3 CPUs per task with no overcommitment). Allocate the 6 CPUs as consumable resources\nfrom a single node in the default partition.slurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core\nCommand line:srun --nodes=1-1 --ntasks=2 --cpus-per-task=3 ...\nComments:The SelectType configuration options define cores as consumable resources.\nThe --nodes=1-1 srun option\n restricts the job to a single node. The following table shows a possible pattern of allocation\n  for this job.\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\nn1\n\n\nn2\n\n\n\n\nNumber\n\t\t\t\tof Allocated CPUs\n\n\n6\n\n\n0\n\n\n0\n\n\n\n\nNumber\n\t\t\t\tof Tasks\n\n\n2\n\n\n0\n\n\n0\n\n\n\n\nExample 3: Consumable resources with balanced allocation across nodes\n\nA job requires 9 CPUs (3 tasks and 3 CPUs per task with no overcommitment).\nAllocate 3 CPUs from each of the 3 nodes in the default partition.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core\nCommand line:srun --nodes=3-3 --ntasks=3 --cpus-per-task=3 ...\nComments:The options specify the following conditions for the job: 3 tasks, 3 unique CPUs\n per task, using exactly 3 nodes. To satisfy these conditions, Slurm must\n  allocate 3 CPUs from each node. The following table shows the allocation\n   for this job.\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\nn1\n\n\nn2\n\n\n\n\nNumber\n\t\t\t\tof Allocated CPUs\n\n\n3\n\n\n3\n\n\n3\n\n\n\n\nNumber\n\t\t\t\tof Tasks\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\nExample 4: Consumable resources with minimization of resource fragmentation\n\nA job requires 12 CPUs (12 tasks and 1 CPU per task with no overcommitment). Allocate\nCPUs using the minimum number of nodes and the minimum number of sockets required for\nthe job in order to minimize fragmentation of allocated/unallocated CPUs in the cluster.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core,CR_CORE_DEFAULT_DIST_BLOCK\nCommand line:srun --ntasks=12 ...\nComments:The default allocation method across nodes is block. This minimizes the number of nodes\n used for the job. The configuration option \n CR_CORE_DEFAULT_DIST_BLOCK sets the default allocation method within a\n node to block. This minimizes the number of sockets used for the job within a node.\n The combination of these two methods causes Slurm to allocate the 12 CPUs using the\n minimum required number of nodes (2 nodes) and sockets (3 sockets).The following\n table shows a possible pattern of allocation for this job.\n\n\n\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\nn1\n\n\nn2\n\n\n\n\nSocket id\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n4\n\n\n4\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nNumber of\n\t\t\t\tTasks\n\n\n8\n\n\n4\n\n\n0\n\n\n\n\nExample 5: Consumable resources with cyclic distribution of tasks to nodes\n\nA job requires 12 CPUs (6 tasks and 2 CPUs per task with no overcommitment). Allocate\n6 CPUs each from 2 nodes in the default partition. Distribute tasks to nodes cyclically.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core\nCommand line:srun --nodes=2-2 --ntasks-per-node=3 --distribution=cyclic \n--ntasks=6 --cpus-per-task=2 ...\nComments:The options specify the following conditions for the job: 6 tasks, 2 unique CPUs per task,\nusing exactly 2 nodes, and with 3 tasks per node. To satisfy these conditions, Slurm\nmust allocate 6 CPUs from each of the 2 nodes. The \n--distribution=cyclic option causes the tasks to be distributed to the nodes in a\nround-robin fashion. The following table shows a possible pattern of allocation and\ndistribution for this job.\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\nn1\n\n\nn2\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n6\n\n\n6\n\n\n0\n\n\n\n\nNumber of\n\t\t\t\tTasks\n\n\n3\n\n\n3\n\n\n0\n\n\n\n\nDistribution\n\t\t\t\tof Tasks to Nodes, by Task id\n\n\n\n\t\t\t\t024\n\n\n\t\t\t\t135\n\n-\n\n\n\nExample 6: Consumable resources with default allocation and\nplane distribution of tasks to nodes\n\nA job requires 16 CPUs (8 tasks and 2 CPUs per task with no overcommitment).\nUse all 3 nodes in the default partition. Distribute tasks to each node in blocks of two in a round-robin fashion.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core\nCommand line:srun --nodes=3-3 --distribution=plane=2 --ntasks=8 --cpus-per-task=2 ...\nComments:The options specify the following conditions for the job: 8 tasks, 2 unique CPUs\nper task, using all 3 nodes in the partition. To satisfy these conditions using\nthe default allocation method across nodes (block), Slurm allocates 8 CPUs from\nthe first node, 6 CPUs from the second node and 2 CPUs from the third node.\nThe --distribution=plane=2 option causes Slurm\nto distribute tasks in blocks of two to each of the nodes in a round-robin fashion,\nsubject to the number of CPUs allocated on each node.  So, for example, only 1 task\nis distributed to the third node because only 2 CPUs were allocated on that node and\neach task requires 2 CPUs. The following table shows a possible pattern of allocation\nand distribution for this job.\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\nn1\n\n\nn2\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n8\n\n\n6\n\n\n2\n\n\n\n\nNumber of\n\t\t\t\tTasks\n\n\n4\n\n\n3\n\n\n1\n\n\n\n\nDistribution\n\t\t\t\tof Tasks to Nodes, by Task id\n\n\n0\n\t\t\t\t 15 6\n\n\n2\n\t\t\t\t 37\n\n\n4\n\n\n\n\n\nExample 7: Consumable resources with overcommitment of CPUs to tasks\n\nA job has 20 tasks. Run the job in a single node.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core\nCommand line:srun --nodes=1-1 --ntasks=20 --overcommit ...\nComments:The\n--overcommit option allows the job to\nrun in only one node by overcommitting CPUs to tasks.The following table shows\n a possible pattern of allocation and distribution for this job.\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\nn1\n\n\nn2\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n8\n\n\n0\n\n\n0\n\n\n\n\nNumber of\n\t\t\t\tTasks\n\n\n20\n\n\n0\n\n\n0\n\n\n\n\nDistribution\n\t\t\t\tof Tasks to Nodes, by Task id\n\n\n0\n\t\t\t\t- 19\n\n\n-\n\n\n-\n\n\n\n\nExample 8: Consumable resources with resource sharing between jobs\n\n2 jobs each require 6 CPUs (6 tasks per job with no overcommitment).\nRun both jobs simultaneously in a single node.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core\nCommand line:srun --nodes=1-1 --nodelist=n0 --ntasks=6 --oversubscribe ...\nsrun --nodes=1-1 --nodelist=n0 --ntasks=6 --oversubscribe ...\nComments:The --nodes=1-1 and --nodelist=n0\nsrun options together restrict both jobs to node n0. The\nOverSubscribe=YES option in the partition definition plus\nthe --oversubscribe srun option allows the two\njobs to oversubscribe CPUs on the node.\nExample 9: Consumable resources on multithreaded node,\nallocating only one thread per core\n\nA job requires 8 CPUs (8 tasks with no overcommitment). Run the job on node n3,\nallocating only one thread per core.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_CPU\nCommand line:srun --partition=hypernode --ntasks=8 --hint=nomultithread ...\nComments:The CR_CPU configuration\noption enables the allocation of only one thread per core.\nThe --hint=nomultithread\nsrun option causes Slurm to allocate only one thread from each core to\nthis job. The following table shows a possible pattern of allocation\nfor this job.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNodename\n\n\nn3\n\n\n\n\nSocket id\n\n\n0\n\n\n1\n\n\n\n\nCore id\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n\n\nCPU id\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n4\n\n\n4\n\n\n\n\nAllocated\n\t\t\t\tCPU ids\n\n\n0\n\t\t\t\t  2   4   6\n\n\n8\n\t\t\t\t 10  12  14\n\n\n\n\nExample 10: Consumable resources with task affinity and core binding\n\nA job requires 6 CPUs (6 tasks with no overcommitment). Run the job in a\nsingle node in the default partition. Apply core binding to each task.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core\nTaskPlugin=task/affinity\nCommand line:srun --nodes=1-1 --ntasks=6 --cpu-bind=cores ...\nComments:Using the default allocation method within nodes (cyclic), Slurm allocates\n3 CPUs on each socket of 1 node. Using the default distribution method\nwithin nodes (cyclic), Slurm distributes and binds each task to an allocated\ncore in a round-robin fashion across the sockets. The following table shows\na possible pattern of allocation, distribution and binding for this job.\nFor example, task id 2 is bound to CPU id 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\n\n\nSocket id\n\n\n0\n\n\n1\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n3\n\n\n3\n\n\n\n\nAllocated\n\t\t\t\tCPU ids\n\n\n0 1 2\n\n\n4 5 6\n\n\n\n\nBinding of\n\t\t\t\tTasks to CPUs\n\n\nCPU id\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n\nTask id\n\n\n0\n\n\n2\n\n\n4\n\n\n-\n\n\n1\n\n\n3\n\n\n5\n\n\n-\n\n\n\n\nExample 11: Consumable resources with task affinity and socket binding, Case 1\n\nA job requires 6 CPUs (6 tasks with no overcommitment). Run the job in\na single node in the default partition. Apply socket binding to each task.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core\nTaskPlugin=task/affinity\nCommand line:srun --nodes=1-1 --ntasks=6 --cpu-bind=sockets ...\nComments:Using the default allocation method within nodes (cyclic), Slurm allocates 3\nCPUs on each socket of 1 node. Using the default distribution method within nodes\n(cyclic), Slurm distributes and binds each task to all of the allocated CPUs in\none socket in a round-robin fashion across the sockets. The following table shows\na possible pattern of allocation, distribution and binding for this job. For\nexample, task ids 1, 3 and 5 are all bound to CPU ids 4, 5 and 6.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\n\n\nSocket id\n\n\n0\n\n\n1\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n3\n\n\n3\n\n\n\n\nAllocated\n\t\t\t\tCPU ids\n\n\n0 1 2\n\n\n4 5 6\n\n\n\n\nBinding of\n\t\t\t\tTasks to CPUs\n\n\nCPU id\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n\nTask ids\n\n\n0\n\t\t\t\t 2  4\n\n\n-\n\n\n1\n\t\t\t\t 3  5\n\n\n-\n\n\n\n\nExample 12: Consumable resources with task affinity and socket binding, Case 2\n\nA job requires 6 CPUs (2 tasks with 3 cpus per task and no overcommitment). Run the job in\na single node in the default partition. Allocate cores using the block allocation method.\nDistribute cores using the block distribution method. Apply socket binding to each task.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core,CR_CORE_DEFAULT_DIST_BLOCK\nTaskPlugin=task/affinity\nCommand line:srun --nodes=1-1 --ntasks=2 --cpus-per-task=3 --cpu-bind=sockets \n--distribution=block:block ...\nComments:Using the block allocation method, Slurm allocates 4\nCPUs on one socket and 2 CPUs on the other socket of one node. Using the block distribution method within\nnodes, Slurm distributes 3 CPUs to each task.  Applying socket binding, Slurm binds each task to all\nallocated CPUs in all sockets in which the task has a distributed CPU. The following table shows\na possible pattern of allocation, distribution and binding for this job. In this example, using the\nblock allocation method CPU ids 0-3 are allocated on socket id 0 and CPU ids 4-5 are allocated on\nsocket id 1.  Using the block distribution method, CPU ids 0-2 were distributed to task id 0, and CPU ids\n3-5 were distributed to task id 1.  Applying socket binding, task id 0 is therefore bound to the allocated\nCPUs on socket 0, and task id 1 is bound to the allocated CPUs on both sockets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\n\n\nSocket id\n\n\n0\n\n\n1\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n4\n\n\n2\n\n\n\n\nAllocated\n\t\t\t\tCPU ids\n\n\n0 1 2 3\n\n\n4 5\n\n\n\n\nBinding of\n\t\t\t\tTasks to CPUs\n\n\nCPU id\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n\nTask ids\n\n\n0 1\n\n\n1\n\n\n-\n\n\n\n\nExample 13: Consumable resources with task affinity and socket binding, Case 3\n\nA job requires 6 CPUs (2 tasks with 3 cpus per task and no overcommitment). Run the job in\na single node in the default partition. Allocate cores using the block allocation method.\nDistribute cores using the cyclic distribution method. Apply socket binding to each task.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core,CR_CORE_DEFAULT_DIST_BLOCK\nTaskPlugin=task/affinity\nCommand line:srun --nodes=1-1 --ntasks=2 --cpus-per-task=3 --cpu-bind=sockets \n--distribution=block:cyclic ...\nComments:Using the block allocation method, Slurm allocates 4\nCPUs on one socket and 2 CPUs on the other socket of one node. Using the cyclic distribution method within\nnodes, Slurm distributes 3 CPUs to each task.  Applying socket binding, Slurm binds each task to all\nallocated CPUs in all sockets in which the task has a distributed CPU. The following table shows\na possible pattern of allocation, distribution and binding for this job. In this example, using the\nblock allocation method CPU ids 0-3 are allocated on socket id 0 and CPU ids 4-5 are allocated on\nsocket id 1.  Using the cyclic distribution method, CPU ids 0, 1 and 4 were distributed to task id 0, and CPU ids\n2, 3 and 5 were distributed to task id 1.  Applying socket binding, both tasks are therefore bound to the\nallocated CPUs on both sockets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\n\n\nSocket id\n\n\n0\n\n\n1\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n4\n\n\n2\n\n\n\n\nAllocated\n\t\t\t\tCPU ids\n\n\n0 1 2 3\n\n\n4 5\n\n\n\n\nBinding of\n\t\t\t\tTasks to CPUs\n\n\nCPU id\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n\nTask ids\n\n\n0 1\n\n\n0 1\n\n\n-\n\n\n\nExample 14: Consumable resources with task affinity and\ncustomized allocation and distribution\n\nA job requires 18 CPUs (18 tasks with no overcommitment). Run the job in the\ndefault partition. Allocate 6 CPUs on each node using block allocation within\nnodes. Use cyclic distribution of tasks to nodes and block distribution of\ntasks for CPU binding.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core,CR_CORE_DEFAULT_DIST_BLOCK\nTaskPlugin=task/affinity\nCommand line:srun --nodes=3-3 --ntasks=18 --ntasks-per-node=6 \n--distribution=cyclic:block --cpu-bind=cores ...\nComments:This example shows the use of task affinity with customized allocation of CPUs and\ndistribution of tasks across nodes and within nodes for binding. The srun options\nspecify the following conditions for the job: 18 tasks, 1 unique CPU per task, using\nall 3 nodes in the partition, with 6 tasks per node.\nThe CR_CORE_DEFAULT_DIST_BLOCK\nconfiguration option specifies block allocation within nodes. To satisfy these\nconditions, Slurm allocates 6 CPUs on each node, with 4 CPUs allocated on one socket\nand 2 CPUs on the other socket. The \n--distribution=cyclic:block option specifies cyclic distribution of\ntasks to nodes and block distribution of tasks to CPUs within nodes for binding.\nThe following table shows a possible pattern of allocation, distribution and binding\nfor this job. For example, task id 10 is bound to CPU id 3 on node n1.\n\n\n\n\nNodename\n\n\nn0\n\n\nn1\n\n\nn2\n\n\n\n\nSocket id\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n4\n\n\n2\n\n\n4\n\n\n2\n\n\n4\n\n\n2\n\n\n\n\nAllocated\n\t\t\t\tCPU ids\n\n\n0 1 2 3 4 5\n\n\n0 1 2 3 4 5\n\n\n0 1 2 3 4 5\n\n\n\n\nNumber of\n\t\t\t\tTasks\n\n\n6\n\n\n6\n\n\n6\n\n\n\n\nDistribution\n\t\t\t\tof Tasks to Nodes, by Task id\n\n\n0\n\t\t\t3\n\t\t\t6\n\t\t\t9\n\t\t\t12\n\t\t\t15\n\t\t\t\n\n\n1\n\t\t\t4\n\t\t\t7\n\t\t\t10\n\t\t\t13\n\t\t\t16\n\t\t\t\n\n\n2\n\t\t\t5\n\t\t\t8\n\t\t\t11\n\t\t\t14\n\t\t\t17\n\t\t\t \n\n\n\n\nBinding of\n\t\t\t\tTasks to CPUs\n\n\nCPU id\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n\nTask id\n\n\n0\n\n\n3\n\n\n6\n\n\n9\n\n\n12\n\n\n15\n\n\n-\n\n\n-\n\n\n1\n\n\n4\n\n\n7\n\n\n10\n\n\n13\n\n\n16\n\n\n-\n\n\n-\n\n\n2\n\n\n5\n\n\n8\n\n\n11\n\n\n14\n\n\n17\n\n\n-\n\n\n-\n\n\n\nExample 15: Consumable resources with task affinity to\noptimize the performance of a multi-task, multi-thread job\n\nA job requires 9 CPUs (3 tasks and 3 CPUs per task with no overcommitment). Run\nthe job in the default partition, managing the CPUs to optimize the performance\nof the job.slurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core,CR_CORE_DEFAULT_DIST_BLOCK\nTaskPlugin=task/affinity\nCommand line:srun --ntasks=3 --cpus-per-task=3 --ntasks-per-node=1 --cpu-bind=cores ...\nComments:To optimize the performance of this job, the user wishes to allocate 3 CPUs from each of\n3 sockets and bind each task to the 3 CPUs in a single socket. The\nSelectTypeParameters configuration option specifies\na consumable resource type of cores and block allocation within nodes. The\nTaskPlugin\nconfiguration option enables task affinity. The srun options specify the following\nconditions for the job: 3 tasks, with 3 unique CPUs per task, with 1 task per node. To satisfy\nthese conditions, Slurm allocates 3 CPUs from one socket in each of the 3 nodes in the default partition. The\n--cpu-bind=cores option causes Slurm to bind\neach task to the 3 allocated CPUs on the node to which it is distributed. The\nfollowing table shows a possible pattern of allocation, distribution and binding\nfor this job. For example, task id 2 is bound to CPU ids 0, 1 and 2 on socket id 0 of node n2.\n\n\n\n\nNodename\n\n\nn0\n\n\nn1\n\n\nn2\n\n\n\n\nSocket id\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n3\n\n\n0\n\n\n3\n\n\n0\n\n\n3\n\n\n0\n\n\n\n\nAllocated\n\t\t\t\tCPU ids\n\n\n0 1 2\n\n\n0 1 2\n\n\n0 1 2\n\n\n\n\nNumber of\n\t\t\t\tTasks\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\nDistribution\n\t\t\t\tof Tasks to Nodes, by Task id\n\n\n0\n\n\n1\n\n\n2\n\n\n\n\nBinding of\n\t\t\t\tTasks to CPUs\n\n\nCPU id\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n\nTask id\n\n\n0\n\n\n-\n\n\n1\n\n\n-\n\n\n2\n\n\n--\n\n\n\n\nExample 16: Consumable resources with task cgroup\n\nA job requires 6 CPUs (6 tasks with no overcommitment). Run the job in a\nsingle node in the default partition.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core\nTaskPlugin=task/cgroup\ncgroup.conf options:ConstrainCores=yes\nCommand line:srun --nodes=1-1 --ntasks=6 ...\nComments:The task/cgroup plugin currently supports only the block method for\nallocating cores within nodes. Slurm distributes tasks to the cores but\nwithout cpu binding, each task has access to all the allocated CPUs.\nThe following table shows a possible pattern of allocation, distribution\nand binding for this job.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\n\n\nSocket id\n\n\n0\n\n\n1\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n4\n\n\n2\n\n\n\n\nAllocated\n\t\t\t\tCPU ids\n\n\n0 1 2 3\n\n\n4 5\n\n\n\n\nBinding of\n\t\t\t\tTasks to CPUs\n\n\nCPU id\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n\nTask id\n\n\n0-5\n\n\n0-5\n\n\n-\n\n\n\n\nThe task/cgroup plugin does not bind tasks to CPUs. To bind tasks to CPUs and\nfor access to all task distribution options, the task/affinity plugin can be\nused with the task/cgroup plugin:\nTaskPlugin=task/cgroup,task/affinityLast modified 04 January 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/wckey.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Workload Characterization Key (WCKey) Management",
                "content": "A WCKey is an orthogonal way to do accounting against possibly\n  unrelated accounts. This can be useful where users from different\n  accounts are all working on the same project.\nslurm(dbd).conf settings\n\nIncluding \"WCKey\" in your AccountingStorageEnforce option in\n  your slurm.conf file will enforce WCKeys per job.  This means only\n  jobs with valid WCKeys (WCKeys previously added through sacctmgr)\n  will be allowed to run.\n\nIf you wish to track the value of a jobs WCKey you must set\n  the TrackWCKey option in both the slurm.conf as well as the\n  slurmdbd.conf files.  This will assure the WCKey is tracked on each\n  job.  If you set \"WCKey\" in your AccountingStorageEnforce\n  line TrackWCKey is set automatically, it still needs to be\n  added to your slurmdbd.conf file though.\n\nsbatch/salloc/srun\n Each submitting tool has the --wckey= option that can set the WCKey for a\n job.  [SBATCH|SALLOC|SLURM]_WCKEY can also be set in the environment\n to set the WCKey.  If no WCKey is given the WCKey for the job will be\n set to the users default WCKey for the cluster, which can be set up\n with sacctmgr.  Also if no WCKey is specified the accounting record\n is appended with a '*' to signify the WCKey was not specified.  This\n is useful for a manager to determine if a user is specifying their\n WCKey or not.\n\nsacct\nSacct can be used to view the WCKey by adding \"wckey\" to the\n  --format option.  You can also single out jobs by using the\n  --wckeys= option which would only send information about jobs that\n  ran with specific WCKeys.\n\nsacctmgr\nSacctmgr is used to manage WCKeys.  You can add and remove WCKeys\n  from users or list them.\n\nYou add a user to a WCKey much like you do an account, only the\n  WCKey doesn't need to be created before hand. i.e.\n\n\nsacctmgr add user da wckey=secret_project\n\n You can remove them from a WCKey in the same fashion.\n\n\nsacctmgr del user da wckey=secret_project\n\n To alter the users default WCKey you can run a line like\n\n\nsacctmgr mod user da cluster=snowflake set defaultwckey=secret_project\n\n\nWhich will change the default WCKey for user \"da\" on cluster\n\"snowflake\" to be \"secret_project\".  If you want this for all clusters\njust remove the cluster= option.\n\nsreport\nInformation about reports available for WCKeys can be\n  found on the sreport manpage.\n\nLast modified 14 November 2014\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "sbatch/salloc/srun",
                "content": " Each submitting tool has the --wckey= option that can set the WCKey for a\n job.  [SBATCH|SALLOC|SLURM]_WCKEY can also be set in the environment\n to set the WCKey.  If no WCKey is given the WCKey for the job will be\n set to the users default WCKey for the cluster, which can be set up\n with sacctmgr.  Also if no WCKey is specified the accounting record\n is appended with a '*' to signify the WCKey was not specified.  This\n is useful for a manager to determine if a user is specifying their\n WCKey or not.\nsacctSacct can be used to view the WCKey by adding \"wckey\" to the\n  --format option.  You can also single out jobs by using the\n  --wckeys= option which would only send information about jobs that\n  ran with specific WCKeys.\nsacctmgrSacctmgr is used to manage WCKeys.  You can add and remove WCKeys\n  from users or list them.\nYou add a user to a WCKey much like you do an account, only the\n  WCKey doesn't need to be created before hand. i.e.\n\nsacctmgr add user da wckey=secret_project\n You can remove them from a WCKey in the same fashion.\n\nsacctmgr del user da wckey=secret_project\n To alter the users default WCKey you can run a line like\n\nsacctmgr mod user da cluster=snowflake set defaultwckey=secret_project\n\nWhich will change the default WCKey for user \"da\" on cluster\n\"snowflake\" to be \"secret_project\".  If you want this for all clusters\njust remove the cluster= option.\nsreportInformation about reports available for WCKeys can be\n  found on the sreport manpage.\nLast modified 14 November 2014"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/authentication.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Authentication Plugins",
                "content": "OverviewIt is important to know that the Remote Procedure Calls (RPCs) that are\nreceived by Slurm are coming from trusted sources. There are a few different\nauthentication mechanisms available within Slurm to verify the legitimacy and\nintegrity of the requests.MUNGEMUNGE can be used to create and validate credentials. It allows Slurm to\nauthenticate the UID and GID of a request from another host that has matching\nusers and groups. MUNGE libraries must exist when building Slurm in order for\nit to be able to use munge for authentication. All hosts in the cluster must\nhave a shared cryptographic key.Setup\nMUNGE requires that there be a shared key on the machines running\nslurmctld, slurmdbd and the nodes. You can create a key file by entering your\nown text or by generating random data:\n\ndd if=/dev/random of=/etc/munge/munge.key bs=1024 count=1\n\n\nThis key should be owned by the \"munge\" user and should not be readable\nor writable by other users:\n\nchown munge:munge /etc/munge/munge.key\nchmod 400 /etc/munge/munge.key\n\n\nDistribute the key file to the machines on the cluster. It needs to be\non the machines running slurmctld, slurmdbd, slurmd and any submit hosts\nyou have configured. Use the distribution method of your choice.\nThe \"munge\" service should be running on any machines that need to use it\nfor authentication. It should be enabled and started on all the machines you\ndistributed a key to:\n\nsystemctl enable munge\nsystemctl start munge\n\n\nChanging the authentication mechanism requires a restart of Slurm daemons.\nThe daemons need to be stopped before updating the slurm.conf so that client\ncommands do not use a mechanism other than what the running daemons are\nexpecting.\nUpdate your slurm.conf and slurmdbd.conf to use MUNGE authentication.\n\nslurm.conf:\n\nAuthType = auth/munge\nCredType = cred/munge\n\nslurmdbd.conf:\n\nAuthType = auth/munge\n\n\n\nStart the Slurm daemons back up with the appropriate method for your\ncluster.\nSlurmBeginning with version 23.11, Slurm has its own plugin that can create and\nvalidate credentials. It validates that the requests come from legitimate UIDs\nand GIDs on other hosts with matching users and groups. All hosts in the\ncluster must have a shared cryptographic key.Single Key Setup\nFor the authentication to happen correctly you must have a shared key on\nthe machine running slurmctld, slurmdbd and the nodes. You can create a key\nfile by entering your own text or by generating random data:\n\ndd if=/dev/random of=/etc/slurm/slurm.key bs=1024 count=1\n\n\nThe slurm.key or slurm.jwks should be owned by SlurmUser and should not be\nreadable or writable by other users. This example assumes the SlurmUser is\n'slurm':\n\nchown slurm:slurm /etc/slurm/slurm.key\nchmod 600 /etc/slurm/slurm.key\n\n\nDistribute the key file to the machines on the cluster. It needs to be\non the machines running slurmctld, slurmdbd, slurmd and sackd.\nUpdate your slurm.conf and slurmdbd.conf to use the Slurm authentication\ntype.\n\nslurm.conf:\n\nAuthType = auth/slurm\nCredType = cred/slurm\n\nslurmdbd.conf:\n\nAuthType = auth/slurm\n\n\n\nStart the daemons back up with the appropriate method for your cluster.\nBeginning with version 24.05, you may alternatively create a slurm.jwks file\nwith multiple keys defined. The slurm.jwks file aids with key rotation, as\nthe cluster does not need to be restarted at once when a key is rotated.\nInstead, an scontrol reconfigure is sufficient. There are no slurm.conf\nparameters required to use the slurm.jwks file, instead, the presence of the\nslurm.jwks file enables this functionality. If the slurm.jwks is not present or\ncannot be read, the cluster defaults to the slurm.key.Multiple Key SetupSetup for multiple keys is very similar to the single key setup with the\nexception of a richer key file. The key file is composed of one jwks-esque\n\"keys\" list, with a number of \"key\" entries into this list. The key entries\nhave many different fields. An example file is below.\n\n{\n  \"keys\": [\n    {\n      \"alg\": \"HS256\",\n      \"kty\": \"oct\",\n      \"kid\": \"key-identifier\",\n      \"k\": \"VGhlIGtleSBiZWxvdyBtZSBuZXZlciBsaWVz\",\n      \"exp\": 1718200800\n    },\n    {\n      \"alg\": \"HS256\",\n      \"kty\": \"oct\",\n      \"kid\": \"key-identifier-2\",\n      \"k\": \"VGhlIGtleSBhYm92ZSBtZSBhbHdheXMgbGllcw==\",\n      \"use\": \"default\"\n    }\n  ]\n}\nThe following fields are used by auth/slurm. (Additional fields can be\npresent, but will be ignored.)\n\nalg \u2014 The cryptographic algorithm used with the key. This\nfield is required, and the value MUST be HS256.\nkty \u2014 The family of cryptographic algorithms used to sign the\nkey. This field is required, and the value MUST be oct.\nkid \u2014 The case-sensitive text identifier used for key\nmatching. This field is required, and the text must be unique.\nk \u2014 The actual key, represented in a Base64 or Base64url\nencoded binary blob. This field is required, and must be longer than 16\nbytes.\nuse \u2014 Determines whether this key is the default key.\nAcceptable values are only \"default\", which denotes this key as the default\nkey. There can only be one default key. This field is optional.\nexp \u2014 The expiration date of the key as a Unix timestamp. This\nfield is optional.\n\nOnce the slurm.jwks file has been created, use the same process for setting\nup auth/slurm as in the single key setup, except use the slurm.jwks file instead\nof the slurm.key file.If the cluster does not have access to consistent user ids from LDAP or\nthe operating system, you can use the\nuse_client_ids option to\nallow it to use the Slurm authentication mechanism.SACKSlurm's internal authentication makes use of a subsystem \u2014 the\nSlurm Auth and Cred Kiosk (SACK) \u2014\nthat is responsible for handling requests from the auth/slurm and\ncred/slurm plugins. This subsystem is automatically started and managed\ninternally by the slurmctld, slurmdbd, and slurmd daemons on each system with no\nneed to run a separate daemon.For login nodes not running one of these Slurm daemons, the sackd\ndaemon should be run to allow the Slurm client commands to authenticate to\nthe rest of the cluster. This daemon can also manage cached configuration files\nfor configless environments.JWTSlurm can be configured to use JSON Web Tokens (JWT) for authentication\npurposes. This is configured with the AuthAltType parameter and is used only\nfor client to server communication. You can read more about this authentication\nmechanism and how to install it here.Last modified 02 July 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/job_state_codes.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Job State Codes",
                "content": "Each job in the Slurm system has a state assigned to it. How the job state is\ndisplayed depends on the method used to identify the state.OverviewIn the Slurm code, there are base states and state flags.\nEach job has a base state and may have additional state flags set. When using\nthe REST API, both the base state and current\nflag(s) will be returned.When the squeue and sacct\ncommand report a job state, they represent it as a single state. Both will\nrecognize all base states but not all state flags. If a recognized flag is\npresent, it will be reported instead of the base state. Refer to the relevant\ncommand documentation for details.This page represents all job codes and flags that are represented in the\ncode. The names provided are the string representations that are used in\nuser-facing output. For most, the names used in the code are identical, with\nJOB_ at the start.\nFor more visibility into the job states and flags, set\nDebugFlags=TraceJobs and SlurmctldDebug=verbose\n(or higher) in slurm.conf.Job statesEach job known to the system will have one of the following states:\n\nNameDescription\nBOOT_FAILterminated due to node boot failure\nCANCELLEDcancelled by user or administrator\nCOMPLETEDcompleted execution successfully;\n\tfinished with an exit code of zero on all nodes\nDEADLINEterminated due to reaching the latest\n\tacceptable start time specified for the job\nFAILEDcompleted execution unsuccessfully;\n\tnon-zero exit code or other failure condition\nNODE_FAILterminated due to node failure\nOUT_OF_MEMORYexperienced out of memory error\nPENDINGqueued and waiting for initiation;\n\twill typically have a reason code\n\tspecifying why it has not yet started\nPREEMPTEDterminated due to\n\tpreemption; may transition to another state\n\tbased on the configured PreemptMode and job characteristics\nRUNNINGallocated resources and executing\nSUSPENDEDallocated resources but execution\n\tsuspended, such as from preemption or a\n\tdirect request from an\n\tauthorized user\nTIMEOUTterminated due to reaching the time limit,\n\tsuch as those configured in slurm.conf or\n\tspecified for the individual job\n\nJob flagsJobs may have additional flags set:\n\nNameDescription\nCOMPLETINGjob has finished or been cancelled\n\tand is performing cleanup tasks, including the\n\tepilog script if present\nCONFIGURINGjob has been allocated nodes and is\n\twaiting for them to boot or reboot\nLAUNCH_FAILEDfailed to launch on the chosen\n\tnode(s); includes prolog failure and\n\tother failure conditions\nPOWER_UP_NODEjob has been allocated powered down\n\tnodes and is waiting for them to boot\nRECONFIG_FAILnode configuration for job failed\nREQUEUEDjob is being requeued,\n\tsuch as from preemption or a\n\tdirect request from an\n\tauthorized user\nREQUEUE_FEDrequeued due to conditions of its\n\tsibling job in a federated setup\nREQUEUE_HOLDsame as REQUEUED but will\n\tnot be considered for scheduling until it is\n\treleased\nRESIZINGthe size of the job is changing; prevents\n\tconflicting job changes from taking place\nRESV_DEL_HOLDheld due to deleted reservation\nREVOKEDrevoked due to conditions of its sibling\n\tjob in a federated setup\nSIGNALINGoutgoing signal to job is pending\nSPECIAL_EXITsame as REQUEUE_HOLD but\n\tused to identify a special situation\n\tthat applies to this job\nSTAGE_OUTstaging out data\n\t(burst buffer)\nSTOPPEDreceived SIGSTOP to suspend the job without\n\treleasing resources\nUPDATE_DBsending an update about the job to the\n\tdatabase\n\nLast modified 01 October 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/upgrades.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Upgrade Guide",
                "content": "Slurm supports in-place upgrades between certain versions. This page provides\nimportant details about the steps necessary to perform an upgrade and the\npotential complications to prepare for.See also Quick Start Administrator GuideContents\nRelease Cycle\n\nCompatibility Window\nEPEL Repository\nPre-Release Versions\n\nReverting an Upgrade\nMinor Upgrades\nUpgrade Procedure\n\nPreparation\nCreate Backups\nslurmdbd (Accounting)\n\nDatabase Server\n\nslurmctld (Controller)\nslurmd (Compute Nodes)\nOther Slurm Commands\nCustomized Slurm Plugins\n\nSeamless Upgrades\nRelease Cycle\nThe Slurm version number contains three period-separated numbers that\nrepresent both the major Slurm release and maintenance release level.\nFor example, Slurm 23.11.4:\n23.11 = major release\n\nThis matches the year and month of initial release (November 2023)\nMajor releases may contain changes to RPCs (remote procedure calls),\n\tstate files, configuration options, and core functionality\n\n.4 = maintenance version\n\nMaintenance releases may contain bug fixes and performance improvements\n\nPrior to the 24.05 release, Slurm operated on a 9-month release cycle for\nmajor versions. Slurm 24.05 represents the first release on the\n\nnew 6-month cycle.Compatibility Window\nUpgrades from the previous two major releases are compatible. For\nexample, slurmdbd 23.11.x is capable of accepting messages from slurmctld\ndaemons and commands with a version of 23.11.x, 23.02.x or 22.05.x. It is also\ncapable of updating the records in the database that were recorded by an\ninstance of slurmdbd running these versions.The Slurm 24.11 release will introduce compatibility with three previous\nmajor releases to provide a similar support duration with the more frequent\n6-month release cycle:\n\n\nSlurm Release\nRevised End of Support(total length)\nCompatible Prior Version\n\n\n23.02\nNovember 2024 (21 months)\n22.05, 21.08\n\n\n23.11\nMay 2025 (18 months)\n23.02, 22.05\n\n\n24.05\nNovember 2025 (18 months)\n23.11, 23.02\n\n\n24.11\nMay 2026 (18 months)\n24.05, 23.11, 23.02\n\n\n25.05\nNovember 2026 (18 months)\n24.11, 24.05, 23.11\n\n\n25.11\nMay 2027 (18 months)\n25.05, 24.11, 24.05\n\n\nUpgrades from incompatible versions will fail immediately upon startup.\nIt is required to perform upgrades from incompatible prior versions in steps,\ngoing to newer versions compatible with the current running version. It may\ntake several steps to upgrade to a current release of Slurm. For example,\ninstead of upgrading directly from Slurm 20.11 to 23.11, first upgrade all\nsystems to Slurm 22.05 and verify functionality, then proceed to upgrade to\n23.11. This ensures that each upgrade performed is tested and can be supported\nby SchedMD. Compatibility requirements apply to running jobs and upgrading\noutside of their compatibility window will result in the jobs being killed and\njob accounting being lost.EPEL Repository\nIn the beginning of 2021, a version of Slurm was added to the\nEPEL repository. This version is not provided by or supported by SchedMD, and is\nnot currently supported for customer use. Unfortunately, this inclusion could\ncause Slurm to be updated to a newer version outside of a planned maintenance\nperiod or result in conflicting packages. In order to prevent Slurm from being\nchanged and broken unintentionally, we recommend you modify the EPEL Repository\nconfiguration to exclude all Slurm packages from automatic updates.Add the following under the [epel]\nsection of /etc/yum.repos.d/epel.repo:\nexclude=slurm*Pre-Release Versions\nWhen installing pre-release versions (e.g., 24.05.0rc1 or\nmaster branch), you should prepare\nfor unexpected crashes, bugs, and loss of state information. SchedMD aims to\nuse the NEWS file to indicate cases in which state information will be lost with\npre-release versions. However, these pre-release versions receive limited\ntesting and are not intended for production clusters. Sites are encouraged\nto actively run pre-release versions on test machines before each major release.\nReverting an Upgrade\nReverting an upgrade (or downgrading) is not supported once any of the\nSlurm daemons have been started. When starting up after an upgrade, the Slurm\ndaemons (slurmctld, slurmdbd, and slurmd) will update their relevant state\nfiles and databases to the structure used in the new version. If you revert to\nan older version, the relevant Slurm daemon will not recognize the new state\nfile or database, resulting in loss or corruption of state information or job\naccounting. The Slurm daemons will likely refuse to start unless configured to\nstart with the risk of possible data loss.By using recovery tools, like comprehensive file backups, disk images, and\nsnapshots, it may be possible to revert components to the pre-upgrade state.\nIn particular, restoring the contents of StateSaveLocation (as defined in\nslurm.conf) and (if configured) the accounting database will be required\nif you wish to revert an upgrade. Reverting an upgrade will wipe out anything\nthat happened after the backups were created.Minor Upgrades\nWhen upgrading to a newer minor maintenance release (as\ndefined above), we recommend following the same\nupgrade procedure as with major releases. You will find that the process takes\nless time, and is more accommodating of mixed versions and in-place\ndowngrades. However, you should always have current backups to solidify your\nrecovery options.Upgrade Procedure\nThe upgrades procedure can be summarized as follows. Note the specific order\nin which the daemons should be upgraded:\nPrepare cluster for the upgrade\nCreate backups\nUpgrade slurmdbd\nUpgrade slurmctld\nUpgrade slurmd (preferably with slurmctld)\nUpgrade login nodes and client commands\nRecompile/upgrade customized Slurm plugins\nTest key functionality\nArchive backup data\nBefore considering the upgrade complete, wait for all jobs that were already\nrunning to finish. Any jobs started before the slurmd system was upgraded\nwill be running with the old version of slurmstepd, so starting another\nupgrade or trying to use new features in the new version may cause problems.NOTE: If multiple daemons are present on the same system, they may\nneed to be upgraded at the same time due to dependencies to the general\nslurm package. After upgrading, daemons should be started in the order\nlisted above. This is not a recommended setup for production; sites are\nstrongly advised to assign a single core Slurm daemon to each system.Preparation\nRELEASE_NOTES and NEWS\nReview relevant release notes in the RELEASE_NOTES file in root of\nSlurm source directory for the target release and any major versions between\nwhat you're currently running and the target you are upgrading to. Pay\nparticular attention to any entries in which items are removed or\nchanged. These are particularly likely to require specific attention or\nchanges during the upgrade. Also look for changes in optional slurm components\nthat you are using. You may also notice new items added to Slurm that you wish\nto start using after the upgrade.Release notes for the latest major version are\navailable here. Release notes for other\nversions can be found in the source, which can be viewed on\nGitHub\nby selecting the branch or tag corresponding to the desired version. More\ndetailed changes, including minor release changes, can be found in the\nNEWS file, but are usually not needed to prepare for upgrades.Configuration Changes\nAlways prepare and test configuration changes in a test environment\nbefore upgrading in production. Changes outlined in the release notes will need\nto be looked up in the man pages (such as slurm.conf\n) for details and new syntax. Certain options in your configuration files\nmay need to be changed as features and functionality are improved in every major\nSlurm release. Typically, new naming and syntax conventions are introduced\nseveral versions before the old ones are removed, so you may be able to make the\nnecessary changes before starting the upgrade process.Plan for Downtime\nRefer to the expected downtime guidance in the\nfollowing sections for each relevant Slurm daemon, particularly the\nslurmdbd. Notify affected users of the estimated\ndowntime for the relevant services and the potential impact on their jobs.\nWhenever possible, try to plan upgrades during SchedMD's support hours.\nIf you encounter an issue outside of these hours there will be a delay before\nassistance can be provided.OpenAPI Changes\nSites using --json or --yaml arguments with any CLI\ncommands or running slurmrestd need to check for format\ncompatibility and data_parser plugin removals before upgrading. The formats for\nthe values parsed and dumped as JSON and YAML are handled by the data_parser\nand openapi plugins. Changes to the formats are tracked in the\nOpenAPI release notes.\n\n\nRelease Notes\nAdded OpenAPI plugins\nAdded Data_Parser plugin\nRemoved in Release\n\n\n20.02\nv0.0.35,dbv0.0.35\n\n22.05\n\n\n20.11\nv0.0.36, dbv0.0.36\n\n23.02\n\n\n21.08\nv0.0.37, dbv0.0.37\n\n23.11\n\n\n22.05\nv0.0.38, dbv0.0.38\n\n24.05\n\n\n23.02\nv0.0.39, dbv0.0.39\nv0.0.39\n24.11\n\n\n23.11\nslurmctld, slurmdbd\nv0.0.40\n25.05\n\n\n24.05\n\nv0.0.41\n25.11\n\n\n24.11\n\nv0.0.42\n26.05\n\n\nNOTE: The unversioned openapi/slurmctld and openapi/slurmdbd plugins\nhave no planned removal release.Any scripts or clients making use of --json or\n--yaml arguments with any CLI commands may need to pass the\ndata_parser version explicitly to avoid issues after an upgrade. The default\ndata_parser used is the latest version which may not have a compatible format\nwith the prior versions. Sites can use the specification generation mode to\ncompare formatting differences.\n\n$CLI_COMMAND --json=v0.0.41+spec_only > /tmp/v41.json;\n$CLI_COMMAND --json=v0.0.40+spec_only > /tmp/v40.json;\njson_diff /tmp/v40.json /tmp/v41.json;\nIn the event of a format incompatibility, the preferred data_parser can be\nrequested explicitly starting with the v0.0.40 plugins in any release before\nthe plugin's removal.\n\n$CLI_COMMAND --json=v0.0.41 $OTHER_ARGS | $SITE_SCRIPT;\n$CLI_COMMAND --json=v0.0.40 $OTHER_ARGS | $SITE_SCRIPT;\n$CLI_COMMAND --yaml=v0.0.41 $OTHER_ARGS | $SITE_SCRIPT;\n$CLI_COMMAND --yaml=v0.0.40 $OTHER_ARGS | $SITE_SCRIPT;\nAny slurmrestd web clients can determine the relevant plugin\nbeing used by looking at the URL being queried. Example URLs:\n\nhttp://$HOST/slurmdb/v0.0.40/jobs\nhttp://$HOST/slurm/v0.0.40/jobs\nThe relevant data_parser plugin in the example URLs is \"v0.0.40\" which\nmatches the data_parser/v0.0.40 plugin. Plugin naming follows the\nnaming schema of vXX.XX.XX where the XX are numbers. The naming\nschema matches the internal naming schema for Slurm's packed binary RPC layer\nbut is not directly related. The URLs for each given data_parser plugins will\nremain a valid query target until the plugin is removed as part of SchedMD's\ncommitment to ensure release limited backwards compatibility. While it should\nbe possible to continue using any client from a prior release while the plugins\nare still supported, sites should always recompile any generated OpenAPI\nclients and test thoroughly before upgrading.Create Backups\nAlways create full backups to restore all parts of Slurm, including\nthe Mysql database, before upgrading in the event the upgrade must be reverted.\nSchedMD aims to make supported upgrades a seamless process but it is possible\nfor unexpected issues to arise and irreversibly corrupt all of the data\nkept by Slurm. If something like this happens, it will not be possible to\nrecover any corrupted data and you will be reliant on backed up data.It is recommended to prepare recovery options (file backups, disk images,\nsnapshots, database dumps) that will take you back to a known working cluster\nstate. How backups are taken is specific to how the systems integrator\ndesigned and setup the cluster and procedures are not provided here.At a minimum, back up the following:\n\nStateSaveLocation as defined in\nslurm.conf, or it can be\nqueried by calling scontrol show config | grep StateSaveLocation\nEntire slurm configuration directory, as defined by\nconfigure --sysconfdir=DIR during compilation.\nThis is usually located in /etc/slurm/\nMySQL database (if slurmdbd is configured). Usually done by calling\n\nmysqldump --databases slurm_acct_db > /path/to/offline/storage/backup.sql\n\nThis assumes that slurmdbd is not running while the dump is running.\nIf you wish to back it up while slurmdbd is running, you may use the\n--single-transaction flag with the following limitations:\n\nDatabase operations may be slower while the dump is running\nRestoring this dump will restore the database at the time the dump was\nstarted, losing any changes made during or after the dump\nCertain cluster operations may lead to an incorrect or failed dump:\n\nCreating a new database\nUpgrading an existing database\nAdding or Removing a cluster in the slurmdbd\n\nArchiving or Purging accounting data\n\n\n\n\nslurmdbd (Accounting)\nIf slurmdbd is used in your environment, it must be at the same or\nhigher major release number as the slurmctld daemon(s), and at a close enough\nversion for compatibility. Thus, when\nperforming upgrades, it should be upgraded first. When a backup slurmdbd host\nis in use, it should be upgraded at the same time as the primary.Upgrades to the slurmdbd may require significant downtime.\nWith large accounting databases, the precautionary database dump will take some\ntime, and the upgraded daemon may be unresponsive for tens of minutes while it\nupdates the database to the new schema. Sites are encouraged to use the\npurge functionality if older\naccounting data is not required for normal operations. Purging old records\nbefore attempting to upgrade can significantly decrease outage time.The non-slurmdbd functionality of the cluster will continue to operate while\nthe upgrade is in process, provided the activity does not fill up the slurmdbd\nAgent queue on the slurmctld node.  While slurmdbd is offline, you should\nmonitor the memory usage of slurmctld, and the DBD Agent queue size, as\nreported by sdiag, to ensure it does not exceed the configured\nMaxDBDMsgs in slurm.conf.\nCli commands sacct and \nsacctmgr will not work while slurmdbd is down.\nslurmrestd queries that include slurmdb in\nthe URL path will fail while slurmdbd is down.It is preferred to create a backup of the database after shutting down the\nslurmdbd daemon, when the MySQL database is no longer changing. If you\nwish to take a backup with mysqldump while the slurmdbd is still\nrunning, you can add --single-transaction to the mysqldump command.\nNote that the slurmdbd will continue to execute operations that will not be\ncontained in the dump, which may cause complications if you need to restore\nthe database to this state.The suggested upgrade procedure is as follows:\nShutdown the slurmdbd daemon(s) gracefully:\nsacctmgr shutdownor via systemd:\nsystemctl stop slurmdbd Wait until slurmdbd is fully down before\nproceeding or there may be data loss from data that was not fully saved.\nsystemctl status slurmdbd\n\nBackup the Slurm database\nVerify that the innodb_buffer_pool_size in my.cnf is greater than the\ndefault. See the recommendation in the\n\n\taccounting page.\nUpgrade the slurmdbd daemon binaries and libraries.\nInstall the new RPM/DEB\n\tpackages only on the slurmdbd system. Do not upgrade the other Slurm\n\tsystems at this time.\nStart the primary slurmdbd daemon.\n\tNOTE: If you typically use systemd, it is recommended to\n\tinitially start the daemon directly as the configured SlurmUser:\n\tsudo -u slurm slurmdbd -D\nWhen the daemon starts up for the first time after upgrading, it\n\twill take some extra time to update existing records in the database. If\n\tit is started with systemd and reaches the configured timeout value, it\n\tmay be killed prematurely potentially causing data loss. After it\n\tfinishes starting up, you can use Ctrl+C to exit, then\n\tstart it normally with systemd.\nStart the backup slurmdbd daemon (if applicable).\nValidate accounting operation, such as retrieving data through\n\tsacct or sacctmgr.\nDatabase Server\nWhen upgrading the database server that is used by slurmdbd (e.g., MySQL or\nMariaDB), usually no special procedures are required. It is recommended to use a\ndatabase server that is supported by the publisher (or that was at the time when\nthe chosen Slurm version was initially released). Database upgrades should be\nperformed while the slurmdbd is stopped and according to the recommended\nprocedure for the database used.When upgrading an existing accounting database to MariaDB 10.2.1 or\nlater from an older version of MariaDB or any version of MySQL, ensure you are\nrunning slurmdbd 22.05.7 or later. These versions will gracefully handle\nchanges to MariaDB default values that can cause problems for slurmdbd.slurmctld (Controller)\nIt is preferred to upgrade the slurmctld system(s) at the same time as slurmd\non the compute nodes and other Slurm commands on client machines and login nodes.\nThe effects of downtime on slurmctld and slurmd daemons are largely the same,\nso upgrading them all together minimizes the total duration of these effects.\nRolling upgrades are also possible if the slurmctld is upgraded first. When\nmultiple slurmctld hosts are used, all should be upgraded simultaneously.Upgrading the slurmctld involves a brief period of downtime during\nwhich job submissions are not accepted, queued jobs are not scheduled, and\ninformation about completing jobs is held. These functions will resume once\nthe upgraded controller is started.The recommended upgrade procedure is below, including optional steps for a\nsimultaneous upgrade of slurmd systems:\nIncrease configured SlurmdTimeout and SlurmctldTimeout values and\n\texecute scontrol reconfig for them to take effect.\n\tThe new timeout should be long enough to perform the upgrade using\n\tyour preferred method. If the timeout is reached, nodes may be marked\n\tDOWN and their jobs killed.\nShutdown the slurmctld daemon(s).\n(opt.) Shutdown the slurmd daemons on the compute nodes.\nBack up the contents of the configured StateSaveLocation.\nUpgrade the slurmctld (and optionally slurmd) daemons.\n(opt.) Restart the slurmd daemons on the compute nodes.\nRestart the slurmctld daemon(s).\nValidate proper operation, such as communication with nodes and a job's\n\tability to successfully start and finish.\nRestore the preferred SlurmdTimeout and SlurmctldTimeout values and\n\texecute scontrol reconfig for them to take effect.\nslurmd (Compute Nodes)\nIt is preferred to upgrade all slurmd nodes at the same time as the slurmctld.\nIt is also possible to perform a rolling upgrade by upgrading the slurmd nodes\nlater in any number of groups. Sites are encouraged to minimize the amount of\ntime during which mixed versions are used in a cluster.Upgrades will not interrupt running jobs as long as SlurmdTimeout\nis not reached during the process. However, while the slurmd is down for\nupgrades, new jobs will not be started and finishing jobs will wait to\nreport back to the controller until it comes back online.If you are upgrading the slurmd nodes separately from the controller, the\nfollowing procedure can be followed:\nIncrease the configured SlurmdTimeout value and execute\n\tscontrol reconfig for it to take effect.\n\tThe new timeout should be long enough to perform the upgrade using\n\tyour preferred method. If the timeout is reached, nodes may be marked\n\tDOWN and their jobs killed.\nShutdown the slurmd daemons on the compute nodes.\nBack up the contents of the configured StateSaveLocation.\nUpgrade the slurmd daemons.\nRestart the slurmd daemons.\nValidate proper operation, such as communication with the controller and a\n\tjob's ability to successfully start and finish.\nRepeat for any other groups of nodes that need to be upgraded.\nRestore the preferred SlurmdTimeout value and\nexecute scontrol reconfig for it to take effect.\nOther Slurm Commands\nOther Slurm commands (including client commands) do not require special\nattention when upgrading, except where specifically noted in the release notes.\nYou should also pay attention to any changes introduced in these additional\ncomponents. After core Slurm components have been upgraded, upgrade additional\ncomponents and client commands using the normal method for your system, then\nrestart any affected daemons.Customized Slurm Plugins\nSlurm's main public API library (libslurm.so.X.0.0) increases its version\nnumber with every major release, so any application linked against it should be\nrecompiled after an upgrade. This includes locally developed Slurm plugins.If you have built your own version of Slurm plugins, besides having to\nrecompile them, they will likely need modification to support the new version\nof Slurm. It is common for plugins to add new functions and function arguments\nduring major updates. See the RELEASE_NOTES file for details about these\nchanges.Slurm's PMI-1 (libpmi.so.0.0.0) and PMI-2 (libpmi2.so.0.0.0) public API\nlibraries do not change between releases and are meant to be permanently\nfixed. This means that linking against either of them will not require you\nto recompile the application after a Slurm upgrade, except in the unlikely\nevent that one of them changes. It is unlikely because these libraries must\nbe compatible with any other PMI-1 and PMI-2 implementations. If there was a\nchange, it would be announced in the RELEASE_NOTES and would only happen on\na major release.As an example, MPI stacks like OpenMPI and MVAPICH2 link against Slurm's\nPMI-1 and/or PMI-2 API, but not against our main public API. This means that at\nthe time of writing this documentation, you don't need to recompile these\nstacks after a Slurm upgrade. One known exception is MPICH. When MPICH is\ncompiled with Slurm support and with the Hydra Process Manager, it will use\nthe Slurm API to obtain job information. This link means you will need to\nrecompile the MPICH stack after an upgrade.One easy way to know if an application requires a recompile is to inspect all\nof its ELF files with 'ldd' and grep for 'slurm'. If you see a versioned\n'libslurm.so.x.y.z' reference, then the application will likely need to be\nrecompiled.Seamless Upgrades\nIn environments where the Slurm build process is customized, it is possible\nto install a new version of Slurm to a unique directory and use a symbolic link\nto point the directory in your PATH to the version of Slurm you would like to\nuse. This allows you to install the new version before you are in a maintenance\nperiod as well as easily switch between versions should you need to roll\nback for any reason. It also avoids potential problems with library conflicts\nthat might arise from installing different versions to the same directory.Last modified 15 August 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/configless_slurm.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "\"Configless\" Slurm",
                "content": "\"Configless\" Slurm is a feature that allows the compute nodes \u2014\nspecifically the slurmd process \u2014 and user commands running on login\nnodes to pull configuration information directly from the slurmctld instead of\nfrom a pre-distributed local file. Your cluster does require a central set of\nconfiguration files on the Slurm controllers \u2014 \"configless\" in Slurm's\nparlance means that the compute nodes, login nodes, and other cluster hosts\ndo not need to be deployed with local copies of these files.The slurmd on startup will reach out to a slurmctld that you specify and the\nconfig files will be pulled to the node. This slurmctld can be identified by\neither an explicit option, or \u2014 preferably \u2014 through DNS SRV\nrecords defined within the cluster itself.If you have a login node you will be running client commands\nfrom, those client commands will have to use the DNS record to get the\nconfiguration information from the controller when they run.\nIf you expect to have a lot of traffic from a login node, this\ncan generate a lot of requests for the configuration files. In cases like\nthis, you may want to consider running slurmd on the machine so it can\nmanage the configuration files, but not allowing it to run jobs.Installation\n\nThere are no extra steps required to install this feature. It is built in\nby default starting with Slurm 20.02.SetupThe slurmctld must first be configured to run in the configless mode.\nThis is handled by setting SlurmctldParameters=enable_configless in\nslurm.conf and restarting slurmctld.Once enabled, you must configure the slurmd to get its configs from the\nslurmctld. This can be accomplished either by launching slurmd with the\n--conf-server option, or by setting a DNS SRV record and ensuring there\nis no local configuration file on the compute node.The --conf-server options takes precedence over the DNS record.The command line option takes \"$host[:$port]\", so an example would look like:\n\nslurmd --conf-server slurmctl-primary:6817\n\nSpecifying the port is optional and will default to 6817 if it is not present.\nMultiple slurmctlds can be specified as a comma-separated list, in priority\norder (highest to lowest).\n\nslurmd --conf-server slurmctl-primary:6817,slurmctl-secondary\nThe same information can be provided in a DNS SRV record. For example:\n_slurmctld._tcp 3600 IN SRV 10 0 6817 slurmctl-backup\n_slurmctld._tcp 3600 IN SRV 0 0 6817 slurmctl-primary\n\nWill provide the required information to the slurmd on startup. As shown above,\nmultiple SRV records can be specified if you have deployed Slurm in an HA\nsetup. The DNS SRV entry with the lowest priority should be your primary\nslurmctld, with higher priority values for backup slurmctlds.\nInitial Testing\n\nWith the slurmctld configured and slurmd started, you can check in a couple\nplaces to make sure the configs are present on the node. Config files will be\nin SlurmdSpoolDir under the /conf-cache/, and a symlink to this\nlocation will be created automatically in /run/slurm/conf. You can\nconfirm that reloading is working by adding a comment to your slurm.conf on the\nslurmctld node and running\nscontrol reconfig and checking that the config\nwas updated.Limitations\n\nUsing \"%n\" in \"SlurmdSpoolDir\" or \"SlurmdPidFile\" will not be properly\nsubstituted for the NodeName unless slurmd is also launched with the \"-N\"\noption.If you are using systemd to launch slurmd, you must ensure that\n\"ConditionPathExists=*\" is not present in the unit file or the slurmd will not\nstart. (The example slurmd.service file shipped in Slurm 20.02 and above does\nnot include this entry.)If any of the supported config files \"Include\" additional config files,\nthe Included configs will ONLY be shipped if their \"Include\" filename\nreference has no path separators and the file is located adjacent to slurm.conf.\nAny additional config files will need to be shared a different way or added to\nthe parent config.\nIf Prolog and Epilog scripts are specified in slurm.conf,\nthe scripts will ONLY be shipped if the filenames referenced have no path\nseparators and the file is located adjacent to slurm.conf.\nNotesThe order of precedence for determining what configuration source to use\nis as follows:\nThe slurmd --conf-server $host[:$port] option\nThe -f $config_file option\nThe SLURM_CONF environment variable (if set)\nA local slurm config file:\n  \nThe default slurm config file (likely /etc/slurm.conf)\nFor user commands, a cached slurm config file\n        (run/slurm/conf/slurm.conf)\n\n\nThe SLURM_CONF_SERVER environment variable (if set)\nAny DNS SRV records (from lowest priority value to highest)\n\nThe TTL (Time To Live) of the SRV record does not affect the validity\n    of the obtained configuration. The nodes will have to be notified of any\n    changes to the configuration file through an\n    scontrol reconfig or a slurmd restart.\n\nSupported configuration files are:\nslurm.conf\nacct_gather.conf\ncgroup.conf\ncli_filter.lua\ngres.conf\nhelpers.conf\njob_container.conf\nmpi.conf\noci.conf\nplugstack.conf\nscrun.lua\ntopology.conf\nLast modified 25 September 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/heterogeneous_jobs.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Heterogeneous Job Support",
                "content": "\nOverview\nSubmitting Jobs\nBurst Buffers\n\nManaging Jobs\nAccounting\n\nLaunching Applications (Job Steps)\nEnvironment Variables\nExamples\nLimitations\nHeterogeneous Steps\nSystem Administrator Information\nOverviewSlurm version 17.11 and later supports the ability to submit and manage\nheterogeneous jobs, in which each component has virtually all job options\navailable including partition, account and QOS (Quality Of Service).\nFor example, part of a job might require four cores and 4 GB for each of 128\ntasks while another part of the job would require 16 GB of memory and one CPU.Submitting Jobs\n\nThe salloc, sbatch and srun commands can all be used\nto submit heterogeneous jobs.\nResource specifications for each component of the heterogeneous job should be\nseparated with \":\" character.\nFor example:\n$ sbatch --cpus-per-task=4 --mem-per-cpu=1  --ntasks=128 : \\\n         --cpus-per-task=1 --mem-per-cpu=16 --ntasks=1 my.bash\nOptions specified for one component of a heterogeneous job (or job step) will\nbe used for subsequent components to the extent which is expected to be helpful.\nPropagated options can be reset as desired for each component (e.g. a different\naccount name could be specified for each hetjob component.\nFor example, --immediate and --job-name are propagated, while\n--ntasks and --mem-per-cpu are reset to default values for each\ncomponent.\nA list of propagated options follows.\n--account\n--acctg-freq\n--begin\n--cluster-constraint\n--clusters\n--comment\n--deadline\n--delay-boot\n--dependency\n--distribution\n--epilog            (option available only in srun)\n--error\n--export\n--export-file\n--exclude\n--get-user-env\n--gid\n--hold\n--ignore-pbs\n--immediate\n--input\n--job-name\n--kill-on-bad-exit  (option available only in srun)\n--label             (option available only in srun)\n--mcs-label\n--mem\n--msg-timeout       (option available only in srun)\n--no-allocate       (option available only in srun)\n--no-requeue\n--nice\n--no-kill\n--open-mode         (option available only in srun)\n--output\n--parsable\n--priority\n--profile\n--propagate\n--prolog            (option available only in srun)\n--pty               (option available only in srun)\n--qos\n--quiet\n--quit-on-interrupt (option available only in srun)\n--reboot\n--reservation\n--requeue\n--signal\n--slurmd-debug      (option available only in srun)\n--task-epilog       (option available only in srun)\n--task-prolog       (option available only in srun)\n--time\n--test-only\n--time-min\n--uid\n--unbuffered        (option available only in srun)\n--verbose\n--wait\n--wait-all-nodes\n--wckey\n--workdir\nThe task distribution specification applies separately within each job\ncomponent. Consider for example a heterogeneous job with each component being\nallocated 4 CPUs on 2 nodes. In our example, job component zero is allocated\n2 CPUs on node \"nid00001\" and 2 CPUs on node \"nid00002\". Job component one is\nallocated 2 CPUs on node \"nid00003\" and 2 CPUs on node \"nid00004\". A task\ndistribution of \"cyclic\" will distribute the first 4 tasks in a cyclic fashion\non nodes \"nid00001\" and \"nid00002\", then distribute the next 4 tasks in a cyclic\nfashion on nodes \"nid00003\" and \"nid00004\" as shown below.\nNode nid00001Node nid00002Node nid00003Node nid00004\nRank 0Rank 1Rank 4Rank 5\nRank 2Rank 3Rank 6Rank 7\nSome options should be specified only in the first hetjob component.\nFor example, specifying a batch job output file in the second hetjob component's\noptions will result in the first hetjob component (where the batch script\nexecutes) using the default output file name.Environment variables used to specify default options for the job submit\ncommand will be applied to every component of the heterogeneous job\n(e.g. SBATCH_ACCOUNT).Batch job options can be included in the submitted script for multiple\nheterogeneous job components. Each component should be separated by a line\ncontaining the line \"#SBATCH hetjob\" as shown below.\n$ cat new.bash\n#!/bin/bash\n#SBATCH --cpus-per-task=4 --mem-per-cpu=16g --ntasks=1\n#SBATCH hetjob\n#SBATCH --cpus-per-task=2 --mem-per-cpu=1g  --ntasks=8\n\nsrun run.app\n\n$ sbatch new.bash\nIs equivalent to the following:\n$ cat my.bash\n#!/bin/bash\nsrun run.app\n\n$ sbatch --cpus-per-task=4 --mem-per-cpu=16g --ntasks=1 : \\\n         --cpus-per-task=2 --mem-per-cpu=1g  --ntasks=8 my.bash\nThe batch script will be executed in the first node in the first component\nof the heterogeneous job. For the above example, that will be the job component\nwith 1 task, 4 CPUs and 64 GB of memory (16 GB for each of the 4 CPUs).If a heterogeneous job is submitted to run in multiple clusters not\npart of a federation (e.g. \"sbatch --cluster=alpha,beta ...\") then the entire\njob will be sent to the cluster expected to be able to start all components\nat the earliest time.A resource limit test is performed when a heterogeneous job is submitted in\norder to immediately reject jobs that will not be able to start with current\nlimits.\nThe individual components of the heterogeneous job are validated, like all\nregular jobs.\nThe heterogeneous job as a whole is also tested, but in a more limited\nfashion with respect to quality of service (QOS) limits.\nEach component of a heterogeneous job counts as a \"job\" with respect to\nresource limits.Burst Buffers\n\nA burst buffer can either be persistent or linked to a specific job ID.\nSince a heterogeneous job consists of multiple job IDs, a job-specific burst\nbuffer will be associated with only one heterogeneous job component.\nEach component can have its own burst buffer directives, and they are processed\nseparately. Only a persistent burst buffer can be accessed by all components\nof a heterogeneous job. Persistent burst buffers are only available in the\ndatawarp plugin. A sample batch script demonstrating this for the datawarp\nplugin is appended.\n#!/bin/bash\n#SBATCH --nodes=1 --constraint=haswell\n#BB create_persistent name=alpha capacity=10 access=striped type=scratch\n#DW persistentdw name=alpha\n#SBATCH hetjob\n#SBATCH --nodes=16 --constraint=knl\n#DW persistentdw name=alpha\n...\nNOTE: Cray's DataWarp interface directly reads the job script, but\nhas no knowledge of \"Slurm's \"hetjob\" directive, so Slurm internally rebuilds\nthe script for each job component so that only that job component's burst buffer\ndirectives are included in that script. The batch script's first component of the\njob will be modified in order to replace the burst buffer directives of other\njob components with \"#EXCLUDED directive\" where directive is \"DW\" or \"BB\"\nfor the datawarp plugin and is the\nconfigured value for the lua plugin.\nThis prevents their interpretation by Cray infrastructure and aids\nadministrators in writing an interface for the lua plugin.\nSince the batch script will only be executed by the first job\ncomponent, the subsequent job components will not include commands from the\noriginal script. These scripts are built and managed by Slurm for internal\npurposes (and visible from various Slurm commands) from a user script as shown\nabove. An example is shown below:\nRebuilt script for first job component\n\n#!/bin/bash\n#SBATCH --nodes=1 --constraint=haswell\n#BB create_persistent name=alpha capacity=10 access=striped type=scratch\n#DW persistentdw name=alpha\n#SBATCH hetjob\n#SBATCH --nodes=16 --constraint=knl\n#EXCLUDED DW persistentdw name=alpha\n...\n\n\nRebuilt script for second job component\n\n#!/bin/bash\n#SBATCH --nodes=16 --constraint=knl\n#DW persistentdw name=alpha\nexit 0\nManaging JobsInformation maintained in Slurm for a heterogeneous job includes:\njob_id: Each component of a heterogeneous job will have its own\nunique job_id.\nhet_job_id: This identification number applies to all components\nof the heterogeneous job. All components of the same job will have the same\nhet_job_id value and it will be equal to the job_id of the\nfirst component. We refer to this as the \"heterogeneous job leader\".\nhet_job_id_set: Regular expression identifying all job_id\nvalues associated with the job.\nhet_job_offset: A unique sequence number applied to each component\nof the heterogeneous job. The first component will have a het_job_offset\nvalue of 0, the next a value of 1, etc.\n\n\njob_id\nhet_job_id\nhet_job_offset\nhet_job_id_set\n\n1231230123-127\n1241231123-127\n1251232123-127\n1261233123-127\n1271234123-127\nTable 1: Example job IDsThe squeue and sview commands report the\ncomponents of a heterogeneous job using the format\n\"<het_job_id>+<het_job_offset>\".\nFor example \"123+4\" would represent heterogeneous job id 123 and its fifth\ncomponent (note: the first component has a het_job_offset value of 0).A request for a specific job ID that identifies a ID of the first component\nof a heterogeneous job (i.e. the \"heterogeneous job leader\") will return\ninformation about all components of that job. For example:\n$ squeue --job=93\nJOBID PARTITION  NAME  USER ST  TIME  NODES NODELIST\n 93+0     debug  bash  adam  R 18:18      1 nid00001\n 93+1     debug  bash  adam  R 18:18      1 nid00011\n 93+2     debug  bash  adam  R 18:18      1 nid00021\nA request to cancel or otherwise signal a heterogeneous job leader will be applied to\nall components of that heterogeneous job. A request to cancel a specific component of\nthe heterogeneous job using the \"#+#\" notation will apply only to that specific component.\nFor example:\n$ squeue --job=93\nJOBID PARTITION  NAME  USER ST  TIME  NODES NODELIST\n 93+0     debug  bash  adam  R 19:18      1 nid00001\n 93+1     debug  bash  adam  R 19:18      1 nid00011\n 93+2     debug  bash  adam  R 19:18      1 nid00021\n$ scancel 93+1\n$ squeue --job=93\nJOBID PARTITION  NAME  USER ST  TIME  NODES NODELIST\n 93+0     debug  bash  adam  R 19:38      1 nid00001\n 93+2     debug  bash  adam  R 19:38      1 nid00021\n$ scancel 93\n$ squeue --job=93\nJOBID PARTITION  NAME  USER ST  TIME  NODES NODELIST\nWhile a heterogeneous job is in pending state, only the entire job can be\ncancelled rather than its individual components.\nA request to cancel an individual component of a heterogeneous job in\npending state will return an error.\nAfter the job has begun execution, the individual component can be cancelled.Email notification for job state changes (the --mail-type option)\nis only supported for a heterogeneous job leader. Requests for email\nnotifications for other components of a heterogeneous job will be silently\nignored.Requests to modify an individual component of a job using the scontrol\ncommand must specify the job ID with the \"#+#\" notation.\nA request to modify a job by specifying the het_job_id will modify all\ncomponents of a heterogeneous job.\nFor example:\n# Change the account of component 2 of heterogeneous job 123:\n$ scontrol update jobid=123+2 account=abc\n\n# Change the time limit of all components of heterogeneous job 123:\n$ scontrol update jobid=123 timelimit=60\nRequests to perform the following operations a job can only be requested for\na heterogeneous job leader and will be applied to all components of that\nheterogeneous job. Requests to operate on individual components of the\nheterogeneous will return an error.\nrequeue\nresume\nsuspend\nThe sbcast command supports heterogeneous job allocations. By default,\nsbcast will copy files to all nodes in the job allocation. The -j/--jobid\noption can be used to copy files to individual components as shown below.\n$ sbcast --jobid=123   data /tmp/data\n$ sbcast --jobid=123.0 app0 /tmp/app0\n$ sbcast --jobid=123.1 app1 /tmp/app1\nThe srun commands --bcast option will transfer files to the nodes associated\nwith the application to be launched as specified by the --het-group option.Slurm has a configuration option to control behavior of some commands with\nrespect to heterogeneous jobs.\nBy default a request to cancel, hold or release a job ID that is not the\nhet_job_id, but that of a job component will only operate that one component\nof the heterogeneous job.\nIf SchedulerParameters configuration parameter includes the option\n\"whole_hetjob\" then the operation would apply to all components of the job if\nany job component is specified to be operated upon. In the below example, the\nscancel command will either cancel all components of job 93 if\nSchedulerParameters=whole_hetjob is configured, otherwise only job 93+1 will be\ncancelled. If a specific heterogeneous job component is specified (e.g. \"scancel\n93+1\"), then only that one component will be effected.\n$ squeue --job=93\nJOBID PARTITION  NAME  USER ST  TIME  NODES NODELIST\n 93+0     debug  bash  adam  R 19:18      1 nid00001\n 93+1     debug  bash  adam  R 19:18      1 nid00011\n 93+2     debug  bash  adam  R 19:18      1 nid00021\n$ scancel 94 (where job ID 94 is equivalent to 93+1)\n# Cancel 93+0, 93+1 and 93+2 if SchedulerParameters includes \"whole_hetjob\"\n# Cancel only 93+1 if SchedulerParameters does not include \"whole_hetjob\"\nAccountingSlurm's accounting database records the het_job_id and het_job_offset\nfields.\nThe sacct command reports job's using the format\n\"<het_job_id>+<het_job_offset>\" and can accept a job ID\nspecification for filtering using the same format.\nIf a het_job_id value is specified as a job filter, then information about\nall components of that job will be reported as shown below by default.\nThe --whole-hetjob=[yes|no] option can be used to force to report\nthe information about all the components of that job or just about the specific\ncomponent requested, no matter if the job filter includes the het_job_id\n(leader) or not.\n\n$ sacct -j 67767\n  JobID JobName Partition Account AllocCPUS     State ExitCode \n------- ------- --------- ------- --------- --------- -------- \n67767+0     foo     debug    test         2 COMPLETED      0:0 \n67767+1     foo     debug    test         4 COMPLETED      0:0 \n\n$  sacct -j 67767+1\n  JobID JobName Partition Account AllocCPUS     State ExitCode \n------- ------- --------- ------- --------- --------- -------- \n67767+1     foo     debug    test         4 COMPLETED      0:0 \n\n$  sacct -j 67767 --whole-hetjob=no\n  JobID JobName Partition Account AllocCPUS     State ExitCode\n------- ------- --------- ------- --------- --------- --------\n67767+0     foo     debug    test         4 COMPLETED      0:0\n\n$ sacct -j 67767+1 --whole-hetjob=yes\n  JobID JobName Partition Account AllocCPUS     State ExitCode\n------- ------- --------- ------- --------- --------- --------\n67767+0     foo     debug    test         2 COMPLETED      0:0\n67767+1     foo     debug    test         4 COMPLETED      0:0\nLaunching Applications (Job Steps)\n\nThe srun command is used to launch applications.\nBy default, the application is launched only on the first component of a\nheterogeneous job, but options are available to support different behaviors.srun's \"--het-group\" option defines which hetjob component(s) are to have\napplications launched for them. The --het-group option takes an expression\ndefining which component(s) are to launch an application for an individual\nexecution of the srun command. The expression can contain one or more component\nindex values in a comma separated list. Ranges of index values can be specified\nin a hyphen separated list. By default, an application is launched only on\ncomponent number zero. Some examples follow:\n--het-group=2\n--het-group=0,4\n--het-group=1,3-5\nIMPORTANT: The ability to execute a single application across more\nthan one job allocation does not work with all MPI implementations or Slurm MPI\nplugins. Slurm's ability to execute such an application can be disabled on the\nentire cluster by adding \"disable_hetjob_steps\" to Slurm's SchedulerParameters\nconfiguration parameter.IMPORTANT: While the srun command can be used to launch heterogeneous\njob steps, mpirun would require substantial modification to support\nheterogeneous applications. We are aware of no such mpirun development efforts\nat this time.By default, the applications launched by a single execution of the srun\ncommand (even for different components of the heterogeneous job) are combined\ninto one MPI_COMM_WORLD with non-overlapping task IDs.As with the salloc and sbatch commands, the \":\" character is used to\nseparate multiple components of a heterogeneous job.\nThis convention means that the stand-alone \":\" character can not be used as an\nargument to an application launched by srun.\nThis includes the ability to execute different applications and arguments\nfor each job component.\nIf some heterogeneous job component lacks an application specification, the next\napplication specification provided will be used for earlier components lacking\none as shown below.\n$ srun --label -n2 : -n1 hostname\n0: nid00012\n1: nid00012\n2: nid00013\nIf multiple srun commands are executed concurrently, this may result in resource\ncontention (e.g. memory limits preventing some job steps components from being\nallocated resources because of two srun commands executing at the same time).\nIf the srun --het-group option is used to create multiple job steps (for the\ndifferent components of a heterogeneous job), those job steps will be created\nsequentially.\nWhen multiple srun commands execute at the same time, this may result in some\nstep allocations taking place, while others are delayed.\nOnly after all job step allocations have been granted will the application\nbeing launched.All components of a job step will have the same step ID value.\nIf job steps are launched on subsets of the job components there may be gaps in\nthe step ID values for individual job components.\n$ salloc -n1 : -n2 beta bash\nsalloc: Pending job allocation 1721\nsalloc: Granted job allocation 1721\n$ srun --het-group=0,1 true   # Launches steps 1721.0 and 1722.0\n$ srun --het-group=0   true   # Launches step  1721.1, no 1722.1\n$ srun --het-group=0,1 true   # Launches steps 1721.2 and 1722.2\nThe maximum het-group specified in a job step allocation (either explicitly\nspecified or implied by the \":\" separator) must not exceed the number of\ncomponents in the heterogeneous job allocation. For example\n$ salloc -n1 -C alpha : -n2 -C beta bash\nsalloc: Pending job allocation 1728\nsalloc: Granted job allocation 1728\n$ srun --het-group=0,1 hostname\nnid00001\nnid00008\nnid00008\n$ srun hostname : date : id\nerror: Attempt to run a job step with het-group value of 2,\n       but the job allocation has maximum value of 1\nEnvironment Variables\n\nSlurm environment variables will be set independently for each component of\nthe job by appending \"_HET_GROUP_\" and a sequence number to the usual name.\nIn addition, the \"SLURM_JOB_ID\" environment variable will contain the job ID\nof the heterogeneous job leader and \"SLURM_HET_SIZE\" will contain the number of\ncomponents in the job. Note that if using srun with a single specific\nhet group (for instance --het-group=1) \"SLURM_JOB_ID\" will contain the job\nID of the heterogeneous job leader. The job ID for a specific heterogeneous\ncomponent is set in \"SLURM_JOB_ID_HET_GROUP_<component_id>\". For example:\n\n$ salloc -N1 : -N2 bash\nsalloc: Pending job allocation 11741\nsalloc: job 11741 queued and waiting for resources\nsalloc: job 11741 has been allocated resources\n$ env | grep SLURM\nSLURM_JOB_ID=11741\nSLURM_HET_SIZE=2\nSLURM_JOB_ID_HET_GROUP_0=11741\nSLURM_JOB_ID_HET_GROUP_1=11742\nSLURM_JOB_NODES_HET_GROUP_0=1\nSLURM_JOB_NODES_HET_GROUP_1=2\nSLURM_JOB_NODELIST_HET_GROUP_0=nid00001\nSLURM_JOB_NODELIST_HET_GROUP_1=nid[00011-00012]\n...\n$ srun --het-group=1 printenv SLURM_JOB_ID\n11741\n11741\n$ srun --het-group=0 printenv SLURM_JOB_ID\n11741\n$ srun --het-group=1 printenv SLURM_JOB_ID_HET_GROUP_1\n11742\n11742\n$ srun --het-group=0 printenv SLURM_JOB_ID_HET_GROUP_0\n11741\nThe various MPI implementations rely heavily upon Slurm environment variables\nfor proper operation.\nA single MPI application executing in a single MPI_COMM_WORLD requires a\nuniform set of environment variables that reflect a single job allocation.\nThe example below shows how Slurm sets environment variables for MPI.\n$ salloc -N1 : -N2 bash\nsalloc: Pending job allocation 11741\nsalloc: job 11751 queued and waiting for resources\nsalloc: job 11751 has been allocated resources\n$ env | grep SLURM\nSLURM_JOB_ID=11751\nSLURM_HET_SIZE=2\nSLURM_JOB_ID_HET_GROUP_0=11751\nSLURM_JOB_ID_HET_GROUP_1=11752\nSLURM_JOB_NODELIST_HET_GROUP_0=nid00001\nSLURM_JOB_NODELIST_HET_GROUP_1=nid[00011-00012]\n...\n$ srun --het-group=0,1 env | grep SLURM\nSLURM_JOB_ID=11751\nSLURM_JOB_NODELIST=nid[00001,00011-00012]\n...\nExamplesCreate a heterogeneous resource allocation containing one node with 256GB\nof memory and a feature of \"haswell\" plus 2176 cores on 32 nodes with a\nfeature of \"knl\". Then launch a program called \"server\" on the \"haswell\" node\nand \"client\" on the \"knl\" nodes. Each application will be in its own\nMPI_COMM_WORLD.\nsalloc -N1 --mem=256GB -C haswell : \\\n       -n2176 -N32 --ntasks-per-core=1 -C knl bash\nsrun server &\nsrun --het-group=1 client &\nwait\nThis variation of the above example launches programs \"server\" and \"client\"\nin a single MPI_COMM_WORLD.\nsalloc -N1 --mem=256GB -C haswell : \\\n       -n2176 -N32 --ntasks-per-core=1 -C knl bash\nsrun server : client\nThe SLURM_PROCID environment variable will be set to reflect a global\ntask rank. Each spawned process will have a unique SLURM_PROCID.Similarly, the SLURM_NPROCS and SLURM_NTASKS environment variables will be set\nto reflect a global task count (both environment variables will have the same\nvalue).\nSLURM_NTASKS will be set to the total count of tasks in all components.\nNote that the task rank and count values are needed by MPI and typically\ndetermined by examining Slurm environment variables.Limitations\n\nThe backfill scheduler has limitations in how it tracks usage of CPUs and\nmemory in the future.\nThis typically requires the backfill scheduler be able to allocate each\ncomponent of a heterogeneous job on a different node in order to begin its\nresource allocation, even if multiple components of the job do actually get\nallocated resources on the same node.In a federation of clusters, a heterogeneous job will execute entirely on\nthe cluster from which the job is submitted. The heterogeneous job will not\nbe eligible to migrate between clusters or to have different components of\nthe job execute on different clusters in the federation.Caution must be taken when submitting heterogeneous jobs that request\nmultiple overlapping partitions. When the partitions share the same resources\nit's possible to starve your own job by having the first job component request\nenough nodes that the scheduler isn't able to fill the subsequent request(s).\nConsider an example where you have partition p1 that contains 10 nodes\nand partition p2 that exists on 5 of the same nodes. If you submit a\nheterogeneous job that requests 5 nodes in p1 and 5 nodes in p2,\nthe scheduler may try to allocate some of the nodes from the p2\npartition for the first job component, preventing the scheduler from being\nable to fulfill the second request, resulting in a job that is never able to\nstart.Magnetic reservations cannot \"attract\" heterogeneous jobs - heterogeneous\njobs will only run in magnetic reservations if they explicitly request the\nreservation.Job arrays of heterogeneous jobs are not supported.The srun command's --no-allocate option is not supported\nfor heterogeneous jobs.Only one job step per heterogeneous job component can be launched by a\nsingle srun command (e.g.\n\"srun --het-group=0 alpha : --het-group=0 beta\" is not supported).The sattach command can only be used to attach to a single component of\na heterogeneous job at a time.License requests are only allowed on the first component\njob (e.g.\n\"sbatch -L ansys:2 : script.sh\").\nHeterogeneous jobs are only scheduled by the backfill scheduler plugin.\nThe more frequently executed scheduling logic only starts jobs on a first-in\nfirst-out (FIFO) basis and lacks logic for concurrently scheduling all\ncomponents of a heterogeneous job.\nHeterogeneous jobs are not supported on GANG scheduling operations.\nSlurm's Perl APIs do not support heterogeneous jobs.\nThe srun --multi-prog option can not be used to span more than one\nheterogeneous job component.\nThe srun --open-mode option is by default set to \"append\".\nAncient versions of OpenMPI and their derivatives (i.e. Cray MPI) are\ndependent upon communication ports being assigned to them by Slurm. Such MPI\njobs will experience step launch failure if any component of a\nheterogeneous job step is unable to acquire the allocated ports.\nNon-heterogeneous job steps will retry step launch using a new set of\ncommunication ports (no change in Slurm behavior).\n\nHeterogeneous Steps\n\n\nSlurm version 20.11 introduces the ability to request heterogeneous job\nsteps from within a non-homogeneous job allocation. This allows you the\nflexibility to have different layouts for job steps without requiring the\nuse of heterogeneous jobs, where having separate jobs for the components\nmay be undesirable.\nSome limitations for heterogeneous steps are that the steps must be able\nto run on unique nodes. You also cannot request heterogeneous steps from within\na heterogeneous job.\nAn example scenario would be if you have a task that needs to use 1 GPU\nper processor while another task needs all the available GPUs on a node with\nonly one processor. This can be accomplished like this:\n\n\n$ salloc -N2 --exclusive --gpus=10\nsalloc: Granted job allocation 61034\n$ srun -N1 -n4 --gpus=4 printenv SLURMD_NODENAME : -N1 -n1 --gpus=6 printenv SLURMD_NODENAME\nnode02\nnode01\nnode01\nnode01\nnode01\n\nSystem Administrator Information\n\n\nThe job submit plugin is invoked independently for each component of a\nheterogeneous job.\nThe spank_init_post_opt() function is invoked once for each component of a\nheterogeneous job. This permits site defined options on a per job component\nbasis.\nScheduling of heterogeneous jobs is performed only by the sched/backfill\nplugin and all heterogeneous job components are either all scheduled at the same\ntime or deferred. The pending reason of heterogeneous jobs isn't set until\nbackfill evaluation.\nIn order to ensure the timely initiation of both heterogeneous and\nnon-heterogeneous jobs, the backfill scheduler alternates between two different\nmodes on each iteration.\nIn the first mode, if a heterogeneous job component can not be initiated\nimmediately, its expected start time is recorded and all subsequent components\nof that job will be considered for starting no earlier than the latest\ncomponent's expected start time.\nIn the second mode, all heterogeneous job components will be considered for\nstarting no earlier than the latest component's expected start time.\nAfter completion of the second mode, all heterogeneous job expected start time\ndata is cleared and the first mode will be used in the next backfill scheduler\niteration.\nRegular (non-heterogeneous jobs) are scheduled independently on each iteration\nof the backfill scheduler.\n For example, consider a heterogeneous job with three components.\nWhen considered as independent jobs, the components could be initiated at times\nnow (component 0), now plus 2 hour (component 1), and now plus 1 hours\n(component 2).\nWhen the backfill scheduler runs in the first mode:\n\nComponent 0 will be noted to possible to start now, but not initiated due\nto the additional components to be initiated\nComponent 1 will be noted to be possible to start in 2 hours\nComponent 2 will not be considered for scheduling until 2 hours in the\nfuture, which leave some additional resources available for scheduling to other\njobs\n\nWhen the backfill scheduler executes next, it will use the second mode and\n(assuming no other state changes) all three job components will be considered\navailable for scheduling no earlier than 2 hours in the future, which may allow\nother jobs to be allocated resources before heterogeneous job component 0\ncould be initiated.\nThe heterogeneous job start time data will be cleared before the first\nmode is used in the next iteration in order to consider system status changes\nwhich might permit the heterogeneous to be initiated at an earlier time than\npreviously determined.\nA resource limit test is performed when a heterogeneous job is submitted in\norder to immediately reject jobs that will not be able to start with current\nlimits.\nThe individual components of the heterogeneous job are validated, like all\nregular jobs.\nThe heterogeneous job as a whole is also tested, but in a more limited\nfashion with respect to quality of service (QOS) limits.\nThis is due to the complexity of each job component having up to three sets of\nlimits (association, job QOS and partition QOS).\nNote that successful submission of any job (heterogeneous or otherwise) does\nnot ensure the job will be able to start without exceeding some limit.\nFor example a job's CPU limit test does not consider that CPUs might not be\nallocated individually, but resource allocations might be performed by whole\ncore, socket or node.\nEach component of a heterogeneous job counts as a \"job\" with respect to\nresource limits.\nFor example, a user might have a limit of 2 concurrent running jobs and submit\na heterogeneous job with 3 components.\nSuch a situation will have an adverse effect upon scheduling other jobs,\nespecially other heterogeneous jobs.\nLast modified 04 January 2024\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Heterogeneous Steps\n\n",
                "content": "Slurm version 20.11 introduces the ability to request heterogeneous job\nsteps from within a non-homogeneous job allocation. This allows you the\nflexibility to have different layouts for job steps without requiring the\nuse of heterogeneous jobs, where having separate jobs for the components\nmay be undesirable.Some limitations for heterogeneous steps are that the steps must be able\nto run on unique nodes. You also cannot request heterogeneous steps from within\na heterogeneous job.An example scenario would be if you have a task that needs to use 1 GPU\nper processor while another task needs all the available GPUs on a node with\nonly one processor. This can be accomplished like this:\n\n\n$ salloc -N2 --exclusive --gpus=10\nsalloc: Granted job allocation 61034\n$ srun -N1 -n4 --gpus=4 printenv SLURMD_NODENAME : -N1 -n1 --gpus=6 printenv SLURMD_NODENAME\nnode02\nnode01\nnode01\nnode01\nnode01\n\nSystem Administrator Information\n\n\nThe job submit plugin is invoked independently for each component of a\nheterogeneous job.\nThe spank_init_post_opt() function is invoked once for each component of a\nheterogeneous job. This permits site defined options on a per job component\nbasis.\nScheduling of heterogeneous jobs is performed only by the sched/backfill\nplugin and all heterogeneous job components are either all scheduled at the same\ntime or deferred. The pending reason of heterogeneous jobs isn't set until\nbackfill evaluation.\nIn order to ensure the timely initiation of both heterogeneous and\nnon-heterogeneous jobs, the backfill scheduler alternates between two different\nmodes on each iteration.\nIn the first mode, if a heterogeneous job component can not be initiated\nimmediately, its expected start time is recorded and all subsequent components\nof that job will be considered for starting no earlier than the latest\ncomponent's expected start time.\nIn the second mode, all heterogeneous job components will be considered for\nstarting no earlier than the latest component's expected start time.\nAfter completion of the second mode, all heterogeneous job expected start time\ndata is cleared and the first mode will be used in the next backfill scheduler\niteration.\nRegular (non-heterogeneous jobs) are scheduled independently on each iteration\nof the backfill scheduler.\n For example, consider a heterogeneous job with three components.\nWhen considered as independent jobs, the components could be initiated at times\nnow (component 0), now plus 2 hour (component 1), and now plus 1 hours\n(component 2).\nWhen the backfill scheduler runs in the first mode:\n\nComponent 0 will be noted to possible to start now, but not initiated due\nto the additional components to be initiated\nComponent 1 will be noted to be possible to start in 2 hours\nComponent 2 will not be considered for scheduling until 2 hours in the\nfuture, which leave some additional resources available for scheduling to other\njobs\n\nWhen the backfill scheduler executes next, it will use the second mode and\n(assuming no other state changes) all three job components will be considered\navailable for scheduling no earlier than 2 hours in the future, which may allow\nother jobs to be allocated resources before heterogeneous job component 0\ncould be initiated.\nThe heterogeneous job start time data will be cleared before the first\nmode is used in the next iteration in order to consider system status changes\nwhich might permit the heterogeneous to be initiated at an earlier time than\npreviously determined.\nA resource limit test is performed when a heterogeneous job is submitted in\norder to immediately reject jobs that will not be able to start with current\nlimits.\nThe individual components of the heterogeneous job are validated, like all\nregular jobs.\nThe heterogeneous job as a whole is also tested, but in a more limited\nfashion with respect to quality of service (QOS) limits.\nThis is due to the complexity of each job component having up to three sets of\nlimits (association, job QOS and partition QOS).\nNote that successful submission of any job (heterogeneous or otherwise) does\nnot ensure the job will be able to start without exceeding some limit.\nFor example a job's CPU limit test does not consider that CPUs might not be\nallocated individually, but resource allocations might be performed by whole\ncore, socket or node.\nEach component of a heterogeneous job counts as a \"job\" with respect to\nresource limits.\nFor example, a user might have a limit of 2 concurrent running jobs and submit\na heterogeneous job with 3 components.\nSuch a situation will have an adverse effect upon scheduling other jobs,\nespecially other heterogeneous jobs.\nLast modified 04 January 2024\n"
            },
            {
                "title": "System Administrator Information\n\n",
                "content": "The job submit plugin is invoked independently for each component of a\nheterogeneous job.The spank_init_post_opt() function is invoked once for each component of a\nheterogeneous job. This permits site defined options on a per job component\nbasis.Scheduling of heterogeneous jobs is performed only by the sched/backfill\nplugin and all heterogeneous job components are either all scheduled at the same\ntime or deferred. The pending reason of heterogeneous jobs isn't set until\nbackfill evaluation.\nIn order to ensure the timely initiation of both heterogeneous and\nnon-heterogeneous jobs, the backfill scheduler alternates between two different\nmodes on each iteration.\nIn the first mode, if a heterogeneous job component can not be initiated\nimmediately, its expected start time is recorded and all subsequent components\nof that job will be considered for starting no earlier than the latest\ncomponent's expected start time.\nIn the second mode, all heterogeneous job components will be considered for\nstarting no earlier than the latest component's expected start time.\nAfter completion of the second mode, all heterogeneous job expected start time\ndata is cleared and the first mode will be used in the next backfill scheduler\niteration.\nRegular (non-heterogeneous jobs) are scheduled independently on each iteration\nof the backfill scheduler. For example, consider a heterogeneous job with three components.\nWhen considered as independent jobs, the components could be initiated at times\nnow (component 0), now plus 2 hour (component 1), and now plus 1 hours\n(component 2).\nWhen the backfill scheduler runs in the first mode:\nComponent 0 will be noted to possible to start now, but not initiated due\nto the additional components to be initiated\nComponent 1 will be noted to be possible to start in 2 hours\nComponent 2 will not be considered for scheduling until 2 hours in the\nfuture, which leave some additional resources available for scheduling to other\njobs\nWhen the backfill scheduler executes next, it will use the second mode and\n(assuming no other state changes) all three job components will be considered\navailable for scheduling no earlier than 2 hours in the future, which may allow\nother jobs to be allocated resources before heterogeneous job component 0\ncould be initiated.The heterogeneous job start time data will be cleared before the first\nmode is used in the next iteration in order to consider system status changes\nwhich might permit the heterogeneous to be initiated at an earlier time than\npreviously determined.A resource limit test is performed when a heterogeneous job is submitted in\norder to immediately reject jobs that will not be able to start with current\nlimits.\nThe individual components of the heterogeneous job are validated, like all\nregular jobs.\nThe heterogeneous job as a whole is also tested, but in a more limited\nfashion with respect to quality of service (QOS) limits.\nThis is due to the complexity of each job component having up to three sets of\nlimits (association, job QOS and partition QOS).\nNote that successful submission of any job (heterogeneous or otherwise) does\nnot ensure the job will be able to start without exceeding some limit.\nFor example a job's CPU limit test does not consider that CPUs might not be\nallocated individually, but resource allocations might be performed by whole\ncore, socket or node.\nEach component of a heterogeneous job counts as a \"job\" with respect to\nresource limits.For example, a user might have a limit of 2 concurrent running jobs and submit\na heterogeneous job with 3 components.\nSuch a situation will have an adverse effect upon scheduling other jobs,\nespecially other heterogeneous jobs.Last modified 04 January 2024"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/configurator.html",
        "sections": [
            {
                "title": "Slurm Version 24.05 Configuration Tool",
                "content": "This form can be used to create a Slurm configuration file with\nyou controlling many of the important configuration parameters.This is the full version of the Slurm configuration tool. This version\nhas all the configuration options to create a Slurm configuration file. There\nis a simplified version of the Slurm configuration tool available at\nconfigurator.easy.html.This tool supports Slurm version 24.05 only.\nConfiguration files for other versions of Slurm should be built\nusing the tool distributed with it in doc/html/configurator.html.\nSome parameters will be set to default values, but you can\nmanually edit the resulting slurm.conf as desired\nfor greater flexibility. See man slurm.conf for more\ndetails about the configuration parameters.Note the while Slurm daemons create log files and other files as needed,\nit treats the lack of parent directories as a fatal error.\nThis prevents the daemons from running if critical file systems are\nnot mounted and will minimize the risk of cold-starting (starting\nwithout preserving jobs).Note that this configuration file must be installed on all nodes\nin your cluster.After you have filled in the fields of interest, use the\n\"Submit\" button on the bottom of the page to build the slurm.conf\nfile. It will appear on your web browser. Save the file in text format\nas slurm.conf for use by Slurm.\n\nFor more information about Slurm, see\nhttps://slurm.schedmd.com/slurm.html\nCluster Name\n ClusterName:\nThe name of your cluster. Using different names for each of your clusters is\nimportant when using a single database to record information from multiple\nSlurm-managed clusters.\n\nControl Machines\nDefine the hostname of the computer on which the Slurm controller and\noptional backup controller will execute.\nHostname values should not be the fully qualified domain\nname (e.g. use tux rather than tux.abc.com).\n\n SlurmctldHost:\nPrimary Controller Hostname\n\n BackupController: Backup\nController Hostname (optional)\n\nCompute Machines\nDefine the machines on which user applications can run.\nYou can also specify addresses of these computers if desired\n(defaults to their hostnames).\nOnly a few of the possible parameters associated with the nodes will\nbe set by this tool, but many others are available.\nExecuting the command slurmd -C on each compute node will print its\nphysical configuration (sockets, cores, real memory size, etc.), which\ncan be used in constructing the slurm.conf file.\nAll of the nodes will be placed into a single partition (or queue)\nwith global access. Many options are available to group nodes into\npartitions with a wide variety of configuration parameters.\nManually edit the slurm.conf produced to exercise these options.\nNode names and addresses may be specified using a numeric range specification.\n\n\n NodeName:\nCompute nodes\n\n NodeAddr: Compute node addresses\n(optional)\n\n PartitionName:\nName of the one partition to be created\n\n MaxTime:\nMaximum time limit of jobs in minutes or INFINITE\n\nThe following parameters describe a node's configuration.\nSet a value for CPUs.\nThe other parameters are optional, but provide more control over scheduled resources:\n\n CPUs: Count of processors\non each compute node.\nIf CPUs is omitted, it will be inferred from:\nSockets, CoresPerSocket, and ThreadsPerCore.\n\n\nSockets:\nNumber of physical processor sockets/chips on the node.\nIf Sockets is omitted, it will be inferred from:\nCPUs, CoresPerSocket, and ThreadsPerCore.\n\n\nCoresPerSocket:\nNumber of cores in a single physical processor socket.\nThe CoresPerSocket value describes physical cores, not\nthe logical number of processors per socket.\n\n\nThreadsPerCore:\nNumber of logical threads in a single physical core.\n\n RealMemory: Amount\nof real memory. This parameter is required when specifying Memory as a\nconsumable resource with the select/cons_tres plug-in. See below\nunder Resource Selection.\n\nSlurm User\nThe Slurm controller (slurmctld) can run without elevated privileges,\nso it is recommended that a user \"slurm\" be created for it. For testing\npurposes any user name can be used.\n\n SlurmUser\n\nSlurm Port Numbers\nThe Slurm controller (slurmctld) requires a unique port for communications\nas do the Slurm compute node daemons (slurmd). If not set, slurm ports\nare set by checking for an entry in /etc/services and if that\nfails by using an interval default set at Slurm build time.\n\n SlurmctldPort\n\n SlurmdPort\n\nState Preservation\nDefine the location of a directory where the slurmctld daemon saves its state.\nThis should be a fully qualified pathname which can be read and written to\nby the Slurm user on both the control machine and backup controller (if configured).\nThe location of a directory where slurmd saves state should also be defined.\nThis must be a unique directory on each compute server (local disk).\nThe use of a highly reliable file system (e.g. RAID) is recommended.\n\n\nStateSaveLocation: Slurmctld state save directory\nMust be writable by all SlurmctldHost nodes\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
            },
            {
                "title": "Cluster Name",
                "content": "ClusterNameControl Machinestuxtux.abc.com\n SlurmctldHost:\nPrimary Controller Hostname\n\n BackupController: Backup\nController Hostname (optional)\n\nCompute Machines\nDefine the machines on which user applications can run.\nYou can also specify addresses of these computers if desired\n(defaults to their hostnames).\nOnly a few of the possible parameters associated with the nodes will\nbe set by this tool, but many others are available.\nExecuting the command slurmd -C on each compute node will print its\nphysical configuration (sockets, cores, real memory size, etc.), which\ncan be used in constructing the slurm.conf file.\nAll of the nodes will be placed into a single partition (or queue)\nwith global access. Many options are available to group nodes into\npartitions with a wide variety of configuration parameters.\nManually edit the slurm.conf produced to exercise these options.\nNode names and addresses may be specified using a numeric range specification.\n\n\n NodeName:\nCompute nodes\n\n NodeAddr: Compute node addresses\n(optional)\n\n PartitionName:\nName of the one partition to be created\n\n MaxTime:\nMaximum time limit of jobs in minutes or INFINITE\n\nThe following parameters describe a node's configuration.\nSet a value for CPUs.\nThe other parameters are optional, but provide more control over scheduled resources:\n\n CPUs: Count of processors\non each compute node.\nIf CPUs is omitted, it will be inferred from:\nSockets, CoresPerSocket, and ThreadsPerCore.\n\n\nSockets:\nNumber of physical processor sockets/chips on the node.\nIf Sockets is omitted, it will be inferred from:\nCPUs, CoresPerSocket, and ThreadsPerCore.\n\n\nCoresPerSocket:\nNumber of cores in a single physical processor socket.\nThe CoresPerSocket value describes physical cores, not\nthe logical number of processors per socket.\n\n\nThreadsPerCore:\nNumber of logical threads in a single physical core.\n\n RealMemory: Amount\nof real memory. This parameter is required when specifying Memory as a\nconsumable resource with the select/cons_tres plug-in. See below\nunder Resource Selection.\n\nSlurm User\nThe Slurm controller (slurmctld) can run without elevated privileges,\nso it is recommended that a user \"slurm\" be created for it. For testing\npurposes any user name can be used.\n\n SlurmUser\n\nSlurm Port Numbers\nThe Slurm controller (slurmctld) requires a unique port for communications\nas do the Slurm compute node daemons (slurmd). If not set, slurm ports\nare set by checking for an entry in /etc/services and if that\nfails by using an interval default set at Slurm build time.\n\n SlurmctldPort\n\n SlurmdPort\n\nState Preservation\nDefine the location of a directory where the slurmctld daemon saves its state.\nThis should be a fully qualified pathname which can be read and written to\nby the Slurm user on both the control machine and backup controller (if configured).\nThe location of a directory where slurmd saves state should also be defined.\nThis must be a unique directory on each compute server (local disk).\nThe use of a highly reliable file system (e.g. RAID) is recommended.\n\n\nStateSaveLocation: Slurmctld state save directory\nMust be writable by all SlurmctldHost nodes\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
            },
            {
                "title": "Compute Machines",
                "content": "slurmd -Cslurm.confslurm.conf\n NodeName:\nCompute nodes\n\n NodeAddr: Compute node addresses\n(optional)\n\n PartitionName:\nName of the one partition to be created\n\n MaxTime:\nMaximum time limit of jobs in minutes or INFINITE\n\nThe following parameters describe a node's configuration.\nSet a value for CPUs.\nThe other parameters are optional, but provide more control over scheduled resources:\n\n CPUs: Count of processors\non each compute node.\nIf CPUs is omitted, it will be inferred from:\nSockets, CoresPerSocket, and ThreadsPerCore.\n\n\nSockets:\nNumber of physical processor sockets/chips on the node.\nIf Sockets is omitted, it will be inferred from:\nCPUs, CoresPerSocket, and ThreadsPerCore.\n\n\nCoresPerSocket:\nNumber of cores in a single physical processor socket.\nThe CoresPerSocket value describes physical cores, not\nthe logical number of processors per socket.\n\n\nThreadsPerCore:\nNumber of logical threads in a single physical core.\n\n RealMemory: Amount\nof real memory. This parameter is required when specifying Memory as a\nconsumable resource with the select/cons_tres plug-in. See below\nunder Resource Selection.\n\nSlurm User\nThe Slurm controller (slurmctld) can run without elevated privileges,\nso it is recommended that a user \"slurm\" be created for it. For testing\npurposes any user name can be used.\n\n SlurmUser\n\nSlurm Port Numbers\nThe Slurm controller (slurmctld) requires a unique port for communications\nas do the Slurm compute node daemons (slurmd). If not set, slurm ports\nare set by checking for an entry in /etc/services and if that\nfails by using an interval default set at Slurm build time.\n\n SlurmctldPort\n\n SlurmdPort\n\nState Preservation\nDefine the location of a directory where the slurmctld daemon saves its state.\nThis should be a fully qualified pathname which can be read and written to\nby the Slurm user on both the control machine and backup controller (if configured).\nThe location of a directory where slurmd saves state should also be defined.\nThis must be a unique directory on each compute server (local disk).\nThe use of a highly reliable file system (e.g. RAID) is recommended.\n\n\nStateSaveLocation: Slurmctld state save directory\nMust be writable by all SlurmctldHost nodes\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
            },
            {
                "title": "Slurm User",
                "content": "\n SlurmUser\n\nSlurm Port Numbers\nThe Slurm controller (slurmctld) requires a unique port for communications\nas do the Slurm compute node daemons (slurmd). If not set, slurm ports\nare set by checking for an entry in /etc/services and if that\nfails by using an interval default set at Slurm build time.\n\n SlurmctldPort\n\n SlurmdPort\n\nState Preservation\nDefine the location of a directory where the slurmctld daemon saves its state.\nThis should be a fully qualified pathname which can be read and written to\nby the Slurm user on both the control machine and backup controller (if configured).\nThe location of a directory where slurmd saves state should also be defined.\nThis must be a unique directory on each compute server (local disk).\nThe use of a highly reliable file system (e.g. RAID) is recommended.\n\n\nStateSaveLocation: Slurmctld state save directory\nMust be writable by all SlurmctldHost nodes\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
            },
            {
                "title": "Slurm Port Numbers",
                "content": "/etc/services\n SlurmctldPort\n\n SlurmdPort\n\nState Preservation\nDefine the location of a directory where the slurmctld daemon saves its state.\nThis should be a fully qualified pathname which can be read and written to\nby the Slurm user on both the control machine and backup controller (if configured).\nThe location of a directory where slurmd saves state should also be defined.\nThis must be a unique directory on each compute server (local disk).\nThe use of a highly reliable file system (e.g. RAID) is recommended.\n\n\nStateSaveLocation: Slurmctld state save directory\nMust be writable by all SlurmctldHost nodes\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
            },
            {
                "title": "State Preservation",
                "content": "\n\nStateSaveLocation: Slurmctld state save directory\nMust be writable by all SlurmctldHost nodes\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
            },
            {
                "title": "Scheduling",
                "content": "SchedulerTypeBackfillBuiltin\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
            },
            {
                "title": "Interconnect",
                "content": "SwitchTypeHPE\n  SlingshotNone\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
            },
            {
                "title": "Default MPI Type",
                "content": "MpiDefaultMPI-PMI2MPI-PMIxNone\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
            },
            {
                "title": "Process Tracking",
                "content": "ProctrackTypeCgroupcgroupcgroup.confLinuxProcPgid\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
            },
            {
                "title": "Resource Selection",
                "content": "SelectTypecons_tres\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
            },
            {
                "title": "Task Launch",
                "content": "TaskPluginNoneAffinityCgroup\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
            },
            {
                "title": "Prolog and Epilog",
                "content": "\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
            },
            {
                "title": "Event Logging",
                "content": "\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
            },
            {
                "title": "Job Completion Logging",
                "content": "JobCompTypeNoneElasticsearchFileTxtKafkaLuaScriptMySQL\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
            },
            {
                "title": "Job Accounting Gather",
                "content": "JobAcctGatherTypeNonecgroupLinuxJobAcctGatherFrequency\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
            },
            {
                "title": "Job Accounting Storage",
                "content": "AccountingStorageTypeNoneSlurmDBDAccountingStorageLocOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
            },
            {
                "title": "Process ID Logging",
                "content": "\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
            },
            {
                "title": "Timers",
                "content": "\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/jwt.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "JSON Web Tokens (JWT) Authentication",
                "content": "Slurm provides a\nRFC7519 compliant\nimplementation of JSON Web Tokens (JWT). This\nauthentication can be used as an AuthAltType, usually alongside\nauth/munge as the AuthType. The only supported communication\ndirection is from a client connecting to slurmctld and slurmdbd.\nThis means that certain scenarios (specifically interactive jobs using\nsrun) are currently not supported for clients with auth/jwt enabled (or\nthat have SLURM_JWT in their environment).Prerequisites\n\nJWT requires libjwt.\nBoth the library and the development headers must be available when Slurm is\ncompiled.Full root trust in JWT creation\n\nThe power to create JWTs is the power of root on a cluster. This is a\nper-site decision on what/who/when/how to trust. If a given authentication\nsystem can not be fully trusted with the powers of root for the entire cluster,\nthen an authenticating proxy will need to be used to divide up the trust and\nimplement the site's specific policies before the requests reach Slurm\n(specifically slurmrestd). While possibly inefficient, there is no technical\nreason that tiers of authenticating proxies can not be used if there is a lack\nof trust but a desire to allow creation of lesser auth tokens. Each site will\nneed to weight the risks and benefits of which JWTs to trust before implementing\nany system. Once a job has been queued, the proxied authentication system will\nno longer be involved and the job will run with that user's permissions and\naccess per Linux/POSIX's ACLs and trusts.Models of trust\n\nThere are several ways to handle controlling JWT authentication and access.\nSlurm JWT plugin implementation is purposefully simple and will not be able to\nsupport most models of trust needed by sites. There already exists a plethora of\nauthentication systems, and the expectation is that any site that wants more\ncomplexity than the default offering will use one of those systems instead.\nExternal JWT generation\nWe provide an example python script for generating new JWTs but they are a\nstandard and most languages have existing tooling for them. This is usually the\neasiest route for sites but does require each site to implement the tooling for\ntheir users directly.\nAuthenticating proxy\nThis is the most versatile option, as any authentication system can be\nplaced in front of slurmrestd. It requires creating a slurmuser/root token that\ncan then be used to proxy for any user. There are existing solutions for this\nwith Nginx and Apache, and probably every other non-trivial proxy. We suggest\nchoosing the preferred proxy and finding an existing setup guide for\nauthenticating via that proxy. The proxy will need to have the\nX-SLURM-USER-TOKEN and X-SLURM-USER-NAME headers defined.\nThere is no requirement that an authenticating proxy implement JWT for\nclients. This is the primary benefit of authenticating proxies; they can use\nany authentication method since they are the trusted point that tells Slurm\nwhich user the request is from. These authentication tokens are only used by\nthe proxy and are not passed to the job. This is generally not an\nissue as once the job is in Slurm, it runs as the Posix user with all of the\ninherent trust of that user and it then uses auth/munge or\nauth/slurm for everything after that.\nJWKS\nThis is like an authentication proxy, as another system is used to create the\ntokens, but it skips having the authentication system in front of Slurm by using\nsigned public keys. This tends to be the preferred solution for sites using\ncloud authentication systems, such as:\n\n\nAmazon Cognito\nKeycloak - Using keycloak is an\noption which doesn't require a cloud auth solution.\n\n\nSetup for Standalone Use\n\n\n\n    Configure and build Slurm with JWT support\nAdd JWT key to controller in StateSaveLocation.\nHere is an example with the JWT key in /var/spool/slurm/statesave/:\n\ndd if=/dev/random of=/var/spool/slurm/statesave/jwt_hs256.key bs=32 count=1\nchown slurm:slurm /var/spool/slurm/statesave/jwt_hs256.key\nchmod 0600 /var/spool/slurm/statesave/jwt_hs256.key\nchown slurm:slurm /var/spool/slurm/statesave\nchmod 0755 /var/spool/slurm/statesave\n\nThe key does not have to be in the StateSaveLocation, but that is a convenient\nlocation if you have multiple controllers since it is shared between them.\nThe key should not be placed in a directory where non-admin users might be\nable to access it.\nThe key file should be owned by SlurmUser or root, with\nrecommended permissions of 0400. The file must not be accessible by 'other'.\n\nAdd JWT as an alternative authentication in slurm.conf and slurmdbd.conf:\n\nAuthAltTypes=auth/jwt\nAuthAltParameters=jwt_key=/var/spool/slurm/statesave/jwt_hs256.key\n\n\nRestart slurmctld\nCreate tokens for users as desired:\n\nscontrol token username=$USER\n\nAn optional lifespan=$LIFESPAN option can be used to change the token\nlifespan from the default 1800 seconds. The root account, or SlurmUser\naccount can be used to generate tokens for any user. Alternatively, a user\nmay use the command to generate tokens for themselves by simply calling\n\nscontrol token\n\nNote that administrators can prevent users from generating tokens by setting\nthe following parameter in slurm.conf:\n\nAuthAltParameters=disable_token_creation\n\nThis functionality is provided to allow sites to control when and how users are\nprovided tokens along with controlling the token lifespans.\n\nExport the SLURM_JWT environment variable before calling any Slurm\ncommand.\nExport the SLURM_JWT=daemon environment variable before starting\nthe slurmrestd daemon to activate AuthAltTypes=auth/jwt as the primary\nauthentication mechanism.\n\nExternal Authentication Integration with JWKS and RS256 Tokens\n\nStarting with the 21.08 release, Slurm can support RS256 tokens such as\nthose generated by\nAmazon Cognito,\nAzure AD, or\nKeycloak.\nTo enable Slurm's RS256 token support, an appropriate JWKS file must be\ndownloaded and configured as such:\n\nAuthAltTypes=auth/jwt\nAuthAltParameters=jwks=/var/spool/slurm/statesave/jwks.json\n\nThe jwks file should be owned by SlurmUser or root, must be\nreadable by SlurmUser, with recommended permissions of 0400.\nThe file must not be writable by 'other'.Note that, by default, the built-in ability to generate HS256 tokens will\nbe disabled when JWKS support is enabled. It can be re-enabled by explicitly\nconfiguring the jwt_key= option alongside jwks=.\nNote: Slurm ignores the x5c and x5t fields and does not\nattempt to verify the certificate chain if presented in the JWKS file. JWTs are\nonly verified against RSA 256 bit keys provided via e and\nn fields.\nJWKS has signing keys that receive trust by being placed in the\njwks.json. Those trusted keys can then create new tokens (which are JWTs) for\nany user by signing them. JWKS does not support adding keys for individual\nusers but only for adding trusted signing keys.\nJWT and JWKS can coexist in Slurm. Slurm will auto-disable JWT when\nJWKS is configured as a safety mechanism, to avoid accidentally having both\nenabled at the same time.\nUser Mapping\nDepending on the service used to generate tokens, you may run into issues\nmapping the token to a username. Slurm defaults to using the sun\n(Slurm UserName) field. If the service uses a different field, you will need to\ncorrect this for it to work with Slurm.Option 1: Change Slurm to use a different field. This can be\ncustomized using \nAuthAltParameters=userclaimfield. For example, using the default field\nfor KeyCloak:\n\nAuthAltParameters=jwks=/local/path/to/jwks.json,userclaimfield=preferred_username\n\nOption 2: Change the identity service to use a different field. In\nKeyCloak 25.0, for example, you should find this option under Clients ->\nClient details -> Dedicated scopes -> Mapper details. Change the username\nmapping to use the sun field.Compatibility\n\nRFC7519\nRequired tokens for Slurm are present:\n\t\niat: Unix timestamp of creation date.\nexp: Unix timestamp of expiration date.\nsun or username: Slurm UserName (\n\t\t\t\n\t\t\t\tPOSIX.1-2017 User Name\n\t\t\t).\n\t\t\n\n\nTokens are signed with HS256 algorithm compliant to RFC7518. RS256 is also\n\tsupported to verify tokens, although Slurm cannot create them\n\tdirectly.\nSigning key is provided to slurmctld and slurmdbd to allow decryption of\n\tthe tokens. Slurm currently only supports a single signing key.\n\n#!/usr/bin/env python3\nimport sys\nimport os\nimport pprint\nimport json\nimport time\nfrom datetime import datetime, timedelta, timezone\n\nfrom jwt import JWT\nfrom jwt.jwa import HS256\nfrom jwt.jwk import jwk_from_dict\nfrom jwt.utils import b64decode,b64encode\n\nif len(sys.argv) != 3:\n    sys.exit(\"gen_jwt.py [user name] [expiration time (seconds)]\");\n\nwith open(\"/var/spool/slurm/statesave/jwt.key\", \"rb\") as f:\n    priv_key = f.read()\n\nsigning_key = jwk_from_dict({\n    'kty': 'oct',\n    'k': b64encode(priv_key)\n})\n\nmessage = {\n    \"exp\": int(time.time() + int(sys.argv[2])),\n    \"iat\": int(time.time()),\n    \"sun\": sys.argv[1]\n}\n\na = JWT()\ncompact_jws = a.encode(message, signing_key, alg='HS256')\nprint(\"SLURM_JWT={}\".format(compact_jws))\n\n#!/usr/bin/env python3\nimport sys\nimport os\nimport pprint\nimport json\nimport time\nfrom datetime import datetime, timedelta, timezone\n\nfrom jwt import JWT\nfrom jwt.jwa import HS256\nfrom jwt.jwk import jwk_from_dict\nfrom jwt.utils import b64decode,b64encode\n\nif len(sys.argv) != 2:\n    sys.exit(\"verify_jwt.py [JWT Token]\");\n\nwith open(\"/var/spool/slurm/statesave/jwt.key\", \"rb\") as f:\n    priv_key = f.read()\n\nsigning_key = jwk_from_dict({\n    'kty': 'oct',\n    'k': b64encode(priv_key)\n})\n\na = JWT()\nb = a.decode(sys.argv[1], signing_key, algorithms=[\"HS256\"])\nprint(b)\nLast modified 15 August 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/cons_tres_share.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Sharing Consumable Resources",
                "content": "CPU Management\n\n\n(Disclaimer: In this \"CPU Management\" section, the term \"consumable resource\"\ndoes not include memory. The management of memory as a consumable resource is\ndiscussed in its own section below.)\n\nThe per-partition OverSubscribe setting applies to the entity\nbeing selected for scheduling:\n\nWhen the select/linear plugin is enabled, the\nper-partition OverSubscribe setting controls whether or not the\nnodes are shared among jobs.\n\nWhen the default select/cons_tres plugin is\nenabled, the per-partition OverSubscribe setting controls\nwhether or not the configured consumable resources are shared among jobs.\nWhen a consumable resource such as a core,\nsocket, or CPU is shared, it means that more than one job can be assigned to it.\n\n\nThe following table describes this new functionality in more detail:\n\nSelection Setting\nPer-partition OverSubscribe Setting\nResulting Behavior\n\nSelectType=select/linear\nOverSubscribe=NO\nWhole nodes are allocated to jobs. No node will run more than one job\nper partition/queue.\n\nOverSubscribe=YES\nBy default same as OverSubscribe=NO. Nodes allocated to a job may be shared with\nother jobs if each job allows sharing via the srun --oversubscribe\noption.\n\nOverSubscribe=FORCE\nEach whole node can be allocated to multiple jobs up to the count specified\nper partition/queue (default 4 jobs per node)\n\nSelectType=select/cons_tres\nPlus one of the following:\nSelectTypeParameters=CR_Core\nSelectTypeParameters=CR_Core_Memory\nOverSubscribe=NO\nCores are allocated to jobs. No core will run more than one job\nper partition/queue.\n\nOverSubscribe=YES\nBy default same as OverSubscribe=NO. Cores allocated to a job may be shared with\nother jobs if each job allows sharing via the srun --oversubscribe\noption.\n\nOverSubscribe=FORCE\nEach core can be allocated to multiple jobs up to the count specified\nper partition/queue (default 4 jobs per core).\n\nSelectType=select/cons_tres\nPlus one of the following:\nSelectTypeParameters=CR_CPU\nSelectTypeParameters=CR_CPU_Memory\nOverSubscribe=NO\nCPUs are allocated to jobs. No CPU will run more than one job\nper partition/queue.\n\nOverSubscribe=YES\nBy default same as OverSubscribe=NO. CPUs allocated to a job may be shared with\nother jobs if each job allows sharing via the srun --oversubscribe\noption.\n\nOverSubscribe=FORCE\nEach CPU can be allocated to multiple jobs up to the count specified\nper partition/queue (default 4 jobs per CPU).\n\nSelectType=select/cons_tres\nPlus one of the following:\nSelectTypeParameters=CR_Socket\nSelectTypeParameters=CR_Socket_Memory\nOverSubscribe=NO\nSockets are allocated to jobs. No Socket will run more than one job\nper partition/queue.\n\nOverSubscribe=YES\nBy default same as OverSubscribe=NO. Sockets allocated to a job may be shared with\nother jobs if each job allows sharing via the srun --oversubscribe\noption.\n\nOverSubscribe=FORCE\nEach socket can be allocated to multiple jobs up to the count specified\nper partition/queue (default 4 jobs per socket).\n\nWhen OverSubscribe=FORCE is configured, the consumable resources are\nscheduled for jobs using a least-loaded algorithm. Thus, idle\nCPUs|cores|sockets will be allocated to a job before busy ones, and\nCPUs|cores|sockets running one job will be allocated to a job before ones\nrunning two or more jobs. This is the same approach that the\nselect/linear plugin uses when allocating \"shared\" nodes.\n\nNote that the granularity of the \"least-loaded\" algorithm is what\ndistinguishes the consumable resource and linear plugins\nwhen OverSubscribe=FORCE is configured. With the\nselect/cons_tres plugin enabled,\nthe CPUs of a node are not\novercommitted until all of the rest of the CPUs are overcommitted on the\nother nodes. Thus if one job allocates half of the CPUs on a node and then a\nsecond job is submitted that requires more than half of the CPUs, the\nconsumable resource plugin will attempt to place this new job on other\nbusy nodes that have more than half of the CPUs available for use. The\nselect/linear plugin simply counts jobs on nodes, and does not\ntrack the CPU usage on each node.\n\nThe sharing functionality in the\nselect/cons_tres plugin also supports the\nnew OverSubscribe=FORCE:<num> syntax. If OverSubscribe=FORCE:3\nis configured with a consumable resource plugin and CR_Core or\nCR_Core_Memory, then the plugin will\nrun up to 3 jobs on each core of each node in the partition. If\nCR_Socket or CR_Socket_Memory is configured, then the\nplugin will run up to 3 jobs on each socket\nof each node in the partition.\nNodes in Multiple Partitions\n\n\nSlurm has supported configuring nodes in more than one partition since version\n0.7.0. The following table describes how nodes configured in two partitions with\ndifferent OverSubscribe settings will be allocated to jobs. Note that\n\"shared\" jobs are jobs that are submitted to partitions configured with\nOverSubscribe=FORCE or with OverSubscribe=YES and the job requested\nsharing with the srun --oversubscribe option. Conversely, \"non-shared\"\njobs are jobs that are submitted to partitions configured with\nOverSubscribe=NO or OverSubscribe=YES and the job did not\nrequest shareable resources.\n\n\u00a0First job \"shareable\"First job not\n\"shareable\"\nSecond job \"shareable\"Both jobs can run on the same nodes and\nmay share resourcesJobs do not run on the same nodes\nSecond job not \"shareable\"Jobs do not run on the same nodes\nJobs can run on the same nodes but will not share resources\n\nThe next table contains several scenarios with the select/cons_tres\nplugin enabled to further\nclarify how a node is used when it is configured in more than one partition and\nthe partitions have different \"OverSubscribe\" policies.\n\nSlurm configuration\nResulting Behavior\n\nTwo OverSubscribe=NO partitions assigned the same set of nodes\nJobs from either partition will be assigned to all available consumable\nresources. No consumable resource will be shared. One node could have 2 jobs\nrunning on it, and each job could be from a different partition.\n\nTwo partitions assigned the same set of nodes: one partition is\nOverSubscribe=FORCE, and the other is OverSubscribe=NO\nA node will only run jobs from one partition at a time. If a node is\nrunning jobs from the OverSubscribe=NO partition, then none of its\nconsumable resources will be shared. If a node is running jobs from the\nOverSubscribe=FORCE partition, then its consumable resources can be\nshared.\n\nTwo OverSubscribe=FORCE partitions assigned the same set of nodes\nJobs from either partition will be assigned consumable resources. All\nconsumable resources can be shared. One node could have 2 jobs running on it,\nand each job could be from a different partition.\n\nTwo partitions assigned the same set of nodes: one partition is\nOverSubscribe=FORCE:3, and the other is OverSubscribe=FORCE:5\nGenerally the same behavior as above. However no consumable resource will\never run more than 3 jobs from the first partition, and no consumable resource\nwill ever run more than 5 jobs from the second partition. A consumable resource\ncould have up to 8 jobs running on it at one time.\n\n\nNote that the \"mixed shared setting\" configuration (row #2 above) introduces the\npossibility of starvation between jobs in each partition. If a set of\nnodes are running jobs from the OverSubscribe=NO partition, then these\nnodes will continue to only be available to jobs from that partition, even if\njobs submitted to the OverSubscribe=FORCE partition have a higher\npriority. This works in reverse also, and in fact it's easier for jobs from the\nOverSubscribe=FORCE partition to hold onto the nodes longer because the\nconsumable resource \"sharing\" provides more resource availability for new jobs\nto begin running \"on top of\" the existing jobs. This happens with the\nselect/linear plugin also, so it's not specific to the\nselect/cons_tres plugin.\nMemory Management\n\n\nThe management of memory as a consumable resource remains unchanged and\ncan be used to prevent oversubscription of memory, which would result in\nhaving memory pages swapped out and severely degraded performance.\n\nSelection Setting\nResulting Behavior\n\nSelectType=select/linear\nMemory allocation is not tracked. Jobs are allocated to nodes without\nconsidering if there is enough free memory. Swapping could occur!\n\nSelectType=select/linear plus\nSelectTypeParameters=CR_Memory\nMemory allocation is tracked.  Nodes that do not have enough available\nmemory to meet the jobs memory requirement will not be allocated to the job.\n\n\nSelectType=select/cons_tres\nPlus one of the following:\nSelectTypeParameters=CR_Core\nSelectTypeParameters=CR_CPU\nSelectTypeParameters=CR_Socket\nMemory allocation is not tracked. Jobs are allocated to consumable resources\nwithout considering if there is enough free memory. Swapping could occur!\n\nSelectType=select/cons_tres\nPlus one of the following:\nSelectTypeParameters=CR_Memory\nSelectTypeParameters=CR_Core_Memory\nSelectTypeParameters=CR_CPU_Memory\nSelectTypeParameters=CR_Socket_Memory\nMemory allocation for all jobs are tracked. Nodes that do not have enough\navailable memory to meet the jobs memory requirement will not be allocated to\nthe job.\n\nUsers can specify their job's memory requirements one of two ways. The\nsrun --mem=<num> option can be used to specify the jobs\nmemory requirement on a per allocated node basis. This option is recommended\nfor use with the select/linear plugin, which allocates\nwhole nodes to jobs. The\nsrun --mem-per-cpu=<num> option can be used to specify the\njobs memory requirement on a per allocated CPU basis. This is recommended\nfor use with the select/cons_tres\nplugin, which can allocate individual CPUs to jobs.Default and maximum values for memory on a per node or per CPU basis can\nbe configured by the system administrator using the following\nslurm.conf options: DefMemPerCPU,\nDefMemPerNode, MaxMemPerCPU and\nMaxMemPerNode.\nUsers can use the --mem or --mem-per-cpu option\nat job submission time to override the default value, but they cannot exceed\nthe maximum value.\n\nEnforcement of a jobs memory allocation is performed by setting the \"maximum\ndata segment size\" and the \"maximum virtual memory size\" system limits to the\nappropriate values before launching the tasks. Enforcement is also managed by\nthe accounting plugin, which periodically gathers data about running jobs. Set\nJobAcctGather and JobAcctFrequency to\nvalues suitable for your system.\nNOTE: The --oversubscribe and --exclusive\noptions are mutually exclusive when used at job submission. If both options are\nset when submitting a job, the job submission command used will fatal.\nLast modified 30 May 2023"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/power_save.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Slurm Power Saving Guide",
                "content": "Contents\nOverview\nConfiguration\nNode Lifecycle\nManual Power Saving\nResume and Suspend Programs\nFault Tolerance\nBooting Different Images\nUse of Allocations\nNode Features\nHybrid Cluster\nCloud Accounting\nOverviewSlurm provides an integrated mechanism for nodes being suspended (powered\ndown, placed into power saving mode) and resumed (powered up, restored from\npower saving mode) on demand or by request. Nodes that remain IDLE for\nSuspendTime will be suspended by SuspendProgram and will be\nunavailable for scheduling for SuspendTimeout. Nodes will\nautomatically be resumed by ResumeProgram to complete work allocated\nto them. Nodes that fail to register within ResumeTimeout will become\nDOWN and their allocated jobs are requeued. Node power saving can be\nmanually requested by scontrol update nodename=<nodename>\nstate=power_<down|up>. The rate of nodes being resumed or\nsuspended can be controlled by ResumeRate and SuspendRate.Slurm can be configured to accomplish power saving by managing compute\nresources in any cloud provider (e.g. Amazon\nWeb Services, Google Cloud\nPlatform, Microsoft Azure) via\ntheir API. These resources can be combined with an existing cluster to\nprocess excess workload (cloud bursting) or it can operate as an independent\nand self-contained cluster.To enable Power Saving operation in Slurm, you must configure the\nfollowing:\nResumeProgram and SuspendProgram must be defined. Their\n  value must be a valid path to a program.\nResumeTimeout and SuspendTimeout must be defined, either\n  globally or on at least one partition.\nSuspendTime must be defined, either globally or on at least one\n  partition, and not be INFINITE or -1.\nResumeRate and SuspendRate must be greater than or equal\n  to 0.\nThe Slurm control daemon, slurmctld, must be restarted to initially\nenable Power Saving operation. Changes in the configuration parameters (e.g.\nSuspendTime) will take effect after modifying the slurm.conf\nconfiguration file and executing scontrol reconfigure.ConfigurationThe following configuration parameters of interest include:\nDebugFlags\n\n\nDefines specific subsystems which should provide more detailed event\n    logging. Options of interest include:\n\nPower\n\n\nPower management plugin and power save (suspend/resume programs)\n        details.\n\n\n\nReconfigFlags\n\n\nFlags to control various actions that may be taken when an\n    scontrol reconfigure command is issued. Options of interest\n    include:\n\nKeepPowerSaveSettings\n\n\nIf set, an scontrol reconfigure command will preserve\n        the current state of SuspendExcNodes, SuspendExcParts,\n        and SuspendExcStates.\n\n\n\nResumeFailProgram\n\n\nProgram to be executed when nodes fail to resume by\n    ResumeTimeout. The argument to the program will be the names of\n    the failed nodes (using Slurm's hostlist expression format).\n\nResumeProgram\n\n\nProgram to be executed to restore nodes from power saving mode. The\n    program executes as SlurmUser (as configured in\n    slurm.conf). The argument to the program will be the names of\n    nodes to be restored from power savings mode (using Slurm's hostlist\n    expression format).\nIf the slurmd daemon fails to respond within the configured\n    ResumeTimeout value with an updated BootTime, the node will be\n    placed in a DOWN state and the job requesting the node will be requeued.\n    If the node isn't actually rebooted (e.g. when multiple-slurmd is\n    configured) you can start slurmd with the \"-b\" option to report the node\n    boot time as now.\nA job to node mapping is available in JSON format by reading the\n    temporary file specified by the SLURM_RESUME_FILE environment\n    variable. This file should be used at the beginning of\n    ResumeProgram - see the Fault Tolerance\n    section for more details. This program may use the scontrol show\n    nodename command to ensure that a node has booted and the\n    slurmd daemon started.\n\nSLURM_RESUME_FILE=/proc/1647372/fd/7:\n{\n  \"all_nodes_resume\" : \"cloud[1-3,7-8]\",\n  \"jobs\" : [\n    {\n      \"extra\" : \"An arbitrary string from --extra\",\n      \"features\" : \"c1,c2\",\n      \"job_id\" : 140814,\n      \"nodes_alloc\" : \"cloud[1-4]\",\n      \"nodes_resume\" : \"cloud[1-3]\",\n      \"oversubscribe\" : \"OK\",\n      \"partition\" : \"cloud\",\n      \"reservation\" : \"resv_1234\"\n    },\n    {\n      \"extra\" : null,\n      \"features\" : \"c1,c2\",\n      \"job_id\" : 140815,\n      \"nodes_alloc\" : \"cloud[1-2]\",\n      \"nodes_resume\" : \"cloud[1-2]\",\n      \"oversubscribe\" : \"OK\",\n      \"partition\" : \"cloud\",\n      \"reservation\" : null\n    },\n    {\n      \"extra\" : null,\n      \"features\" : null\n      \"job_id\" : 140816,\n      \"nodes_alloc\" : \"cloud[7-8]\",\n      \"nodes_resume\" : \"cloud[7-8]\",\n      \"oversubscribe\" : \"NO\",\n      \"partition\" : \"cloud_exclusive\",\n      \"reservation\" : null\n    }\n  ]\n}\n\nSee the squeue man page\n    for possible values for oversubscribe.\nNOTE: The SLURM_RESUME_FILE will only exist and be\n    usable if Slurm was compiled with the JSON-C serializer library.\n\nResumeRate\n\n\nMaximum number of nodes to be removed from power saving mode per\n    minute. A value of zero results in no limits being imposed. The default\n    value is 300. Use this to prevent rapid increases in power\n    consumption.\n\nResumeTimeout\n\n\nMaximum time permitted (in seconds) between when a node resume request\n    is issued and when the node is actually available for use. Nodes which\n    fail to respond in this time frame will be marked DOWN and the jobs\n    scheduled on the node requeued. Nodes which reboot after this time frame\n    will be marked DOWN with a reason of \"Node unexpectedly rebooted.\" The\n    default value is 60 seconds.\n\nSchedulerParameters\n\n\nThe interpretation of this parameter varies by SchedulerType. Multiple\n    options may be comma separated. Options of interest include:\n\nsalloc_wait_nodes\n\n\nIf defined, the salloc command will wait until all allocated nodes\n        are ready for use (i.e. booted) before the command returns. By\n        default, salloc will return as soon as the resource allocation has\n        been made. The salloc command can use the\n        --wait-all-nodes option to override this configuration\n        parameter.\n\nsbatch_wait_nodes\n\n\nIf defined, the sbatch script will wait until all allocated nodes\n        are ready for use (i.e. booted) before the initiation. By default,\n        the sbatch script will be initiated as soon as the first node in the\n        job allocation is ready. The sbatch command can use the\n        --wait-all-nodes option to override this configuration\n        parameter.\n\n\n\nSlurmctldParameters\n\n\nComma-separated options identifying slurmctld options. Options of\n    interest include:\n\ncloud_dns\n\n\nBy default, Slurm expects that the network addresses for cloud\n        nodes won't be known until creation of the node and that Slurm will\n        be notified of the node's address upon registration. Since Slurm\n        communications rely on the node configuration found in the\n        slurm.conf, Slurm will tell the client command, after waiting for all\n        nodes to boot, each node's IP address. However, in environments where\n        the nodes are in DNS, this step can be avoided by configuring this\n        option.\n\nidle_on_node_suspend\n\n\nMark nodes as idle, regardless of current state, when suspending\n        nodes with SuspendProgram so that nodes will be eligible to be\n        resumed at a later time.\n\nnode_reg_mem_percent=#\n\n\nPercentage of memory a node is allowed to register with without\n        being marked as invalid with low memory. Default is 100. For\n        State=CLOUD nodes, the default is 90.\n\npower_save_interval=#\n\n\nHow often the power_save thread looks to resume and suspend nodes.\n        The power_save thread will do work sooner if there are node state\n        changes. Default is 10 seconds.\n\npower_save_min_interval=#\n\n\nHow often the power_save thread, at a minimum, looks to resume and\n        suspend nodes. Default is 0.\n\n\n\nSuspendExcNodes\n\n\nNodes not subject to suspend/resume logic. This may be used to avoid\n    suspending and resuming nodes which are not in the cloud. Alternately the\n    suspend/resume programs can treat local nodes differently from nodes\n    being provisioned from cloud. Use Slurm's hostlist expression to identify\n    nodes with an optional \":\" separator and count of nodes to exclude from\n    the preceding range. For example nid[10-20]:4 will prevent 4\n    usable nodes (i.e IDLE and not DOWN, DRAINING or already powered down) in\n    the set nid[10-20] from being powered down. Multiple sets of\n    nodes can be specified with or without counts in a comma separated list\n    (e.g nid[10-20]:4,nid[80-90]:2). By default, no nodes are\n    excluded. This value may be updated with scontrol. See\n    ReconfigFlags=KeepPowerSaveSettings for setting persistence.\n\nSuspendExcParts\n\n\nList of partitions with nodes to never place in power saving mode.\n    Multiple partitions may be specified using a comma separator. By default,\n    no nodes are excluded. This value may be updated with scontrol. See\n    ReconfigFlags=KeepPowerSaveSettings for setting persistence.\n\nSuspendExcStates\n\n\nSpecifies node states that are not to be powered down automatically.\n    Valid states include CLOUD, DOWN, DRAIN, DYNAMIC_FUTURE, DYNAMIC_NORM,\n    FAIL, INVALID_REG, MAINTENANCE, NOT_RESPONDING, PERFCTRS, PLANNED, and\n    RESERVED. By default, any of these states, if idle for\n    SuspendTime, would be powered down. This value may be updated with\n    scontrol. See ReconfigFlags=KeepPowerSaveSettings for setting\n    persistence.\n\nSuspendProgram\n\n\nProgram to be executed to place nodes into power saving mode. The\n    program executes as SlurmUser (as configured in\n    slurm.conf). The argument to the program will be the names of\n    nodes to be placed into power savings mode (using Slurm's hostlist\n    expression format).\n\nSuspendRate\n\n\nMaximum number of nodes to be placed into power saving mode per\n    minute. A value of zero results in no limits being imposed. The default\n    value is 60. Use this to prevent rapid drops in power consumption.\n\nSuspendTime\n\n\nNodes becomes eligible for power saving mode after being idle or down\n    for this number of seconds. A negative number disables power saving mode.\n    The default value is -1 (disabled).\n\nSuspendTimeout\n\n\nMaximum time permitted (in second) between when a node suspend request\n    is issued and when the node shutdown is complete. At that time the node\n    must ready for a resume request to be issued as needed for new workload.\n    The default value is 30 seconds.\n\nNode ConfigurationNode parameters of interest include:\nFeature\n\n\nA node feature can be associated with resources acquired from the\n    cloud and user jobs can specify their preference for resource use with\n    the --constraint option.\n\nNodeName\n\n\nThis is the name by which Slurm refers to the node. A name containing\n    a numeric suffix is recommended for convenience.\n\nState\n\n\nNodes which are to be added on demand should have a state of\n    CLOUD.\n\nWeight\n\n\nEach node can be configured with a weight indicating the desirability\n    of using that resource. Nodes with lower weights are used before those\n    with higher weights. The default value is 1.\n\nPartition ConfigurationPartition parameters of interest include:\nPowerDownOnIdle\n\n\nIf set to YES and power saving is enabled for the partition,\n    then nodes allocated from this partition will be requested to power down\n    after being allocated at least one job. These nodes will not power down\n    until they transition from COMPLETING to IDLE. If set to NO then\n    power saving will operate as configured for the partition. The default\n    value is NO.\nThe following will cause a transition from COMPLETING to\n    IDLE:\n\nCompleting all running jobs without additional jobs being\n      allocated.\nExclusiveUser=YES and after all running jobs complete but\n      before another user's job is allocated.\nOverSubscribe=EXCLUSIVE and after the running job completes\n      but before another job is allocated.\n\nNOTE: Nodes are still subject to powering down when being IDLE\n    for SuspendTime when PowerDownOnIdle is set to NO.\n\nResumeTimeout\n\n\nMaximum time permitted (in seconds) between when a node resume request\n    is issued and when the node is actually available for use. Nodes which\n    fail to respond in this time frame will be marked DOWN and the jobs\n    scheduled on the node requeued. Nodes which reboot after this time frame\n    will be marked DOWN with a reason of \"Node unexpectedly rebooted.\" The\n    default value is 60 seconds.\nFor nodes that are in multiple partitions with this option set, the\n    highest time will take effect. If not set on any partition, the node will\n    use the ResumeTimeout value set for the entire cluster.\n\nSuspendTime\n\n\nNodes which remain idle or down for this number of seconds will be\n    placed into power saving mode by SuspendProgram.\nFor nodes that are in multiple partitions with this option set, the\n    highest time will take effect. If not set on any partition, the node will\n    use the SuspendTime value set for the entire cluster. Setting\n    SuspendTime to INFINITE will disable suspending of nodes in\n    this partition.\n\nSuspendTimeout\n\n\nMaximum time permitted (in second) between when a node suspend request\n    is issued and when the node shutdown is complete. At that time the node\n    must ready for a resume request to be issued as needed for new workload.\n    The default value is 30 seconds.\nFor nodes that are in multiple partitions with this option set, the\n    highest time will take effect. If not set on any partition, the node will\n    use the SuspendTimeout value set for the entire cluster.\n\nNode LifecycleWhen Slurm is configured for Power Saving operation, nodes have an\nexpanded set of states associated with them. States associated with Power\nSaving are generally labeled with a symbol when viewing node details with\nsinfo.\n\n  Figure 1. Node Lifecycle\nNode states of interest:\n\n\n\nSTATE\n\nPower Saving Symbol\n\nDescription\n\n\n\nPOWER_DOWN\n!\nPower down request. When the node is no longer running job(s),\n        run the SuspendProgram.\n\n\nPOWER_UP\n\u00a0\nPower up request. When possible, run the\n        ResumeProgram.\n\n\nPOWERED_DOWN\n~\nThe node is powered down or in power saving mode.\n\n\nPOWERING_DOWN\n%\nThe node is in the process of powering down, or being put into\n        power saving mode, and is not capable of running any jobs for\n        SuspendTimeout.\n\n\nPOWERING_UP\n#\nThe node is in the process of powering up, or being restored from\n        power saving mode.\n\n\n\nManual Power SavingA node can be manually powered up and down by setting the state of the\nnode to the following states using scontrol:\nscontrol update nodename=<nodename> state=power_<down|down_asap|down_force|up>\nscontrol update command actions/states of interest:\nPOWER_DOWN\n\nWill use the configured SuspendProgram program to explicitly\n  place a node in power saving mode. If a node is already in the process of\n  being powered down, the command will only change the state of the node but\n  won't have any effect until the configured SuspendTimeout is\n  reached.\nPOWER_DOWN_ASAP\n\nWill drain the node and mark it for power down. Currently running jobs\n  will complete first and no additional jobs will be allocated to the\n  node.\nPOWER_DOWN_FORCE\n\nWill cancel all jobs on the node, power it down, and reset its state to\n  IDLE.\nPOWER_UP\n\nWill use the configured ResumeProgram program to explicitly move\n  a node out of power saving mode. If a node is already in the process of\n  being powered up, the command will only change the state of the node but\n  won't have any effect until the configured ResumeTimeout is\n  reached.\nRESUME\n\nNot an actual node state, but will change a node state from DRAIN,\n  DRAINING, DOWN or REBOOT to IDLE and NoResp. slurmctld will then attempt to\n  contact slurmd to request that the node register itself. Once registered,\n  the node state will then remove the NoResp flag and will resume normal\n  operations. It will also clear the POWERING_DOWN state of a node and make\n  it eligible to be allocated.\nResume and Suspend ProgramsThe ResumeProgram and SuspendProgram execute as\nSlurmUser on the node where the slurmctld daemon runs (primary\nand backup server nodes). Use of sudo may be required for\nSlurmUser to power down and restart nodes. If you need to convert\nSlurm's hostlist expression into individual node names, the scontrol\nshow hostnames command may prove useful. The commands used to boot or\nshut down nodes will depend upon your cluster management tools.The ResumeProgram and SuspendProgram are not subject to any\ntime limits but must have Fault Tolerance. They\nshould perform the required action, ideally verify the action (e.g. node boot\nand start the slurmd daemon, thus the node is no longer non-responsive\nto slurmctld) and terminate. Long running programs will be logged by\nslurmctld, but not aborted.Example ResumeProgram:\n#!/bin/bash\n# Example ResumeProgram\nhosts=$(scontrol show hostnames \"$1\")\nlogfile=/var/log/power_save.log\necho \"$(date) Resume invoked $0 $*\" >>$logfile\nfor host in $hosts\ndo\n        sudo node_startup \"$host\"\ndone\nexit 0\nExample SuspendProgram:\n#!/bin/bash\n# Example SuspendProgram\nhosts=$(scontrol show hostnames \"$1\")\nlogfile=/var/log/power_save.log\necho \"$(date) Suspend invoked $0 $*\" >>$logfile\nfor host in $hosts\ndo\n        sudo node_shutdown \"$host\"\ndone\nexit 0\nNOTE: the stderr and stdout of the suspend and resume programs are\nnot logged. If logging is desired, then it should be added to the\nscripts.Fault ToleranceIf the slurmctld daemon is terminated gracefully, it will wait up\nto ten seconds (or the maximum of SuspendTimeout or\nResumeTimeout if less than ten seconds) for any spawned\nSuspendProgram or ResumeProgram to terminate before the daemon\nterminates. If the spawned program does not terminate within that time\nperiod, the event will be logged and slurmctld will exit in order to\npermit another slurmctld daemon to be initiated. Any spawned\nSuspendProgram or ResumeProgram will continue to run.When the slurmctld daemon shuts down, any SLURM_RESUME_FILE\ntemporary files are no longer available, even once slurmctld restarts.\nTherefore, ResumeProgram should use SLURM_RESUME_FILE within\nten seconds of starting to guarantee that it still exists.Booting Different ImagesIf you want ResumeProgram to boot various images according to job\nspecifications, it will need to be a fairly sophisticated program and perform\nthe following actions:\nDetermine which jobs are associated with the nodes to be booted.\n  SLURM_RESUME_FILE will help with this step.\nDetermine which image is required for each job. Images can be mapped\n  with NodeFeaturesPlugins.\n  \nBoot the appropriate image for each node.\nUse of AllocationsA resource allocation request will be granted as soon as resources are\nselected for use, possibly before the nodes are all available for use. The\nlaunching of job steps will be delayed until the required nodes have been\nrestored to service (it prints a warning about waiting for nodes to become\navailable and periodically retries until they are available).In the case of an sbatch command, the batch program will start when\nnode zero of the allocation is ready for use and pre-processing can be\nperformed as needed before using srun to launch job steps. The\nsbatch --wait-all-nodes=<value> option can be used\nto override this behavior on a per-job basis and a system-wide default can be\nset with the SchedulerParameters=sbatch_wait_nodes option.In the case of the salloc command, once the allocation is made a\nnew shell will be created on the login node. The salloc\n--wait-all-nodes=<value> option can be used to override\nthis behavior on a per-job basis and a system-wide default can be set with\nthe SchedulerParameters=salloc_wait_nodes option.Node FeaturesFeatures defined by NodeFeaturesPlugins, and\nassociated to cloud nodes in the slurm.conf, will be available but not\nactive when the node is powered down. If a job requests available nut not\nactive features, the controller will allocate nodes that are powered down and\nhave the features as available. At allocation, the features will be made\nactive. A cloud node will remain with the active features until the node is\npowered down (i.e. the node can't be rebooted to get other features until the\nnode is powered down). When the node is powered down, the those features\nbecome available but not active. Any feature not defined by\nNodeFeaturesPlugins are always active.Example:\nslurm.conf:\nNodeFeaturesPlugins=node_features/helpers\n\nNodeName=cloud[1-5] ... State=CLOUD Feature=f1,f2,l1\nNodeName=cloud[6-10] ... State=CLOUD Feature=f3,f4,l2\n\nhelpers.conf:\nNodeName=cloud[1-5] Feature=f1,f2 Helper=/bin/true\nNodeName=cloud[6-10] Feature=f3,f4 Helper=/bin/true\nFeatures f1, f2, f3, and f4 are changeable features and are defined on the\nnode lines in the slurm.conf because CLOUD nodes do not register\nbefore being allocated. By setting the Helper script to /bin/true, the\nslurmd's will not report any active features to the controller and the\ncontroller will manage all the active features. If the Helper is set\nto a script that reports the active features, the controller will validate\nthat the reported active features are a super set of the node's active\nchangeable features in the controller. Features l1 and l2 will always be\nactive and can be used as selectable labels.Hybrid ClusterCloud nodes to be acquired on demand can be placed into their own Slurm\npartition. This mode of operation can be used to use these nodes only if so\nrequested by the user. Note that jobs can be submitted to multiple partitions\nand will use resources from whichever partition permits faster initiation. A\nsample configuration in which nodes are added from the cloud when the\nworkload exceeds available resources. Users can explicitly request local\nresources or resources from the cloud by using the --constraint\noption.Example:\n# Excerpt of slurm.conf\nSelectType=select/cons_tres\nSelectTypeParameters=CR_CORE_Memory\n\nSuspendProgram=/usr/sbin/slurm_suspend\nResumeProgram=/usr/sbin/slurm_resume\nSuspendTime=600\nSuspendExcNodes=tux[0-127]\nTreeWidth=128\n\nNodeName=DEFAULT    Sockets=1 CoresPerSocket=4 ThreadsPerCore=2\nNodeName=tux[0-127] Weight=1 Feature=local State=UNKNOWN\nNodeName=ec[0-127]  Weight=8 Feature=cloud State=CLOUD\nPartitionName=debug MaxTime=1:00:00 Nodes=tux[0-32] Default=YES\nPartitionName=batch MaxTime=8:00:00 Nodes=tux[0-127],ec[0-127]\nWhen SuspendTime is set globally, Slurm attempts to suspend all\nnodes unless excluded by SuspendExcNodes or SuspendExcParts. It\ncan be tricky to have to remember to add on-premise nodes to the excluded\noptions. By setting the global SuspendTime to INFINITE and\nconfiguring SuspendTime on cloud specific partitions, you can avoid\nhaving to exclude nodes.Example:\n# Excerpt of slurm.conf\nSelectType=select/cons_tres\nSelectTypeParameters=CR_CORE_Memory\n\nSuspendProgram=/usr/sbin/slurm_suspend\nResumeProgram=/usr/sbin/slurm_resume\nTreeWidth=128\n\nNodeName=DEFAULT    Sockets=1 CoresPerSocket=4 ThreadsPerCore=2\nNodeName=tux[0-127] Weight=1 Feature=local State=UNKNOWN\nNodeName=ec[0-127]  Weight=8 Feature=cloud State=CLOUD\nPartitionName=debug MaxTime=1:00:00 Nodes=tux[0-32] Default=YES\nPartitionName=batch MaxTime=8:00:00 Nodes=tux[0-127],ec[0-127]\nPartitionName=cloud Nodes=ec[0-127] SuspendTime=600\nHere we have configured a partition with only cloud nodes and defined\nSuspendTime on that partition. Doing so will allow us to control when\nthose nodes power down without affecting our on-premise nodes, therefore\nSuspendExcNodes or SuspendExcParts are not needed in this\nsetup.Cloud AccountingInformation about cloud instances can be stored in the database. This can\nbe done by configuring instance id/type upon slurmd startup or with\nscontrol update. The node's \"extra\" field will also be stored in the\ndatabase.Configuring cloud information on slurmd startup:\n$ slurmd --instance-id=12345 --instance-type=m7g.medium --extra=\"arbitrary string\" . . .\nConfiguring cloud information with scontrol update:\n$ scontrol update nodename=n1 instanceid=12345 instancetype=m7g.medium extra=\"arbitrary string\"\nThis data can then be seen on the controller with scontrol show\nnode. Past and current data can be seen in the database with sacctmgr\nshow instance, as well as through slurmrestd with the /instance\nand /instances endpoints.Showing cloud information on the controller with scontrol:\n$ scontrol show nodes n1 | grep \"NodeName\\|Extra\\|Instance\"\nNodeName=n1 Arch=x86_64 CoresPerSocket=4\n   Extra=arbitrary string\n   InstanceId=12345 InstanceType=m7g.medium\nShowing cloud information from the database with sacctmgr:\n$ sacctmgr show instance format=nodename,instanceid,instancetype,extra\nNodeName                  InstanceId         InstanceType                Extra\n--------------- -------------------- -------------------- --------------------\nn1                             12345           m7g.medium     arbitrary string\nShowing cloud information from the database with slurmrestd:\n$ curl -k -s \\\n        --request GET \\\n        -H X-SLURM-USER-NAME:$(whoami) \\\n        -H X-SLURM-USER-TOKEN:$SLURM_JWT \\\n        -H \"Content-Type: application/json\" \\\n        --url localhost:8080/slurmdb/v0.0.40/instances \\\n        | jq \".instances\"\n\n[\n  {\n    \"cluster\": \"c1\",\n    \"extra\": \"arbitrary string\",\n    \"instance_id\": \"12345\",\n    \"instance_type\": \"m7g.medium\",\n    \"node_name\": \"n1\",\n    \"time\": {\n      \"time_end\": 0,\n      \"time_start\": 1687213177\n    }\n  }\n]\nLast modified 02 February 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/resource_limits.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Resource Limits",
                "content": "Familiarity with Slurm's Accounting web page\nis strongly recommended before use of this document.HierarchySlurm's hierarchical limits are enforced in the following order\n  with Job QOS and Partition QOS order being reversible by using the QOS\n  flag 'OverPartQOS':\nPartition QOS limit\nJob QOS limit\nUser association\nAccount association(s), ascending the hierarchy\nRoot/Cluster association\nPartition limit\nNone\nNote: If limits are defined at multiple points in this hierarchy,\nthe point in this list where the limit is first defined will be used.\nConsider the following example:\nMaxJobs=20 and MaxSubmitJobs is undefined in the partition QOS\nNo limits are set in the job QOS and\nMaxJobs=4 and MaxSubmitJobs=50 in the user association\nThe limits in effect will be MaxJobs=20 and MaxSubmitJobs=50.Note: The precedence order specified above is respected except for the\nfollowing limits: Max[Time|Wall], [Min|Max]Nodes. For these limits, even\nif the job is enforced with QOS and/or Association limits, it can't\ngo over the limit imposed at Partition level, even if it listed at the bottom.\nSo the default for these 3 types of limits is that they are upper bound by the\nPartition one. This Partition level bound can be ignored if\nthe respective QOS PartitionTimeLimit and/or Partition[Max|Min]Nodes flags\nare set, then the job would be enforced the limits imposed at QOS\nand/or association level respecting the order above.\nGrp* limits are also an exception. A more restrictive limit at the\nAccount level will be enforced before a less restrictive limit at the User\nlevel. This is due to the nature of the limit being enforced, requiring that\nthe limit at the highest level not be exceeded.\nConfigurationScheduling policy information must be stored in a database\nas specified by the AccountingStorageType configuration parameter\nin the slurm.conf configuration file.\nInformation can be recorded in a MySQL or\nMariaDB database.\nFor security and performance reasons, the use of\nSlurmDBD (Slurm Database Daemon) as a front-end to the\ndatabase is strongly recommended.\nSlurmDBD uses a Slurm authentication plugin (e.g. MUNGE).\nSlurmDBD also uses an existing Slurm accounting storage plugin\nto maximize code reuse.\nSlurmDBD uses data caching and prioritization of pending requests\nin order to optimize performance.\nWhile SlurmDBD relies upon existing Slurm plugins for authentication\nand database use, the other Slurm commands and daemons are not required\non the host where SlurmDBD is installed.\nOnly the slurmdbd and slurm-plugins RPMs are required\nfor SlurmDBD execution.Both accounting and scheduling policies are configured based upon\nan association. An association is a 4-tuple consisting\nof the cluster name, bank account, user and (optionally) the Slurm\npartition.\nIn order to enforce scheduling policy, set the value of\nAccountingStorageEnforce.\nThis option contains a comma separated list of options you may want to\nenforce.  The valid options are:\n\nassociations - This will prevent users from running jobs if\ntheir association is not in the database. This option will\nprevent users from accessing invalid accounts.\n\nlimits - This will enforce limits set to associations.  By setting\n  this option, the 'associations' option is also set.\n\nqos - This will require all jobs to specify (either overtly or by\ndefault) a valid qos (Quality of Service).  QOS values are defined for\neach association in the database.  By setting this option, the\n'associations' option is also set.\n\nsafe - This will ensure a job will only be launched when using an\n  association or qos that has a TRES-minutes limit set if the job will be\n  able to run to completion. Without this option set, jobs will be\n  launched as long as their usage hasn't reached the TRES-minutes limit\n  which can lead to jobs being launched but then killed when the limit is\n  reached.\n  With the 'safe' option set, a job won't be killed due to limits,\n  even if the limits are changed after the job was started and the\n  association or qos violates the updated limits.\n  By setting this option, both the 'associations' option and the\n  'limits' option are set automatically.\n\nwckeys - This will prevent users from running jobs under a wckey\n  that they don't have access to.  By using this option, the\n  'associations' option is also set.  The 'TrackWCKey' option is also\n  set to true.\n\n\nNOTE: The association is a combination of cluster, account,\nuser names and optional partition name.\n\nWithout AccountingStorageEnforce being set (the default behavior)\njobs will be executed based upon policies configured in Slurm on each\ncluster.\n\nTools\nThe tool used to manage accounting policy is sacctmgr.\nIt can be used to create and delete cluster, user, bank account,\nand partition records plus their combined association record.\nSee man sacctmgr for details on this tools and examples of\nits use.\nChanges made to the scheduling policy are uploaded to\nthe Slurm control daemons on the various clusters and take effect\nimmediately. When an association is deleted, all running or pending\njobs which belong to that association are immediately canceled.\nWhen limits are lowered, running jobs will not be canceled to\nsatisfy the new limits, but the new lower limits will be enforced.\nAssociation specific limits and scheduling policies\n\n\nThese represent the limits and scheduling policies relevant to Associations.\nWhen dealing with Associations, most of these limits are available\nnot only for the user association, but also for each cluster and account.\nLimits and policies are applied in the following order:\n\n1. The option specified for the user association.\n\n2. The option specified for the account.\n\n3. The option specified for the cluster.\n\n4. If nothing is configured at the above levels, no limit will be applied.\n\nThese are just the limits and policies for Associations. For a more\ncomplete description of the columns available to be displayed, see the\n\nsacctmgr man page.\n\nFairshare\n\nInteger value used for determining priority.\n  Essentially this is the amount of claim this association and its\n  children have to the above system. Can also be the string \"parent\",\n  when used on a user this means that the parent association is used\n  for fairshare.  If Fairshare=parent is set on an account, that\n  account's children will be effectively re-parented for fairshare\n  calculations to the first parent of their parent that is not\n  Fairshare=parent.  Limits remain the same, only its fairshare value\n  is affected.\n\nGrpJobs\n\nThe total number of jobs able to run at any given\n  time from an association and its children.  If\n  this limit is reached, new jobs will be queued but only allowed to\n  run after previous jobs complete from this group.\n\nGrpJobsAccrue\n\nThe total number of pending jobs able to accrue age\n  priority at any given time from an association and its children.  If\n  this limit is reached, new jobs will be queued but not accrue age priority\n  until after previous jobs are removed from pending in this group.\n  This limit does not determine if the job can run or not, it only limits the\n  age factor of the priority.\n\nGrpSubmitJobs\n\nThe total number of jobs able to be submitted\n  to the system at any given time from an association and its children.\n  If this limit is reached, new submission requests will be\n  denied until previous jobs complete from this group.\n\nGrpTRES\n\nThe total count of TRES able to be used at any given\n  time from jobs running from an association and its children. If\n  this limit is reached, new jobs will be queued but only allowed to\n  run after resources have been relinquished from this group.\n\nGrpTRESMins\n\nThe total number of TRES minutes that can\n  possibly be used by past, present and future jobs\n  running from an association and its children. If any limit is reached,\n  all running jobs with that TRES in this group will be killed, and no new\n  jobs will be allowed to run.  This usage is decayed (at a rate of\n  PriorityDecayHalfLife).  It can also be reset (according to\n  PriorityUsageResetPeriod) in order to allow jobs to run against the\n  association tree.\n  This limit only applies when using the Priority Multifactor plugin.\n\nGrpTRESRunMins\n\nUsed to limit the combined total number of TRES\n  minutes used by all jobs running with an association and its\n  children.  This takes into consideration time limit of\n  running jobs and consumes it. If the limit is reached, no new jobs\n  are started until other jobs finish to allow time to free up.\n\nGrpWall\n\nThe maximum wall clock time running jobs are able\n  to be allocated in aggregate for an association and its children.\n  If this limit is reached, future jobs in this association will be\n  queued until they are able to run inside the limit.\n  This usage is decayed (at a rate of\n  PriorityDecayHalfLife).  It can also be reset (according to\n  PriorityUsageResetPeriod) in order to allow jobs to run against the\n  association tree again.\n\nMaxJobs\n\nThe total number of jobs able to run at any given\n  time for the given association.  If this limit is reached, new jobs will\n  be queued but only allowed to run after existing jobs in the association\n  complete.\n\nMaxJobsAccrue\n\nThe maximum number of pending jobs able to accrue age\n  priority at any given time for the given association.  If this limit is\n  reached, new jobs will be queued but will not accrue age priority\n  until after existing jobs in the association are moved from a pending state.\n  This limit does not determine if the job can run, it only limits the\n  age factor of the priority.\n\nMaxSubmitJobs\n\nThe maximum number of jobs able to be submitted\n  to the system at any given time from the given association.  If\n  this limit is reached, new submission requests will be denied until\n  existing jobs in this association complete.\n\nMaxTRESMinsPerJob\n\nA limit of TRES minutes to be used by a job.\n  If this limit is reached, the job will be killed if not running in\n  Safe mode, otherwise the job will pend until enough time is given to\n  complete the job.\n\nMaxTRESPerJob\n\nThe maximum size in TRES any given job can\n  have from the association.\n\nMaxTRESPerNode\n\nThe maximum size in TRES each node in a job\n  allocation can use.\n\n\nMaxWallDurationPerJob\n\nThe maximum wall clock time any individual job\n  can run for in the given association.  If this limit is reached,\n  the job will be denied at submission.\n\nMinPrioThreshold\n\nMinimum priority required to reserve resources\n  in the given association. Used to override bf_min_prio_reserve.\n  See \n  bf_min_prio_reserve for details.\n\nQOS\n\ncomma separated list of QOSs an association is\n  able to run.\n\n\nNOTE: When modifying a TRES field with sacctmgr, one must\nspecify which TRES to modify (see TRES for complete\nlist) as in the following examples: \n\nSET:\nsacctmgr modify user bob set GrpTRES=cpu=1500,mem=200,gres/gpu=50\nUNSET:\nsacctmgr modify user bob set GrpTRES=cpu=-1,mem=-1,gres/gpu=-1\n\nQOS specific limits and scheduling policies\n\n\nAs noted above, the default behavior is that\na limit set on a Partition QOS will be applied before a limit on the job's\nrequested QOS. You can change this behavior with the OverPartQOS\nflag.\nUnless noted, if a job request breaches a given limit\non its own, the job will pend unless the job's QOS has the DenyOnLimit\nflag set, which will cause the job to be denied at submission.  When\nGrp limits are considered with respect to this flag the Grp limit\nis treated as a Max limit.\n\nGraceTime\n\nPreemption grace time to be extended to a job which\n  has been selected for preemption in the format of\n  <hh>:<mm>:<ss>. The default value is zero,\n  meaning no preemption grace time is allowed on this QOS. This value\n  is only meaningful for QOS PreemptMode=CANCEL and PreemptMode=REQUEUE.\n\nGrpJobs\n\nThe total number of jobs able to run at any given time\n  from a QOS. If this limit is reached, new jobs will be queued but only\n  allowed to run after previous jobs complete from this group.\n\nGrpJobsAccrue\n\nThe total number of pending jobs able to accrue age priority at any\n  given time from a QOS. If this limit is reached, new jobs will be queued but\n  will not accrue age based priority until after previous jobs are removed\n  from pending in this group. This limit does not determine if the job can\n  run or not, it only limits the age factor of the priority. This limit only\n  applies to the job's QOS and not the partition's QOS.\n\nGrpSubmitJobs\n\nThe total number of jobs able to be submitted to the system at any\n  given time from a QOS. If this limit is reached, new submission requests\n  will be denied until previous jobs complete from this group.\n\nGrpTRES\n\nThe total count of TRES able to be used at any given time from jobs\n  running from a QOS. If this limit is reached, new jobs will be queued but\n  only allowed to run after resources have been relinquished from this group.\n\nGrpTRESMins\n\nThe total number of TRES minutes that can possibly be used by past,\n  present and future jobs running from a QOS. If any limit is reached,\n  all running jobs with that TRES in this group will be killed, and no new\n  jobs will be allowed to run.  This usage is decayed (at a rate of\n  PriorityDecayHalfLife).  It can also be reset (according to\n  PriorityUsageResetPeriod) in order to allow jobs to run against the\n  QOS again.  QOS that have the NoDecay flag set do not decay GrpTRESMins,\n  see QOS Options for details.\n  This limit only applies when using the Priority Multifactor plugin.\n\nGrpTRESRunMins\n\nUsed to limit the combined total number of TRES\n  minutes used by all jobs running with a QOS.  This takes into\n  consideration the time limit of running jobs and consumes it.\n  If the limit is reached, no new jobs are started until other jobs\n  finish to allow time to free up.\n\nGrpWall\n\nThe maximum wall clock time running jobs are able\n  to be allocated in aggregate for a QOS. If this limit is reached,\n  future jobs in this QOS will be queued until they are able to run\n  inside the limit. This usage is decayed (at a rate of\n  PriorityDecayHalfLife).  It can also be reset (according to\n  PriorityUsageResetPeriod) in order to allow jobs to run against the\n  QOS again.  QOS that have the NoDecay flag set do not decay GrpWall.\n  See QOS Options for details.\n\nLimitFactor\n\nA float that is factored into an associations [Grp|Max]TRES limits.\n  For example, if the LimitFactor is 2, then an association with a GrpTRES of\n  30 CPUs would be allowed to allocate 60 CPUs when running under this QOS.\n\n  NOTE: This factor is only applied to associations running in this\n  QOS and is not applied to any limits in the QOS itself.\n\nMaxJobsAccruePerAccount\n\nThe maximum number of pending jobs an\n  account (or sub-account) can have accruing age priority at any given time.\n  This limit does not determine if the job can run, it only limits the\n  age factor of the priority.\n\nMaxJobsAccruePerUser\n\nThe maximum number of pending jobs a\n  user can have accruing age priority at any given time.\n  This limit does not determine if the job can run, it only limits the\n  age factor of the priority.\n\nMaxJobsPerAccount\n\nThe maximum number of jobs an account (or sub-account) can have running at\n  a given time.\n\nMaxJobsPerUser\n\nThe maximum number of jobs a user can\n  have running at a given time.\n\nMaxSubmitJobsPerAccount\n\nThe maximum number of jobs an account (or sub-account) can have running and\n  pending at a given time.\n\nMaxSubmitJobsPerUser\n\nThe maximum number of jobs a user can\n  have running and pending at a given time.\n\nMaxTRESMinsPerJob\n\nMaximum number of TRES minutes each job is able to use.\n\nMaxTRESPerAccount\n\nThe maximum number of TRES an account can\n  allocate at a given time.\n\nMaxTRESPerJob\n\nThe maximum number of TRES each job is able to use.\n\nMaxTRESPerNode\n\nThe maximum number of TRES each node in a job allocation can use.\n\nMaxTRESPerUser\n\nThe maximum number of TRES a user can\n  allocate at a given time.\n\nMaxWallDurationPerJob\n\nMaximum wall clock time each job is able to use. Format is <min>\n  or <min>:<sec> or <hr>:<min>:<sec> or\n  <days>-<hr>:<min>:<sec> or <days>-<hr>.\n  The value is recorded in minutes with rounding as needed.\n\nMinPrioThreshold\n\nMinimum priority required to reserve resources when scheduling.\n\nMinTRESPerJob\n\nThe minimum size in TRES any given job can\n  have when using the requested QOS.\n\nUsageFactor\n\nA float that is factored into a job's TRES usage (e.g. RawUsage,\n  TRESMins, TRESRunMins). For example, if the usagefactor was 2, for every\n  TRESBillingUnit second a job ran it would count for 2. If the usagefactor\n  was .5, every second would only count for half of the time.\n  A setting of 0 would add no timed usage from the job.\n\n  The usage factor only applies to the job's QOS and not the partition QOS.\n  \n  If the UsageFactorSafe flag is set and AccountingStorageEnforce includes\n  Safe, jobs will only be able to run if the job can run to completion\n  with the UsageFactor applied, and won't be killed due to limits.\n  \n  If the UsageFactorSafe flag is not set and AccountingStorageEnforce includes\n  Safe, a job will be able to be scheduled without the UsageFactor\n  applied and won't be killed due to limits.\n  \n  If the UsageFactorSafe flag is not set and AccountingStorageEnforce does\n  not include Safe, a job will be scheduled as long as the limits are\n  not reached, but could be killed due to limits.\n  \n  See \n  AccountingStorageEnforce in the slurm.conf man page.\n\n\nThe MaxNodes and MaxTime options already exist in\nSlurm's configuration on a per-partition basis, but the above options\nprovide the ability to impose limits on a per-user basis.  The\nMaxJobs option provides an entirely new mechanism for Slurm to\ncontrol the workload any individual may place on a cluster in order to\nachieve some balance between users.\nWhen assigning limits to a QOS to use for a Partition QOS,\nkeep in mind that those limits are enforced at the QOS level, not\nindividually for each partition.  For example, if a QOS has a\nGrpTRES=cpu=20 limit defined and the QOS is assigned to two\nunique partitions, users will be limited to 20 CPUs for the QOS\nrather than being allowed 20 CPUs for each partition.\nFair-share scheduling is based upon the hierarchical bank account\ndata maintained in the Slurm database.  More information can be found\nin the priority/multifactor\nplugin description.\nSpecific limits over GRES\n\n\n When a GRES has a type associated with it and a limit is applied\n  over this specific type (e.g. MaxTRESPerUser=gres/gpu:tesla=1) if a\n  user requests a generic gres, the type's limit will not be enforced. In this\n  situation an additional lua job submit plugin to check the user request may\n  become useful. For example, if one requests --gres=gpu:2 having a\n  limit set of MaxTRESPerUser=gres/gpu:tesla=1, the limit won't be\n  enforced so it will still be possible to get two teslas.\n\n\n  This is due to a design limitation. The only way to enforce such a limit\n  is to combine the specification of the limit with a job submit plugin that\n  forces the user to always request a specific type model.\n\n\n  An example of basic lua job submit plugin function could be:\n\n\nfunction slurm_job_submit(job_desc, part_list, submit_uid)\n   gres_request = \"\"\n   t = {job_desc.tres_per_job,\n\tjob_desc.tres_per_socket,\n\tjob_desc.tres_per_task,\n\tjob_desc.tres_per_node}\n   for k in pairs(t) do\n\tgres_request = gres_request .. t[k] .. \",\"\n   end\n   if (gres_request ~= nil)\n   then\n      for g in gres_request:gmatch(\"[^,]+\")\n      do\n\t bad = string.match(g,'^gres/gpu[:=]*[0-9]*$')\n\t if (bad ~= nil)\n\t then\n\t    slurm.log_info(\"User specified gpu GRES without type: %s\", bad)\n\t    slurm.user_msg(\"You must always specify a type when requesting gpu GRES\")\n\t    return slurm.ERROR\n\t end\n      end\n   end\nend\n\n Having this script and the limit in place will force the users to always\n  specify a gpu with its type, thus enforcing the limits for each specific\n  model.\n\nWhen TRESBillingWeights are defined for a partition, both typed and\nnon-typed resources should be included. For example, if you have 'tesla' GPUs\nin one partition and you only define the billing weights for the 'tesla' typed\nGPU resource, then those weights will not be applied to the generic GPUs.\nIt is also advisable to set AccountingStorageTRES for both generic\n  and specific gres types, otherwise requests that ask for the generic instance\n  of a gres won't be accounted for. For example, to track generic GPUs and\n  Tesla GPUs, you would set this in your slurm.conf:\n\n\n  AccountingStorageTRES=gres/gpu,gres/gpu:tesla\n\n\n  See Trackable Resources TRES for details.\n\nJob Reason Codes\n\n\nWhen a pending job is evaluated by the scheduler but found to exceed a\nconfigured resource limit, a corresponding reason will be assigned to the job.\nMore details can be found on the Job Reason\nCodes page. More details about scheduling can be found in the\n Scheduling Configuration Guide.\nLast modified 27 September 2024\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Tools",
                "content": "The tool used to manage accounting policy is sacctmgr.\nIt can be used to create and delete cluster, user, bank account,\nand partition records plus their combined association record.\nSee man sacctmgr for details on this tools and examples of\nits use.Changes made to the scheduling policy are uploaded to\nthe Slurm control daemons on the various clusters and take effect\nimmediately. When an association is deleted, all running or pending\njobs which belong to that association are immediately canceled.\nWhen limits are lowered, running jobs will not be canceled to\nsatisfy the new limits, but the new lower limits will be enforced.Association specific limits and scheduling policies\n\nThese represent the limits and scheduling policies relevant to Associations.\nWhen dealing with Associations, most of these limits are available\nnot only for the user association, but also for each cluster and account.\nLimits and policies are applied in the following order:\n\n1. The option specified for the user association.\n\n2. The option specified for the account.\n\n3. The option specified for the cluster.\n\n4. If nothing is configured at the above levels, no limit will be applied.\nThese are just the limits and policies for Associations. For a more\ncomplete description of the columns available to be displayed, see the\n\nsacctmgr man page.\nFairshare\n\nInteger value used for determining priority.\n  Essentially this is the amount of claim this association and its\n  children have to the above system. Can also be the string \"parent\",\n  when used on a user this means that the parent association is used\n  for fairshare.  If Fairshare=parent is set on an account, that\n  account's children will be effectively re-parented for fairshare\n  calculations to the first parent of their parent that is not\n  Fairshare=parent.  Limits remain the same, only its fairshare value\n  is affected.\n\nGrpJobs\n\nThe total number of jobs able to run at any given\n  time from an association and its children.  If\n  this limit is reached, new jobs will be queued but only allowed to\n  run after previous jobs complete from this group.\n\nGrpJobsAccrue\n\nThe total number of pending jobs able to accrue age\n  priority at any given time from an association and its children.  If\n  this limit is reached, new jobs will be queued but not accrue age priority\n  until after previous jobs are removed from pending in this group.\n  This limit does not determine if the job can run or not, it only limits the\n  age factor of the priority.\n\nGrpSubmitJobs\n\nThe total number of jobs able to be submitted\n  to the system at any given time from an association and its children.\n  If this limit is reached, new submission requests will be\n  denied until previous jobs complete from this group.\n\nGrpTRES\n\nThe total count of TRES able to be used at any given\n  time from jobs running from an association and its children. If\n  this limit is reached, new jobs will be queued but only allowed to\n  run after resources have been relinquished from this group.\n\nGrpTRESMins\n\nThe total number of TRES minutes that can\n  possibly be used by past, present and future jobs\n  running from an association and its children. If any limit is reached,\n  all running jobs with that TRES in this group will be killed, and no new\n  jobs will be allowed to run.  This usage is decayed (at a rate of\n  PriorityDecayHalfLife).  It can also be reset (according to\n  PriorityUsageResetPeriod) in order to allow jobs to run against the\n  association tree.\n  This limit only applies when using the Priority Multifactor plugin.\n\nGrpTRESRunMins\n\nUsed to limit the combined total number of TRES\n  minutes used by all jobs running with an association and its\n  children.  This takes into consideration time limit of\n  running jobs and consumes it. If the limit is reached, no new jobs\n  are started until other jobs finish to allow time to free up.\n\nGrpWall\n\nThe maximum wall clock time running jobs are able\n  to be allocated in aggregate for an association and its children.\n  If this limit is reached, future jobs in this association will be\n  queued until they are able to run inside the limit.\n  This usage is decayed (at a rate of\n  PriorityDecayHalfLife).  It can also be reset (according to\n  PriorityUsageResetPeriod) in order to allow jobs to run against the\n  association tree again.\n\nMaxJobs\n\nThe total number of jobs able to run at any given\n  time for the given association.  If this limit is reached, new jobs will\n  be queued but only allowed to run after existing jobs in the association\n  complete.\n\nMaxJobsAccrue\n\nThe maximum number of pending jobs able to accrue age\n  priority at any given time for the given association.  If this limit is\n  reached, new jobs will be queued but will not accrue age priority\n  until after existing jobs in the association are moved from a pending state.\n  This limit does not determine if the job can run, it only limits the\n  age factor of the priority.\n\nMaxSubmitJobs\n\nThe maximum number of jobs able to be submitted\n  to the system at any given time from the given association.  If\n  this limit is reached, new submission requests will be denied until\n  existing jobs in this association complete.\n\nMaxTRESMinsPerJob\n\nA limit of TRES minutes to be used by a job.\n  If this limit is reached, the job will be killed if not running in\n  Safe mode, otherwise the job will pend until enough time is given to\n  complete the job.\n\nMaxTRESPerJob\n\nThe maximum size in TRES any given job can\n  have from the association.\n\nMaxTRESPerNode\n\nThe maximum size in TRES each node in a job\n  allocation can use.\n\n\nMaxWallDurationPerJob\n\nThe maximum wall clock time any individual job\n  can run for in the given association.  If this limit is reached,\n  the job will be denied at submission.\n\nMinPrioThreshold\n\nMinimum priority required to reserve resources\n  in the given association. Used to override bf_min_prio_reserve.\n  See \n  bf_min_prio_reserve for details.\n\nQOS\n\ncomma separated list of QOSs an association is\n  able to run.\n\nNOTE: When modifying a TRES field with sacctmgr, one must\nspecify which TRES to modify (see TRES for complete\nlist) as in the following examples: \nSET:\nsacctmgr modify user bob set GrpTRES=cpu=1500,mem=200,gres/gpu=50\nUNSET:\nsacctmgr modify user bob set GrpTRES=cpu=-1,mem=-1,gres/gpu=-1\nQOS specific limits and scheduling policies\n\nAs noted above, the default behavior is that\na limit set on a Partition QOS will be applied before a limit on the job's\nrequested QOS. You can change this behavior with the OverPartQOS\nflag.Unless noted, if a job request breaches a given limit\non its own, the job will pend unless the job's QOS has the DenyOnLimit\nflag set, which will cause the job to be denied at submission.  When\nGrp limits are considered with respect to this flag the Grp limit\nis treated as a Max limit.\nGraceTime\n\nPreemption grace time to be extended to a job which\n  has been selected for preemption in the format of\n  <hh>:<mm>:<ss>. The default value is zero,\n  meaning no preemption grace time is allowed on this QOS. This value\n  is only meaningful for QOS PreemptMode=CANCEL and PreemptMode=REQUEUE.\n\nGrpJobs\n\nThe total number of jobs able to run at any given time\n  from a QOS. If this limit is reached, new jobs will be queued but only\n  allowed to run after previous jobs complete from this group.\n\nGrpJobsAccrue\n\nThe total number of pending jobs able to accrue age priority at any\n  given time from a QOS. If this limit is reached, new jobs will be queued but\n  will not accrue age based priority until after previous jobs are removed\n  from pending in this group. This limit does not determine if the job can\n  run or not, it only limits the age factor of the priority. This limit only\n  applies to the job's QOS and not the partition's QOS.\n\nGrpSubmitJobs\n\nThe total number of jobs able to be submitted to the system at any\n  given time from a QOS. If this limit is reached, new submission requests\n  will be denied until previous jobs complete from this group.\n\nGrpTRES\n\nThe total count of TRES able to be used at any given time from jobs\n  running from a QOS. If this limit is reached, new jobs will be queued but\n  only allowed to run after resources have been relinquished from this group.\n\nGrpTRESMins\n\nThe total number of TRES minutes that can possibly be used by past,\n  present and future jobs running from a QOS. If any limit is reached,\n  all running jobs with that TRES in this group will be killed, and no new\n  jobs will be allowed to run.  This usage is decayed (at a rate of\n  PriorityDecayHalfLife).  It can also be reset (according to\n  PriorityUsageResetPeriod) in order to allow jobs to run against the\n  QOS again.  QOS that have the NoDecay flag set do not decay GrpTRESMins,\n  see QOS Options for details.\n  This limit only applies when using the Priority Multifactor plugin.\n\nGrpTRESRunMins\n\nUsed to limit the combined total number of TRES\n  minutes used by all jobs running with a QOS.  This takes into\n  consideration the time limit of running jobs and consumes it.\n  If the limit is reached, no new jobs are started until other jobs\n  finish to allow time to free up.\n\nGrpWall\n\nThe maximum wall clock time running jobs are able\n  to be allocated in aggregate for a QOS. If this limit is reached,\n  future jobs in this QOS will be queued until they are able to run\n  inside the limit. This usage is decayed (at a rate of\n  PriorityDecayHalfLife).  It can also be reset (according to\n  PriorityUsageResetPeriod) in order to allow jobs to run against the\n  QOS again.  QOS that have the NoDecay flag set do not decay GrpWall.\n  See QOS Options for details.\n\nLimitFactor\n\nA float that is factored into an associations [Grp|Max]TRES limits.\n  For example, if the LimitFactor is 2, then an association with a GrpTRES of\n  30 CPUs would be allowed to allocate 60 CPUs when running under this QOS.\n\n  NOTE: This factor is only applied to associations running in this\n  QOS and is not applied to any limits in the QOS itself.\n\nMaxJobsAccruePerAccount\n\nThe maximum number of pending jobs an\n  account (or sub-account) can have accruing age priority at any given time.\n  This limit does not determine if the job can run, it only limits the\n  age factor of the priority.\n\nMaxJobsAccruePerUser\n\nThe maximum number of pending jobs a\n  user can have accruing age priority at any given time.\n  This limit does not determine if the job can run, it only limits the\n  age factor of the priority.\n\nMaxJobsPerAccount\n\nThe maximum number of jobs an account (or sub-account) can have running at\n  a given time.\n\nMaxJobsPerUser\n\nThe maximum number of jobs a user can\n  have running at a given time.\n\nMaxSubmitJobsPerAccount\n\nThe maximum number of jobs an account (or sub-account) can have running and\n  pending at a given time.\n\nMaxSubmitJobsPerUser\n\nThe maximum number of jobs a user can\n  have running and pending at a given time.\n\nMaxTRESMinsPerJob\n\nMaximum number of TRES minutes each job is able to use.\n\nMaxTRESPerAccount\n\nThe maximum number of TRES an account can\n  allocate at a given time.\n\nMaxTRESPerJob\n\nThe maximum number of TRES each job is able to use.\n\nMaxTRESPerNode\n\nThe maximum number of TRES each node in a job allocation can use.\n\nMaxTRESPerUser\n\nThe maximum number of TRES a user can\n  allocate at a given time.\n\nMaxWallDurationPerJob\n\nMaximum wall clock time each job is able to use. Format is <min>\n  or <min>:<sec> or <hr>:<min>:<sec> or\n  <days>-<hr>:<min>:<sec> or <days>-<hr>.\n  The value is recorded in minutes with rounding as needed.\n\nMinPrioThreshold\n\nMinimum priority required to reserve resources when scheduling.\n\nMinTRESPerJob\n\nThe minimum size in TRES any given job can\n  have when using the requested QOS.\n\nUsageFactor\n\nA float that is factored into a job's TRES usage (e.g. RawUsage,\n  TRESMins, TRESRunMins). For example, if the usagefactor was 2, for every\n  TRESBillingUnit second a job ran it would count for 2. If the usagefactor\n  was .5, every second would only count for half of the time.\n  A setting of 0 would add no timed usage from the job.\n\n  The usage factor only applies to the job's QOS and not the partition QOS.\n  \n  If the UsageFactorSafe flag is set and AccountingStorageEnforce includes\n  Safe, jobs will only be able to run if the job can run to completion\n  with the UsageFactor applied, and won't be killed due to limits.\n  \n  If the UsageFactorSafe flag is not set and AccountingStorageEnforce includes\n  Safe, a job will be able to be scheduled without the UsageFactor\n  applied and won't be killed due to limits.\n  \n  If the UsageFactorSafe flag is not set and AccountingStorageEnforce does\n  not include Safe, a job will be scheduled as long as the limits are\n  not reached, but could be killed due to limits.\n  \n  See \n  AccountingStorageEnforce in the slurm.conf man page.\n\nThe MaxNodes and MaxTime options already exist in\nSlurm's configuration on a per-partition basis, but the above options\nprovide the ability to impose limits on a per-user basis.  The\nMaxJobs option provides an entirely new mechanism for Slurm to\ncontrol the workload any individual may place on a cluster in order to\nachieve some balance between users.When assigning limits to a QOS to use for a Partition QOS,\nkeep in mind that those limits are enforced at the QOS level, not\nindividually for each partition.  For example, if a QOS has a\nGrpTRES=cpu=20 limit defined and the QOS is assigned to two\nunique partitions, users will be limited to 20 CPUs for the QOS\nrather than being allowed 20 CPUs for each partition.Fair-share scheduling is based upon the hierarchical bank account\ndata maintained in the Slurm database.  More information can be found\nin the priority/multifactor\nplugin description.Specific limits over GRES\n\n When a GRES has a type associated with it and a limit is applied\n  over this specific type (e.g. MaxTRESPerUser=gres/gpu:tesla=1) if a\n  user requests a generic gres, the type's limit will not be enforced. In this\n  situation an additional lua job submit plugin to check the user request may\n  become useful. For example, if one requests --gres=gpu:2 having a\n  limit set of MaxTRESPerUser=gres/gpu:tesla=1, the limit won't be\n  enforced so it will still be possible to get two teslas.\n\n  This is due to a design limitation. The only way to enforce such a limit\n  is to combine the specification of the limit with a job submit plugin that\n  forces the user to always request a specific type model.\n\n  An example of basic lua job submit plugin function could be:\n\nfunction slurm_job_submit(job_desc, part_list, submit_uid)\n   gres_request = \"\"\n   t = {job_desc.tres_per_job,\n\tjob_desc.tres_per_socket,\n\tjob_desc.tres_per_task,\n\tjob_desc.tres_per_node}\n   for k in pairs(t) do\n\tgres_request = gres_request .. t[k] .. \",\"\n   end\n   if (gres_request ~= nil)\n   then\n      for g in gres_request:gmatch(\"[^,]+\")\n      do\n\t bad = string.match(g,'^gres/gpu[:=]*[0-9]*$')\n\t if (bad ~= nil)\n\t then\n\t    slurm.log_info(\"User specified gpu GRES without type: %s\", bad)\n\t    slurm.user_msg(\"You must always specify a type when requesting gpu GRES\")\n\t    return slurm.ERROR\n\t end\n      end\n   end\nend\n Having this script and the limit in place will force the users to always\n  specify a gpu with its type, thus enforcing the limits for each specific\n  model.\nWhen TRESBillingWeights are defined for a partition, both typed and\nnon-typed resources should be included. For example, if you have 'tesla' GPUs\nin one partition and you only define the billing weights for the 'tesla' typed\nGPU resource, then those weights will not be applied to the generic GPUs.It is also advisable to set AccountingStorageTRES for both generic\n  and specific gres types, otherwise requests that ask for the generic instance\n  of a gres won't be accounted for. For example, to track generic GPUs and\n  Tesla GPUs, you would set this in your slurm.conf:\n\n  AccountingStorageTRES=gres/gpu,gres/gpu:tesla\n\n  See Trackable Resources TRES for details.\nJob Reason Codes\n\nWhen a pending job is evaluated by the scheduler but found to exceed a\nconfigured resource limit, a corresponding reason will be assigned to the job.\nMore details can be found on the Job Reason\nCodes page. More details about scheduling can be found in the\n Scheduling Configuration Guide.Last modified 27 September 2024\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/mail.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Mailing Lists",
                "content": "SchedMD maintains two Slurm mailing lists:\nslurm-announce@schedmd.com is designated for communications about\nSlurm releases. Only SchedMD may post to this list. Low traffic.\nslurm-users@schedmd.com (formerly slurm-dev@schedmd.com) is for\ndiscussion by the Slurm community, and is typically used by Slurm system\nadministrators for discussion and community-provided support. (Please see the\nSchedMD support page\npage for commercial support options.)\nAny person subscribed to this mailing list may post to it. High traffic.\nYou can subscribe to either list, change your subscription options, or\nunsubscribe through the\nMailman interface here.An archive of the slurm-users list is online:\nhttp://groups.google.com/group/slurm-usersLast modified 2 February 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/man_index.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Man Pages",
                "content": "NOTE: This documentation is for Slurm version 24.05.\nDocumentation for other versions of Slurm is distributed with the codeRefer to this page for an overview of Slurm.Commands\n\nsacct\nDisplays accounting data for all jobs and job steps in the Slurm job\n\taccounting log or Slurm database.\nsacctmgr\nUsed to view and modify Slurm account information.\nsalloc\nObtain a Slurm job allocation (a set of nodes), execute a command,\n\tand then release the allocation when the command is finished.\nsattach\nAttach to a Slurm job step.\nsbatch\nSubmit a batch script to Slurm.\nsbcast\nTransmit a file to the nodes allocated to a Slurm job.\nscancel\nUsed to signal jobs or job steps that are under the control of Slurm.\nscontrol\nView or modify Slurm configuration and state.\nscrontab\nManage Slurm crontab files.\nscrun\nAn OCI runtime proxy for slurm.\nsdiag\nScheduling diagnostic tool.\nsh5util\nMerge utility for acct_gather_profile plugin.\nsinfo\nView information about Slurm nodes and partitions.\nsprio\nView the factors that comprise a job's scheduling priority.\nsqueue\nView information about jobs located in the Slurm scheduling queue.\nsreport\nGenerate reports from the slurm accounting data.\nsrun\nRun parallel jobs.\nsshare\nTool for listing the shares of associations to a cluster.\nsstat\nDisplay the status information of a running job/step.\nstrigger\nUsed to set, get or clear Slurm trigger information.\nsview\nGraphical user interface to view and modify Slurm state.\nConfiguration Files\n\nacct_gather.conf\nSlurm configuration file for the acct_gather plugins.\nburst_buffer.conf\nSlurm burst buffer configuration.\ncgroup.conf\nSlurm configuration file for the cgroup support.\ngres.conf\nSlurm configuration file for generic resource management.\nhelpers.conf\nSlurm configuration file for the node_features/helpers plugin.\njob_container.conf\nSlurm configuration file for configuring the tmpfs job container\n\tplugin.\nknl.conf\nSlurm configuration file for Intel Knights Landing management.\nmpi.conf\nSlurm configuration file to allow the configuration of MPI plugins.\noci.conf\nSlurm configuration file for OCI Containers.\nplugstack.conf\nSlurm configuration file for SPANK plug-in stack.\nslurm.conf\nSlurm configuration file.\nslurmdbd.conf\nSlurm Database Daemon (SlurmDBD) configuration file.\ntopology.conf\nSlurm configuration file for defining the network topology.\nDaemons and Other\n\nsackd\nSlurm Auth and Cred Kiosk Daemon.\nslurmctld\nThe central management daemon of Slurm.\nslurmd\nThe compute node daemon for Slurm.\nslurmdbd\nSlurm Database Daemon.\nslurmrestd\nThe Slurm REST API daemon.\nslurmstepd\nThe job step manager for Slurm.\nSPANK\nSlurm Plug-in Architecture for Node and job (K)control.\nLast modified 10 July 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/prolog_epilog.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Prolog and Epilog Guide",
                "content": "Slurm supports a multitude of prolog and epilog programs.\nNote that for security reasons, these programs do not have a search path set.\nEither specify fully qualified path names in the program or set the\nPATH\nenvironment variable.\nThe first table below identifies what prologs and epilogs are available for job\nallocations, when and where they run.\n\n\n\n\n\n\n\n\nParameter\n\t\t\t\t\n\n\nLocation\n\t\t\t\t\n\n\nInvoked by\n\t\t\t\t\n\n\nUser\n\t\t\t\t\n\n\nWhen executed\n\t\t\t\t\n\n\n\n\n\n\t\t\t\tProlog (from slurm.conf)\n\n\n\n\t\t\t\tCompute or front end node\n\n\n\n\t\t\t\tslurmd daemon\n\n\n\n\t\t\t\tSlurmdUser (normally user root)\n\n\n\n\t\t\t\tFirst job or job step initiation on that node (by default);\n\t\t\t\tPrologFlags=Alloc will force the script to be executed at\n\t\t\t\tjob allocation\n\n\n\n\n\n\t\t\t\tPrologSlurmctld (from slurm.conf)\n\n\n\n\t\t\t\tHead node (where slurmctld daemon runs)\n\n\n\n\t\t\t\tslurmctld daemon\n\n\n\n\t\t\t\tSlurmctldUser\n\n\n\n\t\t\t\tAt job allocation\n\n\n\n\n\n\t\t\t\tEpilog (from slurm.conf)\n\n\n\n\t\t\t\tCompute or front end node\n\n\n\n\t\t\t\tslurmd daemon\n\n\n\n\t\t\t\tSlurmdUser (normally user root)\n\n\n\n\t\t\t\tAt job termination\n\n\n\n\n\n\t\t\t\tEpilogSlurmctld (from slurm.conf)\n\n\n\n\t\t\t\tHead node (where slurmctld daemon runs)\n\n\n\n\t\t\t\tslurmctld daemon\n\n\n\n\t\t\t\tSlurmctldUser\n\n\n\n\t\t\t\tAt job termination\n\n\n\nThis second table below identifies what prologs and epilogs are available for job\nstep allocations, when and where they run.\n\n\n\n\n\n\n\n\nParameter\n\t\t\t\t\n\n\nLocation\n\t\t\t\t\n\n\nInvoked by\n\t\t\t\t\n\n\nUser\n\t\t\t\t\n\n\nWhen executed\n\t\t\t\t\n\n\n\n\n\n\t\t\t\tSrunProlog (from slurm.conf) or srun --prolog\n\n\n\n\t\t\t\tsrun invocation node\n\n\n\n\t\t\t\tsrun command\n\n\n\n\t\t\t\tUser invoking srun command\n\n\n\n\t\t\t\tPrior to launching job step\n\n\n\n\n\n\t\t\t\tTaskProlog (from slurm.conf)\n\n\n\n\t\t\t\tCompute node\n\n\n\n\t\t\t\tslurmstepd daemon\n\n\n\n\t\t\t\tUser invoking srun command\n\n\n\n\t\t\t\tPrior to launching job step\n\n\n\n\n\n\t\t\t\tsrun --task-prolog\n\n\n\n\t\t\t\tCompute node\n\n\n\n\t\t\t\tslurmstepd daemon\n\n\n\n\t\t\t\tUser invoking srun command\n\n\n\n\t\t\t\tPrior to launching job step\n\n\n\n\n\n\t\t\t\tTaskEpilog (from slurm.conf)\n\n\n\n\t\t\t\tCompute node\n\n\n\n\t\t\t\tslurmstepd daemon\n\n\n\n\t\t\t\tUser invoking srun command\n\n\n\n\t\t\t\tCompletion job step\n\n\n\n\n\n\t\t\t\tsrun --task-epilog\n\n\n\n\t\t\t\tCompute node\n\n\n\n\t\t\t\tslurmstepd daemon\n\n\n\n\t\t\t\tUser invoking srun command\n\n\n\n\t\t\t\tCompletion job step\n\n\n\n\n\n\t\t\t\tSrunEpilog (from slurm.conf) or srun --epilog\n\n\n\n\t\t\t\tsrun invocation node\n\n\n\n\t\t\t\tsrun command\n\n\n\n\t\t\t\tUser invoking srun command\n\n\n\n\t\t\t\tCompletion job step\n\n\n\nBy default the Prolog script is only run on any individual\nnode when it first sees a job step from a new allocation; it does not\nrun the Prolog immediately when an allocation is granted.  If no job steps\nfrom an allocation are run on a node, it will never run the Prolog for that\nallocation.  This Prolog behavior can be changed by the\nPrologFlags parameter.  The Epilog, on the other hand, always\nruns on every node of an allocation when the allocation is released.If multiple prolog and/or epilog scripts are specified,\n(e.g. \"/etc/slurm/prolog.d/*\") they will run in reverse alphabetical order\n(z-a -> Z-A -> 9-0).Prolog and Epilog scripts should be designed to be as short as possible\nand should not call Slurm commands (e.g. squeue, scontrol, sacctmgr, etc).\nLong running scripts can cause scheduling problems when jobs take a long time\nto start or finish. Slurm commands in these scripts can potentially lead to\nperformance issues and should not be used.The task prolog is executed with the same environment as the user tasks to\nbe initiated. The standard output of that program is read and processed as\nfollows:\nexport name=value\nsets an environment variable for the user task\nunset name\nclears an environment variable from the user task\nprint ...\nwrites to the task's standard output.A TaskProlog script can just be a bash script. Here is a very basic example:\n\n#!/bin/bash\n\n# The TaskProlog script can be used for any preliminary work needed\n# before running a job step, and it can also be used to modify the\n# user's environment. There are two main mechanisms for that, which\n# rely on printing commands to stdout:\n\n# Make a variable available for the user\necho \"export VARIABLE_1=HelloWorld\"\n\n# Unset variables for the user\necho \"unset MANPATH\"\n\n# We can also print messages if needed\necho \"print This message has been printed with TaskProlog\"\nSpecial treatment is given to the SLURM_PROLOG_CPU_MASK variable when\nset in the task prolog. The variable is interpreted as a coma separated list\nof hex maps. It allows you to specify the CPU(s) that will be bound to a\ntask and is applied using sched_setaffinity.\nThe above functionality is limited to the task prolog script.Unless otherwise specified, these environment variables are available\nto all of the programs.\nCUDA_MPS_ACTIVE_THREAD_PERCENTAGE\nSpecifies the percentage of a GPU that should be allocated to the job.\nThe value is set only if the gres/mps plugin is configured and the job\nrequests those resources.\nAvailable in Prolog and Epilog only.\nCUDA_VISIBLE_DEVICES\nSpecifies the GPU devices for the job allocation.\nThe value is set only if the gres/gpu or gres/mps plugin is configured and the\njob requests those resources.\nNote that the environment variable set for the job may differ from that set for\nthe Prolog and Epilog if Slurm is configured to constrain the device files\nvisible to a job using Linux cgroup.\nThis is because the Prolog and Epilog programs run outside of any Linux\ncgroup while the job runs inside of the cgroup and may thus have a\ndifferent set of visible devices.\nFor example, if a job is allocated the device \"/dev/nvidia1\", then\nCUDA_VISIBLE_DEVICES will be set to a value of\n\"1\" in the Prolog and Epilog while the job's value of\nCUDA_VISIBLE_DEVICES will be set to a\nvalue of \"0\" (i.e. the first GPU device visible to the job).\nCUDA_VISIBLE_DEVICES will be set unless\notherwise excluded via the Flags or AutoDetect options in\ngres.conf. See also SLURM_JOB_GPUS.\nAvailable in Prolog and Epilog only.\nGPU_DEVICE_ORDINAL\nSpecifies the GPU devices for the job allocation. The considerations for\nCUDA_VISIBLE_DEVICES also apply to\nGPU_DEVICE_ORDINAL.\n\nROCR_VISIBLE_DEVICES\nSpecifies the GPU devices for the job allocation. The considerations for\nCUDA_VISIBLE_DEVICES also apply to\nROCR_VISIBLE_DEVICES.\n\nSLURM_ARRAY_JOB_ID\nIf this job is part of a job array, this will be set to the job ID.\nOtherwise it will not be set.\nTo reference this specific task of a job array, combine\nSLURM_ARRAY_JOB_ID with\nSLURM_ARRAY_TASK_ID\n(e.g. scontrol update\n${SLURM_ARRAY_JOB_ID}_{$SLURM_ARRAY_TASK_ID} ...);\nAvailable in PrologSlurmctld, SrunProlog, TaskProlog, EpilogSlurmctld,\nSrunEpilog and TaskEpilog.\nSLURM_ARRAY_TASK_COUNT\nIf this job is part of a job array, this will be set to the number of\ntasks in the array. Otherwise it will not be set.\nAvailable in PrologSlurmctld, SrunProlog, TaskProlog, EpilogSlurmctld,\nSrunEpilog and TaskEpilog.\nSLURM_ARRAY_TASK_ID\nIf this job is part of a job array, this will be set to the task ID.\nOtherwise it will not be set.\nTo reference this specific task of a job array, combine\nSLURM_ARRAY_JOB_ID with\nSLURM_ARRAY_TASK_ID\n(e.g. scontrol update\n${SLURM_ARRAY_JOB_ID}_{$SLURM_ARRAY_TASK_ID} ...);\nAvailable in PrologSlurmctld, SrunProlog, TaskProlog, EpilogSlurmctld,\nSrunEpilog and TaskEpilog.\nSLURM_ARRAY_TASK_MAX\nIf this job is part of a job array, this will be set to the maximum\ntask ID.\nOtherwise it will not be set.\nAvailable in PrologSlurmctld, SrunProlog, TaskProlog, EpilogSlurmctld,\nSrunEpilog and TaskEpilog.\nSLURM_ARRAY_TASK_MIN\nIf this job is part of a job array, this will be set to the minimum\ntask ID.\nOtherwise it will not be set.\nAvailable in PrologSlurmctld, SrunProlog, TaskProlog, EpilogSlurmctld,\nSrunEpilog and TaskEpilog.\nSLURM_ARRAY_TASK_STEP\nIf this job is part of a job array, this will be set to the step\nsize of task IDs.\nOtherwise it will not be set.\nAvailable in PrologSlurmctld, SrunProlog, TaskProlog, EpilogSlurmctld,\nSrunEpilog and TaskEpilog.\nSLURM_CLUSTER_NAME\nName of the cluster executing the job. Available in Prolog, PrologSlurmctld,\nEpilog and EpilogSlurmctld.\nSLURM_CONF\nLocation of the slurm.conf file. Available in Prolog, SrunProlog, TaskProlog,\nEpilog, SrunEpilog and TaskEpilog.\nSLURM_CPUS_ON_NODE\nCount of processors available to the job on current node. Available in\nSrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_DISTRIBUTION\nDistribution type for the job. Available in SrunProlog, TaskProlog, SrunEpilog\nand TaskEpilog.\nSLURMD_NODENAME\nName of the node running the task. In the case of a parallel job executing\non multiple compute nodes, the various tasks will have this environment\nvariable set to different values on each compute node. Available in Prolog,\nSrunProlog, TaskProlog, Epilog, SrunEpilog and TaskEpilog.\nSLURM_GPUS\nCount of the GPUs available to the job. Available in SrunProlog, TaskProlog,\nSrunEpilog and TaskEpilog.\nSLURM_GTID\nGlobal Task IDs running on this node. Zero origin and comma separated.\nAvailable in SrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_JOB_ACCOUNT\nAccount name used for the job.\nSLURM_JOB_COMMENT\nComment added to the job.\nAvailable in Prolog, PrologSlurmctld, Epilog and EpilogSlurmctld.\nSLURM_JOB_CONSTRAINTS\nFeatures required to run the job.\nAvailable in Prolog, PrologSlurmctld, Epilog and EpilogSlurmctld.\nSLURM_JOB_CPUS_PER_NODE\nCount of processors available per node.\nSLURM_JOB_DERIVED_EC\nThe highest exit code of all of the job steps.\nAvailable in Epilog and EpilogSlurmctld.\nSLURM_JOB_END_TIME\nThe UNIX timestamp for a job's end time.\nSLURM_JOB_EXIT_CODE\nThe exit code of the job script (or salloc). The value is the status\nas returned by the wait() system call\n(See wait(2)).\nAvailable in Epilog and EpilogSlurmctld.\nSLURM_JOB_EXIT_CODE2\nThe exit code of the job script (or salloc). The value has the format\n<exit>:<sig>.\nThe first number is the exit code, typically as set by the\nexit() function.\nThe second number is the signal that caused the process to\nterminate if it was terminated by a signal.\nAvailable in Epilog and EpilogSlurmctld.\nSLURM_JOB_EXTRA\nExtra field added to the job.\nAvailable in Prolog, PrologSlurmctld, Epilog, EpilogSlurmctld, and\nResumeProgram (via SLURM_RESUME_FILE).\nSLURM_JOB_GID\nGroup ID of the job's owner.\nSLURM_JOB_GPUS\nThe GPU IDs of GPUs in the job allocation (if any).\nAvailable in the Prolog, SrunProlog, TaskProlog, Epilog, SrunEpilog and\nTaskProlog.\nSLURM_JOB_GROUP\nGroup name of the job's owner.\nAvailable in PrologSlurmctld and EpilogSlurmctld.\nSLURM_JOB_ID\nJob ID.\nSLURM_JOBID\nJob ID.\nSLURM_JOB_LICENSES\nName and count of any license(s) requested.\nSLURM_JOB_NAME\nName of the job.\nAvailable in PrologSlurmctld, SrunProlog, TaskProlog, EpilogSlurmctld,\nSrunEpilog and TaskEpilog.\nSLURM_JOB_NODELIST\nNodes assigned to job. A Slurm hostlist expression.\nscontrol show hostnames\ncan be used to convert this to a\nlist of individual host names.\nSLURM_NTASKS\nNumber of tasks requested by the job.\nAvailable in SrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_JOB_NUM_NODES\nNumber of nodes assigned to a job.\nSLURM_JOB_OVERSUBSCRIBE\nJob OverSubscribe status.\nSee the squeue man page for\npossible values.\nAvailable in Prolog, PrologSlurmctld, Epilog and EpilogSlurmctld.\nSLURM_JOB_PARTITION\nPartition that job runs in.\nSLURM_JOB_QOS\nQOS assigned to job. Available in SrunProlog, TaskProlog, SrunEpilog\nand TaskEpilog.\nSLURM_JOB_RESERVATION\nReservation requested for the job.\nSLURM_JOB_RESTART_COUNT\nNumber of times the job has been restarted.\nAvailable in Prolog, PrologSlurmctld, Epilog and EpilogSlurmctld.\nSLURM_JOB_START_TIME\nThe UNIX timestamp of a job's start time.\nSLURM_JOB_STDERR\nJob's stderr path.\nAvailable in Prolog, PrologSlurmctld, Epilog and EpilogSlurmctld.\nSLURM_JOB_STDIN\nJob's stdin path.\nAvailable in Prolog, PrologSlurmctld, Epilog and EpilogSlurmctld.\nSLURM_JOB_STDOUT\nJob's stdout path.\nAvailable in Prolog, PrologSlurmctld, Epilog and EpilogSlurmctld.\nSLURM_JOB_UID\nUser ID of the job's owner.\nSLURM_JOB_USER\nUser name of the job's owner.\nSLURM_JOB_WORK_DIR\nJob's working directory. Available in Prolog, PrologSlurmctld, Epilog,\nEpilogSlurmctld.\nSLURM_LOCAL_GLOBALS_FILE\nGlobals file used to set up the environment for the testsuite. Available\nin SrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_LOCALID\nNode local task ID for the process within a job. Available in SrunProlog,\nTaskProlog, SrunEpilog and TaskEpilog.\nSLURM_NNODES\nNumber of nodes assigned to a job. Available in SrunProlog, TaskProlog,\nSrunEpilog and TaskEpilog.\nSLURM_NODEID\nID of current node relative to other nodes in a multi-node job. Available in\nSrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_PRIO_PROCESS\nScheduling priority (nice value) at the time of submission. Available in\nSrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_PROCID\nThe MPI rank (or relative process ID) of the current process. Available in\nSrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_RESTART_COUNT\nNumber of times the job has been restarted. This is only set if the job\nhas been restarted at least once. Available in SrunProlog, TaskProlog,\nSrunEpilog and TaskEpilog.\nSLURM_RLIMIT_AS\nResource limit on the job's address space. Available in SrunProlog,\nTaskProlog, SrunEpilog and TaskEpilog.\nSLURM_RLIMIT_CORE\nResource limit on the size of a core file the job is able to produce.\nAvailable in SrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_RLIMIT_CPU\nResource limit on the amount of CPU time a job is able to use. Available\nin SrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_RLIMIT_DATA\nResource limit on the size of a job's data segment. Available in SrunProlog,\nTaskProlog, SrunEpilog and TaskEpilog.\nSLURM_RLIMIT_FSIZE\nResource limit on the maximum size of files a job may create. Available in\nSrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_RLIMIT_MEMLOCK\nResource limit on the bytes of data that may be locked into RAM. Available\nin SrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_RLIMIT_NOFILE\nResource limit on the number of file descriptors that can be opened by the\njob. Available in SrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_RLIMIT_NPROC\nResource limit on the number of processes that can be opened by the calling\nprocess. Available in SrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_RLIMIT_RSS\nResource limit on the job's resident set size. Available in SrunProlog,\nTaskProlog, SrunEpilog and TaskEpilog.\nSLURM_RLIMIT_STACK\nResource limit on the job's process stack. Available in SrunProlog,\nTaskProlog, SrunEpilog and TaskEpilog.\nSLURM_SCRIPT_CONTEXT\nIdentifies which epilog or prolog program is currently running.\nThe value is one of the following:\n\nprolog_slurmctld\nepilog_slurmctld\nprolog_slurmd\nepilog_slurmd\nprolog_task\nepilog_task\nprolog_srun\nepilog_srun\n\n\nSLURM_STEP_ID\nStep ID of the current job. Available in SrunProlog, TaskProlog, SrunEpilog\nand TaskEpilog.\nSLURM_STEPID\nStep ID of the current job. Available in SrunProlog and SrunEpilog.\nSLURM_SUBMIT_DIR\nDirectory from which the job was submitted or, if applicable, the directory\nspecified by the -D, --chdir option. Available in SrunProlog,\nTaskprolog, SrunEpilog and TaskEpilog.\nSLURM_SUBMIT_HOST\nHost from which the job was submitted. Available in SrunProlog, TaskProlog,\nSrunEpilog and TaskEpilog.\nSLURM_TASK_PID\nProcess ID of the process started for the task. Available in SrunProlog,\nTaskProlog, SrunEpilog and TaskEpilog.\nSLURM_TASKS_PER_NODE\nNumber of tasks per node. Available in SrunProlog, TaskProlog, SrunEpilog\nand TaskEpilog.\nSLURM_TOPOLOGY_ADDR\nSet to the names of network switches or nodes that may be involved in the\njob's communications. Starts with the top level switch down to the node name.\nA period is used to separate each hardware component name. Available in\nSrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_TOPOLOGY_ADDR_PATTERN\nSet to the network component types that corresponds with the list of names\nfrom SLURM_TOPOLOGY_ADDR. Each component will be identified as either\nswitch or node. A period is used to separate each component\ntype. Available in SrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_WCKEY\nUser name of the job's wckey (if any).\nAvailable in PrologSlurmctld and EpilogSlurmctld only.\nPlugin functions may also be useful to execute logic at various well-defined\npoints.\nSPANK is another mechanism that may be useful\nto invoke logic in the user commands, slurmd daemon, and slurmstepd daemon.\nFailure Handling\n\n\nIf the Epilog fails (returns a non-zero exit code), this will result in the\nnode being set to a DRAIN state.\nIf the EpilogSlurmctld fails (returns a non-zero exit code), this will only\nbe logged.\nIf the Prolog fails (returns a non-zero exit code), this will result in the\nnode being set to a DRAIN state and the job requeued. The job will be placed\nin a held state unless nohold_on_prolog_fail is configured in\nSchedulerParameters.\nIf the PrologSlurmctld fails (returns a non-zero exit code), this will cause\nthe job to be requeued. Only batch jobs can be requeued. Interactive jobs\n(salloc and srun) will be cancelled if the PrologSlurmctld fails.\nIf a task epilog or srun epilog fails (returns a non-zero exit code) this\nwill only be logged.\nIf a task prolog fails (returns a non-zero exit code), the task will be\ncanceled.\nIf the srun prolog fails (returns a non-zero exit code), the step will be\ncanceled.\n\nBased upon work by Jason Sollom, Cray Inc. and used by permission.\nLast modified 23 September 2024\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Failure Handling\n\n",
                "content": "If the Epilog fails (returns a non-zero exit code), this will result in the\nnode being set to a DRAIN state.\nIf the EpilogSlurmctld fails (returns a non-zero exit code), this will only\nbe logged.\nIf the Prolog fails (returns a non-zero exit code), this will result in the\nnode being set to a DRAIN state and the job requeued. The job will be placed\nin a held state unless nohold_on_prolog_fail is configured in\nSchedulerParameters.\nIf the PrologSlurmctld fails (returns a non-zero exit code), this will cause\nthe job to be requeued. Only batch jobs can be requeued. Interactive jobs\n(salloc and srun) will be cancelled if the PrologSlurmctld fails.If a task epilog or srun epilog fails (returns a non-zero exit code) this\nwill only be logged.\nIf a task prolog fails (returns a non-zero exit code), the task will be\ncanceled.\nIf the srun prolog fails (returns a non-zero exit code), the step will be\ncanceled.Based upon work by Jason Sollom, Cray Inc. and used by permission.Last modified 23 September 2024"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/cgroups.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Control Group in Slurm",
                "content": "Contents\n\n\nControl Group Overview\nSlurm cgroup plugins design\nUse of cgroup in Slurm\nSlurm Cgroup Configuration Overview\nCurrently Available Cgroup Plugins\n\nproctrack/cgroup plugin\ntask/cgroup plugin\njobacct_gather/cgroup plugin\n\n\nUse of cgroup for Resource Specialization\nSlurm cgroup plugins\n\nMain differences between cgroup/v1 and cgroup/v2\nMain differences between controller interfaces\nOther generalities\n\n\nControl Group Overview\n\nControl Group is a mechanism provided by the kernel to organize processes\nhierarchically and distribute system resources along the hierarchy in a\ncontrolled and configurable manner. Slurm can make use of cgroups to constrain\ndifferent resources to jobs, steps and tasks, and to get accounting about these\nresources.A cgroup provides different controllers (formerly \"subsystems\") for different\nresources. Slurm plugins can use several of these controllers, e.g.: memory,\ncpu, devices, freezer, cpuset, cpuacct. Each enabled controller\ngives the ability to constrain resources to a set of processes. If one\ncontroller is not available on the system, then Slurm cannot constrain the\nassociated resources through a cgroup.\"cgroup\" stands for \"control group\" and is never capitalized. The singular\nform is used to designate the whole feature and also as a qualifier as in\n\"cgroup controllers\". When explicitly referring to multiple individual control\ngroups, the plural form \"cgroups\" is used.Slurm supports two cgroup modes, Legacy mode (cgroup v1) and Unified Mode\n(cgroup v2). Hybrid mode where controllers from both version 1 and version 2 are\nmixed in a system is not supported.See the kernel.org documentation for a more comprehensive description of\ncgroup:\n\nKernel's Cgroup v1 documentation\n\n\nKernel's Cgroup v2 documentation \n\nSlurm cgroup plugins design\n\n\ncgroup/v1 plugin documentation (not available yet)\ncgroup/v2 plugin documentation \nUse of cgroup in Slurm  Slurm provides cgroup versions of a number of plugins.\nproctrack/cgroup (for process tracking and management)\ntask/cgroup (for constraining resources at step and task level)\njobacct_gather/cgroup (for gathering statistics)\ncgroups can also be used for resource specialization (constraining daemons to\ncores or memory).Slurm Cgroup Configuration Overview\n\nThere are several sets of configuration options for Slurm cgroups:\nslurm.conf provides options to enable the\ncgroup plugins. Each plugin may be enabled or disabled independently of the\nothers.\n\ncgroup.conf provides general options that are\ncommon to all cgroup plugins, plus additional options that apply only to\nspecific plugins.\n\nSystem-level resource specialization is enabled using node configuration\nparameters.\n\nCurrently Available Cgroup Plugins\n\nproctrack/cgroup plugin\n\nThe proctrack/cgroup plugin is an alternative to other proctrack plugins such\nas proctrack/linux for process tracking and suspend/resume capability.\n\nproctrack/cgroup uses the freezer controller to keep track of all pids of a\njob. It basically stores the pids in a specific hierarchy in the cgroup tree and\ntakes cares of signaling these pids when instructed. For example, if a user\ndecides to cancel a job, Slurm will execute this order internally by calling the\nproctrack plugin and asking it to send a SIGTERM to the job. Since proctrack\nmaintains a hierarchy of all Slurm-related pids in cgroup, it will easily know\nwhich ones will need to be signaled.\n\nProctrack can also respond to queries for getting a list of all the pids of a\njob or a step.\n\nAlternatively, when using proctrack/linux, pids are stored by cgroup in a\nsingle file (cgroup.procs) which is read by the plugin to get all the pids of a\npart of the hierarchy. For example, when using proctrack/cgroup, a single step\nhas its own cgroup.procs file, so getting the pids of the step is instantaneous.\nIn proctrack/linux, we need to read recursively /proc to get all the descendants\nof a parent pid.\nTo enable this plugin, configure the following option in slurm.conf:\nProctrackType=proctrack/cgroup\nThere are no specific options for this plugin in cgroup.conf, but the general\noptions apply. See the cgroup.conf man page for\ndetails.task/cgroup pluginThe task/cgroup plugin allows constraining resources to a job, a step, or a\ntask. This is the only plugin that can ensure that the boundaries of an\nallocation are not violated.\nOnly jobacctgather/linux offers a very simplistic mechanism for\nconstraining memory to a job but it is not reliable (there's a window of time\nwhere jobs can exceed its limits) and only for very rare systems where cgroup is\nnot available.task/cgroup provides the following features:\nConfine jobs and steps to their allocated cpuset.\nConfine jobs and steps to specific memory resources.\nConfine jobs, steps and tasks to their allocated gres, including gpus.\nThe task/cgroup plugin uses the cpuset, memory and devices subsystems.To enable this plugin, add task/cgroup to the TaskPlugin configuration\nparameter in slurm.conf:TaskPlugin=task/cgroupThere are many specific options for this plugin in cgroup.conf. The general\noptions also apply. See the cgroup.conf man page\nfor details.This plugin can be stacked with other task plugins, for example with\ntask/affinity. This will allow it to constrain resources to a job plus\ngetting the advantage of the affinity plugin (order doesn't matter):TaskPlugin=task/cgroup,task/affinityjobacct_gather/cgroup plugin\n\n\nThe jobacct_gather/cgroup plugin is an alternative to the\njobacct_gather/linux plugin for the collection of accounting statistics\nfor jobs, steps and tasks.\n\njobacct_gather/cgroup uses the cpuacct and memory cgroup controllers.\nThe cpu and memory statistics collected by this plugin do not represent the\nsame resources as the cpu and memory statistics collected by the\njobacct_gather/linux. While the cgroup plugin just reads a cgroup.stats\nfile and similar containing the information for the entire subtree of pids, the\nlinux plugin gets information from /proc/pid/stat for every pid and then does\nthe calculations, thus becoming a bit less efficient (thought not noticeable in\nthe practice) than the cgroup one.To enable this plugin, configure the following option in slurm.conf:\nJobacctGatherType=jobacct_gather/cgroup\nThere are no specific options for this plugin in cgroup.conf, but the general\noptions apply. See the cgroup.conf man page for\ndetails.Use of cgroup for Resource Specialization\n\nResource Specialization may be used to reserve a subset of cores or a\nspecific amount of memory on each compute node for exclusive use by the Slurm\ncompute node daemon, slurmd.Slurmstepd is not constrained by this resource specialization, since it is\nconsidered part of the job and its consumption is completely dependent on the\ntypology of the job. For example an MPI job can initialize many ranks with PMI\nand make slurmstepd consume more memory.System-level resource specialization is enabled with special node\nconfiguration parameters. Read slurm.conf and core\nspecialization in core_spec.html for more\ninformation.Slurm cgroup plugins\n\n\nSince 22.05, Slurm supports cgroup/v1 and cgroup/v2. Both plugins have very\ndifferent ways of organizing their hierarchies and respond to different design\nconstraints. The design is the responsibility of the kernel maintainers.\nMain differences between cgroup/v1 and cgroup/v2\n\nThe three main differences between v1 and v2 are:\nUnified mode in v2\nIn cgroup/v1 there's a separate hierarchy for each controller, which\nmeans the job structure must be replicated and managed for every enabled\ncontroller. For example, for the same job, if using\nmemory and freezer controllers, we will need to create the same\nslurm/uid/job_id/step_id/ hierarchy in both controller's directories. For\nexample:\n\n/sys/fs/cgroup/memory/slurm/uid_1000/job_1/step_0/\n/sys/fs/cgroup/freezer/slurm/uid_1000/job_1/step_0/\nIn cgroup/v2 we have a Unified hierarchy, where controllers are\nenabled at the same level and presented to the user as different files.\n/sys/fs/cgroup/system.slice/slurmstepd.scope/job_1/step_0/\n\nTop-down constraint in v2\nResources are distributed top-down and a cgroup can further distribute a\nresource only if the resource has been distributed to it from the parent.\nEnabled controllers are listed in the cgroup.controllers file and\nenabled controllers in a subtree are listed in cgroup.subtree_control.\n\nNo-Internal-Process constraint in v2\nIn cgroup/v1 the hierarchy is free, which means one can create any\ndirectory in the tree and put pids in it. In cgroup/v2 there's a kernel\nrestriction which impedes adding a pid to non-leaf directories.\n\nSystemd dependency on cgroup/v2 - separation of slurmd and stepds\n This is not a kernel limitation but a systemd decision, which imposes an\nimportant restriction on services that decide to use Delegate=yes.\nSystemd, with pid 1, decided to be the complete owner of the cgroup\nhierarchy, /sys/fs/cgroup, trying to impose a single-writer\ndesign. This means that everything related to cgroup must be under control of\nsystemd. If one decides to manually modify the cgroup tree, creating directories\nand moving pids around, it is possible that at some point systemd may decide to\nenable or disable controllers on the entire tree, or move pids around. It's been\nexperienced that a\n\nsystemd reload\n\nor a\n\nsystemd reset-failed\n\nremoved controllers, at any level and directory of the tree, if there was not\nany \"systemd unit\" making use of it and there were not any \"Delegate=Yes\"\nstarted \"systemd unit\" on the system. This is because systemd wants to cleanup\nthe cgroup tree and match it against its internal unit database. In fact,\nlooking at the code of systemd one can see how cgroup directories related to\nunits with \"Delegate=yes\" flag are ignored, while any other cgroup directories\nare modified.  This makes it mandatory to start slurmd and slurmstepd processes\nunder a unit with \"Delegate=yes\". This means we need to start, stop and restart\nslurmd with systemd. If we do that though, since we may have previously modified\nthe tree where slurmd belongs (e.g. adding job directories) systemd will not be\nable to restart slurmd because of the Top-down constraint mentioned\nearlier. It will not be able to put the new slurmd pid into the root cgroup\nwhich is now a non-leaf. This forces us to separate the cgroup hierarchies of\nslurmstepd from the slurmd ones, and since we need to inform systemd about it\nand put slurmstepd into a new unit, we will do a dbus call to systemd to create\na new scope for slurmstepds. See\n\nsystemd ControlGroupInterface for more information.\n\nThe following differences shouldn't affect how other plugins interact with\ncgroup plugins, but instead they only show internal functional differences.\nA controller in cgroup/v2 is enabled by writing to\ncgroup.controllers, while in cgroup/v1 a new mount point must be\nmounted with filesystem type \"-t cgroup\" and corresponding options,\ne.g.\"-o freezer\".\n\nIn cgroup/v2 the freezer controller is inherently present in the\ncgroup.freeze interface. In cgroup/v1 it is a specific and\nseparate controller which needs to be mounted.\n\nThe devices controller does not exist in cgroup/v2, instead a new eBPF\nprogram must be inserted in the kernel.\n\nIn cgroup/v2, memory.stat file has changed and now we do the sum of\nanon+swapcached+anon_thp to match the RSS concept in v1.\n\nIn cgroup/v2, cpu.stat provides metrics in milis while puacct.stat\nin cgroup/v1 provides metrics in USER_HZ.\n\nMain differences between controller interfaces\n\n\n\ncgroup/v1\ncgroup/v2\n\n\nmemory.limit_in_bytes\nmemory.max\n\n\nmemory.soft_limit_in_bytes\nmemory.high\n\n\nmemory.memsw_limit_in_bytes\nmemory.swap.max\n\n\nmemory.swappiness\nnone\n\n\nfreezer.state\ncgroup.freeze\n\n\ncpuset.cpus\ncpuset.cpus.effective and cpuset.cpus\n\n\ncpuset.mems\ncpuset.mems.effective and cpuset.mems\n\n\ncpuacct.stat\ncpu.stat\n\n\ndevice.*\nebpf program\n\nOther generalities\n\n\nWhen using cgroup/v1, some configurations can exclude the swap cgroup\naccounting. This accounting is part of the features provided by the memory\ncontroller.  If this feature is disabled from the kernel or boot parameters,\ntrying to enable swap constraints will produce an error. If this is required,\nadd the following parameters to the kernel command line:\n\ncgroup_enable=memory swapaccount=1\n\nThis can usually be placed in /etc/default/grub inside\nthe GRUB_CMDLINE_LINUX variable. A command such as update-grub\nmust be run after updating the file. This feature can be disabled also at kernel\nconfig with the parameter:\n\nCONFIG_MEMCG_SWAP=\nIn some Linux distributions, it was possible to use the systemd parameter\nJoinControllers, which is actually deprecated. This parameter allowed multiple\ncontrollers to be mounted in a single hierarchy in cgroup/v1, more or\nless trying to emulate the behavior of cgroup/v2 in \"Unified\" mode.\nHowever, Slurm does not work correctly with this configuration, so please make\nsure your system.conf does not use JoinControllers and that all your cgroup\ncontrollers are under separate directories when using\ncgroup/v1 legacy mode.\n\nLast modified 11 October 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/add.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Adding Files or Plugins to Slurm",
                "content": "Adding a File to SlurmThis is the procedure to follow in order to add a new C file to the Slurm\ncode base.  We recommend using a git branch for this purpose.\nAdd your new file to the git repository.\nModify the \"Makefile.am\" file in the file's parent directory.\nExecute \"autoreconf\" in Slurm's top level directory.\nIf you have older versions of the autoconf, automake, libtool or aclocal then\nyou may need to manually modify the Makefile.in file in the file's parent\ndirectory. If you have different versions of the files than were originally\nused by the Slurm team, this may rebuild all of the Makefile.in files in Slurm.\nAdding a Plugin to Slurm\n\nThis is the procedure to follow in order to add a new plugin to the Slurm\ncode base.  We recommend using a git branch for this purpose. In this example,\nwe show which files would need to be modified in order to add a plugin named\n\"topology/4d_torus\".\nCreate a new directory for this plugin\n(e.g. \"src/plugins/topology/4d_torus\").\nAdd this new directory to its parent directory's \"Makefile.am\" file\n(e.g. \"src/plugins/topology/Makefile.am\").\nPut your new file(s) in the appropriate directory\n(e.g. \"src/plugins/topology/4d_torus/topology_4d_torus.c\"). \nCreate a \"Makefile.am\" file in the new directory identifying the new file(s)\n(e.g. \"src/plugins/topology/4d_torus/Makefile.am\"). Use an existing \"Makefile.am\"\nfile as a model.\nIdentify the new Makefile to be built at Slurm configure time in the file\n\"configure.ac\". Please maintain the alphabetic ordering of entries.\nExecute \"autoreconf\" in Slurm's top level directory.\nIf you have older versions of the autoconf, automake, libtool or aclocal then\nyou may need to manually create or modify the Makefile.in files.\nIf you have different versions of the files than were originally used by the\nSlurm team, this may rebuild all of the Makefile.in files in Slurm.\nModify the \"slurm.spec\" file to include the new plugin file in an\nappropriate RPM.\nAdd the new files, including \"Makefile.am\" and \"Makefile.in\", to the git\nrepository.\nLast modified 27 February 2019"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/rest_api.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Slurm REST API",
                "content": "API to access and control SlurmMore information: https://www.schedmd.com/Contact Info: sales@schedmd.comVersion: Slurm-24.05.4&openapi/slurmdbd&openapi/slurmctldBasePath:Apache 2.0https://www.apache.org/licenses/LICENSE-2.0.htmlAccess\nAPIKey KeyParamName:X-SLURM-USER-NAME KeyInQuery:false KeyInHeader:true\nAPIKey KeyParamName:X-SLURM-USER-TOKEN KeyInQuery:false KeyInHeader:true\nHTTP Basic Authentication\nMethodsModelsTable of Contents Slurm\ndelete /slurm/v0.0.41/job/{job_id}\ndelete /slurm/v0.0.41/jobs/\ndelete /slurm/v0.0.41/node/{node_name}\nget /slurm/v0.0.41/diag/\nget /slurm/v0.0.41/job/{job_id}\nget /slurm/v0.0.41/jobs/\nget /slurm/v0.0.41/jobs/state/\nget /slurm/v0.0.41/licenses/\nget /slurm/v0.0.41/node/{node_name}\nget /slurm/v0.0.41/nodes/\nget /slurm/v0.0.41/partition/{partition_name}\nget /slurm/v0.0.41/partitions/\nget /slurm/v0.0.41/ping/\nget /slurm/v0.0.41/reconfigure/\nget /slurm/v0.0.41/reservation/{reservation_name}\nget /slurm/v0.0.41/reservations/\nget /slurm/v0.0.41/shares\npost /slurm/v0.0.41/job/{job_id}\npost /slurm/v0.0.41/job/allocate\npost /slurm/v0.0.41/job/submit\npost /slurm/v0.0.41/node/{node_name}\nSlurmdb\ndelete /slurmdb/v0.0.41/account/{account_name}\ndelete /slurmdb/v0.0.41/association/\ndelete /slurmdb/v0.0.41/associations/\ndelete /slurmdb/v0.0.41/cluster/{cluster_name}\ndelete /slurmdb/v0.0.41/qos/{qos}\ndelete /slurmdb/v0.0.41/user/{name}\ndelete /slurmdb/v0.0.41/wckey/{id}\nget /slurmdb/v0.0.41/account/{account_name}\nget /slurmdb/v0.0.41/accounts/\nget /slurmdb/v0.0.41/association/\nget /slurmdb/v0.0.41/associations/\nget /slurmdb/v0.0.41/cluster/{cluster_name}\nget /slurmdb/v0.0.41/clusters/\nget /slurmdb/v0.0.41/config\nget /slurmdb/v0.0.41/diag/\nget /slurmdb/v0.0.41/instance/\nget /slurmdb/v0.0.41/instances/\nget /slurmdb/v0.0.41/job/{job_id}\nget /slurmdb/v0.0.41/jobs/\nget /slurmdb/v0.0.41/qos/\nget /slurmdb/v0.0.41/qos/{qos}\nget /slurmdb/v0.0.41/tres/\nget /slurmdb/v0.0.41/user/{name}\nget /slurmdb/v0.0.41/users/\nget /slurmdb/v0.0.41/wckey/{id}\nget /slurmdb/v0.0.41/wckeys/\npost /slurmdb/v0.0.41/accounts/\npost /slurmdb/v0.0.41/accounts_association/\npost /slurmdb/v0.0.41/associations/\npost /slurmdb/v0.0.41/clusters/\npost /slurmdb/v0.0.41/config\npost /slurmdb/v0.0.41/qos/\npost /slurmdb/v0.0.41/tres/\npost /slurmdb/v0.0.41/users/\npost /slurmdb/v0.0.41/users_association/\npost /slurmdb/v0.0.41/wckeys/\nSlurm\n\nUp\ndelete /slurm/v0.0.41/job/{job_id}\ncancel or signal job (slurmV0041DeleteJob)\n\nPath parameters\n\njob_id (required)\nPath Parameter \u2014 Job ID default: null \n \nQuery parameters\n\nsignal (optional)\nQuery Parameter \u2014 Signal to send to Job default: null flags (optional)\nQuery Parameter \u2014 Signalling flags default: null \n \nReturn type\n\nv0.0.41_openapi_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    job signal result\n        v0.0.41_openapi_resp\ndefault\n    job signal result\n        v0.0.41_openapi_resp\n\n\nUp\ndelete /slurm/v0.0.41/jobs/\nsend signal to list of jobs (slurmV0041DeleteJobs)\n\nConsumes\n    This API call consumes the following media types via the Content-Type request header:\n    \napplication/json\napplication/x-yaml\n\nRequest body\n\nv0.0.41_kill_jobs_msg v0.0.41_kill_jobs_msg (optional)\nBody Parameter \u2014  \n \nReturn type\n\nv0.0.41_openapi_kill_jobs_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ],\n  \"status\" : [ {\n    \"federation\" : {\n      \"sibling\" : \"sibling\"\n    },\n    \"job_id\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"error\" : {\n      \"code\" : 0,\n      \"string\" : \"string\",\n      \"message\" : \"message\"\n    },\n    \"step_id\" : \"step_id\"\n  }, {\n    \"federation\" : {\n      \"sibling\" : \"sibling\"\n    },\n    \"job_id\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"error\" : {\n      \"code\" : 0,\n      \"string\" : \"string\",\n      \"message\" : \"message\"\n    },\n    \"step_id\" : \"step_id\"\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    description of jobs to signal\n        v0.0.41_openapi_kill_jobs_resp\ndefault\n    description of jobs to signal\n        v0.0.41_openapi_kill_jobs_resp\n\n\nUp\ndelete /slurm/v0.0.41/node/{node_name}\ndelete node (slurmV0041DeleteNode)\n\nPath parameters\n\nnode_name (required)\nPath Parameter \u2014 Node name default: null \n \nReturn type\n\nv0.0.41_openapi_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    node delete request result\n        v0.0.41_openapi_resp\ndefault\n    node delete request result\n        v0.0.41_openapi_resp\n\n\nUp\nget /slurm/v0.0.41/diag/\nget diagnostics (slurmV0041GetDiag)\n\nReturn type\n\nv0.0.41_openapi_diag_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ],\n  \"statistics\" : {\n    \"bf_cycle_max\" : 4,\n    \"rpcs_by_message_type\" : [ {\n      \"cycle_last\" : 6,\n      \"average_time\" : {\n        \"number\" : 3,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"type_id\" : 0,\n      \"queued\" : 5,\n      \"count\" : 7,\n      \"dropped\" : 4,\n      \"message_type\" : \"message_type\",\n      \"total_time\" : 4,\n      \"cycle_max\" : 8\n    }, {\n      \"cycle_last\" : 6,\n      \"average_time\" : {\n        \"number\" : 3,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"type_id\" : 0,\n      \"queued\" : 5,\n      \"count\" : 7,\n      \"dropped\" : 4,\n      \"message_type\" : \"message_type\",\n      \"total_time\" : 4,\n      \"cycle_max\" : 8\n    } ],\n    \"bf_backfilled_het_jobs\" : 3,\n    \"bf_table_size\" : 7,\n    \"schedule_cycle_depth\" : 7,\n    \"bf_depth_sum\" : 0,\n    \"job_states_ts\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"bf_queue_len\" : 4,\n    \"jobs_started\" : 6,\n    \"schedule_cycle_max\" : 2,\n    \"server_thread_count\" : 5,\n    \"bf_queue_len_sum\" : 4,\n    \"bf_cycle_last\" : 0,\n    \"bf_exit\" : {\n      \"state_changed\" : 5,\n      \"bf_max_time\" : 3,\n      \"bf_max_job_start\" : 7,\n      \"bf_node_space_size\" : 7,\n      \"end_job_queue\" : 8,\n      \"bf_max_job_test\" : 3\n    },\n    \"agent_thread_count\" : 7,\n    \"jobs_completed\" : 3,\n    \"bf_depth_mean\" : 0,\n    \"bf_depth_try_sum\" : 6,\n    \"schedule_cycle_mean\" : 1,\n    \"bf_table_size_sum\" : 9,\n    \"agent_queue_size\" : 5,\n    \"jobs_failed\" : 1,\n    \"bf_last_depth_try\" : 4,\n    \"req_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"bf_cycle_counter\" : 3,\n    \"schedule_queue_length\" : 8,\n    \"bf_queue_len_mean\" : 1,\n    \"schedule_exit\" : {\n      \"max_sched_time\" : 9,\n      \"licenses\" : 6,\n      \"default_queue_depth\" : 4,\n      \"max_job_start\" : 5,\n      \"max_rpc_cnt\" : 9,\n      \"end_job_queue\" : 1\n    },\n    \"jobs_canceled\" : 6,\n    \"schedule_cycle_sum\" : 7,\n    \"jobs_submitted\" : 9,\n    \"schedule_cycle_mean_depth\" : 1,\n    \"schedule_cycle_per_minute\" : 6,\n    \"req_time_start\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"jobs_running\" : 6,\n    \"bf_last_backfilled_jobs\" : 6,\n    \"bf_last_depth\" : 3,\n    \"bf_backfilled_jobs\" : 5,\n    \"rpcs_by_user\" : [ {\n      \"average_time\" : {\n        \"number\" : 3,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"user_id\" : 0,\n      \"count\" : 2,\n      \"total_time\" : 1,\n      \"user\" : \"user\"\n    }, {\n      \"average_time\" : {\n        \"number\" : 3,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"user_id\" : 0,\n      \"count\" : 2,\n      \"total_time\" : 1,\n      \"user\" : \"user\"\n    } ],\n    \"bf_cycle_mean\" : 7,\n    \"pending_rpcs_by_hostlist\" : [ {\n      \"type_id\" : 4,\n      \"count\" : [ \"count\", \"count\" ],\n      \"message_type\" : \"message_type\"\n    }, {\n      \"type_id\" : 4,\n      \"count\" : [ \"count\", \"count\" ],\n      \"message_type\" : \"message_type\"\n    } ],\n    \"dbd_agent_queue_size\" : 9,\n    \"bf_table_size_mean\" : 0,\n    \"jobs_pending\" : 2,\n    \"agent_count\" : 2,\n    \"bf_cycle_sum\" : 6,\n    \"parts_packed\" : 0,\n    \"bf_active\" : true,\n    \"bf_depth_mean_try\" : 7,\n    \"gettimeofday_latency\" : 3,\n    \"pending_rpcs\" : [ {\n      \"type_id\" : 8,\n      \"count\" : 6,\n      \"message_type\" : \"message_type\"\n    }, {\n      \"type_id\" : 8,\n      \"count\" : 6,\n      \"message_type\" : \"message_type\"\n    } ],\n    \"schedule_cycle_total\" : 1,\n    \"bf_when_last_cycle\" : {\n      \"number\" : 9,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"schedule_cycle_last\" : 4\n  }\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    diagnostic results\n        v0.0.41_openapi_diag_resp\ndefault\n    diagnostic results\n        v0.0.41_openapi_diag_resp\n\n\nUp\nget /slurm/v0.0.41/job/{job_id}\nget job info (slurmV0041GetJob)\n\nPath parameters\n\njob_id (required)\nPath Parameter \u2014 Job ID default: null \n \nQuery parameters\n\nupdate_time (optional)\nQuery Parameter \u2014 Filter jobs since update timestamp default: null flags (optional)\nQuery Parameter \u2014 Query flags default: null \n \nReturn type\n\nv0.0.41_openapi_job_info_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"last_backfill\" : {\n    \"number\" : 0,\n    \"set\" : true,\n    \"infinite\" : true\n  },\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"jobs\" : [ {\n    \"container\" : \"container\",\n    \"cluster\" : \"cluster\",\n    \"time_minimum\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"memory_per_tres\" : \"memory_per_tres\",\n    \"scheduled_nodes\" : \"scheduled_nodes\",\n    \"minimum_switches\" : 7,\n    \"qos\" : \"qos\",\n    \"resize_time\" : {\n      \"number\" : 5,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"eligible_time\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"exclusive\" : [ \"true\", \"true\" ],\n    \"cpus_per_tres\" : \"cpus_per_tres\",\n    \"preemptable_time\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tasks\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"system_comment\" : \"system_comment\",\n    \"federation_siblings_active\" : \"federation_siblings_active\",\n    \"tasks_per_tres\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tasks_per_core\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"accrue_time\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"dependency\" : \"dependency\",\n    \"group_name\" : \"group_name\",\n    \"profile\" : [ \"NOT_SET\", \"NOT_SET\" ],\n    \"priority\" : {\n      \"number\" : 9,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_per_job\" : \"tres_per_job\",\n    \"failed_node\" : \"failed_node\",\n    \"derived_exit_code\" : {\n      \"return_code\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"signal\" : {\n        \"name\" : \"name\",\n        \"id\" : {\n          \"number\" : 7,\n          \"set\" : true,\n          \"infinite\" : true\n        }\n      },\n      \"status\" : [ \"INVALID\", \"INVALID\" ]\n    },\n    \"maximum_switch_wait_time\" : 9,\n    \"core_spec\" : 5,\n    \"mcs_label\" : \"mcs_label\",\n    \"required_nodes\" : \"required_nodes\",\n    \"tres_bind\" : \"tres_bind\",\n    \"user_id\" : 7,\n    \"selinux_context\" : \"selinux_context\",\n    \"exit_code\" : {\n      \"return_code\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"signal\" : {\n        \"name\" : \"name\",\n        \"id\" : {\n          \"number\" : 7,\n          \"set\" : true,\n          \"infinite\" : true\n        }\n      },\n      \"status\" : [ \"INVALID\", \"INVALID\" ]\n    },\n    \"federation_origin\" : \"federation_origin\",\n    \"container_id\" : \"container_id\",\n    \"shared\" : [ \"none\", \"none\" ],\n    \"tasks_per_board\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"user_name\" : \"user_name\",\n    \"flags\" : [ \"KILL_INVALID_DEPENDENCY\", \"KILL_INVALID_DEPENDENCY\" ],\n    \"standard_input\" : \"standard_input\",\n    \"admin_comment\" : \"admin_comment\",\n    \"cores_per_socket\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"job_state\" : [ \"PENDING\", \"PENDING\" ],\n    \"tasks_per_node\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"current_working_directory\" : \"current_working_directory\",\n    \"standard_error\" : \"standard_error\",\n    \"array_job_id\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"cluster_features\" : \"cluster_features\",\n    \"partition\" : \"partition\",\n    \"threads_per_core\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_alloc_str\" : \"tres_alloc_str\",\n    \"memory_per_cpu\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"cpu_frequency_minimum\" : {\n      \"number\" : 2,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"node_count\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"power\" : {\n      \"flags\" : [ \"\", \"\" ]\n    },\n    \"deadline\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"mail_type\" : [ \"BEGIN\", \"BEGIN\" ],\n    \"memory_per_node\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"state_reason\" : \"state_reason\",\n    \"het_job_offset\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"end_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"sockets_per_board\" : 4,\n    \"nice\" : 5,\n    \"last_sched_evaluation\" : {\n      \"number\" : 2,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_per_node\" : \"tres_per_node\",\n    \"burst_buffer\" : \"burst_buffer\",\n    \"licenses\" : \"licenses\",\n    \"excluded_nodes\" : \"excluded_nodes\",\n    \"array_max_tasks\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"het_job_id\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"sockets_per_node\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"prefer\" : \"prefer\",\n    \"time_limit\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"minimum_cpus_per_node\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tasks_per_socket\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"batch_host\" : \"batch_host\",\n    \"max_cpus\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"job_size_str\" : [ \"job_size_str\", \"job_size_str\" ],\n    \"hold\" : true,\n    \"cpu_frequency_maximum\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"features\" : \"features\",\n    \"het_job_id_set\" : \"het_job_id_set\",\n    \"state_description\" : \"state_description\",\n    \"show_flags\" : [ \"ALL\", \"ALL\" ],\n    \"array_task_id\" : {\n      \"number\" : 5,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"minimum_tmp_disk_per_node\" : {\n      \"number\" : 8,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_req_str\" : \"tres_req_str\",\n    \"burst_buffer_state\" : \"burst_buffer_state\",\n    \"cron\" : \"cron\",\n    \"allocating_node\" : \"allocating_node\",\n    \"tres_per_socket\" : \"tres_per_socket\",\n    \"array_task_string\" : \"array_task_string\",\n    \"submit_time\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"oversubscribe\" : true,\n    \"wckey\" : \"wckey\",\n    \"max_nodes\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"batch_flag\" : true,\n    \"start_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"name\" : \"name\",\n    \"preempt_time\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"contiguous\" : true,\n    \"job_resources\" : {\n      \"nodes\" : {\n        \"allocation\" : [ {\n          \"memory\" : {\n            \"used\" : 8,\n            \"allocated\" : 9\n          },\n          \"cpus\" : {\n            \"count\" : 9,\n            \"used\" : 6\n          },\n          \"name\" : \"name\",\n          \"index\" : 9,\n          \"sockets\" : [ {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          }, {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          } ]\n        }, {\n          \"memory\" : {\n            \"used\" : 8,\n            \"allocated\" : 9\n          },\n          \"cpus\" : {\n            \"count\" : 9,\n            \"used\" : 6\n          },\n          \"name\" : \"name\",\n          \"index\" : 9,\n          \"sockets\" : [ {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          }, {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          } ]\n        } ],\n        \"count\" : 5,\n        \"select_type\" : [ \"AVAILABLE\", \"AVAILABLE\" ],\n        \"whole\" : true,\n        \"list\" : \"list\"\n      },\n      \"threads_per_core\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"cpus\" : 6,\n      \"select_type\" : [ \"CPU\", \"CPU\" ]\n    },\n    \"billable_tres\" : {\n      \"number\" : 9.301444243932576,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"federation_siblings_viable\" : \"federation_siblings_viable\",\n    \"cpus_per_task\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"batch_features\" : \"batch_features\",\n    \"thread_spec\" : 2,\n    \"cpu_frequency_governor\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"gres_detail\" : [ \"gres_detail\", \"gres_detail\" ],\n    \"network\" : \"network\",\n    \"restart_cnt\" : 3,\n    \"resv_name\" : \"resv_name\",\n    \"extra\" : \"extra\",\n    \"delay_boot\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"reboot\" : true,\n    \"cpus\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"standard_output\" : \"standard_output\",\n    \"pre_sus_time\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"suspend_time\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"association_id\" : 5,\n    \"command\" : \"command\",\n    \"tres_freq\" : \"tres_freq\",\n    \"requeue\" : true,\n    \"tres_per_task\" : \"tres_per_task\",\n    \"mail_user\" : \"mail_user\",\n    \"nodes\" : \"nodes\",\n    \"group_id\" : 7,\n    \"job_id\" : 4,\n    \"comment\" : \"comment\",\n    \"account\" : \"account\"\n  }, {\n    \"container\" : \"container\",\n    \"cluster\" : \"cluster\",\n    \"time_minimum\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"memory_per_tres\" : \"memory_per_tres\",\n    \"scheduled_nodes\" : \"scheduled_nodes\",\n    \"minimum_switches\" : 7,\n    \"qos\" : \"qos\",\n    \"resize_time\" : {\n      \"number\" : 5,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"eligible_time\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"exclusive\" : [ \"true\", \"true\" ],\n    \"cpus_per_tres\" : \"cpus_per_tres\",\n    \"preemptable_time\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tasks\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"system_comment\" : \"system_comment\",\n    \"federation_siblings_active\" : \"federation_siblings_active\",\n    \"tasks_per_tres\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tasks_per_core\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"accrue_time\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"dependency\" : \"dependency\",\n    \"group_name\" : \"group_name\",\n    \"profile\" : [ \"NOT_SET\", \"NOT_SET\" ],\n    \"priority\" : {\n      \"number\" : 9,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_per_job\" : \"tres_per_job\",\n    \"failed_node\" : \"failed_node\",\n    \"derived_exit_code\" : {\n      \"return_code\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"signal\" : {\n        \"name\" : \"name\",\n        \"id\" : {\n          \"number\" : 7,\n          \"set\" : true,\n          \"infinite\" : true\n        }\n      },\n      \"status\" : [ \"INVALID\", \"INVALID\" ]\n    },\n    \"maximum_switch_wait_time\" : 9,\n    \"core_spec\" : 5,\n    \"mcs_label\" : \"mcs_label\",\n    \"required_nodes\" : \"required_nodes\",\n    \"tres_bind\" : \"tres_bind\",\n    \"user_id\" : 7,\n    \"selinux_context\" : \"selinux_context\",\n    \"exit_code\" : {\n      \"return_code\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"signal\" : {\n        \"name\" : \"name\",\n        \"id\" : {\n          \"number\" : 7,\n          \"set\" : true,\n          \"infinite\" : true\n        }\n      },\n      \"status\" : [ \"INVALID\", \"INVALID\" ]\n    },\n    \"federation_origin\" : \"federation_origin\",\n    \"container_id\" : \"container_id\",\n    \"shared\" : [ \"none\", \"none\" ],\n    \"tasks_per_board\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"user_name\" : \"user_name\",\n    \"flags\" : [ \"KILL_INVALID_DEPENDENCY\", \"KILL_INVALID_DEPENDENCY\" ],\n    \"standard_input\" : \"standard_input\",\n    \"admin_comment\" : \"admin_comment\",\n    \"cores_per_socket\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"job_state\" : [ \"PENDING\", \"PENDING\" ],\n    \"tasks_per_node\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"current_working_directory\" : \"current_working_directory\",\n    \"standard_error\" : \"standard_error\",\n    \"array_job_id\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"cluster_features\" : \"cluster_features\",\n    \"partition\" : \"partition\",\n    \"threads_per_core\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_alloc_str\" : \"tres_alloc_str\",\n    \"memory_per_cpu\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"cpu_frequency_minimum\" : {\n      \"number\" : 2,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"node_count\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"power\" : {\n      \"flags\" : [ \"\", \"\" ]\n    },\n    \"deadline\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"mail_type\" : [ \"BEGIN\", \"BEGIN\" ],\n    \"memory_per_node\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"state_reason\" : \"state_reason\",\n    \"het_job_offset\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"end_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"sockets_per_board\" : 4,\n    \"nice\" : 5,\n    \"last_sched_evaluation\" : {\n      \"number\" : 2,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_per_node\" : \"tres_per_node\",\n    \"burst_buffer\" : \"burst_buffer\",\n    \"licenses\" : \"licenses\",\n    \"excluded_nodes\" : \"excluded_nodes\",\n    \"array_max_tasks\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"het_job_id\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"sockets_per_node\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"prefer\" : \"prefer\",\n    \"time_limit\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"minimum_cpus_per_node\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tasks_per_socket\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"batch_host\" : \"batch_host\",\n    \"max_cpus\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"job_size_str\" : [ \"job_size_str\", \"job_size_str\" ],\n    \"hold\" : true,\n    \"cpu_frequency_maximum\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"features\" : \"features\",\n    \"het_job_id_set\" : \"het_job_id_set\",\n    \"state_description\" : \"state_description\",\n    \"show_flags\" : [ \"ALL\", \"ALL\" ],\n    \"array_task_id\" : {\n      \"number\" : 5,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"minimum_tmp_disk_per_node\" : {\n      \"number\" : 8,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_req_str\" : \"tres_req_str\",\n    \"burst_buffer_state\" : \"burst_buffer_state\",\n    \"cron\" : \"cron\",\n    \"allocating_node\" : \"allocating_node\",\n    \"tres_per_socket\" : \"tres_per_socket\",\n    \"array_task_string\" : \"array_task_string\",\n    \"submit_time\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"oversubscribe\" : true,\n    \"wckey\" : \"wckey\",\n    \"max_nodes\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"batch_flag\" : true,\n    \"start_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"name\" : \"name\",\n    \"preempt_time\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"contiguous\" : true,\n    \"job_resources\" : {\n      \"nodes\" : {\n        \"allocation\" : [ {\n          \"memory\" : {\n            \"used\" : 8,\n            \"allocated\" : 9\n          },\n          \"cpus\" : {\n            \"count\" : 9,\n            \"used\" : 6\n          },\n          \"name\" : \"name\",\n          \"index\" : 9,\n          \"sockets\" : [ {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          }, {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          } ]\n        }, {\n          \"memory\" : {\n            \"used\" : 8,\n            \"allocated\" : 9\n          },\n          \"cpus\" : {\n            \"count\" : 9,\n            \"used\" : 6\n          },\n          \"name\" : \"name\",\n          \"index\" : 9,\n          \"sockets\" : [ {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          }, {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          } ]\n        } ],\n        \"count\" : 5,\n        \"select_type\" : [ \"AVAILABLE\", \"AVAILABLE\" ],\n        \"whole\" : true,\n        \"list\" : \"list\"\n      },\n      \"threads_per_core\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"cpus\" : 6,\n      \"select_type\" : [ \"CPU\", \"CPU\" ]\n    },\n    \"billable_tres\" : {\n      \"number\" : 9.301444243932576,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"federation_siblings_viable\" : \"federation_siblings_viable\",\n    \"cpus_per_task\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"batch_features\" : \"batch_features\",\n    \"thread_spec\" : 2,\n    \"cpu_frequency_governor\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"gres_detail\" : [ \"gres_detail\", \"gres_detail\" ],\n    \"network\" : \"network\",\n    \"restart_cnt\" : 3,\n    \"resv_name\" : \"resv_name\",\n    \"extra\" : \"extra\",\n    \"delay_boot\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"reboot\" : true,\n    \"cpus\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"standard_output\" : \"standard_output\",\n    \"pre_sus_time\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"suspend_time\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"association_id\" : 5,\n    \"command\" : \"command\",\n    \"tres_freq\" : \"tres_freq\",\n    \"requeue\" : true,\n    \"tres_per_task\" : \"tres_per_task\",\n    \"mail_user\" : \"mail_user\",\n    \"nodes\" : \"nodes\",\n    \"group_id\" : 7,\n    \"job_id\" : 4,\n    \"comment\" : \"comment\",\n    \"account\" : \"account\"\n  } ],\n  \"last_update\" : {\n    \"number\" : 9,\n    \"set\" : true,\n    \"infinite\" : true\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    job(s) information\n        v0.0.41_openapi_job_info_resp\ndefault\n    job(s) information\n        v0.0.41_openapi_job_info_resp\n\n\nUp\nget /slurm/v0.0.41/jobs/\nget list of jobs (slurmV0041GetJobs)\n\nQuery parameters\n\nupdate_time (optional)\nQuery Parameter \u2014 Filter jobs since update timestamp default: null flags (optional)\nQuery Parameter \u2014 Query flags default: null \n \nReturn type\n\nv0.0.41_openapi_job_info_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"last_backfill\" : {\n    \"number\" : 0,\n    \"set\" : true,\n    \"infinite\" : true\n  },\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"jobs\" : [ {\n    \"container\" : \"container\",\n    \"cluster\" : \"cluster\",\n    \"time_minimum\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"memory_per_tres\" : \"memory_per_tres\",\n    \"scheduled_nodes\" : \"scheduled_nodes\",\n    \"minimum_switches\" : 7,\n    \"qos\" : \"qos\",\n    \"resize_time\" : {\n      \"number\" : 5,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"eligible_time\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"exclusive\" : [ \"true\", \"true\" ],\n    \"cpus_per_tres\" : \"cpus_per_tres\",\n    \"preemptable_time\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tasks\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"system_comment\" : \"system_comment\",\n    \"federation_siblings_active\" : \"federation_siblings_active\",\n    \"tasks_per_tres\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tasks_per_core\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"accrue_time\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"dependency\" : \"dependency\",\n    \"group_name\" : \"group_name\",\n    \"profile\" : [ \"NOT_SET\", \"NOT_SET\" ],\n    \"priority\" : {\n      \"number\" : 9,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_per_job\" : \"tres_per_job\",\n    \"failed_node\" : \"failed_node\",\n    \"derived_exit_code\" : {\n      \"return_code\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"signal\" : {\n        \"name\" : \"name\",\n        \"id\" : {\n          \"number\" : 7,\n          \"set\" : true,\n          \"infinite\" : true\n        }\n      },\n      \"status\" : [ \"INVALID\", \"INVALID\" ]\n    },\n    \"maximum_switch_wait_time\" : 9,\n    \"core_spec\" : 5,\n    \"mcs_label\" : \"mcs_label\",\n    \"required_nodes\" : \"required_nodes\",\n    \"tres_bind\" : \"tres_bind\",\n    \"user_id\" : 7,\n    \"selinux_context\" : \"selinux_context\",\n    \"exit_code\" : {\n      \"return_code\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"signal\" : {\n        \"name\" : \"name\",\n        \"id\" : {\n          \"number\" : 7,\n          \"set\" : true,\n          \"infinite\" : true\n        }\n      },\n      \"status\" : [ \"INVALID\", \"INVALID\" ]\n    },\n    \"federation_origin\" : \"federation_origin\",\n    \"container_id\" : \"container_id\",\n    \"shared\" : [ \"none\", \"none\" ],\n    \"tasks_per_board\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"user_name\" : \"user_name\",\n    \"flags\" : [ \"KILL_INVALID_DEPENDENCY\", \"KILL_INVALID_DEPENDENCY\" ],\n    \"standard_input\" : \"standard_input\",\n    \"admin_comment\" : \"admin_comment\",\n    \"cores_per_socket\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"job_state\" : [ \"PENDING\", \"PENDING\" ],\n    \"tasks_per_node\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"current_working_directory\" : \"current_working_directory\",\n    \"standard_error\" : \"standard_error\",\n    \"array_job_id\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"cluster_features\" : \"cluster_features\",\n    \"partition\" : \"partition\",\n    \"threads_per_core\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_alloc_str\" : \"tres_alloc_str\",\n    \"memory_per_cpu\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"cpu_frequency_minimum\" : {\n      \"number\" : 2,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"node_count\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"power\" : {\n      \"flags\" : [ \"\", \"\" ]\n    },\n    \"deadline\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"mail_type\" : [ \"BEGIN\", \"BEGIN\" ],\n    \"memory_per_node\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"state_reason\" : \"state_reason\",\n    \"het_job_offset\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"end_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"sockets_per_board\" : 4,\n    \"nice\" : 5,\n    \"last_sched_evaluation\" : {\n      \"number\" : 2,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_per_node\" : \"tres_per_node\",\n    \"burst_buffer\" : \"burst_buffer\",\n    \"licenses\" : \"licenses\",\n    \"excluded_nodes\" : \"excluded_nodes\",\n    \"array_max_tasks\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"het_job_id\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"sockets_per_node\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"prefer\" : \"prefer\",\n    \"time_limit\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"minimum_cpus_per_node\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tasks_per_socket\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"batch_host\" : \"batch_host\",\n    \"max_cpus\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"job_size_str\" : [ \"job_size_str\", \"job_size_str\" ],\n    \"hold\" : true,\n    \"cpu_frequency_maximum\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"features\" : \"features\",\n    \"het_job_id_set\" : \"het_job_id_set\",\n    \"state_description\" : \"state_description\",\n    \"show_flags\" : [ \"ALL\", \"ALL\" ],\n    \"array_task_id\" : {\n      \"number\" : 5,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"minimum_tmp_disk_per_node\" : {\n      \"number\" : 8,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_req_str\" : \"tres_req_str\",\n    \"burst_buffer_state\" : \"burst_buffer_state\",\n    \"cron\" : \"cron\",\n    \"allocating_node\" : \"allocating_node\",\n    \"tres_per_socket\" : \"tres_per_socket\",\n    \"array_task_string\" : \"array_task_string\",\n    \"submit_time\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"oversubscribe\" : true,\n    \"wckey\" : \"wckey\",\n    \"max_nodes\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"batch_flag\" : true,\n    \"start_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"name\" : \"name\",\n    \"preempt_time\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"contiguous\" : true,\n    \"job_resources\" : {\n      \"nodes\" : {\n        \"allocation\" : [ {\n          \"memory\" : {\n            \"used\" : 8,\n            \"allocated\" : 9\n          },\n          \"cpus\" : {\n            \"count\" : 9,\n            \"used\" : 6\n          },\n          \"name\" : \"name\",\n          \"index\" : 9,\n          \"sockets\" : [ {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          }, {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          } ]\n        }, {\n          \"memory\" : {\n            \"used\" : 8,\n            \"allocated\" : 9\n          },\n          \"cpus\" : {\n            \"count\" : 9,\n            \"used\" : 6\n          },\n          \"name\" : \"name\",\n          \"index\" : 9,\n          \"sockets\" : [ {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          }, {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          } ]\n        } ],\n        \"count\" : 5,\n        \"select_type\" : [ \"AVAILABLE\", \"AVAILABLE\" ],\n        \"whole\" : true,\n        \"list\" : \"list\"\n      },\n      \"threads_per_core\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"cpus\" : 6,\n      \"select_type\" : [ \"CPU\", \"CPU\" ]\n    },\n    \"billable_tres\" : {\n      \"number\" : 9.301444243932576,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"federation_siblings_viable\" : \"federation_siblings_viable\",\n    \"cpus_per_task\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"batch_features\" : \"batch_features\",\n    \"thread_spec\" : 2,\n    \"cpu_frequency_governor\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"gres_detail\" : [ \"gres_detail\", \"gres_detail\" ],\n    \"network\" : \"network\",\n    \"restart_cnt\" : 3,\n    \"resv_name\" : \"resv_name\",\n    \"extra\" : \"extra\",\n    \"delay_boot\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"reboot\" : true,\n    \"cpus\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"standard_output\" : \"standard_output\",\n    \"pre_sus_time\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"suspend_time\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"association_id\" : 5,\n    \"command\" : \"command\",\n    \"tres_freq\" : \"tres_freq\",\n    \"requeue\" : true,\n    \"tres_per_task\" : \"tres_per_task\",\n    \"mail_user\" : \"mail_user\",\n    \"nodes\" : \"nodes\",\n    \"group_id\" : 7,\n    \"job_id\" : 4,\n    \"comment\" : \"comment\",\n    \"account\" : \"account\"\n  }, {\n    \"container\" : \"container\",\n    \"cluster\" : \"cluster\",\n    \"time_minimum\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"memory_per_tres\" : \"memory_per_tres\",\n    \"scheduled_nodes\" : \"scheduled_nodes\",\n    \"minimum_switches\" : 7,\n    \"qos\" : \"qos\",\n    \"resize_time\" : {\n      \"number\" : 5,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"eligible_time\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"exclusive\" : [ \"true\", \"true\" ],\n    \"cpus_per_tres\" : \"cpus_per_tres\",\n    \"preemptable_time\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tasks\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"system_comment\" : \"system_comment\",\n    \"federation_siblings_active\" : \"federation_siblings_active\",\n    \"tasks_per_tres\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tasks_per_core\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"accrue_time\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"dependency\" : \"dependency\",\n    \"group_name\" : \"group_name\",\n    \"profile\" : [ \"NOT_SET\", \"NOT_SET\" ],\n    \"priority\" : {\n      \"number\" : 9,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_per_job\" : \"tres_per_job\",\n    \"failed_node\" : \"failed_node\",\n    \"derived_exit_code\" : {\n      \"return_code\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"signal\" : {\n        \"name\" : \"name\",\n        \"id\" : {\n          \"number\" : 7,\n          \"set\" : true,\n          \"infinite\" : true\n        }\n      },\n      \"status\" : [ \"INVALID\", \"INVALID\" ]\n    },\n    \"maximum_switch_wait_time\" : 9,\n    \"core_spec\" : 5,\n    \"mcs_label\" : \"mcs_label\",\n    \"required_nodes\" : \"required_nodes\",\n    \"tres_bind\" : \"tres_bind\",\n    \"user_id\" : 7,\n    \"selinux_context\" : \"selinux_context\",\n    \"exit_code\" : {\n      \"return_code\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"signal\" : {\n        \"name\" : \"name\",\n        \"id\" : {\n          \"number\" : 7,\n          \"set\" : true,\n          \"infinite\" : true\n        }\n      },\n      \"status\" : [ \"INVALID\", \"INVALID\" ]\n    },\n    \"federation_origin\" : \"federation_origin\",\n    \"container_id\" : \"container_id\",\n    \"shared\" : [ \"none\", \"none\" ],\n    \"tasks_per_board\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"user_name\" : \"user_name\",\n    \"flags\" : [ \"KILL_INVALID_DEPENDENCY\", \"KILL_INVALID_DEPENDENCY\" ],\n    \"standard_input\" : \"standard_input\",\n    \"admin_comment\" : \"admin_comment\",\n    \"cores_per_socket\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"job_state\" : [ \"PENDING\", \"PENDING\" ],\n    \"tasks_per_node\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"current_working_directory\" : \"current_working_directory\",\n    \"standard_error\" : \"standard_error\",\n    \"array_job_id\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"cluster_features\" : \"cluster_features\",\n    \"partition\" : \"partition\",\n    \"threads_per_core\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_alloc_str\" : \"tres_alloc_str\",\n    \"memory_per_cpu\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"cpu_frequency_minimum\" : {\n      \"number\" : 2,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"node_count\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"power\" : {\n      \"flags\" : [ \"\", \"\" ]\n    },\n    \"deadline\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"mail_type\" : [ \"BEGIN\", \"BEGIN\" ],\n    \"memory_per_node\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"state_reason\" : \"state_reason\",\n    \"het_job_offset\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"end_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"sockets_per_board\" : 4,\n    \"nice\" : 5,\n    \"last_sched_evaluation\" : {\n      \"number\" : 2,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_per_node\" : \"tres_per_node\",\n    \"burst_buffer\" : \"burst_buffer\",\n    \"licenses\" : \"licenses\",\n    \"excluded_nodes\" : \"excluded_nodes\",\n    \"array_max_tasks\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"het_job_id\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"sockets_per_node\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"prefer\" : \"prefer\",\n    \"time_limit\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"minimum_cpus_per_node\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tasks_per_socket\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"batch_host\" : \"batch_host\",\n    \"max_cpus\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"job_size_str\" : [ \"job_size_str\", \"job_size_str\" ],\n    \"hold\" : true,\n    \"cpu_frequency_maximum\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"features\" : \"features\",\n    \"het_job_id_set\" : \"het_job_id_set\",\n    \"state_description\" : \"state_description\",\n    \"show_flags\" : [ \"ALL\", \"ALL\" ],\n    \"array_task_id\" : {\n      \"number\" : 5,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"minimum_tmp_disk_per_node\" : {\n      \"number\" : 8,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_req_str\" : \"tres_req_str\",\n    \"burst_buffer_state\" : \"burst_buffer_state\",\n    \"cron\" : \"cron\",\n    \"allocating_node\" : \"allocating_node\",\n    \"tres_per_socket\" : \"tres_per_socket\",\n    \"array_task_string\" : \"array_task_string\",\n    \"submit_time\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"oversubscribe\" : true,\n    \"wckey\" : \"wckey\",\n    \"max_nodes\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"batch_flag\" : true,\n    \"start_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"name\" : \"name\",\n    \"preempt_time\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"contiguous\" : true,\n    \"job_resources\" : {\n      \"nodes\" : {\n        \"allocation\" : [ {\n          \"memory\" : {\n            \"used\" : 8,\n            \"allocated\" : 9\n          },\n          \"cpus\" : {\n            \"count\" : 9,\n            \"used\" : 6\n          },\n          \"name\" : \"name\",\n          \"index\" : 9,\n          \"sockets\" : [ {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          }, {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          } ]\n        }, {\n          \"memory\" : {\n            \"used\" : 8,\n            \"allocated\" : 9\n          },\n          \"cpus\" : {\n            \"count\" : 9,\n            \"used\" : 6\n          },\n          \"name\" : \"name\",\n          \"index\" : 9,\n          \"sockets\" : [ {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          }, {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          } ]\n        } ],\n        \"count\" : 5,\n        \"select_type\" : [ \"AVAILABLE\", \"AVAILABLE\" ],\n        \"whole\" : true,\n        \"list\" : \"list\"\n      },\n      \"threads_per_core\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"cpus\" : 6,\n      \"select_type\" : [ \"CPU\", \"CPU\" ]\n    },\n    \"billable_tres\" : {\n      \"number\" : 9.301444243932576,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"federation_siblings_viable\" : \"federation_siblings_viable\",\n    \"cpus_per_task\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"batch_features\" : \"batch_features\",\n    \"thread_spec\" : 2,\n    \"cpu_frequency_governor\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"gres_detail\" : [ \"gres_detail\", \"gres_detail\" ],\n    \"network\" : \"network\",\n    \"restart_cnt\" : 3,\n    \"resv_name\" : \"resv_name\",\n    \"extra\" : \"extra\",\n    \"delay_boot\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"reboot\" : true,\n    \"cpus\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"standard_output\" : \"standard_output\",\n    \"pre_sus_time\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"suspend_time\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"association_id\" : 5,\n    \"command\" : \"command\",\n    \"tres_freq\" : \"tres_freq\",\n    \"requeue\" : true,\n    \"tres_per_task\" : \"tres_per_task\",\n    \"mail_user\" : \"mail_user\",\n    \"nodes\" : \"nodes\",\n    \"group_id\" : 7,\n    \"job_id\" : 4,\n    \"comment\" : \"comment\",\n    \"account\" : \"account\"\n  } ],\n  \"last_update\" : {\n    \"number\" : 9,\n    \"set\" : true,\n    \"infinite\" : true\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    job(s) information\n        v0.0.41_openapi_job_info_resp\ndefault\n    job(s) information\n        v0.0.41_openapi_job_info_resp\n\n\nUp\nget /slurm/v0.0.41/jobs/state/\nget list of job states (slurmV0041GetJobsState)\n\nQuery parameters\n\nupdate_time (optional)\nQuery Parameter \u2014 Filter jobs since update timestamp default: null flags (optional)\nQuery Parameter \u2014 Query flags default: null \n \nReturn type\n\nv0.0.41_openapi_job_info_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"last_backfill\" : {\n    \"number\" : 0,\n    \"set\" : true,\n    \"infinite\" : true\n  },\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"jobs\" : [ {\n    \"container\" : \"container\",\n    \"cluster\" : \"cluster\",\n    \"time_minimum\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"memory_per_tres\" : \"memory_per_tres\",\n    \"scheduled_nodes\" : \"scheduled_nodes\",\n    \"minimum_switches\" : 7,\n    \"qos\" : \"qos\",\n    \"resize_time\" : {\n      \"number\" : 5,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"eligible_time\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"exclusive\" : [ \"true\", \"true\" ],\n    \"cpus_per_tres\" : \"cpus_per_tres\",\n    \"preemptable_time\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tasks\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"system_comment\" : \"system_comment\",\n    \"federation_siblings_active\" : \"federation_siblings_active\",\n    \"tasks_per_tres\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tasks_per_core\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"accrue_time\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"dependency\" : \"dependency\",\n    \"group_name\" : \"group_name\",\n    \"profile\" : [ \"NOT_SET\", \"NOT_SET\" ],\n    \"priority\" : {\n      \"number\" : 9,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_per_job\" : \"tres_per_job\",\n    \"failed_node\" : \"failed_node\",\n    \"derived_exit_code\" : {\n      \"return_code\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"signal\" : {\n        \"name\" : \"name\",\n        \"id\" : {\n          \"number\" : 7,\n          \"set\" : true,\n          \"infinite\" : true\n        }\n      },\n      \"status\" : [ \"INVALID\", \"INVALID\" ]\n    },\n    \"maximum_switch_wait_time\" : 9,\n    \"core_spec\" : 5,\n    \"mcs_label\" : \"mcs_label\",\n    \"required_nodes\" : \"required_nodes\",\n    \"tres_bind\" : \"tres_bind\",\n    \"user_id\" : 7,\n    \"selinux_context\" : \"selinux_context\",\n    \"exit_code\" : {\n      \"return_code\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"signal\" : {\n        \"name\" : \"name\",\n        \"id\" : {\n          \"number\" : 7,\n          \"set\" : true,\n          \"infinite\" : true\n        }\n      },\n      \"status\" : [ \"INVALID\", \"INVALID\" ]\n    },\n    \"federation_origin\" : \"federation_origin\",\n    \"container_id\" : \"container_id\",\n    \"shared\" : [ \"none\", \"none\" ],\n    \"tasks_per_board\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"user_name\" : \"user_name\",\n    \"flags\" : [ \"KILL_INVALID_DEPENDENCY\", \"KILL_INVALID_DEPENDENCY\" ],\n    \"standard_input\" : \"standard_input\",\n    \"admin_comment\" : \"admin_comment\",\n    \"cores_per_socket\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"job_state\" : [ \"PENDING\", \"PENDING\" ],\n    \"tasks_per_node\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"current_working_directory\" : \"current_working_directory\",\n    \"standard_error\" : \"standard_error\",\n    \"array_job_id\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"cluster_features\" : \"cluster_features\",\n    \"partition\" : \"partition\",\n    \"threads_per_core\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_alloc_str\" : \"tres_alloc_str\",\n    \"memory_per_cpu\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"cpu_frequency_minimum\" : {\n      \"number\" : 2,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"node_count\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"power\" : {\n      \"flags\" : [ \"\", \"\" ]\n    },\n    \"deadline\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"mail_type\" : [ \"BEGIN\", \"BEGIN\" ],\n    \"memory_per_node\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"state_reason\" : \"state_reason\",\n    \"het_job_offset\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"end_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"sockets_per_board\" : 4,\n    \"nice\" : 5,\n    \"last_sched_evaluation\" : {\n      \"number\" : 2,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_per_node\" : \"tres_per_node\",\n    \"burst_buffer\" : \"burst_buffer\",\n    \"licenses\" : \"licenses\",\n    \"excluded_nodes\" : \"excluded_nodes\",\n    \"array_max_tasks\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"het_job_id\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"sockets_per_node\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"prefer\" : \"prefer\",\n    \"time_limit\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"minimum_cpus_per_node\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tasks_per_socket\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"batch_host\" : \"batch_host\",\n    \"max_cpus\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"job_size_str\" : [ \"job_size_str\", \"job_size_str\" ],\n    \"hold\" : true,\n    \"cpu_frequency_maximum\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"features\" : \"features\",\n    \"het_job_id_set\" : \"het_job_id_set\",\n    \"state_description\" : \"state_description\",\n    \"show_flags\" : [ \"ALL\", \"ALL\" ],\n    \"array_task_id\" : {\n      \"number\" : 5,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"minimum_tmp_disk_per_node\" : {\n      \"number\" : 8,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_req_str\" : \"tres_req_str\",\n    \"burst_buffer_state\" : \"burst_buffer_state\",\n    \"cron\" : \"cron\",\n    \"allocating_node\" : \"allocating_node\",\n    \"tres_per_socket\" : \"tres_per_socket\",\n    \"array_task_string\" : \"array_task_string\",\n    \"submit_time\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"oversubscribe\" : true,\n    \"wckey\" : \"wckey\",\n    \"max_nodes\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"batch_flag\" : true,\n    \"start_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"name\" : \"name\",\n    \"preempt_time\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"contiguous\" : true,\n    \"job_resources\" : {\n      \"nodes\" : {\n        \"allocation\" : [ {\n          \"memory\" : {\n            \"used\" : 8,\n            \"allocated\" : 9\n          },\n          \"cpus\" : {\n            \"count\" : 9,\n            \"used\" : 6\n          },\n          \"name\" : \"name\",\n          \"index\" : 9,\n          \"sockets\" : [ {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          }, {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          } ]\n        }, {\n          \"memory\" : {\n            \"used\" : 8,\n            \"allocated\" : 9\n          },\n          \"cpus\" : {\n            \"count\" : 9,\n            \"used\" : 6\n          },\n          \"name\" : \"name\",\n          \"index\" : 9,\n          \"sockets\" : [ {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          }, {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          } ]\n        } ],\n        \"count\" : 5,\n        \"select_type\" : [ \"AVAILABLE\", \"AVAILABLE\" ],\n        \"whole\" : true,\n        \"list\" : \"list\"\n      },\n      \"threads_per_core\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"cpus\" : 6,\n      \"select_type\" : [ \"CPU\", \"CPU\" ]\n    },\n    \"billable_tres\" : {\n      \"number\" : 9.301444243932576,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"federation_siblings_viable\" : \"federation_siblings_viable\",\n    \"cpus_per_task\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"batch_features\" : \"batch_features\",\n    \"thread_spec\" : 2,\n    \"cpu_frequency_governor\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"gres_detail\" : [ \"gres_detail\", \"gres_detail\" ],\n    \"network\" : \"network\",\n    \"restart_cnt\" : 3,\n    \"resv_name\" : \"resv_name\",\n    \"extra\" : \"extra\",\n    \"delay_boot\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"reboot\" : true,\n    \"cpus\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"standard_output\" : \"standard_output\",\n    \"pre_sus_time\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"suspend_time\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"association_id\" : 5,\n    \"command\" : \"command\",\n    \"tres_freq\" : \"tres_freq\",\n    \"requeue\" : true,\n    \"tres_per_task\" : \"tres_per_task\",\n    \"mail_user\" : \"mail_user\",\n    \"nodes\" : \"nodes\",\n    \"group_id\" : 7,\n    \"job_id\" : 4,\n    \"comment\" : \"comment\",\n    \"account\" : \"account\"\n  }, {\n    \"container\" : \"container\",\n    \"cluster\" : \"cluster\",\n    \"time_minimum\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"memory_per_tres\" : \"memory_per_tres\",\n    \"scheduled_nodes\" : \"scheduled_nodes\",\n    \"minimum_switches\" : 7,\n    \"qos\" : \"qos\",\n    \"resize_time\" : {\n      \"number\" : 5,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"eligible_time\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"exclusive\" : [ \"true\", \"true\" ],\n    \"cpus_per_tres\" : \"cpus_per_tres\",\n    \"preemptable_time\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tasks\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"system_comment\" : \"system_comment\",\n    \"federation_siblings_active\" : \"federation_siblings_active\",\n    \"tasks_per_tres\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tasks_per_core\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"accrue_time\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"dependency\" : \"dependency\",\n    \"group_name\" : \"group_name\",\n    \"profile\" : [ \"NOT_SET\", \"NOT_SET\" ],\n    \"priority\" : {\n      \"number\" : 9,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_per_job\" : \"tres_per_job\",\n    \"failed_node\" : \"failed_node\",\n    \"derived_exit_code\" : {\n      \"return_code\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"signal\" : {\n        \"name\" : \"name\",\n        \"id\" : {\n          \"number\" : 7,\n          \"set\" : true,\n          \"infinite\" : true\n        }\n      },\n      \"status\" : [ \"INVALID\", \"INVALID\" ]\n    },\n    \"maximum_switch_wait_time\" : 9,\n    \"core_spec\" : 5,\n    \"mcs_label\" : \"mcs_label\",\n    \"required_nodes\" : \"required_nodes\",\n    \"tres_bind\" : \"tres_bind\",\n    \"user_id\" : 7,\n    \"selinux_context\" : \"selinux_context\",\n    \"exit_code\" : {\n      \"return_code\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"signal\" : {\n        \"name\" : \"name\",\n        \"id\" : {\n          \"number\" : 7,\n          \"set\" : true,\n          \"infinite\" : true\n        }\n      },\n      \"status\" : [ \"INVALID\", \"INVALID\" ]\n    },\n    \"federation_origin\" : \"federation_origin\",\n    \"container_id\" : \"container_id\",\n    \"shared\" : [ \"none\", \"none\" ],\n    \"tasks_per_board\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"user_name\" : \"user_name\",\n    \"flags\" : [ \"KILL_INVALID_DEPENDENCY\", \"KILL_INVALID_DEPENDENCY\" ],\n    \"standard_input\" : \"standard_input\",\n    \"admin_comment\" : \"admin_comment\",\n    \"cores_per_socket\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"job_state\" : [ \"PENDING\", \"PENDING\" ],\n    \"tasks_per_node\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"current_working_directory\" : \"current_working_directory\",\n    \"standard_error\" : \"standard_error\",\n    \"array_job_id\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"cluster_features\" : \"cluster_features\",\n    \"partition\" : \"partition\",\n    \"threads_per_core\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_alloc_str\" : \"tres_alloc_str\",\n    \"memory_per_cpu\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"cpu_frequency_minimum\" : {\n      \"number\" : 2,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"node_count\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"power\" : {\n      \"flags\" : [ \"\", \"\" ]\n    },\n    \"deadline\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"mail_type\" : [ \"BEGIN\", \"BEGIN\" ],\n    \"memory_per_node\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"state_reason\" : \"state_reason\",\n    \"het_job_offset\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"end_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"sockets_per_board\" : 4,\n    \"nice\" : 5,\n    \"last_sched_evaluation\" : {\n      \"number\" : 2,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_per_node\" : \"tres_per_node\",\n    \"burst_buffer\" : \"burst_buffer\",\n    \"licenses\" : \"licenses\",\n    \"excluded_nodes\" : \"excluded_nodes\",\n    \"array_max_tasks\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"het_job_id\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"sockets_per_node\" : {\n      \"number\" : 0,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"prefer\" : \"prefer\",\n    \"time_limit\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"minimum_cpus_per_node\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tasks_per_socket\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"batch_host\" : \"batch_host\",\n    \"max_cpus\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"job_size_str\" : [ \"job_size_str\", \"job_size_str\" ],\n    \"hold\" : true,\n    \"cpu_frequency_maximum\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"features\" : \"features\",\n    \"het_job_id_set\" : \"het_job_id_set\",\n    \"state_description\" : \"state_description\",\n    \"show_flags\" : [ \"ALL\", \"ALL\" ],\n    \"array_task_id\" : {\n      \"number\" : 5,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"minimum_tmp_disk_per_node\" : {\n      \"number\" : 8,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"tres_req_str\" : \"tres_req_str\",\n    \"burst_buffer_state\" : \"burst_buffer_state\",\n    \"cron\" : \"cron\",\n    \"allocating_node\" : \"allocating_node\",\n    \"tres_per_socket\" : \"tres_per_socket\",\n    \"array_task_string\" : \"array_task_string\",\n    \"submit_time\" : {\n      \"number\" : 4,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"oversubscribe\" : true,\n    \"wckey\" : \"wckey\",\n    \"max_nodes\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"batch_flag\" : true,\n    \"start_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"name\" : \"name\",\n    \"preempt_time\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"contiguous\" : true,\n    \"job_resources\" : {\n      \"nodes\" : {\n        \"allocation\" : [ {\n          \"memory\" : {\n            \"used\" : 8,\n            \"allocated\" : 9\n          },\n          \"cpus\" : {\n            \"count\" : 9,\n            \"used\" : 6\n          },\n          \"name\" : \"name\",\n          \"index\" : 9,\n          \"sockets\" : [ {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          }, {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          } ]\n        }, {\n          \"memory\" : {\n            \"used\" : 8,\n            \"allocated\" : 9\n          },\n          \"cpus\" : {\n            \"count\" : 9,\n            \"used\" : 6\n          },\n          \"name\" : \"name\",\n          \"index\" : 9,\n          \"sockets\" : [ {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          }, {\n            \"cores\" : [ {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            }, {\n              \"index\" : 3,\n              \"status\" : [ \"INVALID\", \"INVALID\" ]\n            } ],\n            \"index\" : 6\n          } ]\n        } ],\n        \"count\" : 5,\n        \"select_type\" : [ \"AVAILABLE\", \"AVAILABLE\" ],\n        \"whole\" : true,\n        \"list\" : \"list\"\n      },\n      \"threads_per_core\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"cpus\" : 6,\n      \"select_type\" : [ \"CPU\", \"CPU\" ]\n    },\n    \"billable_tres\" : {\n      \"number\" : 9.301444243932576,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"federation_siblings_viable\" : \"federation_siblings_viable\",\n    \"cpus_per_task\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"batch_features\" : \"batch_features\",\n    \"thread_spec\" : 2,\n    \"cpu_frequency_governor\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"gres_detail\" : [ \"gres_detail\", \"gres_detail\" ],\n    \"network\" : \"network\",\n    \"restart_cnt\" : 3,\n    \"resv_name\" : \"resv_name\",\n    \"extra\" : \"extra\",\n    \"delay_boot\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"reboot\" : true,\n    \"cpus\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"standard_output\" : \"standard_output\",\n    \"pre_sus_time\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"suspend_time\" : {\n      \"number\" : 1,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"association_id\" : 5,\n    \"command\" : \"command\",\n    \"tres_freq\" : \"tres_freq\",\n    \"requeue\" : true,\n    \"tres_per_task\" : \"tres_per_task\",\n    \"mail_user\" : \"mail_user\",\n    \"nodes\" : \"nodes\",\n    \"group_id\" : 7,\n    \"job_id\" : 4,\n    \"comment\" : \"comment\",\n    \"account\" : \"account\"\n  } ],\n  \"last_update\" : {\n    \"number\" : 9,\n    \"set\" : true,\n    \"infinite\" : true\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    job(s) state information\n        v0.0.41_openapi_job_info_resp\ndefault\n    job(s) state information\n        v0.0.41_openapi_job_info_resp\n\n\nUp\nget /slurm/v0.0.41/licenses/\nget all Slurm tracked license info (slurmV0041GetLicenses)\n\nReturn type\n\nv0.0.41_openapi_licenses_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"licenses\" : [ {\n    \"Used\" : 6,\n    \"LastUpdate\" : 7,\n    \"Total\" : 0,\n    \"Remote\" : true,\n    \"LastConsumed\" : 5,\n    \"LastDeficit\" : 2,\n    \"LicenseName\" : \"LicenseName\",\n    \"Free\" : 1,\n    \"Reserved\" : 5\n  }, {\n    \"Used\" : 6,\n    \"LastUpdate\" : 7,\n    \"Total\" : 0,\n    \"Remote\" : true,\n    \"LastConsumed\" : 5,\n    \"LastDeficit\" : 2,\n    \"LicenseName\" : \"LicenseName\",\n    \"Free\" : 1,\n    \"Reserved\" : 5\n  } ],\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"last_update\" : {\n    \"number\" : 9,\n    \"set\" : true,\n    \"infinite\" : true\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    results of get all licenses\n        v0.0.41_openapi_licenses_resp\ndefault\n    results of get all licenses\n        v0.0.41_openapi_licenses_resp\n\n\nUp\nget /slurm/v0.0.41/node/{node_name}\nget node info (slurmV0041GetNode)\n\nPath parameters\n\nnode_name (required)\nPath Parameter \u2014 Node name default: null \n \nQuery parameters\n\nupdate_time (optional)\nQuery Parameter \u2014 Filter jobs since update timestamp default: null flags (optional)\nQuery Parameter \u2014 Query flags default: null \n \nReturn type\n\nv0.0.41_openapi_nodes_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"nodes\" : [ {\n    \"reason\" : \"reason\",\n    \"gpu_spec\" : \"gpu_spec\",\n    \"slurmd_start_time\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"features\" : [ \"features\", \"features\" ],\n    \"hostname\" : \"hostname\",\n    \"cores\" : 1,\n    \"reason_changed_at\" : {\n      \"number\" : 9,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"reservation\" : \"reservation\",\n    \"tres\" : \"tres\",\n    \"cpu_binding\" : 5,\n    \"state\" : [ \"INVALID\", \"INVALID\" ],\n    \"sockets\" : 6,\n    \"energy\" : {\n      \"current_watts\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"base_consumed_energy\" : 4,\n      \"last_collected\" : 1,\n      \"consumed_energy\" : 7,\n      \"previous_consumed_energy\" : 1,\n      \"average_watts\" : 2\n    },\n    \"partitions\" : [ \"partitions\", \"partitions\" ],\n    \"gres_drained\" : \"gres_drained\",\n    \"weight\" : 6,\n    \"version\" : \"version\",\n    \"gres_used\" : \"gres_used\",\n    \"mcs_label\" : \"mcs_label\",\n    \"real_memory\" : 4,\n    \"instance_id\" : \"instance_id\",\n    \"burstbuffer_network_address\" : \"burstbuffer_network_address\",\n    \"port\" : 1,\n    \"name\" : \"name\",\n    \"resume_after\" : {\n      \"number\" : 9,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"temporary_disk\" : 2,\n    \"tres_used\" : \"tres_used\",\n    \"effective_cpus\" : 3,\n    \"instance_type\" : \"instance_type\",\n    \"external_sensors\" : \"{}\",\n    \"res_cores_per_gpu\" : 5,\n    \"boards\" : 0,\n    \"alloc_cpus\" : 8,\n    \"active_features\" : [ \"active_features\", \"active_features\" ],\n    \"reason_set_by_user\" : \"reason_set_by_user\",\n    \"free_mem\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"alloc_idle_cpus\" : 9,\n    \"extra\" : \"extra\",\n    \"operating_system\" : \"operating_system\",\n    \"power\" : \"{}\",\n    \"architecture\" : \"architecture\",\n    \"owner\" : \"owner\",\n    \"cluster_name\" : \"cluster_name\",\n    \"address\" : \"address\",\n    \"cpus\" : 9,\n    \"tres_weighted\" : 6.438423552598547,\n    \"gres\" : \"gres\",\n    \"threads\" : 1,\n    \"boot_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"alloc_memory\" : 6,\n    \"specialized_memory\" : 7,\n    \"specialized_cpus\" : \"specialized_cpus\",\n    \"specialized_cores\" : 5,\n    \"last_busy\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"comment\" : \"comment\",\n    \"next_state_after_reboot\" : [ \"INVALID\", \"INVALID\" ],\n    \"cpu_load\" : 2\n  }, {\n    \"reason\" : \"reason\",\n    \"gpu_spec\" : \"gpu_spec\",\n    \"slurmd_start_time\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"features\" : [ \"features\", \"features\" ],\n    \"hostname\" : \"hostname\",\n    \"cores\" : 1,\n    \"reason_changed_at\" : {\n      \"number\" : 9,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"reservation\" : \"reservation\",\n    \"tres\" : \"tres\",\n    \"cpu_binding\" : 5,\n    \"state\" : [ \"INVALID\", \"INVALID\" ],\n    \"sockets\" : 6,\n    \"energy\" : {\n      \"current_watts\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"base_consumed_energy\" : 4,\n      \"last_collected\" : 1,\n      \"consumed_energy\" : 7,\n      \"previous_consumed_energy\" : 1,\n      \"average_watts\" : 2\n    },\n    \"partitions\" : [ \"partitions\", \"partitions\" ],\n    \"gres_drained\" : \"gres_drained\",\n    \"weight\" : 6,\n    \"version\" : \"version\",\n    \"gres_used\" : \"gres_used\",\n    \"mcs_label\" : \"mcs_label\",\n    \"real_memory\" : 4,\n    \"instance_id\" : \"instance_id\",\n    \"burstbuffer_network_address\" : \"burstbuffer_network_address\",\n    \"port\" : 1,\n    \"name\" : \"name\",\n    \"resume_after\" : {\n      \"number\" : 9,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"temporary_disk\" : 2,\n    \"tres_used\" : \"tres_used\",\n    \"effective_cpus\" : 3,\n    \"instance_type\" : \"instance_type\",\n    \"external_sensors\" : \"{}\",\n    \"res_cores_per_gpu\" : 5,\n    \"boards\" : 0,\n    \"alloc_cpus\" : 8,\n    \"active_features\" : [ \"active_features\", \"active_features\" ],\n    \"reason_set_by_user\" : \"reason_set_by_user\",\n    \"free_mem\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"alloc_idle_cpus\" : 9,\n    \"extra\" : \"extra\",\n    \"operating_system\" : \"operating_system\",\n    \"power\" : \"{}\",\n    \"architecture\" : \"architecture\",\n    \"owner\" : \"owner\",\n    \"cluster_name\" : \"cluster_name\",\n    \"address\" : \"address\",\n    \"cpus\" : 9,\n    \"tres_weighted\" : 6.438423552598547,\n    \"gres\" : \"gres\",\n    \"threads\" : 1,\n    \"boot_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"alloc_memory\" : 6,\n    \"specialized_memory\" : 7,\n    \"specialized_cpus\" : \"specialized_cpus\",\n    \"specialized_cores\" : 5,\n    \"last_busy\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"comment\" : \"comment\",\n    \"next_state_after_reboot\" : [ \"INVALID\", \"INVALID\" ],\n    \"cpu_load\" : 2\n  } ],\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"last_update\" : {\n    \"number\" : 6,\n    \"set\" : true,\n    \"infinite\" : true\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    node information\n        v0.0.41_openapi_nodes_resp\ndefault\n    node information\n        v0.0.41_openapi_nodes_resp\n\n\nUp\nget /slurm/v0.0.41/nodes/\nget node(s) info (slurmV0041GetNodes)\n\nQuery parameters\n\nupdate_time (optional)\nQuery Parameter \u2014 Filter jobs since update timestamp default: null flags (optional)\nQuery Parameter \u2014 Query flags default: null \n \nReturn type\n\nv0.0.41_openapi_nodes_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"nodes\" : [ {\n    \"reason\" : \"reason\",\n    \"gpu_spec\" : \"gpu_spec\",\n    \"slurmd_start_time\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"features\" : [ \"features\", \"features\" ],\n    \"hostname\" : \"hostname\",\n    \"cores\" : 1,\n    \"reason_changed_at\" : {\n      \"number\" : 9,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"reservation\" : \"reservation\",\n    \"tres\" : \"tres\",\n    \"cpu_binding\" : 5,\n    \"state\" : [ \"INVALID\", \"INVALID\" ],\n    \"sockets\" : 6,\n    \"energy\" : {\n      \"current_watts\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"base_consumed_energy\" : 4,\n      \"last_collected\" : 1,\n      \"consumed_energy\" : 7,\n      \"previous_consumed_energy\" : 1,\n      \"average_watts\" : 2\n    },\n    \"partitions\" : [ \"partitions\", \"partitions\" ],\n    \"gres_drained\" : \"gres_drained\",\n    \"weight\" : 6,\n    \"version\" : \"version\",\n    \"gres_used\" : \"gres_used\",\n    \"mcs_label\" : \"mcs_label\",\n    \"real_memory\" : 4,\n    \"instance_id\" : \"instance_id\",\n    \"burstbuffer_network_address\" : \"burstbuffer_network_address\",\n    \"port\" : 1,\n    \"name\" : \"name\",\n    \"resume_after\" : {\n      \"number\" : 9,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"temporary_disk\" : 2,\n    \"tres_used\" : \"tres_used\",\n    \"effective_cpus\" : 3,\n    \"instance_type\" : \"instance_type\",\n    \"external_sensors\" : \"{}\",\n    \"res_cores_per_gpu\" : 5,\n    \"boards\" : 0,\n    \"alloc_cpus\" : 8,\n    \"active_features\" : [ \"active_features\", \"active_features\" ],\n    \"reason_set_by_user\" : \"reason_set_by_user\",\n    \"free_mem\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"alloc_idle_cpus\" : 9,\n    \"extra\" : \"extra\",\n    \"operating_system\" : \"operating_system\",\n    \"power\" : \"{}\",\n    \"architecture\" : \"architecture\",\n    \"owner\" : \"owner\",\n    \"cluster_name\" : \"cluster_name\",\n    \"address\" : \"address\",\n    \"cpus\" : 9,\n    \"tres_weighted\" : 6.438423552598547,\n    \"gres\" : \"gres\",\n    \"threads\" : 1,\n    \"boot_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"alloc_memory\" : 6,\n    \"specialized_memory\" : 7,\n    \"specialized_cpus\" : \"specialized_cpus\",\n    \"specialized_cores\" : 5,\n    \"last_busy\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"comment\" : \"comment\",\n    \"next_state_after_reboot\" : [ \"INVALID\", \"INVALID\" ],\n    \"cpu_load\" : 2\n  }, {\n    \"reason\" : \"reason\",\n    \"gpu_spec\" : \"gpu_spec\",\n    \"slurmd_start_time\" : {\n      \"number\" : 3,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"features\" : [ \"features\", \"features\" ],\n    \"hostname\" : \"hostname\",\n    \"cores\" : 1,\n    \"reason_changed_at\" : {\n      \"number\" : 9,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"reservation\" : \"reservation\",\n    \"tres\" : \"tres\",\n    \"cpu_binding\" : 5,\n    \"state\" : [ \"INVALID\", \"INVALID\" ],\n    \"sockets\" : 6,\n    \"energy\" : {\n      \"current_watts\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"base_consumed_energy\" : 4,\n      \"last_collected\" : 1,\n      \"consumed_energy\" : 7,\n      \"previous_consumed_energy\" : 1,\n      \"average_watts\" : 2\n    },\n    \"partitions\" : [ \"partitions\", \"partitions\" ],\n    \"gres_drained\" : \"gres_drained\",\n    \"weight\" : 6,\n    \"version\" : \"version\",\n    \"gres_used\" : \"gres_used\",\n    \"mcs_label\" : \"mcs_label\",\n    \"real_memory\" : 4,\n    \"instance_id\" : \"instance_id\",\n    \"burstbuffer_network_address\" : \"burstbuffer_network_address\",\n    \"port\" : 1,\n    \"name\" : \"name\",\n    \"resume_after\" : {\n      \"number\" : 9,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"temporary_disk\" : 2,\n    \"tres_used\" : \"tres_used\",\n    \"effective_cpus\" : 3,\n    \"instance_type\" : \"instance_type\",\n    \"external_sensors\" : \"{}\",\n    \"res_cores_per_gpu\" : 5,\n    \"boards\" : 0,\n    \"alloc_cpus\" : 8,\n    \"active_features\" : [ \"active_features\", \"active_features\" ],\n    \"reason_set_by_user\" : \"reason_set_by_user\",\n    \"free_mem\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"alloc_idle_cpus\" : 9,\n    \"extra\" : \"extra\",\n    \"operating_system\" : \"operating_system\",\n    \"power\" : \"{}\",\n    \"architecture\" : \"architecture\",\n    \"owner\" : \"owner\",\n    \"cluster_name\" : \"cluster_name\",\n    \"address\" : \"address\",\n    \"cpus\" : 9,\n    \"tres_weighted\" : 6.438423552598547,\n    \"gres\" : \"gres\",\n    \"threads\" : 1,\n    \"boot_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"alloc_memory\" : 6,\n    \"specialized_memory\" : 7,\n    \"specialized_cpus\" : \"specialized_cpus\",\n    \"specialized_cores\" : 5,\n    \"last_busy\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"comment\" : \"comment\",\n    \"next_state_after_reboot\" : [ \"INVALID\", \"INVALID\" ],\n    \"cpu_load\" : 2\n  } ],\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"last_update\" : {\n    \"number\" : 6,\n    \"set\" : true,\n    \"infinite\" : true\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    node(s) information\n        v0.0.41_openapi_nodes_resp\ndefault\n    node(s) information\n        v0.0.41_openapi_nodes_resp\n\n\nUp\nget /slurm/v0.0.41/partition/{partition_name}\nget partition info (slurmV0041GetPartition)\n\nPath parameters\n\npartition_name (required)\nPath Parameter \u2014 Partition name default: null \n \nQuery parameters\n\nupdate_time (optional)\nQuery Parameter \u2014 Filter partitions since update timestamp default: null flags (optional)\nQuery Parameter \u2014 Query flags default: null \n \nReturn type\n\nv0.0.41_openapi_partition_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"partitions\" : [ {\n    \"cluster\" : \"cluster\",\n    \"cpus\" : {\n      \"task_binding\" : 6,\n      \"total\" : 1\n    },\n    \"timeouts\" : {\n      \"resume\" : {\n        \"number\" : 9,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"suspend\" : {\n        \"number\" : 6,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"groups\" : {\n      \"allowed\" : \"allowed\"\n    },\n    \"alternate\" : \"alternate\",\n    \"select_type\" : [ \"CPU\", \"CPU\" ],\n    \"suspend_time\" : {\n      \"number\" : 8,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"priority\" : {\n      \"tier\" : 9,\n      \"job_factor\" : 5\n    },\n    \"node_sets\" : \"node_sets\",\n    \"maximums\" : {\n      \"shares\" : 1,\n      \"nodes\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"over_time_limit\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"cpus_per_node\" : {\n        \"number\" : 3,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"cpus_per_socket\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"partition_memory_per_node\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"oversubscribe\" : {\n        \"jobs\" : 6,\n        \"flags\" : [ \"force\", \"force\" ]\n      },\n      \"memory_per_cpu\" : 4,\n      \"time\" : {\n        \"number\" : 7,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"partition_memory_per_cpu\" : {\n        \"number\" : 7,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"nodes\" : {\n      \"configured\" : \"configured\",\n      \"total\" : 0,\n      \"allowed_allocation\" : \"allowed_allocation\"\n    },\n    \"partition\" : {\n      \"state\" : [ \"INACTIVE\", \"INACTIVE\" ]\n    },\n    \"qos\" : {\n      \"deny\" : \"deny\",\n      \"allowed\" : \"allowed\",\n      \"assigned\" : \"assigned\"\n    },\n    \"defaults\" : {\n      \"partition_memory_per_node\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"memory_per_cpu\" : 5,\n      \"time\" : {\n        \"number\" : 7,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"job\" : \"job\",\n      \"partition_memory_per_cpu\" : {\n        \"number\" : 5,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"name\" : \"name\",\n    \"tres\" : {\n      \"configured\" : \"configured\",\n      \"billing_weights\" : \"billing_weights\"\n    },\n    \"accounts\" : {\n      \"deny\" : \"deny\",\n      \"allowed\" : \"allowed\"\n    },\n    \"minimums\" : {\n      \"nodes\" : 4\n    },\n    \"grace_time\" : 9\n  }, {\n    \"cluster\" : \"cluster\",\n    \"cpus\" : {\n      \"task_binding\" : 6,\n      \"total\" : 1\n    },\n    \"timeouts\" : {\n      \"resume\" : {\n        \"number\" : 9,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"suspend\" : {\n        \"number\" : 6,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"groups\" : {\n      \"allowed\" : \"allowed\"\n    },\n    \"alternate\" : \"alternate\",\n    \"select_type\" : [ \"CPU\", \"CPU\" ],\n    \"suspend_time\" : {\n      \"number\" : 8,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"priority\" : {\n      \"tier\" : 9,\n      \"job_factor\" : 5\n    },\n    \"node_sets\" : \"node_sets\",\n    \"maximums\" : {\n      \"shares\" : 1,\n      \"nodes\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"over_time_limit\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"cpus_per_node\" : {\n        \"number\" : 3,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"cpus_per_socket\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"partition_memory_per_node\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"oversubscribe\" : {\n        \"jobs\" : 6,\n        \"flags\" : [ \"force\", \"force\" ]\n      },\n      \"memory_per_cpu\" : 4,\n      \"time\" : {\n        \"number\" : 7,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"partition_memory_per_cpu\" : {\n        \"number\" : 7,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"nodes\" : {\n      \"configured\" : \"configured\",\n      \"total\" : 0,\n      \"allowed_allocation\" : \"allowed_allocation\"\n    },\n    \"partition\" : {\n      \"state\" : [ \"INACTIVE\", \"INACTIVE\" ]\n    },\n    \"qos\" : {\n      \"deny\" : \"deny\",\n      \"allowed\" : \"allowed\",\n      \"assigned\" : \"assigned\"\n    },\n    \"defaults\" : {\n      \"partition_memory_per_node\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"memory_per_cpu\" : 5,\n      \"time\" : {\n        \"number\" : 7,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"job\" : \"job\",\n      \"partition_memory_per_cpu\" : {\n        \"number\" : 5,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"name\" : \"name\",\n    \"tres\" : {\n      \"configured\" : \"configured\",\n      \"billing_weights\" : \"billing_weights\"\n    },\n    \"accounts\" : {\n      \"deny\" : \"deny\",\n      \"allowed\" : \"allowed\"\n    },\n    \"minimums\" : {\n      \"nodes\" : 4\n    },\n    \"grace_time\" : 9\n  } ],\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"last_update\" : {\n    \"number\" : 9,\n    \"set\" : true,\n    \"infinite\" : true\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    partition information\n        v0.0.41_openapi_partition_resp\ndefault\n    partition information\n        v0.0.41_openapi_partition_resp\n\n\nUp\nget /slurm/v0.0.41/partitions/\nget all partition info (slurmV0041GetPartitions)\n\nQuery parameters\n\nupdate_time (optional)\nQuery Parameter \u2014 Filter partitions since update timestamp default: null flags (optional)\nQuery Parameter \u2014 Query flags default: null \n \nReturn type\n\nv0.0.41_openapi_partition_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"partitions\" : [ {\n    \"cluster\" : \"cluster\",\n    \"cpus\" : {\n      \"task_binding\" : 6,\n      \"total\" : 1\n    },\n    \"timeouts\" : {\n      \"resume\" : {\n        \"number\" : 9,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"suspend\" : {\n        \"number\" : 6,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"groups\" : {\n      \"allowed\" : \"allowed\"\n    },\n    \"alternate\" : \"alternate\",\n    \"select_type\" : [ \"CPU\", \"CPU\" ],\n    \"suspend_time\" : {\n      \"number\" : 8,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"priority\" : {\n      \"tier\" : 9,\n      \"job_factor\" : 5\n    },\n    \"node_sets\" : \"node_sets\",\n    \"maximums\" : {\n      \"shares\" : 1,\n      \"nodes\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"over_time_limit\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"cpus_per_node\" : {\n        \"number\" : 3,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"cpus_per_socket\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"partition_memory_per_node\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"oversubscribe\" : {\n        \"jobs\" : 6,\n        \"flags\" : [ \"force\", \"force\" ]\n      },\n      \"memory_per_cpu\" : 4,\n      \"time\" : {\n        \"number\" : 7,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"partition_memory_per_cpu\" : {\n        \"number\" : 7,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"nodes\" : {\n      \"configured\" : \"configured\",\n      \"total\" : 0,\n      \"allowed_allocation\" : \"allowed_allocation\"\n    },\n    \"partition\" : {\n      \"state\" : [ \"INACTIVE\", \"INACTIVE\" ]\n    },\n    \"qos\" : {\n      \"deny\" : \"deny\",\n      \"allowed\" : \"allowed\",\n      \"assigned\" : \"assigned\"\n    },\n    \"defaults\" : {\n      \"partition_memory_per_node\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"memory_per_cpu\" : 5,\n      \"time\" : {\n        \"number\" : 7,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"job\" : \"job\",\n      \"partition_memory_per_cpu\" : {\n        \"number\" : 5,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"name\" : \"name\",\n    \"tres\" : {\n      \"configured\" : \"configured\",\n      \"billing_weights\" : \"billing_weights\"\n    },\n    \"accounts\" : {\n      \"deny\" : \"deny\",\n      \"allowed\" : \"allowed\"\n    },\n    \"minimums\" : {\n      \"nodes\" : 4\n    },\n    \"grace_time\" : 9\n  }, {\n    \"cluster\" : \"cluster\",\n    \"cpus\" : {\n      \"task_binding\" : 6,\n      \"total\" : 1\n    },\n    \"timeouts\" : {\n      \"resume\" : {\n        \"number\" : 9,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"suspend\" : {\n        \"number\" : 6,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"groups\" : {\n      \"allowed\" : \"allowed\"\n    },\n    \"alternate\" : \"alternate\",\n    \"select_type\" : [ \"CPU\", \"CPU\" ],\n    \"suspend_time\" : {\n      \"number\" : 8,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"priority\" : {\n      \"tier\" : 9,\n      \"job_factor\" : 5\n    },\n    \"node_sets\" : \"node_sets\",\n    \"maximums\" : {\n      \"shares\" : 1,\n      \"nodes\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"over_time_limit\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"cpus_per_node\" : {\n        \"number\" : 3,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"cpus_per_socket\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"partition_memory_per_node\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"oversubscribe\" : {\n        \"jobs\" : 6,\n        \"flags\" : [ \"force\", \"force\" ]\n      },\n      \"memory_per_cpu\" : 4,\n      \"time\" : {\n        \"number\" : 7,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"partition_memory_per_cpu\" : {\n        \"number\" : 7,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"nodes\" : {\n      \"configured\" : \"configured\",\n      \"total\" : 0,\n      \"allowed_allocation\" : \"allowed_allocation\"\n    },\n    \"partition\" : {\n      \"state\" : [ \"INACTIVE\", \"INACTIVE\" ]\n    },\n    \"qos\" : {\n      \"deny\" : \"deny\",\n      \"allowed\" : \"allowed\",\n      \"assigned\" : \"assigned\"\n    },\n    \"defaults\" : {\n      \"partition_memory_per_node\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"memory_per_cpu\" : 5,\n      \"time\" : {\n        \"number\" : 7,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"job\" : \"job\",\n      \"partition_memory_per_cpu\" : {\n        \"number\" : 5,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"name\" : \"name\",\n    \"tres\" : {\n      \"configured\" : \"configured\",\n      \"billing_weights\" : \"billing_weights\"\n    },\n    \"accounts\" : {\n      \"deny\" : \"deny\",\n      \"allowed\" : \"allowed\"\n    },\n    \"minimums\" : {\n      \"nodes\" : 4\n    },\n    \"grace_time\" : 9\n  } ],\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"last_update\" : {\n    \"number\" : 9,\n    \"set\" : true,\n    \"infinite\" : true\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    partition information\n        v0.0.41_openapi_partition_resp\ndefault\n    partition information\n        v0.0.41_openapi_partition_resp\n\n\nUp\nget /slurm/v0.0.41/ping/\nping test (slurmV0041GetPing)\n\nReturn type\n\nv0.0.41_openapi_ping_array_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"pings\" : [ {\n    \"mode\" : \"mode\",\n    \"hostname\" : \"hostname\",\n    \"latency\" : 0,\n    \"pinged\" : \"pinged\"\n  }, {\n    \"mode\" : \"mode\",\n    \"hostname\" : \"hostname\",\n    \"latency\" : 0,\n    \"pinged\" : \"pinged\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    results of ping test\n        v0.0.41_openapi_ping_array_resp\ndefault\n    results of ping test\n        v0.0.41_openapi_ping_array_resp\n\n\nUp\nget /slurm/v0.0.41/reconfigure/\nrequest slurmctld reconfigure (slurmV0041GetReconfigure)\n\nReturn type\n\nv0.0.41_openapi_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    reconfigure request result\n        v0.0.41_openapi_resp\ndefault\n    reconfigure request result\n        v0.0.41_openapi_resp\n\n\nUp\nget /slurm/v0.0.41/reservation/{reservation_name}\nget reservation info (slurmV0041GetReservation)\n\nPath parameters\n\nreservation_name (required)\nPath Parameter \u2014 Reservation name default: null \n \nQuery parameters\n\nupdate_time (optional)\nQuery Parameter \u2014 Filter reservations since update timestamp default: null \n \nReturn type\n\nv0.0.41_openapi_reservation_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"reservations\" : [ {\n    \"end_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"flags\" : [ \"MAINT\", \"MAINT\" ],\n    \"groups\" : \"groups\",\n    \"users\" : \"users\",\n    \"max_start_delay\" : 1,\n    \"features\" : \"features\",\n    \"start_time\" : {\n      \"number\" : 2,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"burst_buffer\" : \"burst_buffer\",\n    \"licenses\" : \"licenses\",\n    \"partition\" : \"partition\",\n    \"watts\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"core_specializations\" : [ {\n      \"node\" : \"node\",\n      \"core\" : \"core\"\n    }, {\n      \"node\" : \"node\",\n      \"core\" : \"core\"\n    } ],\n    \"name\" : \"name\",\n    \"tres\" : \"tres\",\n    \"accounts\" : \"accounts\",\n    \"node_count\" : 5,\n    \"node_list\" : \"node_list\",\n    \"purge_completed\" : {\n      \"time\" : {\n        \"number\" : 5,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"core_count\" : 0\n  }, {\n    \"end_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"flags\" : [ \"MAINT\", \"MAINT\" ],\n    \"groups\" : \"groups\",\n    \"users\" : \"users\",\n    \"max_start_delay\" : 1,\n    \"features\" : \"features\",\n    \"start_time\" : {\n      \"number\" : 2,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"burst_buffer\" : \"burst_buffer\",\n    \"licenses\" : \"licenses\",\n    \"partition\" : \"partition\",\n    \"watts\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"core_specializations\" : [ {\n      \"node\" : \"node\",\n      \"core\" : \"core\"\n    }, {\n      \"node\" : \"node\",\n      \"core\" : \"core\"\n    } ],\n    \"name\" : \"name\",\n    \"tres\" : \"tres\",\n    \"accounts\" : \"accounts\",\n    \"node_count\" : 5,\n    \"node_list\" : \"node_list\",\n    \"purge_completed\" : {\n      \"time\" : {\n        \"number\" : 5,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"core_count\" : 0\n  } ],\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"last_update\" : {\n    \"number\" : 9,\n    \"set\" : true,\n    \"infinite\" : true\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    reservation information\n        v0.0.41_openapi_reservation_resp\ndefault\n    reservation information\n        v0.0.41_openapi_reservation_resp\n\n\nUp\nget /slurm/v0.0.41/reservations/\nget all reservation info (slurmV0041GetReservations)\n\nQuery parameters\n\nupdate_time (optional)\nQuery Parameter \u2014 Filter reservations since update timestamp default: null \n \nReturn type\n\nv0.0.41_openapi_reservation_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"reservations\" : [ {\n    \"end_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"flags\" : [ \"MAINT\", \"MAINT\" ],\n    \"groups\" : \"groups\",\n    \"users\" : \"users\",\n    \"max_start_delay\" : 1,\n    \"features\" : \"features\",\n    \"start_time\" : {\n      \"number\" : 2,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"burst_buffer\" : \"burst_buffer\",\n    \"licenses\" : \"licenses\",\n    \"partition\" : \"partition\",\n    \"watts\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"core_specializations\" : [ {\n      \"node\" : \"node\",\n      \"core\" : \"core\"\n    }, {\n      \"node\" : \"node\",\n      \"core\" : \"core\"\n    } ],\n    \"name\" : \"name\",\n    \"tres\" : \"tres\",\n    \"accounts\" : \"accounts\",\n    \"node_count\" : 5,\n    \"node_list\" : \"node_list\",\n    \"purge_completed\" : {\n      \"time\" : {\n        \"number\" : 5,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"core_count\" : 0\n  }, {\n    \"end_time\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"flags\" : [ \"MAINT\", \"MAINT\" ],\n    \"groups\" : \"groups\",\n    \"users\" : \"users\",\n    \"max_start_delay\" : 1,\n    \"features\" : \"features\",\n    \"start_time\" : {\n      \"number\" : 2,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"burst_buffer\" : \"burst_buffer\",\n    \"licenses\" : \"licenses\",\n    \"partition\" : \"partition\",\n    \"watts\" : {\n      \"number\" : 7,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"core_specializations\" : [ {\n      \"node\" : \"node\",\n      \"core\" : \"core\"\n    }, {\n      \"node\" : \"node\",\n      \"core\" : \"core\"\n    } ],\n    \"name\" : \"name\",\n    \"tres\" : \"tres\",\n    \"accounts\" : \"accounts\",\n    \"node_count\" : 5,\n    \"node_list\" : \"node_list\",\n    \"purge_completed\" : {\n      \"time\" : {\n        \"number\" : 5,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"core_count\" : 0\n  } ],\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"last_update\" : {\n    \"number\" : 9,\n    \"set\" : true,\n    \"infinite\" : true\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    reservation information\n        v0.0.41_openapi_reservation_resp\ndefault\n    reservation information\n        v0.0.41_openapi_reservation_resp\n\n\nUp\nget /slurm/v0.0.41/shares\nget fairshare info (slurmV0041GetShares)\n\nQuery parameters\n\naccounts (optional)\nQuery Parameter \u2014 Accounts to query default: null users (optional)\nQuery Parameter \u2014 Users to query default: null \n \nReturn type\n\nv0.0.41_openapi_shares_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"shares\" : {\n    \"shares\" : [ {\n      \"cluster\" : \"cluster\",\n      \"parent\" : \"parent\",\n      \"shares_normalized\" : {\n        \"number\" : 6.027456183070403,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"usage\" : 9,\n      \"fairshare\" : {\n        \"level\" : 2.027123023002322,\n        \"factor\" : 3.616076749251911\n      },\n      \"type\" : [ \"USER\", \"USER\" ],\n      \"effective_usage\" : 2.3021358869347655,\n      \"shares\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"partition\" : \"partition\",\n      \"usage_normalized\" : {\n        \"number\" : 7.061401241503109,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"name\" : \"name\",\n      \"tres\" : {\n        \"run_seconds\" : [ {\n          \"name\" : \"name\",\n          \"value\" : {\n            \"number\" : 5,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }, {\n          \"name\" : \"name\",\n          \"value\" : {\n            \"number\" : 5,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        } ],\n        \"usage\" : [ {\n          \"name\" : \"name\",\n          \"value\" : 5.637376656633329\n        }, {\n          \"name\" : \"name\",\n          \"value\" : 5.637376656633329\n        } ],\n        \"group_minutes\" : [ {\n          \"name\" : \"name\",\n          \"value\" : {\n            \"number\" : 5,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }, {\n          \"name\" : \"name\",\n          \"value\" : {\n            \"number\" : 5,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        } ]\n      },\n      \"id\" : 0\n    }, {\n      \"cluster\" : \"cluster\",\n      \"parent\" : \"parent\",\n      \"shares_normalized\" : {\n        \"number\" : 6.027456183070403,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"usage\" : 9,\n      \"fairshare\" : {\n        \"level\" : 2.027123023002322,\n        \"factor\" : 3.616076749251911\n      },\n      \"type\" : [ \"USER\", \"USER\" ],\n      \"effective_usage\" : 2.3021358869347655,\n      \"shares\" : {\n        \"number\" : 1,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"partition\" : \"partition\",\n      \"usage_normalized\" : {\n        \"number\" : 7.061401241503109,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"name\" : \"name\",\n      \"tres\" : {\n        \"run_seconds\" : [ {\n          \"name\" : \"name\",\n          \"value\" : {\n            \"number\" : 5,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }, {\n          \"name\" : \"name\",\n          \"value\" : {\n            \"number\" : 5,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        } ],\n        \"usage\" : [ {\n          \"name\" : \"name\",\n          \"value\" : 5.637376656633329\n        }, {\n          \"name\" : \"name\",\n          \"value\" : 5.637376656633329\n        } ],\n        \"group_minutes\" : [ {\n          \"name\" : \"name\",\n          \"value\" : {\n            \"number\" : 5,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }, {\n          \"name\" : \"name\",\n          \"value\" : {\n            \"number\" : 5,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        } ]\n      },\n      \"id\" : 0\n    } ],\n    \"total_shares\" : 4\n  },\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    shares information\n        v0.0.41_openapi_shares_resp\ndefault\n    shares information\n        v0.0.41_openapi_shares_resp\n\n\nUp\npost /slurm/v0.0.41/job/{job_id}\nupdate job (slurmV0041PostJob)\n\nPath parameters\n\njob_id (required)\nPath Parameter \u2014 Job ID default: null \n \nConsumes\n    This API call consumes the following media types via the Content-Type request header:\n    \napplication/json\napplication/x-yaml\n\nRequest body\n\nv0.0.41_job_desc_msg v0.0.41_job_desc_msg (optional)\nBody Parameter \u2014  \n \nReturn type\n\nv0.0.41_openapi_job_post_response\n\n\nExample data\nContent-Type: application/json\n{\n  \"job_id\" : \"job_id\",\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"step_id\" : \"step_id\",\n  \"results\" : [ {\n    \"job_id\" : 0,\n    \"why\" : \"why\",\n    \"error_code\" : 6,\n    \"step_id\" : \"step_id\",\n    \"error\" : \"error\"\n  }, {\n    \"job_id\" : 0,\n    \"why\" : \"why\",\n    \"error_code\" : 6,\n    \"step_id\" : \"step_id\",\n    \"error\" : \"error\"\n  } ],\n  \"job_submit_user_msg\" : \"job_submit_user_msg\",\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    job update result\n        v0.0.41_openapi_job_post_response\ndefault\n    job update result\n        v0.0.41_openapi_job_post_response\n\n\nUp\npost /slurm/v0.0.41/job/allocate\nsubmit new job allocation without any steps that must be signaled to stop (slurmV0041PostJobAllocate)\n\nConsumes\n    This API call consumes the following media types via the Content-Type request header:\n    \napplication/json\napplication/x-yaml\n\nRequest body\n\nv0.0.41_job_alloc_req v0.0.41_job_alloc_req (optional)\nBody Parameter \u2014  \n \nReturn type\n\nv0.0.41_openapi_job_alloc_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"job_id\" : 0,\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"job_submit_user_msg\" : \"job_submit_user_msg\",\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    job allocation response\n        v0.0.41_openapi_job_alloc_resp\ndefault\n    job allocation response\n        v0.0.41_openapi_job_alloc_resp\n\n\nUp\npost /slurm/v0.0.41/job/submit\nsubmit new job (slurmV0041PostJobSubmit)\n\nConsumes\n    This API call consumes the following media types via the Content-Type request header:\n    \napplication/json\napplication/x-yaml\n\nRequest body\n\nv0.0.41_job_submit_req v0.0.41_job_submit_req (optional)\nBody Parameter \u2014  \n \nReturn type\n\nv0.0.41_openapi_job_submit_response\n\n\nExample data\nContent-Type: application/json\n{\n  \"result\" : {\n    \"job_id\" : 0,\n    \"error_code\" : 6,\n    \"step_id\" : \"step_id\",\n    \"error\" : \"error\",\n    \"job_submit_user_msg\" : \"job_submit_user_msg\"\n  },\n  \"job_id\" : 1,\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"step_id\" : \"step_id\",\n  \"job_submit_user_msg\" : \"job_submit_user_msg\",\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    job submission response\n        v0.0.41_openapi_job_submit_response\ndefault\n    job submission response\n        v0.0.41_openapi_job_submit_response\n\n\nUp\npost /slurm/v0.0.41/node/{node_name}\nupdate node properties (slurmV0041PostNode)\n\nPath parameters\n\nnode_name (required)\nPath Parameter \u2014 Node name default: null \n \nConsumes\n    This API call consumes the following media types via the Content-Type request header:\n    \napplication/json\napplication/x-yaml\n\nRequest body\n\nv0.0.41_update_node_msg v0.0.41_update_node_msg (optional)\nBody Parameter \u2014  \n \nReturn type\n\nv0.0.41_openapi_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    node update request result\n        v0.0.41_openapi_resp\ndefault\n    node update request result\n        v0.0.41_openapi_resp\nSlurmdb\n\nUp\ndelete /slurmdb/v0.0.41/account/{account_name}\nDelete account (slurmdbV0041DeleteAccount)\n\nPath parameters\n\naccount_name (required)\nPath Parameter \u2014 Account name default: null \n \nReturn type\n\nv0.0.41_openapi_accounts_removed_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"removed_accounts\" : [ \"removed_accounts\", \"removed_accounts\" ],\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    Status of account deletion request\n        v0.0.41_openapi_accounts_removed_resp\ndefault\n    Status of account deletion request\n        v0.0.41_openapi_accounts_removed_resp\n\n\nUp\ndelete /slurmdb/v0.0.41/association/\nDelete association (slurmdbV0041DeleteAssociation)\n\nQuery parameters\n\naccount (optional)\nQuery Parameter \u2014 CSV accounts list default: null cluster (optional)\nQuery Parameter \u2014 CSV clusters list default: null default_qos (optional)\nQuery Parameter \u2014 CSV QOS list default: null format (optional)\nQuery Parameter \u2014 CSV format list default: null id (optional)\nQuery Parameter \u2014 CSV id list default: null only_defaults (optional)\nQuery Parameter \u2014 Filter to only defaults default: null parent_account (optional)\nQuery Parameter \u2014 CSV names of parent account default: null partition (optional)\nQuery Parameter \u2014 CSV partition name list default: null qos (optional)\nQuery Parameter \u2014 CSV QOS list default: null usage_end (optional)\nQuery Parameter \u2014 Usage end (UNIX timestamp) default: null usage_start (optional)\nQuery Parameter \u2014 Usage start (UNIX timestamp) default: null user (optional)\nQuery Parameter \u2014 CSV user list default: null with_usage (optional)\nQuery Parameter \u2014 Include usage default: null with_deleted (optional)\nQuery Parameter \u2014 Include deleted associations default: null with_raw_qos (optional)\nQuery Parameter \u2014 Include a raw qos or delta_qos default: null with_sub_accts (optional)\nQuery Parameter \u2014 Include sub acct information default: null without_parent_info (optional)\nQuery Parameter \u2014 Exclude parent id/name default: null without_parent_limits (optional)\nQuery Parameter \u2014 Exclude limits from parents default: null \n \nReturn type\n\nv0.0.41_openapi_assocs_removed_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"removed_associations\" : [ \"removed_associations\", \"removed_associations\" ],\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    Status of associations delete request\n        v0.0.41_openapi_assocs_removed_resp\ndefault\n    Status of associations delete request\n        v0.0.41_openapi_assocs_removed_resp\n\n\nUp\ndelete /slurmdb/v0.0.41/associations/\nDelete associations (slurmdbV0041DeleteAssociations)\n\nQuery parameters\n\naccount (optional)\nQuery Parameter \u2014 CSV accounts list default: null cluster (optional)\nQuery Parameter \u2014 CSV clusters list default: null default_qos (optional)\nQuery Parameter \u2014 CSV QOS list default: null format (optional)\nQuery Parameter \u2014 CSV format list default: null id (optional)\nQuery Parameter \u2014 CSV id list default: null only_defaults (optional)\nQuery Parameter \u2014 Filter to only defaults default: null parent_account (optional)\nQuery Parameter \u2014 CSV names of parent account default: null partition (optional)\nQuery Parameter \u2014 CSV partition name list default: null qos (optional)\nQuery Parameter \u2014 CSV QOS list default: null usage_end (optional)\nQuery Parameter \u2014 Usage end (UNIX timestamp) default: null usage_start (optional)\nQuery Parameter \u2014 Usage start (UNIX timestamp) default: null user (optional)\nQuery Parameter \u2014 CSV user list default: null with_usage (optional)\nQuery Parameter \u2014 Include usage default: null with_deleted (optional)\nQuery Parameter \u2014 Include deleted associations default: null with_raw_qos (optional)\nQuery Parameter \u2014 Include a raw qos or delta_qos default: null with_sub_accts (optional)\nQuery Parameter \u2014 Include sub acct information default: null without_parent_info (optional)\nQuery Parameter \u2014 Exclude parent id/name default: null without_parent_limits (optional)\nQuery Parameter \u2014 Exclude limits from parents default: null \n \nReturn type\n\nv0.0.41_openapi_assocs_removed_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"removed_associations\" : [ \"removed_associations\", \"removed_associations\" ],\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    List of associations deleted\n        v0.0.41_openapi_assocs_removed_resp\ndefault\n    List of associations deleted\n        v0.0.41_openapi_assocs_removed_resp\n\n\nUp\ndelete /slurmdb/v0.0.41/cluster/{cluster_name}\nDelete cluster (slurmdbV0041DeleteCluster)\n\nPath parameters\n\ncluster_name (required)\nPath Parameter \u2014 Cluster name default: null \n \nQuery parameters\n\nclassification (optional)\nQuery Parameter \u2014 Type of machine default: null cluster (optional)\nQuery Parameter \u2014 CSV cluster list default: null federation (optional)\nQuery Parameter \u2014 CSV federation list default: null flags (optional)\nQuery Parameter \u2014 Query flags default: null format (optional)\nQuery Parameter \u2014 CSV format list default: null rpc_version (optional)\nQuery Parameter \u2014 CSV RPC version list default: null usage_end (optional)\nQuery Parameter \u2014 Usage end (UNIX timestamp) default: null usage_start (optional)\nQuery Parameter \u2014 Usage start (UNIX timestamp) default: null with_deleted (optional)\nQuery Parameter \u2014 Include deleted clusters default: null with_usage (optional)\nQuery Parameter \u2014 Include usage default: null \n \nReturn type\n\nv0.0.41_openapi_clusters_removed_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"deleted_clusters\" : [ \"deleted_clusters\", \"deleted_clusters\" ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    Result of delete cluster request\n        v0.0.41_openapi_clusters_removed_resp\ndefault\n    Result of delete cluster request\n        v0.0.41_openapi_clusters_removed_resp\n\n\nUp\ndelete /slurmdb/v0.0.41/qos/{qos}\nDelete QOS (slurmdbV0041DeleteSingleQos)\n\nPath parameters\n\nqos (required)\nPath Parameter \u2014 QOS name default: null \n \nReturn type\n\nv0.0.41_openapi_slurmdbd_qos_removed_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"removed_qos\" : [ \"removed_qos\", \"removed_qos\" ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    results of ping test\n        v0.0.41_openapi_slurmdbd_qos_removed_resp\ndefault\n    results of ping test\n        v0.0.41_openapi_slurmdbd_qos_removed_resp\n\n\nUp\ndelete /slurmdb/v0.0.41/user/{name}\nDelete user (slurmdbV0041DeleteUser)\n\nPath parameters\n\nname (required)\nPath Parameter \u2014 User name default: null \n \nReturn type\n\nv0.0.41_openapi_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    Result of user delete request\n        v0.0.41_openapi_resp\ndefault\n    Result of user delete request\n        v0.0.41_openapi_resp\n\n\nUp\ndelete /slurmdb/v0.0.41/wckey/{id}\nDelete wckey (slurmdbV0041DeleteWckey)\n\nPath parameters\n\nid (required)\nPath Parameter \u2014 wckey id default: null \n \nReturn type\n\nv0.0.41_openapi_wckey_removed_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"deleted_wckeys\" : [ \"deleted_wckeys\", \"deleted_wckeys\" ],\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    Result of wckey deletion request\n        v0.0.41_openapi_wckey_removed_resp\ndefault\n    Result of wckey deletion request\n        v0.0.41_openapi_wckey_removed_resp\n\n\nUp\nget /slurmdb/v0.0.41/account/{account_name}\nGet account info (slurmdbV0041GetAccount)\n\nPath parameters\n\naccount_name (required)\nPath Parameter \u2014 Account name default: null \n \nQuery parameters\n\nwith_assocs (optional)\nQuery Parameter \u2014 Include associations default: null with_coords (optional)\nQuery Parameter \u2014 Include coordinators default: null with_deleted (optional)\nQuery Parameter \u2014 Include deleted default: null \n \nReturn type\n\nv0.0.41_openapi_accounts_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"accounts\" : [ {\n    \"associations\" : [ {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    }, {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    } ],\n    \"coordinators\" : [ {\n      \"name\" : \"name\",\n      \"direct\" : true\n    }, {\n      \"name\" : \"name\",\n      \"direct\" : true\n    } ],\n    \"organization\" : \"organization\",\n    \"name\" : \"name\",\n    \"flags\" : [ \"DELETED\", \"DELETED\" ],\n    \"description\" : \"description\"\n  }, {\n    \"associations\" : [ {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    }, {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    } ],\n    \"coordinators\" : [ {\n      \"name\" : \"name\",\n      \"direct\" : true\n    }, {\n      \"name\" : \"name\",\n      \"direct\" : true\n    } ],\n    \"organization\" : \"organization\",\n    \"name\" : \"name\",\n    \"flags\" : [ \"DELETED\", \"DELETED\" ],\n    \"description\" : \"description\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    List of accounts\n        v0.0.41_openapi_accounts_resp\ndefault\n    List of accounts\n        v0.0.41_openapi_accounts_resp\n\n\nUp\nget /slurmdb/v0.0.41/accounts/\nGet account list (slurmdbV0041GetAccounts)\n\nQuery parameters\n\ndescription (optional)\nQuery Parameter \u2014 CSV description list default: null DELETED (optional)\nQuery Parameter \u2014 include deleted associations default: null WithAssociations (optional)\nQuery Parameter \u2014 query includes associations default: null WithCoordinators (optional)\nQuery Parameter \u2014 query includes coordinators default: null NoUsersAreCoords (optional)\nQuery Parameter \u2014 remove users as coordinators default: null UsersAreCoords (optional)\nQuery Parameter \u2014 users are coordinators default: null \n \nReturn type\n\nv0.0.41_openapi_accounts_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"accounts\" : [ {\n    \"associations\" : [ {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    }, {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    } ],\n    \"coordinators\" : [ {\n      \"name\" : \"name\",\n      \"direct\" : true\n    }, {\n      \"name\" : \"name\",\n      \"direct\" : true\n    } ],\n    \"organization\" : \"organization\",\n    \"name\" : \"name\",\n    \"flags\" : [ \"DELETED\", \"DELETED\" ],\n    \"description\" : \"description\"\n  }, {\n    \"associations\" : [ {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    }, {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    } ],\n    \"coordinators\" : [ {\n      \"name\" : \"name\",\n      \"direct\" : true\n    }, {\n      \"name\" : \"name\",\n      \"direct\" : true\n    } ],\n    \"organization\" : \"organization\",\n    \"name\" : \"name\",\n    \"flags\" : [ \"DELETED\", \"DELETED\" ],\n    \"description\" : \"description\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    List of accounts\n        v0.0.41_openapi_accounts_resp\ndefault\n    List of accounts\n        v0.0.41_openapi_accounts_resp\n\n\nUp\nget /slurmdb/v0.0.41/association/\nGet association info (slurmdbV0041GetAssociation)\n\nQuery parameters\n\naccount (optional)\nQuery Parameter \u2014 CSV accounts list default: null cluster (optional)\nQuery Parameter \u2014 CSV clusters list default: null default_qos (optional)\nQuery Parameter \u2014 CSV QOS list default: null format (optional)\nQuery Parameter \u2014 CSV format list default: null id (optional)\nQuery Parameter \u2014 CSV id list default: null only_defaults (optional)\nQuery Parameter \u2014 Filter to only defaults default: null parent_account (optional)\nQuery Parameter \u2014 CSV names of parent account default: null partition (optional)\nQuery Parameter \u2014 CSV partition name list default: null qos (optional)\nQuery Parameter \u2014 CSV QOS list default: null usage_end (optional)\nQuery Parameter \u2014 Usage end (UNIX timestamp) default: null usage_start (optional)\nQuery Parameter \u2014 Usage start (UNIX timestamp) default: null user (optional)\nQuery Parameter \u2014 CSV user list default: null with_usage (optional)\nQuery Parameter \u2014 Include usage default: null with_deleted (optional)\nQuery Parameter \u2014 Include deleted associations default: null with_raw_qos (optional)\nQuery Parameter \u2014 Include a raw qos or delta_qos default: null with_sub_accts (optional)\nQuery Parameter \u2014 Include sub acct information default: null without_parent_info (optional)\nQuery Parameter \u2014 Exclude parent id/name default: null without_parent_limits (optional)\nQuery Parameter \u2014 Exclude limits from parents default: null \n \nReturn type\n\nv0.0.41_openapi_assocs_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"associations\" : [ {\n    \"lineage\" : \"lineage\",\n    \"cluster\" : \"cluster\",\n    \"shares_raw\" : 3,\n    \"max\" : {\n      \"jobs\" : {\n        \"total\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"active\" : {\n          \"number\" : 2,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"accruing\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"per\" : {\n          \"submitted\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"count\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"accruing\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"wall_clock\" : {\n            \"number\" : 6,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      },\n      \"tres\" : {\n        \"total\" : [ {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        }, {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        } ],\n        \"minutes\" : {\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"per\" : {\n            \"job\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ]\n          }\n        },\n        \"per\" : {\n          \"node\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"job\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"group\" : {\n          \"minutes\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"active\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        }\n      },\n      \"per\" : {\n        \"account\" : {\n          \"wall_clock\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      }\n    },\n    \"flags\" : [ \"DELETED\", \"DELETED\" ],\n    \"accounting\" : [ {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    }, {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    } ],\n    \"is_default\" : true,\n    \"priority\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"parent_account\" : \"parent_account\",\n    \"default\" : {\n      \"qos\" : \"qos\"\n    },\n    \"min\" : {\n      \"priority_threshold\" : {\n        \"number\" : 8,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"partition\" : \"partition\",\n    \"qos\" : [ \"qos\", \"qos\" ],\n    \"comment\" : \"comment\",\n    \"id\" : 5,\n    \"user\" : \"user\",\n    \"account\" : \"account\"\n  }, {\n    \"lineage\" : \"lineage\",\n    \"cluster\" : \"cluster\",\n    \"shares_raw\" : 3,\n    \"max\" : {\n      \"jobs\" : {\n        \"total\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"active\" : {\n          \"number\" : 2,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"accruing\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"per\" : {\n          \"submitted\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"count\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"accruing\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"wall_clock\" : {\n            \"number\" : 6,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      },\n      \"tres\" : {\n        \"total\" : [ {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        }, {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        } ],\n        \"minutes\" : {\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"per\" : {\n            \"job\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ]\n          }\n        },\n        \"per\" : {\n          \"node\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"job\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"group\" : {\n          \"minutes\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"active\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        }\n      },\n      \"per\" : {\n        \"account\" : {\n          \"wall_clock\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      }\n    },\n    \"flags\" : [ \"DELETED\", \"DELETED\" ],\n    \"accounting\" : [ {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    }, {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    } ],\n    \"is_default\" : true,\n    \"priority\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"parent_account\" : \"parent_account\",\n    \"default\" : {\n      \"qos\" : \"qos\"\n    },\n    \"min\" : {\n      \"priority_threshold\" : {\n        \"number\" : 8,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"partition\" : \"partition\",\n    \"qos\" : [ \"qos\", \"qos\" ],\n    \"comment\" : \"comment\",\n    \"id\" : 5,\n    \"user\" : \"user\",\n    \"account\" : \"account\"\n  } ],\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    List of associations\n        v0.0.41_openapi_assocs_resp\ndefault\n    List of associations\n        v0.0.41_openapi_assocs_resp\n\n\nUp\nget /slurmdb/v0.0.41/associations/\nGet association list (slurmdbV0041GetAssociations)\n\nQuery parameters\n\naccount (optional)\nQuery Parameter \u2014 CSV accounts list default: null cluster (optional)\nQuery Parameter \u2014 CSV clusters list default: null default_qos (optional)\nQuery Parameter \u2014 CSV QOS list default: null format (optional)\nQuery Parameter \u2014 CSV format list default: null id (optional)\nQuery Parameter \u2014 CSV id list default: null only_defaults (optional)\nQuery Parameter \u2014 Filter to only defaults default: null parent_account (optional)\nQuery Parameter \u2014 CSV names of parent account default: null partition (optional)\nQuery Parameter \u2014 CSV partition name list default: null qos (optional)\nQuery Parameter \u2014 CSV QOS list default: null usage_end (optional)\nQuery Parameter \u2014 Usage end (UNIX timestamp) default: null usage_start (optional)\nQuery Parameter \u2014 Usage start (UNIX timestamp) default: null user (optional)\nQuery Parameter \u2014 CSV user list default: null with_usage (optional)\nQuery Parameter \u2014 Include usage default: null with_deleted (optional)\nQuery Parameter \u2014 Include deleted associations default: null with_raw_qos (optional)\nQuery Parameter \u2014 Include a raw qos or delta_qos default: null with_sub_accts (optional)\nQuery Parameter \u2014 Include sub acct information default: null without_parent_info (optional)\nQuery Parameter \u2014 Exclude parent id/name default: null without_parent_limits (optional)\nQuery Parameter \u2014 Exclude limits from parents default: null \n \nReturn type\n\nv0.0.41_openapi_assocs_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"associations\" : [ {\n    \"lineage\" : \"lineage\",\n    \"cluster\" : \"cluster\",\n    \"shares_raw\" : 3,\n    \"max\" : {\n      \"jobs\" : {\n        \"total\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"active\" : {\n          \"number\" : 2,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"accruing\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"per\" : {\n          \"submitted\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"count\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"accruing\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"wall_clock\" : {\n            \"number\" : 6,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      },\n      \"tres\" : {\n        \"total\" : [ {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        }, {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        } ],\n        \"minutes\" : {\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"per\" : {\n            \"job\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ]\n          }\n        },\n        \"per\" : {\n          \"node\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"job\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"group\" : {\n          \"minutes\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"active\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        }\n      },\n      \"per\" : {\n        \"account\" : {\n          \"wall_clock\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      }\n    },\n    \"flags\" : [ \"DELETED\", \"DELETED\" ],\n    \"accounting\" : [ {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    }, {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    } ],\n    \"is_default\" : true,\n    \"priority\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"parent_account\" : \"parent_account\",\n    \"default\" : {\n      \"qos\" : \"qos\"\n    },\n    \"min\" : {\n      \"priority_threshold\" : {\n        \"number\" : 8,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"partition\" : \"partition\",\n    \"qos\" : [ \"qos\", \"qos\" ],\n    \"comment\" : \"comment\",\n    \"id\" : 5,\n    \"user\" : \"user\",\n    \"account\" : \"account\"\n  }, {\n    \"lineage\" : \"lineage\",\n    \"cluster\" : \"cluster\",\n    \"shares_raw\" : 3,\n    \"max\" : {\n      \"jobs\" : {\n        \"total\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"active\" : {\n          \"number\" : 2,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"accruing\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"per\" : {\n          \"submitted\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"count\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"accruing\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"wall_clock\" : {\n            \"number\" : 6,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      },\n      \"tres\" : {\n        \"total\" : [ {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        }, {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        } ],\n        \"minutes\" : {\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"per\" : {\n            \"job\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ]\n          }\n        },\n        \"per\" : {\n          \"node\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"job\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"group\" : {\n          \"minutes\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"active\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        }\n      },\n      \"per\" : {\n        \"account\" : {\n          \"wall_clock\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      }\n    },\n    \"flags\" : [ \"DELETED\", \"DELETED\" ],\n    \"accounting\" : [ {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    }, {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    } ],\n    \"is_default\" : true,\n    \"priority\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"parent_account\" : \"parent_account\",\n    \"default\" : {\n      \"qos\" : \"qos\"\n    },\n    \"min\" : {\n      \"priority_threshold\" : {\n        \"number\" : 8,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"partition\" : \"partition\",\n    \"qos\" : [ \"qos\", \"qos\" ],\n    \"comment\" : \"comment\",\n    \"id\" : 5,\n    \"user\" : \"user\",\n    \"account\" : \"account\"\n  } ],\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    List of associations\n        v0.0.41_openapi_assocs_resp\ndefault\n    List of associations\n        v0.0.41_openapi_assocs_resp\n\n\nUp\nget /slurmdb/v0.0.41/cluster/{cluster_name}\nGet cluster info (slurmdbV0041GetCluster)\n\nPath parameters\n\ncluster_name (required)\nPath Parameter \u2014 Cluster name default: null \n \nQuery parameters\n\nclassification (optional)\nQuery Parameter \u2014 Type of machine default: null cluster (optional)\nQuery Parameter \u2014 CSV cluster list default: null federation (optional)\nQuery Parameter \u2014 CSV federation list default: null flags (optional)\nQuery Parameter \u2014 Query flags default: null format (optional)\nQuery Parameter \u2014 CSV format list default: null rpc_version (optional)\nQuery Parameter \u2014 CSV RPC version list default: null usage_end (optional)\nQuery Parameter \u2014 Usage end (UNIX timestamp) default: null usage_start (optional)\nQuery Parameter \u2014 Usage start (UNIX timestamp) default: null with_deleted (optional)\nQuery Parameter \u2014 Include deleted clusters default: null with_usage (optional)\nQuery Parameter \u2014 Include usage default: null \n \nReturn type\n\nv0.0.41_openapi_clusters_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"clusters\" : [ {\n    \"associations\" : {\n      \"root\" : {\n        \"cluster\" : \"cluster\",\n        \"partition\" : \"partition\",\n        \"id\" : 6,\n        \"user\" : \"user\",\n        \"account\" : \"account\"\n      }\n    },\n    \"controller\" : {\n      \"port\" : 0,\n      \"host\" : \"host\"\n    },\n    \"nodes\" : \"nodes\",\n    \"flags\" : [ \"REGISTERING\", \"REGISTERING\" ],\n    \"name\" : \"name\",\n    \"rpc_version\" : 1,\n    \"tres\" : [ {\n      \"name\" : \"name\",\n      \"count\" : 7,\n      \"id\" : 3,\n      \"type\" : \"type\"\n    }, {\n      \"name\" : \"name\",\n      \"count\" : 7,\n      \"id\" : 3,\n      \"type\" : \"type\"\n    } ],\n    \"select_plugin\" : \"select_plugin\"\n  }, {\n    \"associations\" : {\n      \"root\" : {\n        \"cluster\" : \"cluster\",\n        \"partition\" : \"partition\",\n        \"id\" : 6,\n        \"user\" : \"user\",\n        \"account\" : \"account\"\n      }\n    },\n    \"controller\" : {\n      \"port\" : 0,\n      \"host\" : \"host\"\n    },\n    \"nodes\" : \"nodes\",\n    \"flags\" : [ \"REGISTERING\", \"REGISTERING\" ],\n    \"name\" : \"name\",\n    \"rpc_version\" : 1,\n    \"tres\" : [ {\n      \"name\" : \"name\",\n      \"count\" : 7,\n      \"id\" : 3,\n      \"type\" : \"type\"\n    }, {\n      \"name\" : \"name\",\n      \"count\" : 7,\n      \"id\" : 3,\n      \"type\" : \"type\"\n    } ],\n    \"select_plugin\" : \"select_plugin\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    Cluster information\n        v0.0.41_openapi_clusters_resp\ndefault\n    Cluster information\n        v0.0.41_openapi_clusters_resp\n\n\nUp\nget /slurmdb/v0.0.41/clusters/\nGet cluster list (slurmdbV0041GetClusters)\n\nQuery parameters\n\nupdate_time (optional)\nQuery Parameter \u2014 Filter reservations since update timestamp default: null \n \nReturn type\n\nv0.0.41_openapi_clusters_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"clusters\" : [ {\n    \"associations\" : {\n      \"root\" : {\n        \"cluster\" : \"cluster\",\n        \"partition\" : \"partition\",\n        \"id\" : 6,\n        \"user\" : \"user\",\n        \"account\" : \"account\"\n      }\n    },\n    \"controller\" : {\n      \"port\" : 0,\n      \"host\" : \"host\"\n    },\n    \"nodes\" : \"nodes\",\n    \"flags\" : [ \"REGISTERING\", \"REGISTERING\" ],\n    \"name\" : \"name\",\n    \"rpc_version\" : 1,\n    \"tres\" : [ {\n      \"name\" : \"name\",\n      \"count\" : 7,\n      \"id\" : 3,\n      \"type\" : \"type\"\n    }, {\n      \"name\" : \"name\",\n      \"count\" : 7,\n      \"id\" : 3,\n      \"type\" : \"type\"\n    } ],\n    \"select_plugin\" : \"select_plugin\"\n  }, {\n    \"associations\" : {\n      \"root\" : {\n        \"cluster\" : \"cluster\",\n        \"partition\" : \"partition\",\n        \"id\" : 6,\n        \"user\" : \"user\",\n        \"account\" : \"account\"\n      }\n    },\n    \"controller\" : {\n      \"port\" : 0,\n      \"host\" : \"host\"\n    },\n    \"nodes\" : \"nodes\",\n    \"flags\" : [ \"REGISTERING\", \"REGISTERING\" ],\n    \"name\" : \"name\",\n    \"rpc_version\" : 1,\n    \"tres\" : [ {\n      \"name\" : \"name\",\n      \"count\" : 7,\n      \"id\" : 3,\n      \"type\" : \"type\"\n    }, {\n      \"name\" : \"name\",\n      \"count\" : 7,\n      \"id\" : 3,\n      \"type\" : \"type\"\n    } ],\n    \"select_plugin\" : \"select_plugin\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    List of clusters\n        v0.0.41_openapi_clusters_resp\ndefault\n    List of clusters\n        v0.0.41_openapi_clusters_resp\n\n\nUp\nget /slurmdb/v0.0.41/config\nDump all configuration information (slurmdbV0041GetConfig)\n\nReturn type\n\nv0.0.41_openapi_slurmdbd_config_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"associations\" : [ {\n    \"lineage\" : \"lineage\",\n    \"cluster\" : \"cluster\",\n    \"shares_raw\" : 3,\n    \"max\" : {\n      \"jobs\" : {\n        \"total\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"active\" : {\n          \"number\" : 2,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"accruing\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"per\" : {\n          \"submitted\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"count\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"accruing\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"wall_clock\" : {\n            \"number\" : 6,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      },\n      \"tres\" : {\n        \"total\" : [ {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        }, {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        } ],\n        \"minutes\" : {\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"per\" : {\n            \"job\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ]\n          }\n        },\n        \"per\" : {\n          \"node\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"job\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"group\" : {\n          \"minutes\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"active\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        }\n      },\n      \"per\" : {\n        \"account\" : {\n          \"wall_clock\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      }\n    },\n    \"flags\" : [ \"DELETED\", \"DELETED\" ],\n    \"accounting\" : [ {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    }, {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    } ],\n    \"is_default\" : true,\n    \"priority\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"parent_account\" : \"parent_account\",\n    \"default\" : {\n      \"qos\" : \"qos\"\n    },\n    \"min\" : {\n      \"priority_threshold\" : {\n        \"number\" : 8,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"partition\" : \"partition\",\n    \"qos\" : [ \"qos\", \"qos\" ],\n    \"comment\" : \"comment\",\n    \"id\" : 5,\n    \"user\" : \"user\",\n    \"account\" : \"account\"\n  }, {\n    \"lineage\" : \"lineage\",\n    \"cluster\" : \"cluster\",\n    \"shares_raw\" : 3,\n    \"max\" : {\n      \"jobs\" : {\n        \"total\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"active\" : {\n          \"number\" : 2,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"accruing\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"per\" : {\n          \"submitted\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"count\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"accruing\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"wall_clock\" : {\n            \"number\" : 6,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      },\n      \"tres\" : {\n        \"total\" : [ {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        }, {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        } ],\n        \"minutes\" : {\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"per\" : {\n            \"job\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ]\n          }\n        },\n        \"per\" : {\n          \"node\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"job\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"group\" : {\n          \"minutes\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"active\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        }\n      },\n      \"per\" : {\n        \"account\" : {\n          \"wall_clock\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      }\n    },\n    \"flags\" : [ \"DELETED\", \"DELETED\" ],\n    \"accounting\" : [ {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    }, {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    } ],\n    \"is_default\" : true,\n    \"priority\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"parent_account\" : \"parent_account\",\n    \"default\" : {\n      \"qos\" : \"qos\"\n    },\n    \"min\" : {\n      \"priority_threshold\" : {\n        \"number\" : 8,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"partition\" : \"partition\",\n    \"qos\" : [ \"qos\", \"qos\" ],\n    \"comment\" : \"comment\",\n    \"id\" : 5,\n    \"user\" : \"user\",\n    \"account\" : \"account\"\n  } ],\n  \"qos\" : [ {\n    \"flags\" : [ \"NOT_SET\", \"NOT_SET\" ],\n    \"name\" : \"name\",\n    \"usage_threshold\" : {\n      \"number\" : 6.965117697638846,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"description\" : \"description\",\n    \"usage_factor\" : {\n      \"number\" : 3.5571952270680973,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"id\" : 4,\n    \"priority\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"limits\" : {\n      \"min\" : {\n        \"priority_threshold\" : {\n          \"number\" : 8,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"tres\" : {\n          \"per\" : {\n            \"job\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ]\n          }\n        }\n      },\n      \"max\" : {\n        \"jobs\" : {\n          \"per\" : {\n            \"user\" : {\n              \"number\" : 5,\n              \"set\" : true,\n              \"infinite\" : true\n            },\n            \"account\" : {\n              \"number\" : 4,\n              \"set\" : true,\n              \"infinite\" : true\n            }\n          },\n          \"active_jobs\" : {\n            \"per\" : {\n              \"user\" : {\n                \"number\" : 1,\n                \"set\" : true,\n                \"infinite\" : true\n              },\n              \"account\" : {\n                \"number\" : 7,\n                \"set\" : true,\n                \"infinite\" : true\n              }\n            }\n          }\n        },\n        \"accruing\" : {\n          \"per\" : {\n            \"user\" : {\n              \"number\" : 9,\n              \"set\" : true,\n              \"infinite\" : true\n            },\n            \"account\" : {\n              \"number\" : 9,\n              \"set\" : true,\n              \"infinite\" : true\n            }\n          }\n        },\n        \"tres\" : {\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"minutes\" : {\n            \"per\" : {\n              \"qos\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ],\n              \"job\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ],\n              \"user\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ],\n              \"account\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ]\n            }\n          },\n          \"per\" : {\n            \"node\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ],\n            \"job\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ],\n            \"user\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ],\n            \"account\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ]\n          }\n        },\n        \"wall_clock\" : {\n          \"per\" : {\n            \"qos\" : {\n              \"number\" : 1,\n              \"set\" : true,\n              \"infinite\" : true\n            },\n            \"job\" : {\n              \"number\" : 6,\n              \"set\" : true,\n              \"infinite\" : true\n            }\n          }\n        },\n        \"active_jobs\" : {\n          \"count\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"accruing\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      },\n      \"factor\" : {\n        \"number\" : 6.683562403749608,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"grace_time\" : 7\n    },\n    \"preempt\" : {\n      \"mode\" : [ \"DISABLED\", \"DISABLED\" ],\n      \"exempt_time\" : {\n        \"number\" : 9,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"list\" : [ \"list\", \"list\" ]\n    }\n  }, {\n    \"flags\" : [ \"NOT_SET\", \"NOT_SET\" ],\n    \"name\" : \"name\",\n    \"usage_threshold\" : {\n      \"number\" : 6.965117697638846,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"description\" : \"description\",\n    \"usage_factor\" : {\n      \"number\" : 3.5571952270680973,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"id\" : 4,\n    \"priority\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"limits\" : {\n      \"min\" : {\n        \"priority_threshold\" : {\n          \"number\" : 8,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"tres\" : {\n          \"per\" : {\n            \"job\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ]\n          }\n        }\n      },\n      \"max\" : {\n        \"jobs\" : {\n          \"per\" : {\n            \"user\" : {\n              \"number\" : 5,\n              \"set\" : true,\n              \"infinite\" : true\n            },\n            \"account\" : {\n              \"number\" : 4,\n              \"set\" : true,\n              \"infinite\" : true\n            }\n          },\n          \"active_jobs\" : {\n            \"per\" : {\n              \"user\" : {\n                \"number\" : 1,\n                \"set\" : true,\n                \"infinite\" : true\n              },\n              \"account\" : {\n                \"number\" : 7,\n                \"set\" : true,\n                \"infinite\" : true\n              }\n            }\n          }\n        },\n        \"accruing\" : {\n          \"per\" : {\n            \"user\" : {\n              \"number\" : 9,\n              \"set\" : true,\n              \"infinite\" : true\n            },\n            \"account\" : {\n              \"number\" : 9,\n              \"set\" : true,\n              \"infinite\" : true\n            }\n          }\n        },\n        \"tres\" : {\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"minutes\" : {\n            \"per\" : {\n              \"qos\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ],\n              \"job\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ],\n              \"user\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ],\n              \"account\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ]\n            }\n          },\n          \"per\" : {\n            \"node\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ],\n            \"job\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ],\n            \"user\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ],\n            \"account\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ]\n          }\n        },\n        \"wall_clock\" : {\n          \"per\" : {\n            \"qos\" : {\n              \"number\" : 1,\n              \"set\" : true,\n              \"infinite\" : true\n            },\n            \"job\" : {\n              \"number\" : 6,\n              \"set\" : true,\n              \"infinite\" : true\n            }\n          }\n        },\n        \"active_jobs\" : {\n          \"count\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"accruing\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      },\n      \"factor\" : {\n        \"number\" : 6.683562403749608,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"grace_time\" : 7\n    },\n    \"preempt\" : {\n      \"mode\" : [ \"DISABLED\", \"DISABLED\" ],\n      \"exempt_time\" : {\n        \"number\" : 9,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"list\" : [ \"list\", \"list\" ]\n    }\n  } ],\n  \"wckeys\" : [ {\n    \"cluster\" : \"cluster\",\n    \"name\" : \"name\",\n    \"flags\" : [ \"DELETED\", \"DELETED\" ],\n    \"accounting\" : [ {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    }, {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    } ],\n    \"id\" : 2,\n    \"user\" : \"user\"\n  }, {\n    \"cluster\" : \"cluster\",\n    \"name\" : \"name\",\n    \"flags\" : [ \"DELETED\", \"DELETED\" ],\n    \"accounting\" : [ {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    }, {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    } ],\n    \"id\" : 2,\n    \"user\" : \"user\"\n  } ],\n  \"instances\" : [ {\n    \"cluster\" : \"cluster\",\n    \"instance_id\" : \"instance_id\",\n    \"extra\" : \"extra\",\n    \"node_name\" : \"node_name\",\n    \"time\" : {\n      \"time_start\" : 7,\n      \"time_end\" : 3\n    },\n    \"instance_type\" : \"instance_type\"\n  }, {\n    \"cluster\" : \"cluster\",\n    \"instance_id\" : \"instance_id\",\n    \"extra\" : \"extra\",\n    \"node_name\" : \"node_name\",\n    \"time\" : {\n      \"time_start\" : 7,\n      \"time_end\" : 3\n    },\n    \"instance_type\" : \"instance_type\"\n  } ],\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"tres\" : [ {\n    \"name\" : \"name\",\n    \"count\" : 7,\n    \"id\" : 3,\n    \"type\" : \"type\"\n  }, {\n    \"name\" : \"name\",\n    \"count\" : 7,\n    \"id\" : 3,\n    \"type\" : \"type\"\n  } ],\n  \"accounts\" : [ {\n    \"associations\" : [ {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    }, {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    } ],\n    \"coordinators\" : [ {\n      \"name\" : \"name\",\n      \"direct\" : true\n    }, {\n      \"name\" : \"name\",\n      \"direct\" : true\n    } ],\n    \"organization\" : \"organization\",\n    \"name\" : \"name\",\n    \"flags\" : [ \"DELETED\", \"DELETED\" ],\n    \"description\" : \"description\"\n  }, {\n    \"associations\" : [ {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    }, {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    } ],\n    \"coordinators\" : [ {\n      \"name\" : \"name\",\n      \"direct\" : true\n    }, {\n      \"name\" : \"name\",\n      \"direct\" : true\n    } ],\n    \"organization\" : \"organization\",\n    \"name\" : \"name\",\n    \"flags\" : [ \"DELETED\", \"DELETED\" ],\n    \"description\" : \"description\"\n  } ],\n  \"clusters\" : [ {\n    \"associations\" : {\n      \"root\" : {\n        \"cluster\" : \"cluster\",\n        \"partition\" : \"partition\",\n        \"id\" : 6,\n        \"user\" : \"user\",\n        \"account\" : \"account\"\n      }\n    },\n    \"controller\" : {\n      \"port\" : 0,\n      \"host\" : \"host\"\n    },\n    \"nodes\" : \"nodes\",\n    \"flags\" : [ \"REGISTERING\", \"REGISTERING\" ],\n    \"name\" : \"name\",\n    \"rpc_version\" : 1,\n    \"tres\" : [ {\n      \"name\" : \"name\",\n      \"count\" : 7,\n      \"id\" : 3,\n      \"type\" : \"type\"\n    }, {\n      \"name\" : \"name\",\n      \"count\" : 7,\n      \"id\" : 3,\n      \"type\" : \"type\"\n    } ],\n    \"select_plugin\" : \"select_plugin\"\n  }, {\n    \"associations\" : {\n      \"root\" : {\n        \"cluster\" : \"cluster\",\n        \"partition\" : \"partition\",\n        \"id\" : 6,\n        \"user\" : \"user\",\n        \"account\" : \"account\"\n      }\n    },\n    \"controller\" : {\n      \"port\" : 0,\n      \"host\" : \"host\"\n    },\n    \"nodes\" : \"nodes\",\n    \"flags\" : [ \"REGISTERING\", \"REGISTERING\" ],\n    \"name\" : \"name\",\n    \"rpc_version\" : 1,\n    \"tres\" : [ {\n      \"name\" : \"name\",\n      \"count\" : 7,\n      \"id\" : 3,\n      \"type\" : \"type\"\n    }, {\n      \"name\" : \"name\",\n      \"count\" : 7,\n      \"id\" : 3,\n      \"type\" : \"type\"\n    } ],\n    \"select_plugin\" : \"select_plugin\"\n  } ],\n  \"users\" : [ {\n    \"associations\" : [ {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    }, {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    } ],\n    \"default\" : {\n      \"wckey\" : \"wckey\",\n      \"account\" : \"account\"\n    },\n    \"administrator_level\" : [ \"Not Set\", \"Not Set\" ],\n    \"old_name\" : \"old_name\",\n    \"wckeys\" : [ {\n      \"cluster\" : \"cluster\",\n      \"name\" : \"name\",\n      \"flags\" : [ \"DELETED\", \"DELETED\" ],\n      \"accounting\" : [ {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      }, {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      } ],\n      \"id\" : 2,\n      \"user\" : \"user\"\n    }, {\n      \"cluster\" : \"cluster\",\n      \"name\" : \"name\",\n      \"flags\" : [ \"DELETED\", \"DELETED\" ],\n      \"accounting\" : [ {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      }, {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      } ],\n      \"id\" : 2,\n      \"user\" : \"user\"\n    } ],\n    \"coordinators\" : [ {\n      \"name\" : \"name\",\n      \"direct\" : true\n    }, {\n      \"name\" : \"name\",\n      \"direct\" : true\n    } ],\n    \"flags\" : [ \"NONE\", \"NONE\" ],\n    \"name\" : \"name\"\n  }, {\n    \"associations\" : [ {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    }, {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    } ],\n    \"default\" : {\n      \"wckey\" : \"wckey\",\n      \"account\" : \"account\"\n    },\n    \"administrator_level\" : [ \"Not Set\", \"Not Set\" ],\n    \"old_name\" : \"old_name\",\n    \"wckeys\" : [ {\n      \"cluster\" : \"cluster\",\n      \"name\" : \"name\",\n      \"flags\" : [ \"DELETED\", \"DELETED\" ],\n      \"accounting\" : [ {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      }, {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      } ],\n      \"id\" : 2,\n      \"user\" : \"user\"\n    }, {\n      \"cluster\" : \"cluster\",\n      \"name\" : \"name\",\n      \"flags\" : [ \"DELETED\", \"DELETED\" ],\n      \"accounting\" : [ {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      }, {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      } ],\n      \"id\" : 2,\n      \"user\" : \"user\"\n    } ],\n    \"coordinators\" : [ {\n      \"name\" : \"name\",\n      \"direct\" : true\n    }, {\n      \"name\" : \"name\",\n      \"direct\" : true\n    } ],\n    \"flags\" : [ \"NONE\", \"NONE\" ],\n    \"name\" : \"name\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    slurmdbd configuration\n        v0.0.41_openapi_slurmdbd_config_resp\ndefault\n    slurmdbd configuration\n        v0.0.41_openapi_slurmdbd_config_resp\n\n\nUp\nget /slurmdb/v0.0.41/diag/\nGet slurmdb diagnostics (slurmdbV0041GetDiag)\n\nReturn type\n\nv0.0.41_openapi_slurmdbd_stats_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ],\n  \"statistics\" : {\n    \"time_start\" : 0,\n    \"RPCs\" : [ {\n      \"rpc\" : \"rpc\",\n      \"count\" : 7,\n      \"time\" : {\n        \"average\" : 1,\n        \"total\" : 4\n      }\n    }, {\n      \"rpc\" : \"rpc\",\n      \"count\" : 7,\n      \"time\" : {\n        \"average\" : 1,\n        \"total\" : 4\n      }\n    } ],\n    \"rollups\" : {\n      \"daily\" : {\n        \"duration\" : {\n          \"last\" : 3,\n          \"max\" : 2,\n          \"time\" : 4\n        },\n        \"count\" : 7,\n        \"last_run\" : 9\n      },\n      \"monthly\" : {\n        \"duration\" : {\n          \"last\" : 1,\n          \"max\" : 1,\n          \"time\" : 6\n        },\n        \"count\" : 7,\n        \"last_run\" : 1\n      },\n      \"hourly\" : {\n        \"duration\" : {\n          \"last\" : 5,\n          \"max\" : 5,\n          \"time\" : 2\n        },\n        \"count\" : 6,\n        \"last_run\" : 1\n      }\n    },\n    \"users\" : [ {\n      \"count\" : 5,\n      \"time\" : {\n        \"average\" : 1,\n        \"total\" : 4\n      },\n      \"user\" : \"user\"\n    }, {\n      \"count\" : 5,\n      \"time\" : {\n        \"average\" : 1,\n        \"total\" : 4\n      },\n      \"user\" : \"user\"\n    } ]\n  }\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    Dictionary of statistics\n        v0.0.41_openapi_slurmdbd_stats_resp\ndefault\n    Dictionary of statistics\n        v0.0.41_openapi_slurmdbd_stats_resp\n\n\nUp\nget /slurmdb/v0.0.41/instance/\nGet instance info (slurmdbV0041GetInstance)\n\nQuery parameters\n\ncluster (optional)\nQuery Parameter \u2014 CSV clusters list default: null extra (optional)\nQuery Parameter \u2014 CSV extra list default: null format (optional)\nQuery Parameter \u2014 CSV format list default: null instance_id (optional)\nQuery Parameter \u2014 CSV instance_id list default: null instance_type (optional)\nQuery Parameter \u2014 CSV instance_type list default: null node_list (optional)\nQuery Parameter \u2014 Ranged node string default: null time_end (optional)\nQuery Parameter \u2014 Time end (UNIX timestamp) default: null time_start (optional)\nQuery Parameter \u2014 Time start (UNIX timestamp) default: null \n \nReturn type\n\nv0.0.41_openapi_instances_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"instances\" : [ {\n    \"cluster\" : \"cluster\",\n    \"instance_id\" : \"instance_id\",\n    \"extra\" : \"extra\",\n    \"node_name\" : \"node_name\",\n    \"time\" : {\n      \"time_start\" : 7,\n      \"time_end\" : 3\n    },\n    \"instance_type\" : \"instance_type\"\n  }, {\n    \"cluster\" : \"cluster\",\n    \"instance_id\" : \"instance_id\",\n    \"extra\" : \"extra\",\n    \"node_name\" : \"node_name\",\n    \"time\" : {\n      \"time_start\" : 7,\n      \"time_end\" : 3\n    },\n    \"instance_type\" : \"instance_type\"\n  } ],\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    List of instances\n        v0.0.41_openapi_instances_resp\ndefault\n    List of instances\n        v0.0.41_openapi_instances_resp\n\n\nUp\nget /slurmdb/v0.0.41/instances/\nGet instance list (slurmdbV0041GetInstances)\n\nQuery parameters\n\ncluster (optional)\nQuery Parameter \u2014 CSV clusters list default: null extra (optional)\nQuery Parameter \u2014 CSV extra list default: null format (optional)\nQuery Parameter \u2014 CSV format list default: null instance_id (optional)\nQuery Parameter \u2014 CSV instance_id list default: null instance_type (optional)\nQuery Parameter \u2014 CSV instance_type list default: null node_list (optional)\nQuery Parameter \u2014 Ranged node string default: null time_end (optional)\nQuery Parameter \u2014 Time end (UNIX timestamp) default: null time_start (optional)\nQuery Parameter \u2014 Time start (UNIX timestamp) default: null \n \nReturn type\n\nv0.0.41_openapi_instances_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"instances\" : [ {\n    \"cluster\" : \"cluster\",\n    \"instance_id\" : \"instance_id\",\n    \"extra\" : \"extra\",\n    \"node_name\" : \"node_name\",\n    \"time\" : {\n      \"time_start\" : 7,\n      \"time_end\" : 3\n    },\n    \"instance_type\" : \"instance_type\"\n  }, {\n    \"cluster\" : \"cluster\",\n    \"instance_id\" : \"instance_id\",\n    \"extra\" : \"extra\",\n    \"node_name\" : \"node_name\",\n    \"time\" : {\n      \"time_start\" : 7,\n      \"time_end\" : 3\n    },\n    \"instance_type\" : \"instance_type\"\n  } ],\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    List of instances\n        v0.0.41_openapi_instances_resp\ndefault\n    List of instances\n        v0.0.41_openapi_instances_resp\n\n\nUp\nget /slurmdb/v0.0.41/job/{job_id}\nGet job info (slurmdbV0041GetJob)\nThis endpoint may return multiple job entries since job_id is not a unique key - only the tuple (cluster, job_id, start_time) is unique. If the requested job_id is a component of a heterogeneous job all components are returned.\nPath parameters\n\njob_id (required)\nPath Parameter \u2014 Job id default: null \n \nReturn type\n\nv0.0.41_openapi_slurmdbd_jobs_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"jobs\" : [ {\n    \"container\" : \"container\",\n    \"cluster\" : \"cluster\",\n    \"stdin_expanded\" : \"stdin_expanded\",\n    \"stdin\" : \"stdin\",\n    \"stdout\" : \"stdout\",\n    \"stderr_expanded\" : \"stderr_expanded\",\n    \"flags\" : [ \"NONE\", \"NONE\" ],\n    \"used_gres\" : \"used_gres\",\n    \"association\" : {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    },\n    \"allocation_nodes\" : 0,\n    \"working_directory\" : \"working_directory\",\n    \"constraints\" : \"constraints\",\n    \"required\" : {\n      \"memory_per_node\" : {\n        \"number\" : 6,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"CPUs\" : 6,\n      \"memory_per_cpu\" : {\n        \"number\" : 3,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"stdout_expanded\" : \"stdout_expanded\",\n    \"hold\" : true,\n    \"partition\" : \"partition\",\n    \"qos\" : \"qos\",\n    \"array\" : {\n      \"task\" : \"task\",\n      \"job_id\" : 6,\n      \"task_id\" : {\n        \"number\" : 5,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"limits\" : {\n        \"max\" : {\n          \"running\" : {\n            \"tasks\" : 1\n          }\n        }\n      }\n    },\n    \"het\" : {\n      \"job_id\" : 9,\n      \"job_offset\" : {\n        \"number\" : 6,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"submit_line\" : \"submit_line\",\n    \"extra\" : \"extra\",\n    \"reservation\" : {\n      \"name\" : \"name\",\n      \"id\" : 1\n    },\n    \"block\" : \"block\",\n    \"tres\" : {\n      \"requested\" : [ {\n        \"name\" : \"name\",\n        \"count\" : 7,\n        \"id\" : 3,\n        \"type\" : \"type\"\n      }, {\n        \"name\" : \"name\",\n        \"count\" : 7,\n        \"id\" : 3,\n        \"type\" : \"type\"\n      } ],\n      \"allocated\" : [ {\n        \"name\" : \"name\",\n        \"count\" : 7,\n        \"id\" : 3,\n        \"type\" : \"type\"\n      }, {\n        \"name\" : \"name\",\n        \"count\" : 7,\n        \"id\" : 3,\n        \"type\" : \"type\"\n      } ]\n    },\n    \"state\" : {\n      \"reason\" : \"reason\",\n      \"current\" : [ \"PENDING\", \"PENDING\" ]\n    },\n    \"mcs\" : {\n      \"label\" : \"label\"\n    },\n    \"group\" : \"group\",\n    \"wckey\" : {\n      \"wckey\" : \"wckey\",\n      \"flags\" : [ \"ASSIGNED_DEFAULT\", \"ASSIGNED_DEFAULT\" ]\n    },\n    \"priority\" : {\n      \"number\" : 9,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"stderr\" : \"stderr\",\n    \"steps\" : [ {\n      \"nodes\" : {\n        \"count\" : 6,\n        \"range\" : \"range\",\n        \"list\" : [ \"list\", \"list\" ]\n      },\n      \"task\" : {\n        \"distribution\" : \"distribution\"\n      },\n      \"exit_code\" : {\n        \"return_code\" : {\n          \"number\" : 2,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"signal\" : {\n          \"name\" : \"name\",\n          \"id\" : {\n            \"number\" : 7,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        },\n        \"status\" : [ \"INVALID\", \"INVALID\" ]\n      },\n      \"kill_request_user\" : \"kill_request_user\",\n      \"CPU\" : {\n        \"governor\" : \"governor\",\n        \"requested_frequency\" : {\n          \"min\" : {\n            \"number\" : 4,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"max\" : {\n            \"number\" : 8,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      },\n      \"pid\" : \"pid\",\n      \"step\" : {\n        \"name\" : \"name\",\n        \"id\" : \"id\"\n      },\n      \"tres\" : {\n        \"consumed\" : {\n          \"average\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"min\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"max\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"requested\" : {\n          \"average\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"min\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"max\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"allocated\" : [ {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        }, {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        } ]\n      },\n      \"time\" : {\n        \"elapsed\" : 2,\n        \"total\" : {\n          \"seconds\" : 3,\n          \"microseconds\" : 7\n        },\n        \"system\" : {\n          \"seconds\" : 6,\n          \"microseconds\" : 3\n        },\n        \"start\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"end\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"user\" : {\n          \"seconds\" : 0,\n          \"microseconds\" : 7\n        },\n        \"suspended\" : 5\n      },\n      \"state\" : [ \"PENDING\", \"PENDING\" ],\n      \"tasks\" : {\n        \"count\" : 0\n      },\n      \"statistics\" : {\n        \"CPU\" : {\n          \"actual_frequency\" : 7\n        },\n        \"energy\" : {\n          \"consumed\" : {\n            \"number\" : 3,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      }\n    }, {\n      \"nodes\" : {\n        \"count\" : 6,\n        \"range\" : \"range\",\n        \"list\" : [ \"list\", \"list\" ]\n      },\n      \"task\" : {\n        \"distribution\" : \"distribution\"\n      },\n      \"exit_code\" : {\n        \"return_code\" : {\n          \"number\" : 2,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"signal\" : {\n          \"name\" : \"name\",\n          \"id\" : {\n            \"number\" : 7,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        },\n        \"status\" : [ \"INVALID\", \"INVALID\" ]\n      },\n      \"kill_request_user\" : \"kill_request_user\",\n      \"CPU\" : {\n        \"governor\" : \"governor\",\n        \"requested_frequency\" : {\n          \"min\" : {\n            \"number\" : 4,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"max\" : {\n            \"number\" : 8,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      },\n      \"pid\" : \"pid\",\n      \"step\" : {\n        \"name\" : \"name\",\n        \"id\" : \"id\"\n      },\n      \"tres\" : {\n        \"consumed\" : {\n          \"average\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"min\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"max\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"requested\" : {\n          \"average\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"min\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"max\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"allocated\" : [ {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        }, {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        } ]\n      },\n      \"time\" : {\n        \"elapsed\" : 2,\n        \"total\" : {\n          \"seconds\" : 3,\n          \"microseconds\" : 7\n        },\n        \"system\" : {\n          \"seconds\" : 6,\n          \"microseconds\" : 3\n        },\n        \"start\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"end\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"user\" : {\n          \"seconds\" : 0,\n          \"microseconds\" : 7\n        },\n        \"suspended\" : 5\n      },\n      \"state\" : [ \"PENDING\", \"PENDING\" ],\n      \"tasks\" : {\n        \"count\" : 0\n      },\n      \"statistics\" : {\n        \"CPU\" : {\n          \"actual_frequency\" : 7\n        },\n        \"energy\" : {\n          \"consumed\" : {\n            \"number\" : 3,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      }\n    } ],\n    \"script\" : \"script\",\n    \"failed_node\" : \"failed_node\",\n    \"derived_exit_code\" : {\n      \"return_code\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"signal\" : {\n        \"name\" : \"name\",\n        \"id\" : {\n          \"number\" : 7,\n          \"set\" : true,\n          \"infinite\" : true\n        }\n      },\n      \"status\" : [ \"INVALID\", \"INVALID\" ]\n    },\n    \"licenses\" : \"licenses\",\n    \"nodes\" : \"nodes\",\n    \"job_id\" : 8,\n    \"exit_code\" : {\n      \"return_code\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"signal\" : {\n        \"name\" : \"name\",\n        \"id\" : {\n          \"number\" : 7,\n          \"set\" : true,\n          \"infinite\" : true\n        }\n      },\n      \"status\" : [ \"INVALID\", \"INVALID\" ]\n    },\n    \"name\" : \"name\",\n    \"kill_request_user\" : \"kill_request_user\",\n    \"comment\" : {\n      \"administrator\" : \"administrator\",\n      \"system\" : \"system\",\n      \"job\" : \"job\"\n    },\n    \"time\" : {\n      \"elapsed\" : 9,\n      \"total\" : {\n        \"seconds\" : 1,\n        \"microseconds\" : 4\n      },\n      \"system\" : {\n        \"seconds\" : 1,\n        \"microseconds\" : 6\n      },\n      \"eligible\" : 3,\n      \"start\" : 7,\n      \"limit\" : {\n        \"number\" : 7,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"end\" : 2,\n      \"submission\" : 1,\n      \"planned\" : {\n        \"number\" : 4,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"user\" : {\n        \"seconds\" : 5,\n        \"microseconds\" : 9\n      },\n      \"suspended\" : 1\n    },\n    \"user\" : \"user\",\n    \"account\" : \"account\"\n  }, {\n    \"container\" : \"container\",\n    \"cluster\" : \"cluster\",\n    \"stdin_expanded\" : \"stdin_expanded\",\n    \"stdin\" : \"stdin\",\n    \"stdout\" : \"stdout\",\n    \"stderr_expanded\" : \"stderr_expanded\",\n    \"flags\" : [ \"NONE\", \"NONE\" ],\n    \"used_gres\" : \"used_gres\",\n    \"association\" : {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    },\n    \"allocation_nodes\" : 0,\n    \"working_directory\" : \"working_directory\",\n    \"constraints\" : \"constraints\",\n    \"required\" : {\n      \"memory_per_node\" : {\n        \"number\" : 6,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"CPUs\" : 6,\n      \"memory_per_cpu\" : {\n        \"number\" : 3,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"stdout_expanded\" : \"stdout_expanded\",\n    \"hold\" : true,\n    \"partition\" : \"partition\",\n    \"qos\" : \"qos\",\n    \"array\" : {\n      \"task\" : \"task\",\n      \"job_id\" : 6,\n      \"task_id\" : {\n        \"number\" : 5,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"limits\" : {\n        \"max\" : {\n          \"running\" : {\n            \"tasks\" : 1\n          }\n        }\n      }\n    },\n    \"het\" : {\n      \"job_id\" : 9,\n      \"job_offset\" : {\n        \"number\" : 6,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"submit_line\" : \"submit_line\",\n    \"extra\" : \"extra\",\n    \"reservation\" : {\n      \"name\" : \"name\",\n      \"id\" : 1\n    },\n    \"block\" : \"block\",\n    \"tres\" : {\n      \"requested\" : [ {\n        \"name\" : \"name\",\n        \"count\" : 7,\n        \"id\" : 3,\n        \"type\" : \"type\"\n      }, {\n        \"name\" : \"name\",\n        \"count\" : 7,\n        \"id\" : 3,\n        \"type\" : \"type\"\n      } ],\n      \"allocated\" : [ {\n        \"name\" : \"name\",\n        \"count\" : 7,\n        \"id\" : 3,\n        \"type\" : \"type\"\n      }, {\n        \"name\" : \"name\",\n        \"count\" : 7,\n        \"id\" : 3,\n        \"type\" : \"type\"\n      } ]\n    },\n    \"state\" : {\n      \"reason\" : \"reason\",\n      \"current\" : [ \"PENDING\", \"PENDING\" ]\n    },\n    \"mcs\" : {\n      \"label\" : \"label\"\n    },\n    \"group\" : \"group\",\n    \"wckey\" : {\n      \"wckey\" : \"wckey\",\n      \"flags\" : [ \"ASSIGNED_DEFAULT\", \"ASSIGNED_DEFAULT\" ]\n    },\n    \"priority\" : {\n      \"number\" : 9,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"stderr\" : \"stderr\",\n    \"steps\" : [ {\n      \"nodes\" : {\n        \"count\" : 6,\n        \"range\" : \"range\",\n        \"list\" : [ \"list\", \"list\" ]\n      },\n      \"task\" : {\n        \"distribution\" : \"distribution\"\n      },\n      \"exit_code\" : {\n        \"return_code\" : {\n          \"number\" : 2,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"signal\" : {\n          \"name\" : \"name\",\n          \"id\" : {\n            \"number\" : 7,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        },\n        \"status\" : [ \"INVALID\", \"INVALID\" ]\n      },\n      \"kill_request_user\" : \"kill_request_user\",\n      \"CPU\" : {\n        \"governor\" : \"governor\",\n        \"requested_frequency\" : {\n          \"min\" : {\n            \"number\" : 4,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"max\" : {\n            \"number\" : 8,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      },\n      \"pid\" : \"pid\",\n      \"step\" : {\n        \"name\" : \"name\",\n        \"id\" : \"id\"\n      },\n      \"tres\" : {\n        \"consumed\" : {\n          \"average\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"min\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"max\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"requested\" : {\n          \"average\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"min\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"max\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"allocated\" : [ {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        }, {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        } ]\n      },\n      \"time\" : {\n        \"elapsed\" : 2,\n        \"total\" : {\n          \"seconds\" : 3,\n          \"microseconds\" : 7\n        },\n        \"system\" : {\n          \"seconds\" : 6,\n          \"microseconds\" : 3\n        },\n        \"start\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"end\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"user\" : {\n          \"seconds\" : 0,\n          \"microseconds\" : 7\n        },\n        \"suspended\" : 5\n      },\n      \"state\" : [ \"PENDING\", \"PENDING\" ],\n      \"tasks\" : {\n        \"count\" : 0\n      },\n      \"statistics\" : {\n        \"CPU\" : {\n          \"actual_frequency\" : 7\n        },\n        \"energy\" : {\n          \"consumed\" : {\n            \"number\" : 3,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      }\n    }, {\n      \"nodes\" : {\n        \"count\" : 6,\n        \"range\" : \"range\",\n        \"list\" : [ \"list\", \"list\" ]\n      },\n      \"task\" : {\n        \"distribution\" : \"distribution\"\n      },\n      \"exit_code\" : {\n        \"return_code\" : {\n          \"number\" : 2,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"signal\" : {\n          \"name\" : \"name\",\n          \"id\" : {\n            \"number\" : 7,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        },\n        \"status\" : [ \"INVALID\", \"INVALID\" ]\n      },\n      \"kill_request_user\" : \"kill_request_user\",\n      \"CPU\" : {\n        \"governor\" : \"governor\",\n        \"requested_frequency\" : {\n          \"min\" : {\n            \"number\" : 4,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"max\" : {\n            \"number\" : 8,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      },\n      \"pid\" : \"pid\",\n      \"step\" : {\n        \"name\" : \"name\",\n        \"id\" : \"id\"\n      },\n      \"tres\" : {\n        \"consumed\" : {\n          \"average\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"min\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"max\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"requested\" : {\n          \"average\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"min\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"max\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"allocated\" : [ {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        }, {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        } ]\n      },\n      \"time\" : {\n        \"elapsed\" : 2,\n        \"total\" : {\n          \"seconds\" : 3,\n          \"microseconds\" : 7\n        },\n        \"system\" : {\n          \"seconds\" : 6,\n          \"microseconds\" : 3\n        },\n        \"start\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"end\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"user\" : {\n          \"seconds\" : 0,\n          \"microseconds\" : 7\n        },\n        \"suspended\" : 5\n      },\n      \"state\" : [ \"PENDING\", \"PENDING\" ],\n      \"tasks\" : {\n        \"count\" : 0\n      },\n      \"statistics\" : {\n        \"CPU\" : {\n          \"actual_frequency\" : 7\n        },\n        \"energy\" : {\n          \"consumed\" : {\n            \"number\" : 3,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      }\n    } ],\n    \"script\" : \"script\",\n    \"failed_node\" : \"failed_node\",\n    \"derived_exit_code\" : {\n      \"return_code\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"signal\" : {\n        \"name\" : \"name\",\n        \"id\" : {\n          \"number\" : 7,\n          \"set\" : true,\n          \"infinite\" : true\n        }\n      },\n      \"status\" : [ \"INVALID\", \"INVALID\" ]\n    },\n    \"licenses\" : \"licenses\",\n    \"nodes\" : \"nodes\",\n    \"job_id\" : 8,\n    \"exit_code\" : {\n      \"return_code\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"signal\" : {\n        \"name\" : \"name\",\n        \"id\" : {\n          \"number\" : 7,\n          \"set\" : true,\n          \"infinite\" : true\n        }\n      },\n      \"status\" : [ \"INVALID\", \"INVALID\" ]\n    },\n    \"name\" : \"name\",\n    \"kill_request_user\" : \"kill_request_user\",\n    \"comment\" : {\n      \"administrator\" : \"administrator\",\n      \"system\" : \"system\",\n      \"job\" : \"job\"\n    },\n    \"time\" : {\n      \"elapsed\" : 9,\n      \"total\" : {\n        \"seconds\" : 1,\n        \"microseconds\" : 4\n      },\n      \"system\" : {\n        \"seconds\" : 1,\n        \"microseconds\" : 6\n      },\n      \"eligible\" : 3,\n      \"start\" : 7,\n      \"limit\" : {\n        \"number\" : 7,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"end\" : 2,\n      \"submission\" : 1,\n      \"planned\" : {\n        \"number\" : 4,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"user\" : {\n        \"seconds\" : 5,\n        \"microseconds\" : 9\n      },\n      \"suspended\" : 1\n    },\n    \"user\" : \"user\",\n    \"account\" : \"account\"\n  } ],\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    Job description\n        v0.0.41_openapi_slurmdbd_jobs_resp\ndefault\n    Job description\n        v0.0.41_openapi_slurmdbd_jobs_resp\n\n\nUp\nget /slurmdb/v0.0.41/jobs/\nGet job list (slurmdbV0041GetJobs)\n\nQuery parameters\n\naccount (optional)\nQuery Parameter \u2014 CSV account list default: null association (optional)\nQuery Parameter \u2014 CSV association list default: null cluster (optional)\nQuery Parameter \u2014 CSV cluster list default: null constraints (optional)\nQuery Parameter \u2014 CSV constraint list default: null scheduler_unset (optional)\nQuery Parameter \u2014 Schedule bits not set default: null scheduled_on_submit (optional)\nQuery Parameter \u2014 Job was started on submit default: null scheduled_by_main (optional)\nQuery Parameter \u2014 Job was started from main scheduler default: null scheduled_by_backfill (optional)\nQuery Parameter \u2014 Job was started from backfill default: null job_started (optional)\nQuery Parameter \u2014 Job start RPC was received default: null exit_code (optional)\nQuery Parameter \u2014 Job exit code (numeric) default: null show_duplicates (optional)\nQuery Parameter \u2014 Include duplicate job entries default: null skip_steps (optional)\nQuery Parameter \u2014 Exclude job step details default: null disable_truncate_usage_time (optional)\nQuery Parameter \u2014 Do not truncate the time to usage_start and usage_end default: null whole_hetjob (optional)\nQuery Parameter \u2014 Include details on all hetjob components default: null disable_whole_hetjob (optional)\nQuery Parameter \u2014 Only show details on specified hetjob components default: null disable_wait_for_result (optional)\nQuery Parameter \u2014 Tell dbd not to wait for the result default: null usage_time_as_submit_time (optional)\nQuery Parameter \u2014 Use usage_time as the submit_time of the job default: null show_batch_script (optional)\nQuery Parameter \u2014 Include job script default: null show_job_environment (optional)\nQuery Parameter \u2014 Include job environment default: null format (optional)\nQuery Parameter \u2014 CSV format list default: null groups (optional)\nQuery Parameter \u2014 CSV group list default: null job_name (optional)\nQuery Parameter \u2014 CSV job name list default: null partition (optional)\nQuery Parameter \u2014 CSV partition name list default: null qos (optional)\nQuery Parameter \u2014 CSV QOS name list default: null reason (optional)\nQuery Parameter \u2014 CSV reason list default: null reservation (optional)\nQuery Parameter \u2014 CSV reservation name list default: null reservation_id (optional)\nQuery Parameter \u2014 CSV reservation ID list default: null state (optional)\nQuery Parameter \u2014 CSV state list default: null step (optional)\nQuery Parameter \u2014 CSV step id list default: null end_time (optional)\nQuery Parameter \u2014 Usage end (UNIX timestamp) default: null start_time (optional)\nQuery Parameter \u2014 Usage start (UNIX timestamp) default: null node (optional)\nQuery Parameter \u2014 Ranged node string where jobs ran default: null users (optional)\nQuery Parameter \u2014 CSV user name list default: null wckey (optional)\nQuery Parameter \u2014 CSV wckey list default: null \n \nReturn type\n\nv0.0.41_openapi_slurmdbd_jobs_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"jobs\" : [ {\n    \"container\" : \"container\",\n    \"cluster\" : \"cluster\",\n    \"stdin_expanded\" : \"stdin_expanded\",\n    \"stdin\" : \"stdin\",\n    \"stdout\" : \"stdout\",\n    \"stderr_expanded\" : \"stderr_expanded\",\n    \"flags\" : [ \"NONE\", \"NONE\" ],\n    \"used_gres\" : \"used_gres\",\n    \"association\" : {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    },\n    \"allocation_nodes\" : 0,\n    \"working_directory\" : \"working_directory\",\n    \"constraints\" : \"constraints\",\n    \"required\" : {\n      \"memory_per_node\" : {\n        \"number\" : 6,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"CPUs\" : 6,\n      \"memory_per_cpu\" : {\n        \"number\" : 3,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"stdout_expanded\" : \"stdout_expanded\",\n    \"hold\" : true,\n    \"partition\" : \"partition\",\n    \"qos\" : \"qos\",\n    \"array\" : {\n      \"task\" : \"task\",\n      \"job_id\" : 6,\n      \"task_id\" : {\n        \"number\" : 5,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"limits\" : {\n        \"max\" : {\n          \"running\" : {\n            \"tasks\" : 1\n          }\n        }\n      }\n    },\n    \"het\" : {\n      \"job_id\" : 9,\n      \"job_offset\" : {\n        \"number\" : 6,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"submit_line\" : \"submit_line\",\n    \"extra\" : \"extra\",\n    \"reservation\" : {\n      \"name\" : \"name\",\n      \"id\" : 1\n    },\n    \"block\" : \"block\",\n    \"tres\" : {\n      \"requested\" : [ {\n        \"name\" : \"name\",\n        \"count\" : 7,\n        \"id\" : 3,\n        \"type\" : \"type\"\n      }, {\n        \"name\" : \"name\",\n        \"count\" : 7,\n        \"id\" : 3,\n        \"type\" : \"type\"\n      } ],\n      \"allocated\" : [ {\n        \"name\" : \"name\",\n        \"count\" : 7,\n        \"id\" : 3,\n        \"type\" : \"type\"\n      }, {\n        \"name\" : \"name\",\n        \"count\" : 7,\n        \"id\" : 3,\n        \"type\" : \"type\"\n      } ]\n    },\n    \"state\" : {\n      \"reason\" : \"reason\",\n      \"current\" : [ \"PENDING\", \"PENDING\" ]\n    },\n    \"mcs\" : {\n      \"label\" : \"label\"\n    },\n    \"group\" : \"group\",\n    \"wckey\" : {\n      \"wckey\" : \"wckey\",\n      \"flags\" : [ \"ASSIGNED_DEFAULT\", \"ASSIGNED_DEFAULT\" ]\n    },\n    \"priority\" : {\n      \"number\" : 9,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"stderr\" : \"stderr\",\n    \"steps\" : [ {\n      \"nodes\" : {\n        \"count\" : 6,\n        \"range\" : \"range\",\n        \"list\" : [ \"list\", \"list\" ]\n      },\n      \"task\" : {\n        \"distribution\" : \"distribution\"\n      },\n      \"exit_code\" : {\n        \"return_code\" : {\n          \"number\" : 2,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"signal\" : {\n          \"name\" : \"name\",\n          \"id\" : {\n            \"number\" : 7,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        },\n        \"status\" : [ \"INVALID\", \"INVALID\" ]\n      },\n      \"kill_request_user\" : \"kill_request_user\",\n      \"CPU\" : {\n        \"governor\" : \"governor\",\n        \"requested_frequency\" : {\n          \"min\" : {\n            \"number\" : 4,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"max\" : {\n            \"number\" : 8,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      },\n      \"pid\" : \"pid\",\n      \"step\" : {\n        \"name\" : \"name\",\n        \"id\" : \"id\"\n      },\n      \"tres\" : {\n        \"consumed\" : {\n          \"average\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"min\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"max\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"requested\" : {\n          \"average\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"min\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"max\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"allocated\" : [ {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        }, {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        } ]\n      },\n      \"time\" : {\n        \"elapsed\" : 2,\n        \"total\" : {\n          \"seconds\" : 3,\n          \"microseconds\" : 7\n        },\n        \"system\" : {\n          \"seconds\" : 6,\n          \"microseconds\" : 3\n        },\n        \"start\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"end\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"user\" : {\n          \"seconds\" : 0,\n          \"microseconds\" : 7\n        },\n        \"suspended\" : 5\n      },\n      \"state\" : [ \"PENDING\", \"PENDING\" ],\n      \"tasks\" : {\n        \"count\" : 0\n      },\n      \"statistics\" : {\n        \"CPU\" : {\n          \"actual_frequency\" : 7\n        },\n        \"energy\" : {\n          \"consumed\" : {\n            \"number\" : 3,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      }\n    }, {\n      \"nodes\" : {\n        \"count\" : 6,\n        \"range\" : \"range\",\n        \"list\" : [ \"list\", \"list\" ]\n      },\n      \"task\" : {\n        \"distribution\" : \"distribution\"\n      },\n      \"exit_code\" : {\n        \"return_code\" : {\n          \"number\" : 2,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"signal\" : {\n          \"name\" : \"name\",\n          \"id\" : {\n            \"number\" : 7,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        },\n        \"status\" : [ \"INVALID\", \"INVALID\" ]\n      },\n      \"kill_request_user\" : \"kill_request_user\",\n      \"CPU\" : {\n        \"governor\" : \"governor\",\n        \"requested_frequency\" : {\n          \"min\" : {\n            \"number\" : 4,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"max\" : {\n            \"number\" : 8,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      },\n      \"pid\" : \"pid\",\n      \"step\" : {\n        \"name\" : \"name\",\n        \"id\" : \"id\"\n      },\n      \"tres\" : {\n        \"consumed\" : {\n          \"average\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"min\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"max\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"requested\" : {\n          \"average\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"min\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"max\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"allocated\" : [ {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        }, {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        } ]\n      },\n      \"time\" : {\n        \"elapsed\" : 2,\n        \"total\" : {\n          \"seconds\" : 3,\n          \"microseconds\" : 7\n        },\n        \"system\" : {\n          \"seconds\" : 6,\n          \"microseconds\" : 3\n        },\n        \"start\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"end\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"user\" : {\n          \"seconds\" : 0,\n          \"microseconds\" : 7\n        },\n        \"suspended\" : 5\n      },\n      \"state\" : [ \"PENDING\", \"PENDING\" ],\n      \"tasks\" : {\n        \"count\" : 0\n      },\n      \"statistics\" : {\n        \"CPU\" : {\n          \"actual_frequency\" : 7\n        },\n        \"energy\" : {\n          \"consumed\" : {\n            \"number\" : 3,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      }\n    } ],\n    \"script\" : \"script\",\n    \"failed_node\" : \"failed_node\",\n    \"derived_exit_code\" : {\n      \"return_code\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"signal\" : {\n        \"name\" : \"name\",\n        \"id\" : {\n          \"number\" : 7,\n          \"set\" : true,\n          \"infinite\" : true\n        }\n      },\n      \"status\" : [ \"INVALID\", \"INVALID\" ]\n    },\n    \"licenses\" : \"licenses\",\n    \"nodes\" : \"nodes\",\n    \"job_id\" : 8,\n    \"exit_code\" : {\n      \"return_code\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"signal\" : {\n        \"name\" : \"name\",\n        \"id\" : {\n          \"number\" : 7,\n          \"set\" : true,\n          \"infinite\" : true\n        }\n      },\n      \"status\" : [ \"INVALID\", \"INVALID\" ]\n    },\n    \"name\" : \"name\",\n    \"kill_request_user\" : \"kill_request_user\",\n    \"comment\" : {\n      \"administrator\" : \"administrator\",\n      \"system\" : \"system\",\n      \"job\" : \"job\"\n    },\n    \"time\" : {\n      \"elapsed\" : 9,\n      \"total\" : {\n        \"seconds\" : 1,\n        \"microseconds\" : 4\n      },\n      \"system\" : {\n        \"seconds\" : 1,\n        \"microseconds\" : 6\n      },\n      \"eligible\" : 3,\n      \"start\" : 7,\n      \"limit\" : {\n        \"number\" : 7,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"end\" : 2,\n      \"submission\" : 1,\n      \"planned\" : {\n        \"number\" : 4,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"user\" : {\n        \"seconds\" : 5,\n        \"microseconds\" : 9\n      },\n      \"suspended\" : 1\n    },\n    \"user\" : \"user\",\n    \"account\" : \"account\"\n  }, {\n    \"container\" : \"container\",\n    \"cluster\" : \"cluster\",\n    \"stdin_expanded\" : \"stdin_expanded\",\n    \"stdin\" : \"stdin\",\n    \"stdout\" : \"stdout\",\n    \"stderr_expanded\" : \"stderr_expanded\",\n    \"flags\" : [ \"NONE\", \"NONE\" ],\n    \"used_gres\" : \"used_gres\",\n    \"association\" : {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    },\n    \"allocation_nodes\" : 0,\n    \"working_directory\" : \"working_directory\",\n    \"constraints\" : \"constraints\",\n    \"required\" : {\n      \"memory_per_node\" : {\n        \"number\" : 6,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"CPUs\" : 6,\n      \"memory_per_cpu\" : {\n        \"number\" : 3,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"stdout_expanded\" : \"stdout_expanded\",\n    \"hold\" : true,\n    \"partition\" : \"partition\",\n    \"qos\" : \"qos\",\n    \"array\" : {\n      \"task\" : \"task\",\n      \"job_id\" : 6,\n      \"task_id\" : {\n        \"number\" : 5,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"limits\" : {\n        \"max\" : {\n          \"running\" : {\n            \"tasks\" : 1\n          }\n        }\n      }\n    },\n    \"het\" : {\n      \"job_id\" : 9,\n      \"job_offset\" : {\n        \"number\" : 6,\n        \"set\" : true,\n        \"infinite\" : true\n      }\n    },\n    \"submit_line\" : \"submit_line\",\n    \"extra\" : \"extra\",\n    \"reservation\" : {\n      \"name\" : \"name\",\n      \"id\" : 1\n    },\n    \"block\" : \"block\",\n    \"tres\" : {\n      \"requested\" : [ {\n        \"name\" : \"name\",\n        \"count\" : 7,\n        \"id\" : 3,\n        \"type\" : \"type\"\n      }, {\n        \"name\" : \"name\",\n        \"count\" : 7,\n        \"id\" : 3,\n        \"type\" : \"type\"\n      } ],\n      \"allocated\" : [ {\n        \"name\" : \"name\",\n        \"count\" : 7,\n        \"id\" : 3,\n        \"type\" : \"type\"\n      }, {\n        \"name\" : \"name\",\n        \"count\" : 7,\n        \"id\" : 3,\n        \"type\" : \"type\"\n      } ]\n    },\n    \"state\" : {\n      \"reason\" : \"reason\",\n      \"current\" : [ \"PENDING\", \"PENDING\" ]\n    },\n    \"mcs\" : {\n      \"label\" : \"label\"\n    },\n    \"group\" : \"group\",\n    \"wckey\" : {\n      \"wckey\" : \"wckey\",\n      \"flags\" : [ \"ASSIGNED_DEFAULT\", \"ASSIGNED_DEFAULT\" ]\n    },\n    \"priority\" : {\n      \"number\" : 9,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"stderr\" : \"stderr\",\n    \"steps\" : [ {\n      \"nodes\" : {\n        \"count\" : 6,\n        \"range\" : \"range\",\n        \"list\" : [ \"list\", \"list\" ]\n      },\n      \"task\" : {\n        \"distribution\" : \"distribution\"\n      },\n      \"exit_code\" : {\n        \"return_code\" : {\n          \"number\" : 2,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"signal\" : {\n          \"name\" : \"name\",\n          \"id\" : {\n            \"number\" : 7,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        },\n        \"status\" : [ \"INVALID\", \"INVALID\" ]\n      },\n      \"kill_request_user\" : \"kill_request_user\",\n      \"CPU\" : {\n        \"governor\" : \"governor\",\n        \"requested_frequency\" : {\n          \"min\" : {\n            \"number\" : 4,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"max\" : {\n            \"number\" : 8,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      },\n      \"pid\" : \"pid\",\n      \"step\" : {\n        \"name\" : \"name\",\n        \"id\" : \"id\"\n      },\n      \"tres\" : {\n        \"consumed\" : {\n          \"average\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"min\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"max\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"requested\" : {\n          \"average\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"min\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"max\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"allocated\" : [ {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        }, {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        } ]\n      },\n      \"time\" : {\n        \"elapsed\" : 2,\n        \"total\" : {\n          \"seconds\" : 3,\n          \"microseconds\" : 7\n        },\n        \"system\" : {\n          \"seconds\" : 6,\n          \"microseconds\" : 3\n        },\n        \"start\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"end\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"user\" : {\n          \"seconds\" : 0,\n          \"microseconds\" : 7\n        },\n        \"suspended\" : 5\n      },\n      \"state\" : [ \"PENDING\", \"PENDING\" ],\n      \"tasks\" : {\n        \"count\" : 0\n      },\n      \"statistics\" : {\n        \"CPU\" : {\n          \"actual_frequency\" : 7\n        },\n        \"energy\" : {\n          \"consumed\" : {\n            \"number\" : 3,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      }\n    }, {\n      \"nodes\" : {\n        \"count\" : 6,\n        \"range\" : \"range\",\n        \"list\" : [ \"list\", \"list\" ]\n      },\n      \"task\" : {\n        \"distribution\" : \"distribution\"\n      },\n      \"exit_code\" : {\n        \"return_code\" : {\n          \"number\" : 2,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"signal\" : {\n          \"name\" : \"name\",\n          \"id\" : {\n            \"number\" : 7,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        },\n        \"status\" : [ \"INVALID\", \"INVALID\" ]\n      },\n      \"kill_request_user\" : \"kill_request_user\",\n      \"CPU\" : {\n        \"governor\" : \"governor\",\n        \"requested_frequency\" : {\n          \"min\" : {\n            \"number\" : 4,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"max\" : {\n            \"number\" : 8,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      },\n      \"pid\" : \"pid\",\n      \"step\" : {\n        \"name\" : \"name\",\n        \"id\" : \"id\"\n      },\n      \"tres\" : {\n        \"consumed\" : {\n          \"average\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"min\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"max\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"requested\" : {\n          \"average\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"min\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"max\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ]\n        },\n        \"allocated\" : [ {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        }, {\n          \"name\" : \"name\",\n          \"count\" : 7,\n          \"id\" : 3,\n          \"type\" : \"type\"\n        } ]\n      },\n      \"time\" : {\n        \"elapsed\" : 2,\n        \"total\" : {\n          \"seconds\" : 3,\n          \"microseconds\" : 7\n        },\n        \"system\" : {\n          \"seconds\" : 6,\n          \"microseconds\" : 3\n        },\n        \"start\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"end\" : {\n          \"number\" : 6,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"user\" : {\n          \"seconds\" : 0,\n          \"microseconds\" : 7\n        },\n        \"suspended\" : 5\n      },\n      \"state\" : [ \"PENDING\", \"PENDING\" ],\n      \"tasks\" : {\n        \"count\" : 0\n      },\n      \"statistics\" : {\n        \"CPU\" : {\n          \"actual_frequency\" : 7\n        },\n        \"energy\" : {\n          \"consumed\" : {\n            \"number\" : 3,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      }\n    } ],\n    \"script\" : \"script\",\n    \"failed_node\" : \"failed_node\",\n    \"derived_exit_code\" : {\n      \"return_code\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"signal\" : {\n        \"name\" : \"name\",\n        \"id\" : {\n          \"number\" : 7,\n          \"set\" : true,\n          \"infinite\" : true\n        }\n      },\n      \"status\" : [ \"INVALID\", \"INVALID\" ]\n    },\n    \"licenses\" : \"licenses\",\n    \"nodes\" : \"nodes\",\n    \"job_id\" : 8,\n    \"exit_code\" : {\n      \"return_code\" : {\n        \"number\" : 2,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"signal\" : {\n        \"name\" : \"name\",\n        \"id\" : {\n          \"number\" : 7,\n          \"set\" : true,\n          \"infinite\" : true\n        }\n      },\n      \"status\" : [ \"INVALID\", \"INVALID\" ]\n    },\n    \"name\" : \"name\",\n    \"kill_request_user\" : \"kill_request_user\",\n    \"comment\" : {\n      \"administrator\" : \"administrator\",\n      \"system\" : \"system\",\n      \"job\" : \"job\"\n    },\n    \"time\" : {\n      \"elapsed\" : 9,\n      \"total\" : {\n        \"seconds\" : 1,\n        \"microseconds\" : 4\n      },\n      \"system\" : {\n        \"seconds\" : 1,\n        \"microseconds\" : 6\n      },\n      \"eligible\" : 3,\n      \"start\" : 7,\n      \"limit\" : {\n        \"number\" : 7,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"end\" : 2,\n      \"submission\" : 1,\n      \"planned\" : {\n        \"number\" : 4,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"user\" : {\n        \"seconds\" : 5,\n        \"microseconds\" : 9\n      },\n      \"suspended\" : 1\n    },\n    \"user\" : \"user\",\n    \"account\" : \"account\"\n  } ],\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    List of jobs\n        v0.0.41_openapi_slurmdbd_jobs_resp\ndefault\n    List of jobs\n        v0.0.41_openapi_slurmdbd_jobs_resp\n\n\nUp\nget /slurmdb/v0.0.41/qos/\nGet QOS list (slurmdbV0041GetQos)\n\nQuery parameters\n\ndescription (optional)\nQuery Parameter \u2014 CSV description list default: null id (optional)\nQuery Parameter \u2014 CSV QOS id list default: null format (optional)\nQuery Parameter \u2014 CSV format list default: null name (optional)\nQuery Parameter \u2014 CSV QOS name list default: null preempt_mode (optional)\nQuery Parameter \u2014 PreemptMode used when jobs in this QOS are preempted default: null with_deleted (optional)\nQuery Parameter \u2014 Include deleted QOS default: null \n \nReturn type\n\nv0.0.41_openapi_slurmdbd_qos_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"qos\" : [ {\n    \"flags\" : [ \"NOT_SET\", \"NOT_SET\" ],\n    \"name\" : \"name\",\n    \"usage_threshold\" : {\n      \"number\" : 6.965117697638846,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"description\" : \"description\",\n    \"usage_factor\" : {\n      \"number\" : 3.5571952270680973,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"id\" : 4,\n    \"priority\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"limits\" : {\n      \"min\" : {\n        \"priority_threshold\" : {\n          \"number\" : 8,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"tres\" : {\n          \"per\" : {\n            \"job\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ]\n          }\n        }\n      },\n      \"max\" : {\n        \"jobs\" : {\n          \"per\" : {\n            \"user\" : {\n              \"number\" : 5,\n              \"set\" : true,\n              \"infinite\" : true\n            },\n            \"account\" : {\n              \"number\" : 4,\n              \"set\" : true,\n              \"infinite\" : true\n            }\n          },\n          \"active_jobs\" : {\n            \"per\" : {\n              \"user\" : {\n                \"number\" : 1,\n                \"set\" : true,\n                \"infinite\" : true\n              },\n              \"account\" : {\n                \"number\" : 7,\n                \"set\" : true,\n                \"infinite\" : true\n              }\n            }\n          }\n        },\n        \"accruing\" : {\n          \"per\" : {\n            \"user\" : {\n              \"number\" : 9,\n              \"set\" : true,\n              \"infinite\" : true\n            },\n            \"account\" : {\n              \"number\" : 9,\n              \"set\" : true,\n              \"infinite\" : true\n            }\n          }\n        },\n        \"tres\" : {\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"minutes\" : {\n            \"per\" : {\n              \"qos\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ],\n              \"job\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ],\n              \"user\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ],\n              \"account\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ]\n            }\n          },\n          \"per\" : {\n            \"node\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ],\n            \"job\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ],\n            \"user\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ],\n            \"account\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ]\n          }\n        },\n        \"wall_clock\" : {\n          \"per\" : {\n            \"qos\" : {\n              \"number\" : 1,\n              \"set\" : true,\n              \"infinite\" : true\n            },\n            \"job\" : {\n              \"number\" : 6,\n              \"set\" : true,\n              \"infinite\" : true\n            }\n          }\n        },\n        \"active_jobs\" : {\n          \"count\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"accruing\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      },\n      \"factor\" : {\n        \"number\" : 6.683562403749608,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"grace_time\" : 7\n    },\n    \"preempt\" : {\n      \"mode\" : [ \"DISABLED\", \"DISABLED\" ],\n      \"exempt_time\" : {\n        \"number\" : 9,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"list\" : [ \"list\", \"list\" ]\n    }\n  }, {\n    \"flags\" : [ \"NOT_SET\", \"NOT_SET\" ],\n    \"name\" : \"name\",\n    \"usage_threshold\" : {\n      \"number\" : 6.965117697638846,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"description\" : \"description\",\n    \"usage_factor\" : {\n      \"number\" : 3.5571952270680973,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"id\" : 4,\n    \"priority\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"limits\" : {\n      \"min\" : {\n        \"priority_threshold\" : {\n          \"number\" : 8,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"tres\" : {\n          \"per\" : {\n            \"job\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ]\n          }\n        }\n      },\n      \"max\" : {\n        \"jobs\" : {\n          \"per\" : {\n            \"user\" : {\n              \"number\" : 5,\n              \"set\" : true,\n              \"infinite\" : true\n            },\n            \"account\" : {\n              \"number\" : 4,\n              \"set\" : true,\n              \"infinite\" : true\n            }\n          },\n          \"active_jobs\" : {\n            \"per\" : {\n              \"user\" : {\n                \"number\" : 1,\n                \"set\" : true,\n                \"infinite\" : true\n              },\n              \"account\" : {\n                \"number\" : 7,\n                \"set\" : true,\n                \"infinite\" : true\n              }\n            }\n          }\n        },\n        \"accruing\" : {\n          \"per\" : {\n            \"user\" : {\n              \"number\" : 9,\n              \"set\" : true,\n              \"infinite\" : true\n            },\n            \"account\" : {\n              \"number\" : 9,\n              \"set\" : true,\n              \"infinite\" : true\n            }\n          }\n        },\n        \"tres\" : {\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"minutes\" : {\n            \"per\" : {\n              \"qos\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ],\n              \"job\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ],\n              \"user\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ],\n              \"account\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ]\n            }\n          },\n          \"per\" : {\n            \"node\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ],\n            \"job\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ],\n            \"user\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ],\n            \"account\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ]\n          }\n        },\n        \"wall_clock\" : {\n          \"per\" : {\n            \"qos\" : {\n              \"number\" : 1,\n              \"set\" : true,\n              \"infinite\" : true\n            },\n            \"job\" : {\n              \"number\" : 6,\n              \"set\" : true,\n              \"infinite\" : true\n            }\n          }\n        },\n        \"active_jobs\" : {\n          \"count\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"accruing\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      },\n      \"factor\" : {\n        \"number\" : 6.683562403749608,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"grace_time\" : 7\n    },\n    \"preempt\" : {\n      \"mode\" : [ \"DISABLED\", \"DISABLED\" ],\n      \"exempt_time\" : {\n        \"number\" : 9,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"list\" : [ \"list\", \"list\" ]\n    }\n  } ],\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    List of QOS\n        v0.0.41_openapi_slurmdbd_qos_resp\ndefault\n    List of QOS\n        v0.0.41_openapi_slurmdbd_qos_resp\n\n\nUp\nget /slurmdb/v0.0.41/qos/{qos}\nGet QOS info (slurmdbV0041GetSingleQos)\n\nPath parameters\n\nqos (required)\nPath Parameter \u2014 QOS name default: null \n \nQuery parameters\n\nwith_deleted (optional)\nQuery Parameter \u2014 Query includes deleted QOS default: null \n \nReturn type\n\nv0.0.41_openapi_slurmdbd_qos_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"qos\" : [ {\n    \"flags\" : [ \"NOT_SET\", \"NOT_SET\" ],\n    \"name\" : \"name\",\n    \"usage_threshold\" : {\n      \"number\" : 6.965117697638846,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"description\" : \"description\",\n    \"usage_factor\" : {\n      \"number\" : 3.5571952270680973,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"id\" : 4,\n    \"priority\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"limits\" : {\n      \"min\" : {\n        \"priority_threshold\" : {\n          \"number\" : 8,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"tres\" : {\n          \"per\" : {\n            \"job\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ]\n          }\n        }\n      },\n      \"max\" : {\n        \"jobs\" : {\n          \"per\" : {\n            \"user\" : {\n              \"number\" : 5,\n              \"set\" : true,\n              \"infinite\" : true\n            },\n            \"account\" : {\n              \"number\" : 4,\n              \"set\" : true,\n              \"infinite\" : true\n            }\n          },\n          \"active_jobs\" : {\n            \"per\" : {\n              \"user\" : {\n                \"number\" : 1,\n                \"set\" : true,\n                \"infinite\" : true\n              },\n              \"account\" : {\n                \"number\" : 7,\n                \"set\" : true,\n                \"infinite\" : true\n              }\n            }\n          }\n        },\n        \"accruing\" : {\n          \"per\" : {\n            \"user\" : {\n              \"number\" : 9,\n              \"set\" : true,\n              \"infinite\" : true\n            },\n            \"account\" : {\n              \"number\" : 9,\n              \"set\" : true,\n              \"infinite\" : true\n            }\n          }\n        },\n        \"tres\" : {\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"minutes\" : {\n            \"per\" : {\n              \"qos\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ],\n              \"job\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ],\n              \"user\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ],\n              \"account\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ]\n            }\n          },\n          \"per\" : {\n            \"node\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ],\n            \"job\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ],\n            \"user\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ],\n            \"account\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ]\n          }\n        },\n        \"wall_clock\" : {\n          \"per\" : {\n            \"qos\" : {\n              \"number\" : 1,\n              \"set\" : true,\n              \"infinite\" : true\n            },\n            \"job\" : {\n              \"number\" : 6,\n              \"set\" : true,\n              \"infinite\" : true\n            }\n          }\n        },\n        \"active_jobs\" : {\n          \"count\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"accruing\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      },\n      \"factor\" : {\n        \"number\" : 6.683562403749608,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"grace_time\" : 7\n    },\n    \"preempt\" : {\n      \"mode\" : [ \"DISABLED\", \"DISABLED\" ],\n      \"exempt_time\" : {\n        \"number\" : 9,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"list\" : [ \"list\", \"list\" ]\n    }\n  }, {\n    \"flags\" : [ \"NOT_SET\", \"NOT_SET\" ],\n    \"name\" : \"name\",\n    \"usage_threshold\" : {\n      \"number\" : 6.965117697638846,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"description\" : \"description\",\n    \"usage_factor\" : {\n      \"number\" : 3.5571952270680973,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"id\" : 4,\n    \"priority\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"limits\" : {\n      \"min\" : {\n        \"priority_threshold\" : {\n          \"number\" : 8,\n          \"set\" : true,\n          \"infinite\" : true\n        },\n        \"tres\" : {\n          \"per\" : {\n            \"job\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ]\n          }\n        }\n      },\n      \"max\" : {\n        \"jobs\" : {\n          \"per\" : {\n            \"user\" : {\n              \"number\" : 5,\n              \"set\" : true,\n              \"infinite\" : true\n            },\n            \"account\" : {\n              \"number\" : 4,\n              \"set\" : true,\n              \"infinite\" : true\n            }\n          },\n          \"active_jobs\" : {\n            \"per\" : {\n              \"user\" : {\n                \"number\" : 1,\n                \"set\" : true,\n                \"infinite\" : true\n              },\n              \"account\" : {\n                \"number\" : 7,\n                \"set\" : true,\n                \"infinite\" : true\n              }\n            }\n          }\n        },\n        \"accruing\" : {\n          \"per\" : {\n            \"user\" : {\n              \"number\" : 9,\n              \"set\" : true,\n              \"infinite\" : true\n            },\n            \"account\" : {\n              \"number\" : 9,\n              \"set\" : true,\n              \"infinite\" : true\n            }\n          }\n        },\n        \"tres\" : {\n          \"total\" : [ {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          }, {\n            \"name\" : \"name\",\n            \"count\" : 7,\n            \"id\" : 3,\n            \"type\" : \"type\"\n          } ],\n          \"minutes\" : {\n            \"per\" : {\n              \"qos\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ],\n              \"job\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ],\n              \"user\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ],\n              \"account\" : [ {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              }, {\n                \"name\" : \"name\",\n                \"count\" : 7,\n                \"id\" : 3,\n                \"type\" : \"type\"\n              } ]\n            }\n          },\n          \"per\" : {\n            \"node\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ],\n            \"job\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ],\n            \"user\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ],\n            \"account\" : [ {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            }, {\n              \"name\" : \"name\",\n              \"count\" : 7,\n              \"id\" : 3,\n              \"type\" : \"type\"\n            } ]\n          }\n        },\n        \"wall_clock\" : {\n          \"per\" : {\n            \"qos\" : {\n              \"number\" : 1,\n              \"set\" : true,\n              \"infinite\" : true\n            },\n            \"job\" : {\n              \"number\" : 6,\n              \"set\" : true,\n              \"infinite\" : true\n            }\n          }\n        },\n        \"active_jobs\" : {\n          \"count\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          },\n          \"accruing\" : {\n            \"number\" : 1,\n            \"set\" : true,\n            \"infinite\" : true\n          }\n        }\n      },\n      \"factor\" : {\n        \"number\" : 6.683562403749608,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"grace_time\" : 7\n    },\n    \"preempt\" : {\n      \"mode\" : [ \"DISABLED\", \"DISABLED\" ],\n      \"exempt_time\" : {\n        \"number\" : 9,\n        \"set\" : true,\n        \"infinite\" : true\n      },\n      \"list\" : [ \"list\", \"list\" ]\n    }\n  } ],\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    QOS information\n        v0.0.41_openapi_slurmdbd_qos_resp\ndefault\n    QOS information\n        v0.0.41_openapi_slurmdbd_qos_resp\n\n\nUp\nget /slurmdb/v0.0.41/tres/\nGet TRES info (slurmdbV0041GetTres)\n\nReturn type\n\nv0.0.41_openapi_tres_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"TRES\" : [ {\n    \"name\" : \"name\",\n    \"count\" : 7,\n    \"id\" : 3,\n    \"type\" : \"type\"\n  }, {\n    \"name\" : \"name\",\n    \"count\" : 7,\n    \"id\" : 3,\n    \"type\" : \"type\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    List of TRES\n        v0.0.41_openapi_tres_resp\ndefault\n    List of TRES\n        v0.0.41_openapi_tres_resp\n\n\nUp\nget /slurmdb/v0.0.41/user/{name}\nGet user info (slurmdbV0041GetUser)\n\nPath parameters\n\nname (required)\nPath Parameter \u2014 User name default: null \n \nQuery parameters\n\nwith_deleted (optional)\nQuery Parameter \u2014 Include deleted users default: null with_assocs (optional)\nQuery Parameter \u2014 Include associations default: null with_coords (optional)\nQuery Parameter \u2014 Include coordinators default: null with_wckeys (optional)\nQuery Parameter \u2014 Include wckeys default: null \n \nReturn type\n\nv0.0.41_openapi_users_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"users\" : [ {\n    \"associations\" : [ {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    }, {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    } ],\n    \"default\" : {\n      \"wckey\" : \"wckey\",\n      \"account\" : \"account\"\n    },\n    \"administrator_level\" : [ \"Not Set\", \"Not Set\" ],\n    \"old_name\" : \"old_name\",\n    \"wckeys\" : [ {\n      \"cluster\" : \"cluster\",\n      \"name\" : \"name\",\n      \"flags\" : [ \"DELETED\", \"DELETED\" ],\n      \"accounting\" : [ {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      }, {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      } ],\n      \"id\" : 2,\n      \"user\" : \"user\"\n    }, {\n      \"cluster\" : \"cluster\",\n      \"name\" : \"name\",\n      \"flags\" : [ \"DELETED\", \"DELETED\" ],\n      \"accounting\" : [ {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      }, {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      } ],\n      \"id\" : 2,\n      \"user\" : \"user\"\n    } ],\n    \"coordinators\" : [ {\n      \"name\" : \"name\",\n      \"direct\" : true\n    }, {\n      \"name\" : \"name\",\n      \"direct\" : true\n    } ],\n    \"flags\" : [ \"NONE\", \"NONE\" ],\n    \"name\" : \"name\"\n  }, {\n    \"associations\" : [ {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    }, {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    } ],\n    \"default\" : {\n      \"wckey\" : \"wckey\",\n      \"account\" : \"account\"\n    },\n    \"administrator_level\" : [ \"Not Set\", \"Not Set\" ],\n    \"old_name\" : \"old_name\",\n    \"wckeys\" : [ {\n      \"cluster\" : \"cluster\",\n      \"name\" : \"name\",\n      \"flags\" : [ \"DELETED\", \"DELETED\" ],\n      \"accounting\" : [ {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      }, {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      } ],\n      \"id\" : 2,\n      \"user\" : \"user\"\n    }, {\n      \"cluster\" : \"cluster\",\n      \"name\" : \"name\",\n      \"flags\" : [ \"DELETED\", \"DELETED\" ],\n      \"accounting\" : [ {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      }, {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      } ],\n      \"id\" : 2,\n      \"user\" : \"user\"\n    } ],\n    \"coordinators\" : [ {\n      \"name\" : \"name\",\n      \"direct\" : true\n    }, {\n      \"name\" : \"name\",\n      \"direct\" : true\n    } ],\n    \"flags\" : [ \"NONE\", \"NONE\" ],\n    \"name\" : \"name\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    List of users\n        v0.0.41_openapi_users_resp\ndefault\n    List of users\n        v0.0.41_openapi_users_resp\n\n\nUp\nget /slurmdb/v0.0.41/users/\nGet user list (slurmdbV0041GetUsers)\n\nQuery parameters\n\nadmin_level (optional)\nQuery Parameter \u2014 Administrator level default: null default_account (optional)\nQuery Parameter \u2014 CSV default account list default: null default_wckey (optional)\nQuery Parameter \u2014 CSV default wckey list default: null with_assocs (optional)\nQuery Parameter \u2014 With associations default: null with_coords (optional)\nQuery Parameter \u2014 With coordinators default: null with_deleted (optional)\nQuery Parameter \u2014 With deleted default: null with_wckeys (optional)\nQuery Parameter \u2014 With wckeys default: null without_defaults (optional)\nQuery Parameter \u2014 Exclude defaults default: null \n \nReturn type\n\nv0.0.41_openapi_users_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"users\" : [ {\n    \"associations\" : [ {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    }, {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    } ],\n    \"default\" : {\n      \"wckey\" : \"wckey\",\n      \"account\" : \"account\"\n    },\n    \"administrator_level\" : [ \"Not Set\", \"Not Set\" ],\n    \"old_name\" : \"old_name\",\n    \"wckeys\" : [ {\n      \"cluster\" : \"cluster\",\n      \"name\" : \"name\",\n      \"flags\" : [ \"DELETED\", \"DELETED\" ],\n      \"accounting\" : [ {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      }, {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      } ],\n      \"id\" : 2,\n      \"user\" : \"user\"\n    }, {\n      \"cluster\" : \"cluster\",\n      \"name\" : \"name\",\n      \"flags\" : [ \"DELETED\", \"DELETED\" ],\n      \"accounting\" : [ {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      }, {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      } ],\n      \"id\" : 2,\n      \"user\" : \"user\"\n    } ],\n    \"coordinators\" : [ {\n      \"name\" : \"name\",\n      \"direct\" : true\n    }, {\n      \"name\" : \"name\",\n      \"direct\" : true\n    } ],\n    \"flags\" : [ \"NONE\", \"NONE\" ],\n    \"name\" : \"name\"\n  }, {\n    \"associations\" : [ {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    }, {\n      \"cluster\" : \"cluster\",\n      \"partition\" : \"partition\",\n      \"id\" : 5,\n      \"user\" : \"user\",\n      \"account\" : \"account\"\n    } ],\n    \"default\" : {\n      \"wckey\" : \"wckey\",\n      \"account\" : \"account\"\n    },\n    \"administrator_level\" : [ \"Not Set\", \"Not Set\" ],\n    \"old_name\" : \"old_name\",\n    \"wckeys\" : [ {\n      \"cluster\" : \"cluster\",\n      \"name\" : \"name\",\n      \"flags\" : [ \"DELETED\", \"DELETED\" ],\n      \"accounting\" : [ {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      }, {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      } ],\n      \"id\" : 2,\n      \"user\" : \"user\"\n    }, {\n      \"cluster\" : \"cluster\",\n      \"name\" : \"name\",\n      \"flags\" : [ \"DELETED\", \"DELETED\" ],\n      \"accounting\" : [ {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      }, {\n        \"start\" : 7,\n        \"id\" : 2,\n        \"TRES\" : {\n          \"name\" : \"name\",\n          \"count\" : 3,\n          \"id\" : 9,\n          \"type\" : \"type\"\n        },\n        \"allocated\" : {\n          \"seconds\" : 5\n        }\n      } ],\n      \"id\" : 2,\n      \"user\" : \"user\"\n    } ],\n    \"coordinators\" : [ {\n      \"name\" : \"name\",\n      \"direct\" : true\n    }, {\n      \"name\" : \"name\",\n      \"direct\" : true\n    } ],\n    \"flags\" : [ \"NONE\", \"NONE\" ],\n    \"name\" : \"name\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    List of users\n        v0.0.41_openapi_users_resp\ndefault\n    List of users\n        v0.0.41_openapi_users_resp\n\n\nUp\nget /slurmdb/v0.0.41/wckey/{id}\nGet wckey info (slurmdbV0041GetWckey)\n\nPath parameters\n\nid (required)\nPath Parameter \u2014 wckey id default: null \n \nReturn type\n\nv0.0.41_openapi_wckey_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"wckeys\" : [ {\n    \"cluster\" : \"cluster\",\n    \"name\" : \"name\",\n    \"flags\" : [ \"DELETED\", \"DELETED\" ],\n    \"accounting\" : [ {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    }, {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    } ],\n    \"id\" : 2,\n    \"user\" : \"user\"\n  }, {\n    \"cluster\" : \"cluster\",\n    \"name\" : \"name\",\n    \"flags\" : [ \"DELETED\", \"DELETED\" ],\n    \"accounting\" : [ {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    }, {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    } ],\n    \"id\" : 2,\n    \"user\" : \"user\"\n  } ],\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    Description of wckey\n        v0.0.41_openapi_wckey_resp\ndefault\n    Description of wckey\n        v0.0.41_openapi_wckey_resp\n\n\nUp\nget /slurmdb/v0.0.41/wckeys/\nGet wckey list (slurmdbV0041GetWckeys)\n\nQuery parameters\n\ncluster (optional)\nQuery Parameter \u2014 CSV cluster name list default: null format (optional)\nQuery Parameter \u2014 CSV format name list default: null id (optional)\nQuery Parameter \u2014 CSV id list default: null name (optional)\nQuery Parameter \u2014 CSV name list default: null only_defaults (optional)\nQuery Parameter \u2014 Only query defaults default: null usage_end (optional)\nQuery Parameter \u2014 Usage end (UNIX timestamp) default: null usage_start (optional)\nQuery Parameter \u2014 Usage start (UNIX timestamp) default: null user (optional)\nQuery Parameter \u2014 CSV user list default: null with_usage (optional)\nQuery Parameter \u2014 Include usage default: null with_deleted (optional)\nQuery Parameter \u2014 Include deleted wckeys default: null \n \nReturn type\n\nv0.0.41_openapi_wckey_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"wckeys\" : [ {\n    \"cluster\" : \"cluster\",\n    \"name\" : \"name\",\n    \"flags\" : [ \"DELETED\", \"DELETED\" ],\n    \"accounting\" : [ {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    }, {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    } ],\n    \"id\" : 2,\n    \"user\" : \"user\"\n  }, {\n    \"cluster\" : \"cluster\",\n    \"name\" : \"name\",\n    \"flags\" : [ \"DELETED\", \"DELETED\" ],\n    \"accounting\" : [ {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    }, {\n      \"start\" : 7,\n      \"id\" : 2,\n      \"TRES\" : {\n        \"name\" : \"name\",\n        \"count\" : 3,\n        \"id\" : 9,\n        \"type\" : \"type\"\n      },\n      \"allocated\" : {\n        \"seconds\" : 5\n      }\n    } ],\n    \"id\" : 2,\n    \"user\" : \"user\"\n  } ],\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    List of wckeys\n        v0.0.41_openapi_wckey_resp\ndefault\n    List of wckeys\n        v0.0.41_openapi_wckey_resp\n\n\nUp\npost /slurmdb/v0.0.41/accounts/\nAdd/update list of accounts (slurmdbV0041PostAccounts)\n\nConsumes\n    This API call consumes the following media types via the Content-Type request header:\n    \napplication/json\napplication/x-yaml\n\nRequest body\n\nv0.0.41_openapi_accounts_resp v0.0.41_openapi_accounts_resp (optional)\nBody Parameter \u2014  \n \nReturn type\n\nv0.0.41_openapi_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    Status of account update request\n        v0.0.41_openapi_resp\ndefault\n    Status of account update request\n        v0.0.41_openapi_resp\n\n\nUp\npost /slurmdb/v0.0.41/accounts_association/\nAdd accounts with conditional association (slurmdbV0041PostAccountsAssociation)\n\nConsumes\n    This API call consumes the following media types via the Content-Type request header:\n    \napplication/json\napplication/x-yaml\n\nRequest body\n\nv0.0.41_openapi_accounts_add_cond_resp v0.0.41_openapi_accounts_add_cond_resp (optional)\nBody Parameter \u2014  \n \nReturn type\n\nv0.0.41_openapi_accounts_add_cond_resp_str\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ],\n  \"added_accounts\" : \"added_accounts\"\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    Status of account addition request\n        v0.0.41_openapi_accounts_add_cond_resp_str\ndefault\n    Status of account addition request\n        v0.0.41_openapi_accounts_add_cond_resp_str\n\n\nUp\npost /slurmdb/v0.0.41/associations/\nSet associations info (slurmdbV0041PostAssociations)\n\nConsumes\n    This API call consumes the following media types via the Content-Type request header:\n    \napplication/json\napplication/x-yaml\n\nRequest body\n\nv0.0.41_openapi_assocs_resp v0.0.41_openapi_assocs_resp (optional)\nBody Parameter \u2014  \n \nReturn type\n\nv0.0.41_openapi_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    status of associations update\n        v0.0.41_openapi_resp\ndefault\n    status of associations update\n        v0.0.41_openapi_resp\n\n\nUp\npost /slurmdb/v0.0.41/clusters/\nGet cluster list (slurmdbV0041PostClusters)\n\nConsumes\n    This API call consumes the following media types via the Content-Type request header:\n    \napplication/json\napplication/x-yaml\n\nRequest body\n\nv0.0.41_openapi_clusters_resp v0.0.41_openapi_clusters_resp (optional)\nBody Parameter \u2014  \n \nQuery parameters\n\nupdate_time (optional)\nQuery Parameter \u2014 Filter reservations since update timestamp default: null \n \nReturn type\n\nv0.0.41_openapi_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    Result of modify clusters request\n        v0.0.41_openapi_resp\ndefault\n    Result of modify clusters request\n        v0.0.41_openapi_resp\n\n\nUp\npost /slurmdb/v0.0.41/config\nLoad all configuration information (slurmdbV0041PostConfig)\n\nConsumes\n    This API call consumes the following media types via the Content-Type request header:\n    \napplication/json\napplication/x-yaml\n\nRequest body\n\nv0.0.41_openapi_slurmdbd_config_resp v0.0.41_openapi_slurmdbd_config_resp (optional)\nBody Parameter \u2014  \n \nReturn type\n\nv0.0.41_openapi_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    slurmdbd configuration\n        v0.0.41_openapi_resp\ndefault\n    slurmdbd configuration\n        v0.0.41_openapi_resp\n\n\nUp\npost /slurmdb/v0.0.41/qos/\nAdd or update QOSs (slurmdbV0041PostQos)\n\nConsumes\n    This API call consumes the following media types via the Content-Type request header:\n    \napplication/json\napplication/x-yaml\n\nRequest body\n\nv0.0.41_openapi_slurmdbd_qos_resp v0.0.41_openapi_slurmdbd_qos_resp (optional)\nBody Parameter \u2014  \n \nQuery parameters\n\ndescription (optional)\nQuery Parameter \u2014 CSV description list default: null id (optional)\nQuery Parameter \u2014 CSV QOS id list default: null format (optional)\nQuery Parameter \u2014 CSV format list default: null name (optional)\nQuery Parameter \u2014 CSV QOS name list default: null preempt_mode (optional)\nQuery Parameter \u2014 PreemptMode used when jobs in this QOS are preempted default: null with_deleted (optional)\nQuery Parameter \u2014 Include deleted QOS default: null \n \nReturn type\n\nv0.0.41_openapi_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    QOS update response\n        v0.0.41_openapi_resp\ndefault\n    QOS update response\n        v0.0.41_openapi_resp\n\n\nUp\npost /slurmdb/v0.0.41/tres/\nAdd TRES (slurmdbV0041PostTres)\n\nConsumes\n    This API call consumes the following media types via the Content-Type request header:\n    \napplication/json\napplication/x-yaml\n\nRequest body\n\nv0.0.41_openapi_tres_resp v0.0.41_openapi_tres_resp (optional)\nBody Parameter \u2014  \n \nReturn type\n\nv0.0.41_openapi_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    TRES update result\n        v0.0.41_openapi_resp\ndefault\n    TRES update result\n        v0.0.41_openapi_resp\n\n\nUp\npost /slurmdb/v0.0.41/users/\nUpdate users (slurmdbV0041PostUsers)\n\nConsumes\n    This API call consumes the following media types via the Content-Type request header:\n    \napplication/json\napplication/x-yaml\n\nRequest body\n\nv0.0.41_openapi_users_resp v0.0.41_openapi_users_resp (optional)\nBody Parameter \u2014  \n \nReturn type\n\nv0.0.41_openapi_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    Status of user update request\n        v0.0.41_openapi_resp\ndefault\n    Status of user update request\n        v0.0.41_openapi_resp\n\n\nUp\npost /slurmdb/v0.0.41/users_association/\nAdd users with conditional association (slurmdbV0041PostUsersAssociation)\n\nConsumes\n    This API call consumes the following media types via the Content-Type request header:\n    \napplication/json\napplication/x-yaml\n\nRequest body\n\nv0.0.41_openapi_users_add_cond_resp v0.0.41_openapi_users_add_cond_resp (optional)\nBody Parameter \u2014  \n \nQuery parameters\n\nupdate_time (optional)\nQuery Parameter \u2014 Filter partitions since update timestamp default: null flags (optional)\nQuery Parameter \u2014 Query flags default: null \n \nReturn type\n\nv0.0.41_openapi_users_add_cond_resp_str\n\n\nExample data\nContent-Type: application/json\n{\n  \"added_users\" : \"added_users\",\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    Add list of users with conditional association\n        v0.0.41_openapi_users_add_cond_resp_str\ndefault\n    Add list of users with conditional association\n        v0.0.41_openapi_users_add_cond_resp_str\n\n\nUp\npost /slurmdb/v0.0.41/wckeys/\nAdd or update wckeys (slurmdbV0041PostWckeys)\n\nConsumes\n    This API call consumes the following media types via the Content-Type request header:\n    \napplication/json\napplication/x-yaml\n\nRequest body\n\nv0.0.41_openapi_wckey_resp v0.0.41_openapi_wckey_resp (optional)\nBody Parameter \u2014  \n \nQuery parameters\n\ncluster (optional)\nQuery Parameter \u2014 CSV cluster name list default: null format (optional)\nQuery Parameter \u2014 CSV format name list default: null id (optional)\nQuery Parameter \u2014 CSV id list default: null name (optional)\nQuery Parameter \u2014 CSV name list default: null only_defaults (optional)\nQuery Parameter \u2014 Only query defaults default: null usage_end (optional)\nQuery Parameter \u2014 Usage end (UNIX timestamp) default: null usage_start (optional)\nQuery Parameter \u2014 Usage start (UNIX timestamp) default: null user (optional)\nQuery Parameter \u2014 CSV user list default: null with_usage (optional)\nQuery Parameter \u2014 Include usage default: null with_deleted (optional)\nQuery Parameter \u2014 Include deleted wckeys default: null \n \nReturn type\n\nv0.0.41_openapi_resp\n\n\nExample data\nContent-Type: application/json\n{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}\nExample data\nContent-Type: application/x-yaml\nCustom MIME type example not yet supported: application/x-yaml\nProduces\n    This API call produces the following media types according to the Accept request header;\n    the media type will be conveyed by the Content-Type response header.\n    \napplication/json\napplication/x-yaml\n\nResponses\n200\n    Result of wckey addition or update request\n        v0.0.41_openapi_resp\ndefault\n    Result of wckey addition or update request\n        v0.0.41_openapi_resp\nModelsMethodsTable of Contents\nv0.0.41_job_alloc_req - \nv0.0.41_job_desc_msg - \nv0.0.41_job_submit_req - \nv0.0.41_kill_jobs_msg - \nv0.0.41_openapi_accounts_add_cond_resp - \nv0.0.41_openapi_accounts_add_cond_resp_str - \nv0.0.41_openapi_accounts_removed_resp - \nv0.0.41_openapi_accounts_resp - \nv0.0.41_openapi_assocs_removed_resp - \nv0.0.41_openapi_assocs_resp - \nv0.0.41_openapi_clusters_removed_resp - \nv0.0.41_openapi_clusters_resp - \nv0.0.41_openapi_diag_resp - \nv0.0.41_openapi_instances_resp - \nv0.0.41_openapi_job_alloc_resp - \nv0.0.41_openapi_job_info_resp - \nv0.0.41_openapi_job_post_response - \nv0.0.41_openapi_job_submit_response - \nv0.0.41_openapi_kill_jobs_resp - \nv0.0.41_openapi_licenses_resp - \nv0.0.41_openapi_nodes_resp - \nv0.0.41_openapi_partition_resp - \nv0.0.41_openapi_ping_array_resp - \nv0.0.41_openapi_reservation_resp - \nv0.0.41_openapi_resp - \nv0.0.41_openapi_shares_resp - \nv0.0.41_openapi_slurmdbd_config_resp - \nv0.0.41_openapi_slurmdbd_jobs_resp - \nv0.0.41_openapi_slurmdbd_qos_removed_resp - \nv0.0.41_openapi_slurmdbd_qos_resp - \nv0.0.41_openapi_slurmdbd_stats_resp - \nv0.0.41_openapi_tres_resp - \nv0.0.41_openapi_users_add_cond_resp - \nv0.0.41_openapi_users_add_cond_resp_str - \nv0.0.41_openapi_users_resp - \nv0.0.41_openapi_wckey_removed_resp - \nv0.0.41_openapi_wckey_resp - \nv0.0.41_update_node_msg - \nv0_0_41_job_desc_msg_begin_time - \nv0_0_41_job_desc_msg_crontab - \nv0_0_41_job_desc_msg_crontab_line - \nv0_0_41_job_desc_msg_distribution_plane_size - \nv0_0_41_job_desc_msg_kill_warning_delay - \nv0_0_41_job_desc_msg_required_switches - \nv0_0_41_job_desc_msg_rlimits - \nv0_0_41_job_desc_msg_rlimits_as - \nv0_0_41_job_desc_msg_rlimits_core - \nv0_0_41_job_desc_msg_rlimits_cpu - \nv0_0_41_job_desc_msg_rlimits_data - \nv0_0_41_job_desc_msg_rlimits_fsize - \nv0_0_41_job_desc_msg_rlimits_memlock - \nv0_0_41_job_desc_msg_rlimits_nofile - \nv0_0_41_job_desc_msg_rlimits_nproc - \nv0_0_41_job_desc_msg_rlimits_rss - \nv0_0_41_job_desc_msg_rlimits_stack - \nv0_0_41_job_desc_msg_segment_size - \nv0_0_41_job_desc_msg_time_minimum - \nv0_0_41_openapi_accounts_add_cond_resp_account - \nv0_0_41_openapi_accounts_add_cond_resp_association_condition - \nv0_0_41_openapi_diag_resp_statistics - \nv0_0_41_openapi_diag_resp_statistics_bf_exit - \nv0_0_41_openapi_diag_resp_statistics_bf_when_last_cycle - \nv0_0_41_openapi_diag_resp_statistics_job_states_ts - \nv0_0_41_openapi_diag_resp_statistics_pending_rpcs_by_hostlist_inner - \nv0_0_41_openapi_diag_resp_statistics_pending_rpcs_inner - \nv0_0_41_openapi_diag_resp_statistics_req_time - \nv0_0_41_openapi_diag_resp_statistics_req_time_start - \nv0_0_41_openapi_diag_resp_statistics_rpcs_by_message_type_inner - \nv0_0_41_openapi_diag_resp_statistics_rpcs_by_message_type_inner_average_time - \nv0_0_41_openapi_diag_resp_statistics_rpcs_by_user_inner - \nv0_0_41_openapi_diag_resp_statistics_schedule_exit - \nv0_0_41_openapi_job_info_resp_jobs_inner - \nv0_0_41_openapi_job_info_resp_jobs_inner_accrue_time - \nv0_0_41_openapi_job_info_resp_jobs_inner_array_job_id - \nv0_0_41_openapi_job_info_resp_jobs_inner_array_max_tasks - \nv0_0_41_openapi_job_info_resp_jobs_inner_billable_tres - \nv0_0_41_openapi_job_info_resp_jobs_inner_cores_per_socket - \nv0_0_41_openapi_job_info_resp_jobs_inner_cpu_frequency_governor - \nv0_0_41_openapi_job_info_resp_jobs_inner_cpu_frequency_maximum - \nv0_0_41_openapi_job_info_resp_jobs_inner_cpu_frequency_minimum - \nv0_0_41_openapi_job_info_resp_jobs_inner_cpus - \nv0_0_41_openapi_job_info_resp_jobs_inner_cpus_per_task - \nv0_0_41_openapi_job_info_resp_jobs_inner_deadline - \nv0_0_41_openapi_job_info_resp_jobs_inner_delay_boot - \nv0_0_41_openapi_job_info_resp_jobs_inner_eligible_time - \nv0_0_41_openapi_job_info_resp_jobs_inner_end_time - \nv0_0_41_openapi_job_info_resp_jobs_inner_exit_code - \nv0_0_41_openapi_job_info_resp_jobs_inner_het_job_id - \nv0_0_41_openapi_job_info_resp_jobs_inner_job_resources - \nv0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes - \nv0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner - \nv0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner_cpus - \nv0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner_memory - \nv0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner_sockets_inner - \nv0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner_sockets_inner_cores_inner - \nv0_0_41_openapi_job_info_resp_jobs_inner_job_resources_threads_per_core - \nv0_0_41_openapi_job_info_resp_jobs_inner_last_sched_evaluation - \nv0_0_41_openapi_job_info_resp_jobs_inner_max_cpus - \nv0_0_41_openapi_job_info_resp_jobs_inner_max_nodes - \nv0_0_41_openapi_job_info_resp_jobs_inner_minimum_cpus_per_node - \nv0_0_41_openapi_job_info_resp_jobs_inner_minimum_tmp_disk_per_node - \nv0_0_41_openapi_job_info_resp_jobs_inner_node_count - \nv0_0_41_openapi_job_info_resp_jobs_inner_power - \nv0_0_41_openapi_job_info_resp_jobs_inner_pre_sus_time - \nv0_0_41_openapi_job_info_resp_jobs_inner_preempt_time - \nv0_0_41_openapi_job_info_resp_jobs_inner_preemptable_time - \nv0_0_41_openapi_job_info_resp_jobs_inner_resize_time - \nv0_0_41_openapi_job_info_resp_jobs_inner_sockets_per_node - \nv0_0_41_openapi_job_info_resp_jobs_inner_start_time - \nv0_0_41_openapi_job_info_resp_jobs_inner_submit_time - \nv0_0_41_openapi_job_info_resp_jobs_inner_suspend_time - \nv0_0_41_openapi_job_info_resp_jobs_inner_tasks - \nv0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_board - \nv0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_core - \nv0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_node - \nv0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_socket - \nv0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_tres - \nv0_0_41_openapi_job_info_resp_jobs_inner_threads_per_core - \nv0_0_41_openapi_job_info_resp_last_backfill - \nv0_0_41_openapi_job_info_resp_last_update - \nv0_0_41_openapi_job_post_response_results_inner - \nv0_0_41_openapi_job_submit_response_result - \nv0_0_41_openapi_kill_jobs_resp_status_inner - \nv0_0_41_openapi_kill_jobs_resp_status_inner_error - \nv0_0_41_openapi_kill_jobs_resp_status_inner_federation - \nv0_0_41_openapi_kill_jobs_resp_status_inner_job_id - \nv0_0_41_openapi_licenses_resp_last_update - \nv0_0_41_openapi_licenses_resp_licenses_inner - \nv0_0_41_openapi_nodes_resp_last_update - \nv0_0_41_openapi_nodes_resp_nodes_inner - \nv0_0_41_openapi_nodes_resp_nodes_inner_boot_time - \nv0_0_41_openapi_nodes_resp_nodes_inner_energy - \nv0_0_41_openapi_nodes_resp_nodes_inner_energy_current_watts - \nv0_0_41_openapi_nodes_resp_nodes_inner_free_mem - \nv0_0_41_openapi_nodes_resp_nodes_inner_last_busy - \nv0_0_41_openapi_nodes_resp_nodes_inner_reason_changed_at - \nv0_0_41_openapi_nodes_resp_nodes_inner_resume_after - \nv0_0_41_openapi_nodes_resp_nodes_inner_slurmd_start_time - \nv0_0_41_openapi_partition_resp_last_update - \nv0_0_41_openapi_partition_resp_partitions_inner - \nv0_0_41_openapi_partition_resp_partitions_inner_accounts - \nv0_0_41_openapi_partition_resp_partitions_inner_cpus - \nv0_0_41_openapi_partition_resp_partitions_inner_defaults - \nv0_0_41_openapi_partition_resp_partitions_inner_defaults_partition_memory_per_cpu - \nv0_0_41_openapi_partition_resp_partitions_inner_defaults_partition_memory_per_node - \nv0_0_41_openapi_partition_resp_partitions_inner_defaults_time - \nv0_0_41_openapi_partition_resp_partitions_inner_groups - \nv0_0_41_openapi_partition_resp_partitions_inner_maximums - \nv0_0_41_openapi_partition_resp_partitions_inner_maximums_cpus_per_node - \nv0_0_41_openapi_partition_resp_partitions_inner_maximums_cpus_per_socket - \nv0_0_41_openapi_partition_resp_partitions_inner_maximums_nodes - \nv0_0_41_openapi_partition_resp_partitions_inner_maximums_over_time_limit - \nv0_0_41_openapi_partition_resp_partitions_inner_maximums_oversubscribe - \nv0_0_41_openapi_partition_resp_partitions_inner_maximums_partition_memory_per_cpu - \nv0_0_41_openapi_partition_resp_partitions_inner_maximums_partition_memory_per_node - \nv0_0_41_openapi_partition_resp_partitions_inner_maximums_time - \nv0_0_41_openapi_partition_resp_partitions_inner_minimums - \nv0_0_41_openapi_partition_resp_partitions_inner_nodes - \nv0_0_41_openapi_partition_resp_partitions_inner_partition - \nv0_0_41_openapi_partition_resp_partitions_inner_priority - \nv0_0_41_openapi_partition_resp_partitions_inner_qos - \nv0_0_41_openapi_partition_resp_partitions_inner_suspend_time - \nv0_0_41_openapi_partition_resp_partitions_inner_timeouts - \nv0_0_41_openapi_partition_resp_partitions_inner_timeouts_resume - \nv0_0_41_openapi_partition_resp_partitions_inner_timeouts_suspend - \nv0_0_41_openapi_partition_resp_partitions_inner_tres - \nv0_0_41_openapi_ping_array_resp_pings_inner - \nv0_0_41_openapi_reservation_resp_last_update - \nv0_0_41_openapi_reservation_resp_reservations_inner - \nv0_0_41_openapi_reservation_resp_reservations_inner_core_specializations_inner - \nv0_0_41_openapi_reservation_resp_reservations_inner_end_time - \nv0_0_41_openapi_reservation_resp_reservations_inner_purge_completed - \nv0_0_41_openapi_reservation_resp_reservations_inner_purge_completed_time - \nv0_0_41_openapi_reservation_resp_reservations_inner_start_time - \nv0_0_41_openapi_reservation_resp_reservations_inner_watts - \nv0_0_41_openapi_shares_resp_shares - \nv0_0_41_openapi_shares_resp_shares_shares_inner - \nv0_0_41_openapi_shares_resp_shares_shares_inner_fairshare - \nv0_0_41_openapi_shares_resp_shares_shares_inner_shares - \nv0_0_41_openapi_shares_resp_shares_shares_inner_shares_normalized - \nv0_0_41_openapi_shares_resp_shares_shares_inner_tres - \nv0_0_41_openapi_shares_resp_shares_shares_inner_tres_run_seconds_inner - \nv0_0_41_openapi_shares_resp_shares_shares_inner_tres_run_seconds_inner_value - \nv0_0_41_openapi_shares_resp_shares_shares_inner_tres_usage_inner - \nv0_0_41_openapi_shares_resp_shares_shares_inner_usage_normalized - \nv0_0_41_openapi_slurmdbd_config_resp_accounts_inner - \nv0_0_41_openapi_slurmdbd_config_resp_accounts_inner_associations_inner - \nv0_0_41_openapi_slurmdbd_config_resp_accounts_inner_coordinators_inner - \nv0_0_41_openapi_slurmdbd_config_resp_associations_inner - \nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_default - \nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max - \nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs - \nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_accruing - \nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_active - \nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_per - \nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_per_submitted - \nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_total - \nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_per - \nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_per_account - \nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres - \nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres_group - \nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres_minutes - \nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres_minutes_per - \nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres_per - \nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_min - \nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_priority - \nv0_0_41_openapi_slurmdbd_config_resp_clusters_inner - \nv0_0_41_openapi_slurmdbd_config_resp_clusters_inner_associations - \nv0_0_41_openapi_slurmdbd_config_resp_clusters_inner_associations_root - \nv0_0_41_openapi_slurmdbd_config_resp_clusters_inner_controller - \nv0_0_41_openapi_slurmdbd_config_resp_instances_inner - \nv0_0_41_openapi_slurmdbd_config_resp_instances_inner_time - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_factor - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_accruing - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_accruing_per - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_accruing_per_account - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_accruing_per_user - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_active_jobs - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_active_jobs_accruing - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_active_jobs_count - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_active_jobs - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_active_jobs_per - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_active_jobs_per_account - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_active_jobs_per_user - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_per - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_per_account - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_per_user - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_tres - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_tres_minutes - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_tres_minutes_per - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_tres_per - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock_per - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock_per_job - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock_per_qos - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min_priority_threshold - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min_tres - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min_tres_per - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_preempt - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_preempt_exempt_time - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_priority - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_usage_factor - \nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_usage_threshold - \nv0_0_41_openapi_slurmdbd_config_resp_users_inner - \nv0_0_41_openapi_slurmdbd_config_resp_users_inner_default - \nv0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner - \nv0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner_accounting_inner - \nv0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner_accounting_inner_TRES - \nv0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner_accounting_inner_allocated - \nv0_0_41_openapi_slurmdbd_jobs_resp_errors_inner - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_limits - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_limits_max - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_limits_max_running - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_task_id - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_association - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_comment - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_return_code - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_signal - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_signal_id - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_exit_code - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_het - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_het_job_offset - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_mcs - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_priority - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required_memory_per_cpu - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required_memory_per_node - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_reservation - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_state - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_CPU - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_CPU_requested_frequency - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_CPU_requested_frequency_max - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_CPU_requested_frequency_min - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_nodes - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_statistics - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_statistics_CPU - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_statistics_energy - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_statistics_energy_consumed - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_step - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_task - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tasks - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_end - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_start - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_system - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_total - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_user - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_consumed - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_limit - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_planned - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_system - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_total - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_user - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_tres - \nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_wckey - \nv0_0_41_openapi_slurmdbd_jobs_resp_meta - \nv0_0_41_openapi_slurmdbd_jobs_resp_meta_client - \nv0_0_41_openapi_slurmdbd_jobs_resp_meta_plugin - \nv0_0_41_openapi_slurmdbd_jobs_resp_meta_slurm - \nv0_0_41_openapi_slurmdbd_jobs_resp_meta_slurm_version - \nv0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner - \nv0_0_41_openapi_slurmdbd_stats_resp_statistics - \nv0_0_41_openapi_slurmdbd_stats_resp_statistics_RPCs_inner - \nv0_0_41_openapi_slurmdbd_stats_resp_statistics_RPCs_inner_time - \nv0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups - \nv0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_daily - \nv0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_daily_duration - \nv0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_hourly - \nv0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_hourly_duration - \nv0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_monthly - \nv0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_monthly_duration - \nv0_0_41_openapi_slurmdbd_stats_resp_statistics_users_inner - \nv0_0_41_openapi_users_add_cond_resp_association_condition - \nv0_0_41_openapi_users_add_cond_resp_association_condition_association - \nv0_0_41_openapi_users_add_cond_resp_association_condition_association_grpjobs - \nv0_0_41_openapi_users_add_cond_resp_association_condition_association_grpjobsaccrue - \nv0_0_41_openapi_users_add_cond_resp_association_condition_association_grpsubmitjobs - \nv0_0_41_openapi_users_add_cond_resp_association_condition_association_grpwall - \nv0_0_41_openapi_users_add_cond_resp_association_condition_association_maxjobs - \nv0_0_41_openapi_users_add_cond_resp_association_condition_association_maxjobsaccrue - \nv0_0_41_openapi_users_add_cond_resp_association_condition_association_maxsubmitjobs - \nv0_0_41_openapi_users_add_cond_resp_association_condition_association_maxwalldurationperjob - \nv0_0_41_openapi_users_add_cond_resp_association_condition_association_minpriothresh - \nv0_0_41_openapi_users_add_cond_resp_user - \nv0_0_41_update_node_msg_resume_after - \nv0_0_41_update_node_msg_weight - \n\nv0.0.41_job_alloc_req -  Up\n\n\nhetjob (optional)array[v0.0.41_job_desc_msg] HetJob description \njob (optional)v0.0.41_job_desc_msg \n \n\nv0.0.41_job_desc_msg -  Up\n\n\naccount (optional)String Account associated with the job \naccount_gather_frequency (optional)String Job accounting and profiling sampling intervals in seconds \nadmin_comment (optional)String Arbitrary comment made by administrator \nallocation_node_list (optional)String Local node making the resource allocation \nallocation_node_port (optional)Integer Port to send allocation confirmation to format: int32\nargv (optional)array[String] Arguments to the script \narray (optional)String Job array index value specification \nbatch_features (optional)String Features required for batch script's node \nbegin_time (optional)v0_0_41_job_desc_msg_begin_time \nflags (optional)array[String] Job flags \nEnum:\nburst_buffer (optional)String Burst buffer specifications \nclusters (optional)String Clusters that a federated job can run on \ncluster_constraint (optional)String Required features that a federated cluster must have to have a sibling job submitted to it \ncomment (optional)String Arbitrary comment made by user \ncontiguous (optional)Boolean True if job requires contiguous nodes \ncontainer (optional)String Absolute path to OCI container bundle \ncontainer_id (optional)String OCI container ID \ncore_specification (optional)Integer Specialized core count format: int32\nthread_specification (optional)Integer Specialized thread count format: int32\ncpu_binding (optional)String Method for binding tasks to allocated CPUs \ncpu_binding_flags (optional)array[String] Flags for CPU binding \nEnum:\ncpu_frequency (optional)String Requested CPU frequency range [-p2][:p3] \ncpus_per_tres (optional)String Semicolon delimited list of TRES=# values values indicating how many CPUs should be allocated for each specified TRES (currently only used for gres/gpu) \ncrontab (optional)v0_0_41_job_desc_msg_crontab \ndeadline (optional)Long Latest time that the job may start (UNIX timestamp) format: int64\ndelay_boot (optional)Integer Number of seconds after job eligible start that nodes will be rebooted to satisfy feature specification format: int32\ndependency (optional)String Other jobs that must meet certain criteria before this job can start \nend_time (optional)Long Expected end time (UNIX timestamp) format: int64\nenvironment (optional)array[String] Environment variables to be set for the job \nrlimits (optional)v0_0_41_job_desc_msg_rlimits \nexcluded_nodes (optional)array[String] Comma separated list of nodes that may not be used \nextra (optional)String Arbitrary string used for node filtering if extra constraints are enabled \nconstraints (optional)String Comma separated list of features that are required \ngroup_id (optional)String Group ID of the user that owns the job \nhetjob_group (optional)Integer Unique sequence number applied to this component of the heterogeneous job format: int32\nimmediate (optional)Boolean If true, exit if resources are not available within the time period specified \njob_id (optional)Integer Job ID format: int32\nkill_on_node_fail (optional)Boolean If true, kill job on node failure \nlicenses (optional)String License(s) required by the job \nmail_type (optional)array[String] Mail event type(s) \nEnum:\nmail_user (optional)String User to receive email notifications \nmcs_label (optional)String Multi-Category Security label on the job \nmemory_binding (optional)String Binding map for map/mask_cpu \nmemory_binding_type (optional)array[String] Method for binding tasks to memory \nEnum:\nmemory_per_tres (optional)String Semicolon delimited list of TRES=# values indicating how much memory in megabytes should be allocated for each specified TRES (currently only used for gres/gpu) \nname (optional)String Job name \nnetwork (optional)String Network specs for job step \nnice (optional)Integer Requested job priority change format: int32\ntasks (optional)Integer Number of tasks format: int32\nopen_mode (optional)array[String] Open mode used for stdout and stderr files \nEnum:\nreserve_ports (optional)Integer Port to send various notification msg to format: int32\novercommit (optional)Boolean Overcommit resources \npartition (optional)String Partition assigned to the job \ndistribution_plane_size (optional)v0_0_41_job_desc_msg_distribution_plane_size \npower_flags (optional)array[oas_any_type_not_mapped] \nprefer (optional)String Comma separated list of features that are preferred but not required \nhold (optional)Boolean Hold (true) or release (false) job \npriority (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_priority \nprofile (optional)array[String] Profile used by the acct_gather_profile plugin \nEnum:\nqos (optional)String Quality of Service assigned to the job \nreboot (optional)Boolean Node reboot requested before start \nrequired_nodes (optional)array[String] Comma separated list of required nodes \nrequeue (optional)Boolean Determines whether the job may be requeued \nreservation (optional)String Name of reservation to use \nresv_mpi_ports (optional)Integer Number of reserved communication ports; can only be used if slurmstepd step manager is enabled format: int32\nscript (optional)String Job batch script; only the first component in a HetJob is populated or honored \nshared (optional)array[String] How the job can share resources with other jobs, if at all \nEnum:\nexclusive (optional)array[String] \nEnum:\noversubscribe (optional)Boolean \nsite_factor (optional)Integer Site-specific priority factor format: int32\nspank_environment (optional)array[String] Environment variables for job prolog/epilog scripts as set by SPANK plugins \ndistribution (optional)String Layout \ntime_limit (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_limit \ntime_minimum (optional)v0_0_41_job_desc_msg_time_minimum \ntres_bind (optional)String Task to TRES binding directives \ntres_freq (optional)String TRES frequency directives \ntres_per_job (optional)String Comma separated list of TRES=# values to be allocated for every job \ntres_per_node (optional)String Comma separated list of TRES=# values to be allocated for every node \ntres_per_socket (optional)String Comma separated list of TRES=# values to be allocated for every socket \ntres_per_task (optional)String Comma separated list of TRES=# values to be allocated for every task \nuser_id (optional)String User ID that owns the job \nwait_all_nodes (optional)Boolean If true, wait to start until after all nodes have booted \nkill_warning_flags (optional)array[String] Flags related to job signals \nEnum:\nkill_warning_signal (optional)String Signal to send when approaching end time (e.g. \"10\" or \"USR1\") \nkill_warning_delay (optional)v0_0_41_job_desc_msg_kill_warning_delay \ncurrent_working_directory (optional)String Working directory to use for the job \ncpus_per_task (optional)Integer Number of CPUs required by each task format: int32\nminimum_cpus (optional)Integer Minimum number of CPUs required format: int32\nmaximum_cpus (optional)Integer Maximum number of CPUs required format: int32\nnodes (optional)String Node count range specification (e.g. 1-15:4) \nminimum_nodes (optional)Integer Minimum node count format: int32\nmaximum_nodes (optional)Integer Maximum node count format: int32\nminimum_boards_per_node (optional)Integer Boards per node required format: int32\nminimum_sockets_per_board (optional)Integer Sockets per board required format: int32\nsockets_per_node (optional)Integer Sockets per node required format: int32\nthreads_per_core (optional)Integer Threads per core required format: int32\ntasks_per_node (optional)Integer Number of tasks to invoke on each node format: int32\ntasks_per_socket (optional)Integer Number of tasks to invoke on each socket format: int32\ntasks_per_core (optional)Integer Number of tasks to invoke on each core format: int32\ntasks_per_board (optional)Integer Number of tasks to invoke on each board format: int32\nntasks_per_tres (optional)Integer Number of tasks that can access each GPU format: int32\nminimum_cpus_per_node (optional)Integer Minimum number of CPUs per node format: int32\nmemory_per_cpu (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required_memory_per_cpu \nmemory_per_node (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required_memory_per_cpu \ntemporary_disk_per_node (optional)Integer Minimum tmp disk space required per node format: int32\nselinux_context (optional)String SELinux context \nrequired_switches (optional)v0_0_41_job_desc_msg_required_switches \nsegment_size (optional)v0_0_41_job_desc_msg_segment_size \nstandard_error (optional)String Path to stderr file \nstandard_input (optional)String Path to stdin file \nstandard_output (optional)String Path to stdout file \nwait_for_switch (optional)Integer Maximum time to wait for switches in seconds format: int32\nwckey (optional)String Workload characterization key \nx11 (optional)array[String] X11 forwarding options \nEnum:\nx11_magic_cookie (optional)String Magic cookie for X11 forwarding \nx11_target_host (optional)String Hostname or UNIX socket if x11_target_port=0 \nx11_target_port (optional)Integer TCP port format: int32\n \n\nv0.0.41_job_submit_req -  Up\n\n\nscript (optional)String Deprecated; Populate script field in jobs[0] or job \njobs (optional)array[v0.0.41_job_desc_msg] HetJob description \njob (optional)v0.0.41_job_desc_msg \n \n\nv0.0.41_kill_jobs_msg -  Up\n\n\naccount (optional)String Filter jobs to a specific account \nflags (optional)array[String] Filter jobs according to flags \nEnum:\njob_name (optional)String Filter jobs to a specific name \njobs (optional)array[String] List of jobs to signal \npartition (optional)String Filter jobs to a specific partition \nqos (optional)String Filter jobs to a specific QOS \nreservation (optional)String Filter jobs to a specific reservation \nsignal (optional)String Signal to send to jobs \njob_state (optional)array[String] Filter jobs to a specific state \nEnum:\nuser_id (optional)String Filter jobs to a specific numeric user id \nuser_name (optional)String Filter jobs to a specific user name \nwckey (optional)String Filter jobs to a specific wckey \nnodes (optional)array[String] Filter jobs to a set of nodes \n \n\nv0.0.41_openapi_accounts_add_cond_resp -  Up\n\n\nassociation_condition (optional)v0_0_41_openapi_accounts_add_cond_resp_association_condition \naccount (optional)v0_0_41_openapi_accounts_add_cond_resp_account \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_accounts_add_cond_resp_str -  Up\n\n\nadded_accounts String added_accounts \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_accounts_removed_resp -  Up\n\n\nremoved_accounts array[String] removed_accounts \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_accounts_resp -  Up\n\n\naccounts array[v0_0_41_openapi_slurmdbd_config_resp_accounts_inner] accounts \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_assocs_removed_resp -  Up\n\n\nremoved_associations array[String] removed_associations \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_assocs_resp -  Up\n\n\nassociations array[v0_0_41_openapi_slurmdbd_config_resp_associations_inner] associations \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_clusters_removed_resp -  Up\n\n\ndeleted_clusters array[String] deleted_clusters \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_clusters_resp -  Up\n\n\nclusters array[v0_0_41_openapi_slurmdbd_config_resp_clusters_inner] clusters \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_diag_resp -  Up\n\n\nstatistics v0_0_41_openapi_diag_resp_statistics \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_instances_resp -  Up\n\n\ninstances array[v0_0_41_openapi_slurmdbd_config_resp_instances_inner] instances \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_job_alloc_resp -  Up\n\n\njob_id (optional)Integer Submitted Job ID format: int32\njob_submit_user_msg (optional)String Job submission user message \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_job_info_resp -  Up\n\n\njobs array[v0_0_41_openapi_job_info_resp_jobs_inner] List of jobs \nlast_backfill v0_0_41_openapi_job_info_resp_last_backfill \nlast_update v0_0_41_openapi_job_info_resp_last_update \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_job_post_response -  Up\n\n\nresults (optional)array[v0_0_41_openapi_job_post_response_results_inner] Job update results \njob_id (optional)String First updated Job ID - Use results instead \nstep_id (optional)String First updated Step ID - Use results instead \njob_submit_user_msg (optional)String First updated Job submission user message - Use results instead \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_job_submit_response -  Up\n\n\nresult (optional)v0_0_41_openapi_job_submit_response_result \njob_id (optional)Integer Submitted Job ID format: int32\nstep_id (optional)String Submitted Step ID \njob_submit_user_msg (optional)String Job submission user message \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_kill_jobs_resp -  Up\n\n\nstatus array[v0_0_41_openapi_kill_jobs_resp_status_inner] resultant status of signal request \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_licenses_resp -  Up\n\n\nlicenses array[v0_0_41_openapi_licenses_resp_licenses_inner] List of licenses \nlast_update v0_0_41_openapi_licenses_resp_last_update \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_nodes_resp -  Up\n\n\nnodes array[v0_0_41_openapi_nodes_resp_nodes_inner] List of nodes \nlast_update v0_0_41_openapi_nodes_resp_last_update \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_partition_resp -  Up\n\n\npartitions array[v0_0_41_openapi_partition_resp_partitions_inner] List of partitions \nlast_update v0_0_41_openapi_partition_resp_last_update \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_ping_array_resp -  Up\n\n\npings array[v0_0_41_openapi_ping_array_resp_pings_inner] pings \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_reservation_resp -  Up\n\n\nreservations array[v0_0_41_openapi_reservation_resp_reservations_inner] List of reservations \nlast_update v0_0_41_openapi_reservation_resp_last_update \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_resp -  Up\n\n\nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_shares_resp -  Up\n\n\nshares v0_0_41_openapi_shares_resp_shares \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_slurmdbd_config_resp -  Up\n\n\nclusters (optional)array[v0_0_41_openapi_slurmdbd_config_resp_clusters_inner] Clusters \ntres (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] TRES \naccounts (optional)array[v0_0_41_openapi_slurmdbd_config_resp_accounts_inner] Accounts \nusers (optional)array[v0_0_41_openapi_slurmdbd_config_resp_users_inner] Users \nqos (optional)array[v0_0_41_openapi_slurmdbd_config_resp_qos_inner] QOS \nwckeys (optional)array[v0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner] WCKeys \nassociations (optional)array[v0_0_41_openapi_slurmdbd_config_resp_associations_inner] Associations \ninstances (optional)array[v0_0_41_openapi_slurmdbd_config_resp_instances_inner] Instances \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_slurmdbd_jobs_resp -  Up\n\n\njobs array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner] jobs \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_slurmdbd_qos_removed_resp -  Up\n\n\nremoved_qos array[String] removed QOS \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_slurmdbd_qos_resp -  Up\n\n\nqos array[v0_0_41_openapi_slurmdbd_config_resp_qos_inner] List of QOS \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_slurmdbd_stats_resp -  Up\n\n\nstatistics v0_0_41_openapi_slurmdbd_stats_resp_statistics \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_tres_resp -  Up\n\n\nTRES array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] TRES \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_users_add_cond_resp -  Up\n\n\nassociation_condition v0_0_41_openapi_users_add_cond_resp_association_condition \nuser v0_0_41_openapi_users_add_cond_resp_user \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_users_add_cond_resp_str -  Up\n\n\nadded_users String added_users \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_users_resp -  Up\n\n\nusers array[v0_0_41_openapi_slurmdbd_config_resp_users_inner] users \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_wckey_removed_resp -  Up\n\n\ndeleted_wckeys array[String] deleted wckeys \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_openapi_wckey_resp -  Up\n\n\nwckeys array[v0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner] wckeys \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n \n\nv0.0.41_update_node_msg -  Up\n\n\ncomment (optional)String Arbitrary comment \ncpu_bind (optional)Integer Default method for binding tasks to allocated CPUs format: int32\nextra (optional)String Arbitrary string used for node filtering if extra constraints are enabled \nfeatures (optional)array[String] Available features \nfeatures_act (optional)array[String] Currently active features \ngres (optional)String Generic resources \naddress (optional)array[String] NodeAddr, used to establish a communication path \nhostname (optional)array[String] NodeHostname \nname (optional)array[String] NodeName \nstate (optional)array[String] New state to assign to the node \nEnum:\nreason (optional)String Reason for node being DOWN or DRAINING \nreason_uid (optional)String User ID to associate with the reason (needed if user root is sending message) \nresume_after (optional)v0_0_41_update_node_msg_resume_after \nweight (optional)v0_0_41_update_node_msg_weight \n \n\nv0_0_41_job_desc_msg_begin_time -  Up\nDefer the allocation of the job until the specified time (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_job_desc_msg_crontab -  Up\nSpecification for scrontab job\n\nflags (optional)array[String] Flags \nEnum:\nminute (optional)String Ranged string specifying eligible minute values (e.g. 0-10,50) \nhour (optional)String Ranged string specifying eligible hour values (e.g. 0-5,23) \nday_of_month (optional)String Ranged string specifying eligible day of month values (e.g. 0-10,29) \nmonth (optional)String Ranged string specifying eligible month values (e.g. 0-5,12) \nday_of_week (optional)String Ranged string specifying eligible day of week values (e.g.0-3,7) \nspecification (optional)String Time specification (* means valid for all allowed values) - minute hour day_of_month month day_of_week \ncommand (optional)String Command to run \nline (optional)v0_0_41_job_desc_msg_crontab_line \n \n\nv0_0_41_job_desc_msg_crontab_line -  Up\n\n\nstart (optional)Integer Start of this entry in file format: int32\nend (optional)Integer End of this entry in file format: int32\n \n\nv0_0_41_job_desc_msg_distribution_plane_size -  Up\nPlane size specification when distribution specifies plane\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_job_desc_msg_kill_warning_delay -  Up\nNumber of seconds before end time to send the warning signal\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_job_desc_msg_required_switches -  Up\nMaximum number of switches\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_job_desc_msg_rlimits -  Up\n\n\ncpu (optional)v0_0_41_job_desc_msg_rlimits_cpu \nfsize (optional)v0_0_41_job_desc_msg_rlimits_fsize \ndata (optional)v0_0_41_job_desc_msg_rlimits_data \nstack (optional)v0_0_41_job_desc_msg_rlimits_stack \ncore (optional)v0_0_41_job_desc_msg_rlimits_core \nrss (optional)v0_0_41_job_desc_msg_rlimits_rss \nnproc (optional)v0_0_41_job_desc_msg_rlimits_nproc \nnofile (optional)v0_0_41_job_desc_msg_rlimits_nofile \nmemlock (optional)v0_0_41_job_desc_msg_rlimits_memlock \nas (optional)v0_0_41_job_desc_msg_rlimits_as \n \n\nv0_0_41_job_desc_msg_rlimits_as -  Up\nAddress space limit.\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_job_desc_msg_rlimits_core -  Up\nLargest core file that can be created, in bytes.\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_job_desc_msg_rlimits_cpu -  Up\nPer-process CPU limit, in seconds.\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_job_desc_msg_rlimits_data -  Up\nMaximum size of data segment, in bytes.\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_job_desc_msg_rlimits_fsize -  Up\nLargest file that can be created, in bytes.\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_job_desc_msg_rlimits_memlock -  Up\nLocked-in-memory address space\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_job_desc_msg_rlimits_nofile -  Up\nNumber of open files.\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_job_desc_msg_rlimits_nproc -  Up\nNumber of processes.\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_job_desc_msg_rlimits_rss -  Up\nLargest resident set size, in bytes. This affects swapping; processes that are exceeding their resident set size will be more likely to have physical memory taken from them.\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_job_desc_msg_rlimits_stack -  Up\nMaximum size of stack segment, in bytes.\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_job_desc_msg_segment_size -  Up\nSegment size for topology/block\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_job_desc_msg_time_minimum -  Up\nMinimum run time in minutes\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_accounts_add_cond_resp_account -  Up\nAccount organization and description\n\ndescription (optional)String Arbitrary string describing the account \norganization (optional)String Organization to which the account belongs \n \n\nv0_0_41_openapi_accounts_add_cond_resp_association_condition -  Up\nCSV list of accounts, association limits and options, CSV list of clusters\n\naccounts array[String] CSV accounts list \nassociation (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association \nclusters (optional)array[String] CSV clusters list \n \n\nv0_0_41_openapi_diag_resp_statistics -  Up\nstatistics\n\nparts_packed (optional)Integer Zero if only RPC statistic included format: int32\nreq_time (optional)v0_0_41_openapi_diag_resp_statistics_req_time \nreq_time_start (optional)v0_0_41_openapi_diag_resp_statistics_req_time_start \nserver_thread_count (optional)Integer Number of current active slurmctld threads format: int32\nagent_queue_size (optional)Integer Number of enqueued outgoing RPC requests in an internal retry list format: int32\nagent_count (optional)Integer Number of agent threads format: int32\nagent_thread_count (optional)Integer Total number of active threads created by all agent threads format: int32\ndbd_agent_queue_size (optional)Integer Number of messages for SlurmDBD that are queued format: int32\ngettimeofday_latency (optional)Integer Latency of 1000 calls to the gettimeofday() syscall in microseconds, as measured at controller startup format: int32\nschedule_cycle_max (optional)Integer Max time of any scheduling cycle in microseconds since last reset format: int32\nschedule_cycle_last (optional)Integer Time in microseconds for last scheduling cycle format: int32\nschedule_cycle_sum (optional)Integer Total run time in microseconds for all scheduling cycles since last reset format: int32\nschedule_cycle_total (optional)Integer Number of scheduling cycles since last reset format: int32\nschedule_cycle_mean (optional)Long Mean time in microseconds for all scheduling cycles since last reset format: int64\nschedule_cycle_mean_depth (optional)Long Mean of the number of jobs processed in a scheduling cycle format: int64\nschedule_cycle_per_minute (optional)Long Number of scheduling executions per minute format: int64\nschedule_cycle_depth (optional)Integer Total number of jobs processed in scheduling cycles format: int32\nschedule_exit (optional)v0_0_41_openapi_diag_resp_statistics_schedule_exit \nschedule_queue_length (optional)Integer Number of jobs pending in queue format: int32\njobs_submitted (optional)Integer Number of jobs submitted since last reset format: int32\njobs_started (optional)Integer Number of jobs started since last reset format: int32\njobs_completed (optional)Integer Number of jobs completed since last reset format: int32\njobs_canceled (optional)Integer Number of jobs canceled since the last reset format: int32\njobs_failed (optional)Integer Number of jobs failed due to slurmd or other internal issues since last reset format: int32\njobs_pending (optional)Integer Number of jobs pending at the time of listed in job_state_ts format: int32\njobs_running (optional)Integer Number of jobs running at the time of listed in job_state_ts format: int32\njob_states_ts (optional)v0_0_41_openapi_diag_resp_statistics_job_states_ts \nbf_backfilled_jobs (optional)Integer Number of jobs started through backfilling since last slurm start format: int32\nbf_last_backfilled_jobs (optional)Integer Number of jobs started through backfilling since last reset format: int32\nbf_backfilled_het_jobs (optional)Integer Number of heterogeneous job components started through backfilling since last Slurm start format: int32\nbf_cycle_counter (optional)Integer Number of backfill scheduling cycles since last reset format: int32\nbf_cycle_mean (optional)Long Mean time in microseconds of backfilling scheduling cycles since last reset format: int64\nbf_depth_mean (optional)Long Mean number of eligible to run jobs processed during all backfilling scheduling cycles since last reset format: int64\nbf_depth_mean_try (optional)Long The subset of Depth Mean that the backfill scheduler attempted to schedule format: int64\nbf_cycle_sum (optional)Long Total time in microseconds of backfilling scheduling cycles since last reset format: int64\nbf_cycle_last (optional)Integer Execution time in microseconds of last backfill scheduling cycle format: int32\nbf_cycle_max (optional)Integer Execution time in microseconds of longest backfill scheduling cycle format: int32\nbf_exit (optional)v0_0_41_openapi_diag_resp_statistics_bf_exit \nbf_last_depth (optional)Integer Number of processed jobs during last backfilling scheduling cycle format: int32\nbf_last_depth_try (optional)Integer Number of processed jobs during last backfilling scheduling cycle that had a chance to start using available resources format: int32\nbf_depth_sum (optional)Integer Total number of jobs processed during all backfilling scheduling cycles since last reset format: int32\nbf_depth_try_sum (optional)Integer Subset of bf_depth_sum that the backfill scheduler attempted to schedule format: int32\nbf_queue_len (optional)Integer Number of jobs pending to be processed by backfilling algorithm format: int32\nbf_queue_len_mean (optional)Long Mean number of jobs pending to be processed by backfilling algorithm format: int64\nbf_queue_len_sum (optional)Integer Total number of jobs pending to be processed by backfilling algorithm since last reset format: int32\nbf_table_size (optional)Integer Number of different time slots tested by the backfill scheduler in its last iteration format: int32\nbf_table_size_sum (optional)Integer Total number of different time slots tested by the backfill scheduler format: int32\nbf_table_size_mean (optional)Long Mean number of different time slots tested by the backfill scheduler format: int64\nbf_when_last_cycle (optional)v0_0_41_openapi_diag_resp_statistics_bf_when_last_cycle \nbf_active (optional)Boolean Backfill scheduler currently running \nrpcs_by_message_type (optional)array[v0_0_41_openapi_diag_resp_statistics_rpcs_by_message_type_inner] Most frequently issued remote procedure calls (RPCs) \nrpcs_by_user (optional)array[v0_0_41_openapi_diag_resp_statistics_rpcs_by_user_inner] RPCs issued by user ID \npending_rpcs (optional)array[v0_0_41_openapi_diag_resp_statistics_pending_rpcs_inner] Pending RPC statistics \npending_rpcs_by_hostlist (optional)array[v0_0_41_openapi_diag_resp_statistics_pending_rpcs_by_hostlist_inner] Pending RPCs hostlists \n \n\nv0_0_41_openapi_diag_resp_statistics_bf_exit -  Up\nReasons for which the backfill scheduling cycle exited since last reset\n\nend_job_queue (optional)Integer Reached end of queue format: int32\nbf_max_job_start (optional)Integer Reached number of jobs allowed to start format: int32\nbf_max_job_test (optional)Integer Reached number of jobs allowed to be tested format: int32\nbf_max_time (optional)Integer Reached maximum allowed scheduler time format: int32\nbf_node_space_size (optional)Integer Reached table size limit format: int32\nstate_changed (optional)Integer System state changed format: int32\n \n\nv0_0_41_openapi_diag_resp_statistics_bf_when_last_cycle -  Up\nWhen the last backfill scheduling cycle happened (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_diag_resp_statistics_job_states_ts -  Up\nWhen the job state counts were gathered (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_diag_resp_statistics_pending_rpcs_by_hostlist_inner -  Up\nPending RPCs by hostlist\n\ntype_id Integer Message type as integer format: int32\nmessage_type String Message type as string \ncount array[String] Number of RPCs received \n \n\nv0_0_41_openapi_diag_resp_statistics_pending_rpcs_inner -  Up\nPending RPCs\n\ntype_id Integer Message type as integer format: int32\nmessage_type String Message type as string \ncount Integer Number of pending RPCs queued format: int32\n \n\nv0_0_41_openapi_diag_resp_statistics_req_time -  Up\nWhen the request was made (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_diag_resp_statistics_req_time_start -  Up\nWhen the data in the report started (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_diag_resp_statistics_rpcs_by_message_type_inner -  Up\nRPCs by type\n\ntype_id Integer Message type as integer format: int32\nmessage_type String Message type as string \ncount Integer Number of RPCs received format: int32\nqueued Integer Number of RPCs queued format: int32\ndropped Long Number of RPCs dropped format: int64\ncycle_last Integer Number of RPCs processed within the last RPC queue cycle format: int32\ncycle_max Integer Maximum number of RPCs processed within a RPC queue cycle since start format: int32\ntotal_time Long Total time spent processing RPC in seconds format: int64\naverage_time v0_0_41_openapi_diag_resp_statistics_rpcs_by_message_type_inner_average_time \n \n\nv0_0_41_openapi_diag_resp_statistics_rpcs_by_message_type_inner_average_time -  Up\nAverage time spent processing RPC in seconds\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_diag_resp_statistics_rpcs_by_user_inner -  Up\nRPCs by user\n\nuser_id Integer User ID (numeric) format: int32\nuser String User name \ncount Integer Number of RPCs received format: int32\ntotal_time Long Total time spent processing RPC in seconds format: int64\naverage_time v0_0_41_openapi_diag_resp_statistics_rpcs_by_message_type_inner_average_time \n \n\nv0_0_41_openapi_diag_resp_statistics_schedule_exit -  Up\nReasons for which the scheduling cycle exited since last reset\n\nend_job_queue (optional)Integer Reached end of queue format: int32\ndefault_queue_depth (optional)Integer Reached number of jobs allowed to be tested format: int32\nmax_job_start (optional)Integer Reached number of jobs allowed to start format: int32\nmax_rpc_cnt (optional)Integer Reached RPC limit format: int32\nmax_sched_time (optional)Integer Reached maximum allowed scheduler time format: int32\nlicenses (optional)Integer Blocked on licenses format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner -  Up\n\n\naccount (optional)String Account associated with the job \naccrue_time (optional)v0_0_41_openapi_job_info_resp_jobs_inner_accrue_time \nadmin_comment (optional)String Arbitrary comment made by administrator \nallocating_node (optional)String Local node making the resource allocation \narray_job_id (optional)v0_0_41_openapi_job_info_resp_jobs_inner_array_job_id \narray_task_id (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_task_id \narray_max_tasks (optional)v0_0_41_openapi_job_info_resp_jobs_inner_array_max_tasks \narray_task_string (optional)String String expression of task IDs in this record \nassociation_id (optional)Integer Unique identifier for the association format: int32\nbatch_features (optional)String Features required for batch script's node \nbatch_flag (optional)Boolean True if batch job \nbatch_host (optional)String Name of host running batch script \nflags (optional)array[String] Job flags \nEnum:\nburst_buffer (optional)String Burst buffer specifications \nburst_buffer_state (optional)String Burst buffer state details \ncluster (optional)String Cluster name \ncluster_features (optional)String List of required cluster features \ncommand (optional)String Executed command \ncomment (optional)String Arbitrary comment \ncontainer (optional)String Absolute path to OCI container bundle \ncontainer_id (optional)String OCI container ID \ncontiguous (optional)Boolean True if job requires contiguous nodes \ncore_spec (optional)Integer Specialized core count format: int32\nthread_spec (optional)Integer Specialized thread count format: int32\ncores_per_socket (optional)v0_0_41_openapi_job_info_resp_jobs_inner_cores_per_socket \nbillable_tres (optional)v0_0_41_openapi_job_info_resp_jobs_inner_billable_tres \ncpus_per_task (optional)v0_0_41_openapi_job_info_resp_jobs_inner_cpus_per_task \ncpu_frequency_minimum (optional)v0_0_41_openapi_job_info_resp_jobs_inner_cpu_frequency_minimum \ncpu_frequency_maximum (optional)v0_0_41_openapi_job_info_resp_jobs_inner_cpu_frequency_maximum \ncpu_frequency_governor (optional)v0_0_41_openapi_job_info_resp_jobs_inner_cpu_frequency_governor \ncpus_per_tres (optional)String Semicolon delimited list of TRES=# values indicating how many CPUs should be allocated for each specified TRES (currently only used for gres/gpu) \ncron (optional)String Time specification for scrontab job \ndeadline (optional)v0_0_41_openapi_job_info_resp_jobs_inner_deadline \ndelay_boot (optional)v0_0_41_openapi_job_info_resp_jobs_inner_delay_boot \ndependency (optional)String Other jobs that must meet certain criteria before this job can start \nderived_exit_code (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code \neligible_time (optional)v0_0_41_openapi_job_info_resp_jobs_inner_eligible_time \nend_time (optional)v0_0_41_openapi_job_info_resp_jobs_inner_end_time \nexcluded_nodes (optional)String Comma separated list of nodes that may not be used \nexit_code (optional)v0_0_41_openapi_job_info_resp_jobs_inner_exit_code \nextra (optional)String Arbitrary string used for node filtering if extra constraints are enabled \nfailed_node (optional)String Name of node that caused job failure \nfeatures (optional)String Comma separated list of features that are required \nfederation_origin (optional)String Origin cluster's name (when using federation) \nfederation_siblings_active (optional)String Active sibling job names \nfederation_siblings_viable (optional)String Viable sibling job names \ngres_detail (optional)array[String] List of GRES index and counts allocated per node \ngroup_id (optional)Integer Group ID of the user that owns the job format: int32\ngroup_name (optional)String Group name of the user that owns the job \nhet_job_id (optional)v0_0_41_openapi_job_info_resp_jobs_inner_het_job_id \nhet_job_id_set (optional)String Job ID range for all heterogeneous job components \nhet_job_offset (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_het_job_offset \njob_id (optional)Integer Job ID format: int32\njob_resources (optional)v0_0_41_openapi_job_info_resp_jobs_inner_job_resources \njob_size_str (optional)array[String] Number of nodes (in a range) required for this job \njob_state (optional)array[String] Current state \nEnum:\nlast_sched_evaluation (optional)v0_0_41_openapi_job_info_resp_jobs_inner_last_sched_evaluation \nlicenses (optional)String License(s) required by the job \nmail_type (optional)array[String] Mail event type(s) \nEnum:\nmail_user (optional)String User to receive email notifications \nmax_cpus (optional)v0_0_41_openapi_job_info_resp_jobs_inner_max_cpus \nmax_nodes (optional)v0_0_41_openapi_job_info_resp_jobs_inner_max_nodes \nmcs_label (optional)String Multi-Category Security label on the job \nmemory_per_tres (optional)String Semicolon delimited list of TRES=# values indicating how much memory in megabytes should be allocated for each specified TRES (currently only used for gres/gpu) \nname (optional)String Job name \nnetwork (optional)String Network specs for the job \nnodes (optional)String Node(s) allocated to the job \nnice (optional)Integer Requested job priority change format: int32\ntasks_per_core (optional)v0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_core \ntasks_per_tres (optional)v0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_tres \ntasks_per_node (optional)v0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_node \ntasks_per_socket (optional)v0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_socket \ntasks_per_board (optional)v0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_board \ncpus (optional)v0_0_41_openapi_job_info_resp_jobs_inner_cpus \nnode_count (optional)v0_0_41_openapi_job_info_resp_jobs_inner_node_count \ntasks (optional)v0_0_41_openapi_job_info_resp_jobs_inner_tasks \npartition (optional)String Partition assigned to the job \nprefer (optional)String Feature(s) the job requested but that are not required \nmemory_per_cpu (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required_memory_per_cpu \nmemory_per_node (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required_memory_per_node \nminimum_cpus_per_node (optional)v0_0_41_openapi_job_info_resp_jobs_inner_minimum_cpus_per_node \nminimum_tmp_disk_per_node (optional)v0_0_41_openapi_job_info_resp_jobs_inner_minimum_tmp_disk_per_node \npower (optional)v0_0_41_openapi_job_info_resp_jobs_inner_power \npreempt_time (optional)v0_0_41_openapi_job_info_resp_jobs_inner_preempt_time \npreemptable_time (optional)v0_0_41_openapi_job_info_resp_jobs_inner_preemptable_time \npre_sus_time (optional)v0_0_41_openapi_job_info_resp_jobs_inner_pre_sus_time \nhold (optional)Boolean Hold (true) or release (false) job \npriority (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_priority \nprofile (optional)array[String] Profile used by the acct_gather_profile plugin \nEnum:\nqos (optional)String Quality of Service assigned to the job \nreboot (optional)Boolean Node reboot requested before start \nrequired_nodes (optional)String Comma separated list of required nodes \nminimum_switches (optional)Integer Maximum number of switches (the 'minimum' in the key is incorrect) format: int32\nrequeue (optional)Boolean Determines whether the job may be requeued \nresize_time (optional)v0_0_41_openapi_job_info_resp_jobs_inner_resize_time \nrestart_cnt (optional)Integer Number of job restarts format: int32\nresv_name (optional)String Name of reservation to use \nscheduled_nodes (optional)String List of nodes scheduled to be used for the job \nselinux_context (optional)String SELinux context \nshared (optional)array[String] How the job can share resources with other jobs, if at all \nEnum:\nexclusive (optional)array[String] \nEnum:\noversubscribe (optional)Boolean \nshow_flags (optional)array[String] Job details shown in this response \nEnum:\nsockets_per_board (optional)Integer Number of sockets per board required format: int32\nsockets_per_node (optional)v0_0_41_openapi_job_info_resp_jobs_inner_sockets_per_node \nstart_time (optional)v0_0_41_openapi_job_info_resp_jobs_inner_start_time \nstate_description (optional)String Optional details for state_reason \nstate_reason (optional)String Reason for current Pending or Failed state \nstandard_error (optional)String Path to stderr file \nstandard_input (optional)String Path to stdin file \nstandard_output (optional)String Path to stdout file \nsubmit_time (optional)v0_0_41_openapi_job_info_resp_jobs_inner_submit_time \nsuspend_time (optional)v0_0_41_openapi_job_info_resp_jobs_inner_suspend_time \nsystem_comment (optional)String Arbitrary comment from slurmctld \ntime_limit (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_limit \ntime_minimum (optional)v0_0_41_job_desc_msg_time_minimum \nthreads_per_core (optional)v0_0_41_openapi_job_info_resp_jobs_inner_threads_per_core \ntres_bind (optional)String Task to TRES binding directives \ntres_freq (optional)String TRES frequency directives \ntres_per_job (optional)String Comma separated list of TRES=# values to be allocated per job \ntres_per_node (optional)String Comma separated list of TRES=# values to be allocated per node \ntres_per_socket (optional)String Comma separated list of TRES=# values to be allocated per socket \ntres_per_task (optional)String Comma separated list of TRES=# values to be allocated per task \ntres_req_str (optional)String TRES requested by the job \ntres_alloc_str (optional)String TRES used by the job \nuser_id (optional)Integer User ID that owns the job format: int32\nuser_name (optional)String User name that owns the job \nmaximum_switch_wait_time (optional)Integer Maximum time to wait for switches in seconds format: int32\nwckey (optional)String Workload characterization key \ncurrent_working_directory (optional)String Working directory to use for the job \n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_accrue_time -  Up\nWhen the job started accruing age priority (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_array_job_id -  Up\nJob ID of job array, or 0 if N/A\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_array_max_tasks -  Up\nMaximum number of simultaneously running array tasks, 0 if no limit\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_billable_tres -  Up\nBillable TRES\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Double If \"set\" is True the number will be set with value; otherwise ignore number contents format: double\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_cores_per_socket -  Up\nCores per socket required\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_cpu_frequency_governor -  Up\nCPU frequency governor\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_cpu_frequency_maximum -  Up\nMaximum CPU frequency\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_cpu_frequency_minimum -  Up\nMinimum CPU frequency\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_cpus -  Up\nMinimum number of CPUs required\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_cpus_per_task -  Up\nNumber of CPUs required by each task\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_deadline -  Up\nLatest time that the job may start (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_delay_boot -  Up\nNumber of seconds after job eligible start that nodes will be rebooted to satisfy feature specification\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_eligible_time -  Up\nTime when the job became eligible to run (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_end_time -  Up\nEnd time, real or expected (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_exit_code -  Up\nExit code of the job\n\nstatus (optional)array[String] Status given by return code \nEnum:\nreturn_code (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_return_code \nsignal (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_signal \n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_het_job_id -  Up\nHeterogeneous job ID, if applicable\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_job_resources -  Up\nResources used by the job\n\nselect_type array[String] Scheduler consumable resource selection type \nEnum:\nnodes (optional)v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes \ncpus Integer Number of allocated CPUs format: int32\nthreads_per_core v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_threads_per_core \n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes -  Up\n\n\ncount (optional)Integer Number of allocated nodes format: int32\nselect_type (optional)array[String] Node scheduling selection method \nEnum:\nlist (optional)String Node(s) allocated to the job \nwhole (optional)Boolean Whether whole nodes were allocated \nallocation (optional)array[v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner] Allocated node resources \n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner -  Up\nJob resources for a node\n\nindex Integer Node index format: int32\nname String Node name \ncpus (optional)v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner_cpus \nmemory (optional)v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner_memory \nsockets array[v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner_sockets_inner] Socket allocations in node \n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner_cpus -  Up\n\n\ncount (optional)Integer Total number of CPUs assigned to job format: int32\nused (optional)Integer Total number of CPUs used by job format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner_memory -  Up\n\n\nused (optional)Long Total memory (MiB) used by job format: int64\nallocated (optional)Long Total memory (MiB) allocated to job format: int64\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner_sockets_inner -  Up\n\n\nindex Integer Core index format: int32\ncores array[v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner_sockets_inner_cores_inner] Core in socket \n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner_sockets_inner_cores_inner -  Up\n\n\nindex Integer Core index format: int32\nstatus array[String] Core status \nEnum:\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_job_resources_threads_per_core -  Up\nNumber of processor threads per CPU core\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_last_sched_evaluation -  Up\nLast time job was evaluated for scheduling (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_max_cpus -  Up\nMaximum number of CPUs usable by the job\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_max_nodes -  Up\nMaximum number of nodes usable by the job\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_minimum_cpus_per_node -  Up\nMinimum number of CPUs per node\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_minimum_tmp_disk_per_node -  Up\nMinimum tmp disk space required per node\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_node_count -  Up\nMinimum number of nodes required\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_power -  Up\n\n\nflags (optional)array[oas_any_type_not_mapped] \n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_pre_sus_time -  Up\nTotal run time prior to last suspend in seconds\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_preempt_time -  Up\nTime job received preemption signal (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_preemptable_time -  Up\nTime job becomes eligible for preemption (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_resize_time -  Up\nTime of last size change (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_sockets_per_node -  Up\nNumber of sockets per node required\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_start_time -  Up\nTime execution began, or is expected to begin (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_submit_time -  Up\nTime when the job was submitted (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_suspend_time -  Up\nTime the job was last suspended or resumed (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_tasks -  Up\nNumber of tasks\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_board -  Up\nNumber of tasks invoked on each board\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_core -  Up\nNumber of tasks invoked on each core\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_node -  Up\nNumber of tasks invoked on each node\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_socket -  Up\nNumber of tasks invoked on each socket\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_tres -  Up\nNumber of tasks that can assess each GPU\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_jobs_inner_threads_per_core -  Up\nNumber of processor threads per CPU core required\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_job_info_resp_last_backfill -  Up\nTime of last backfill scheduler run (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_job_info_resp_last_update -  Up\nTime of last job change (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_job_post_response_results_inner -  Up\n\n\njob_id (optional)Integer Job ID for updated job format: int32\nstep_id (optional)String Step ID for updated job \nerror (optional)String Verbose update status or error \nerror_code (optional)Integer Verbose update status or error format: int32\nwhy (optional)String Update response message \n \n\nv0_0_41_openapi_job_submit_response_result -  Up\nJob submission\n\njob_id (optional)Integer New job ID format: int32\nstep_id (optional)String New job step ID \nerror_code (optional)Integer Error code format: int32\nerror (optional)String Error message \njob_submit_user_msg (optional)String Message to user from job_submit plugin \n \n\nv0_0_41_openapi_kill_jobs_resp_status_inner -  Up\nList of jobs signal responses\n\nerror (optional)v0_0_41_openapi_kill_jobs_resp_status_inner_error \nstep_id String Job or Step ID that signaling failed \njob_id v0_0_41_openapi_kill_jobs_resp_status_inner_job_id \nfederation (optional)v0_0_41_openapi_kill_jobs_resp_status_inner_federation \n \n\nv0_0_41_openapi_kill_jobs_resp_status_inner_error -  Up\n\n\nstring (optional)String String error encountered signaling job \ncode (optional)Integer Numeric error encountered signaling job format: int32\nmessage (optional)String Error message why signaling job failed \n \n\nv0_0_41_openapi_kill_jobs_resp_status_inner_federation -  Up\n\n\nsibling (optional)String Name of federation sibling (may be empty for non-federation) \n \n\nv0_0_41_openapi_kill_jobs_resp_status_inner_job_id -  Up\nJob ID that signaling failed\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_licenses_resp_last_update -  Up\nTime of last licenses change (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_licenses_resp_licenses_inner -  Up\n\n\nLicenseName (optional)String Name of the license \nTotal (optional)Integer Total number of licenses present format: int32\nUsed (optional)Integer Number of licenses in use format: int32\nFree (optional)Integer Number of licenses currently available format: int32\nRemote (optional)Boolean Indicates whether licenses are served by the database \nReserved (optional)Integer Number of licenses reserved format: int32\nLastConsumed (optional)Integer Last known number of licenses that were consumed in the license manager (Remote Only) format: int32\nLastDeficit (optional)Integer Number of \"missing licenses\" from the cluster's perspective format: int32\nLastUpdate (optional)Long When the license information was last updated (UNIX Timestamp) format: int64\n \n\nv0_0_41_openapi_nodes_resp_last_update -  Up\nTime of last node change (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_nodes_resp_nodes_inner -  Up\n\n\narchitecture (optional)String Computer architecture \nburstbuffer_network_address (optional)String Alternate network path to be used for sbcast network traffic \nboards (optional)Integer Number of Baseboards in nodes with a baseboard controller format: int32\nboot_time (optional)v0_0_41_openapi_nodes_resp_nodes_inner_boot_time \ncluster_name (optional)String Cluster name (only set in federated environments) \ncores (optional)Integer Number of cores in a single physical processor socket format: int32\nspecialized_cores (optional)Integer Number of cores reserved for system use format: int32\ncpu_binding (optional)Integer Default method for binding tasks to allocated CPUs format: int32\ncpu_load (optional)Integer CPU load as reported by the OS format: int32\nfree_mem (optional)v0_0_41_openapi_nodes_resp_nodes_inner_free_mem \ncpus (optional)Integer Total CPUs, including cores and threads format: int32\neffective_cpus (optional)Integer Number of effective CPUs (excluding specialized CPUs) format: int32\nspecialized_cpus (optional)String Abstract CPU IDs on this node reserved for exclusive use by slurmd and slurmstepd \nenergy (optional)v0_0_41_openapi_nodes_resp_nodes_inner_energy \nexternal_sensors (optional)Object \nextra (optional)String Arbitrary string used for node filtering if extra constraints are enabled \npower (optional)Object \nfeatures (optional)array[String] Available features \nactive_features (optional)array[String] Currently active features \ngpu_spec (optional)String CPU cores reserved for jobs that also use a GPU \ngres (optional)String Generic resources \ngres_drained (optional)String Drained generic resources \ngres_used (optional)String Generic resources currently in use \ninstance_id (optional)String Cloud instance ID \ninstance_type (optional)String Cloud instance type \nlast_busy (optional)v0_0_41_openapi_nodes_resp_nodes_inner_last_busy \nmcs_label (optional)String Multi-Category Security label \nspecialized_memory (optional)Long Combined memory limit, in MB, for Slurm compute node daemons format: int64\nname (optional)String NodeName \nnext_state_after_reboot (optional)array[String] The state the node will be assigned after rebooting \nEnum:\naddress (optional)String NodeAddr, used to establish a communication path \nhostname (optional)String NodeHostname \nstate (optional)array[String] Node state(s) applicable to this node \nEnum:\noperating_system (optional)String Operating system reported by the node \nowner (optional)String User allowed to run jobs on this node (unset if no restriction) \npartitions (optional)array[String] Partitions containing this node \nport (optional)Integer TCP port number of the slurmd format: int32\nreal_memory (optional)Long Total memory in MB on the node format: int64\nres_cores_per_gpu (optional)Integer Number of CPU cores per GPU restricted to GPU jobs format: int32\ncomment (optional)String Arbitrary comment \nreason (optional)String Describes why the node is in a \"DOWN\", \"DRAINED\", \"DRAINING\", \"FAILING\" or \"FAIL\" state \nreason_changed_at (optional)v0_0_41_openapi_nodes_resp_nodes_inner_reason_changed_at \nreason_set_by_user (optional)String User who set the reason \nresume_after (optional)v0_0_41_openapi_nodes_resp_nodes_inner_resume_after \nreservation (optional)String Name of reservation containing this node \nalloc_memory (optional)Long Total memory in MB currently allocated for jobs format: int64\nalloc_cpus (optional)Integer Total number of CPUs currently allocated for jobs format: int32\nalloc_idle_cpus (optional)Integer Total number of idle CPUs format: int32\ntres_used (optional)String Trackable resources currently allocated for jobs \ntres_weighted (optional)Double Weighted number of billable trackable resources allocated format: double\nslurmd_start_time (optional)v0_0_41_openapi_nodes_resp_nodes_inner_slurmd_start_time \nsockets (optional)Integer Number of physical processor sockets/chips on the node format: int32\nthreads (optional)Integer Number of logical threads in a single physical core format: int32\ntemporary_disk (optional)Integer Total size in MB of temporary disk storage in TmpFS format: int32\nweight (optional)Integer Weight of the node for scheduling purposes format: int32\ntres (optional)String Configured trackable resources \nversion (optional)String Slurmd version \n \n\nv0_0_41_openapi_nodes_resp_nodes_inner_boot_time -  Up\nTime when the node booted (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_nodes_resp_nodes_inner_energy -  Up\nEnergy usage data\n\naverage_watts (optional)Integer Average power consumption, in watts format: int32\nbase_consumed_energy (optional)Long The energy consumed between when the node was powered on and the last time it was registered by slurmd, in joules format: int64\nconsumed_energy (optional)Long The energy consumed between the last time the node was registered by the slurmd daemon and the last node energy accounting sample, in joules format: int64\ncurrent_watts (optional)v0_0_41_openapi_nodes_resp_nodes_inner_energy_current_watts \nprevious_consumed_energy (optional)Long Previous value of consumed_energy format: int64\nlast_collected (optional)Long Time when energy data was last retrieved (UNIX timestamp) format: int64\n \n\nv0_0_41_openapi_nodes_resp_nodes_inner_energy_current_watts -  Up\nThe instantaneous power consumption at the time of the last node energy accounting sample, in watts\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_nodes_resp_nodes_inner_free_mem -  Up\nTotal memory in MB currently free as reported by the OS\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_nodes_resp_nodes_inner_last_busy -  Up\nTime when the node was last busy (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_nodes_resp_nodes_inner_reason_changed_at -  Up\nWhen the reason changed (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_nodes_resp_nodes_inner_resume_after -  Up\nNumber of seconds after the node's state is updated to \"DOWN\" or \"DRAIN\" before scheduling a node state resume\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_nodes_resp_nodes_inner_slurmd_start_time -  Up\nTime when the slurmd started (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_partition_resp_last_update -  Up\nTime of last partition change (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_partition_resp_partitions_inner -  Up\n\n\nnodes (optional)v0_0_41_openapi_partition_resp_partitions_inner_nodes \naccounts (optional)v0_0_41_openapi_partition_resp_partitions_inner_accounts \ngroups (optional)v0_0_41_openapi_partition_resp_partitions_inner_groups \nqos (optional)v0_0_41_openapi_partition_resp_partitions_inner_qos \nalternate (optional)String Alternate \ntres (optional)v0_0_41_openapi_partition_resp_partitions_inner_tres \ncluster (optional)String Cluster name \nselect_type (optional)array[String] Scheduler consumable resource selection type \nEnum:\ncpus (optional)v0_0_41_openapi_partition_resp_partitions_inner_cpus \ndefaults (optional)v0_0_41_openapi_partition_resp_partitions_inner_defaults \ngrace_time (optional)Integer GraceTime format: int32\nmaximums (optional)v0_0_41_openapi_partition_resp_partitions_inner_maximums \nminimums (optional)v0_0_41_openapi_partition_resp_partitions_inner_minimums \nname (optional)String PartitionName \nnode_sets (optional)String NodeSets \npriority (optional)v0_0_41_openapi_partition_resp_partitions_inner_priority \ntimeouts (optional)v0_0_41_openapi_partition_resp_partitions_inner_timeouts \npartition (optional)v0_0_41_openapi_partition_resp_partitions_inner_partition \nsuspend_time (optional)v0_0_41_openapi_partition_resp_partitions_inner_suspend_time \n \n\nv0_0_41_openapi_partition_resp_partitions_inner_accounts -  Up\n\n\nallowed (optional)String AllowAccounts \ndeny (optional)String DenyAccounts \n \n\nv0_0_41_openapi_partition_resp_partitions_inner_cpus -  Up\n\n\ntask_binding (optional)Integer CpuBind format: int32\ntotal (optional)Integer TotalCPUs format: int32\n \n\nv0_0_41_openapi_partition_resp_partitions_inner_defaults -  Up\n\n\nmemory_per_cpu (optional)Long DefMemPerCPU or DefMemPerNode format: int64\npartition_memory_per_cpu (optional)v0_0_41_openapi_partition_resp_partitions_inner_defaults_partition_memory_per_cpu \npartition_memory_per_node (optional)v0_0_41_openapi_partition_resp_partitions_inner_defaults_partition_memory_per_node \ntime (optional)v0_0_41_openapi_partition_resp_partitions_inner_defaults_time \njob (optional)String JobDefaults \n \n\nv0_0_41_openapi_partition_resp_partitions_inner_defaults_partition_memory_per_cpu -  Up\nDefMemPerCPU\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_partition_resp_partitions_inner_defaults_partition_memory_per_node -  Up\nDefMemPerNode\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_partition_resp_partitions_inner_defaults_time -  Up\nDefaultTime in minutes\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_partition_resp_partitions_inner_groups -  Up\n\n\nallowed (optional)String AllowGroups \n \n\nv0_0_41_openapi_partition_resp_partitions_inner_maximums -  Up\n\n\ncpus_per_node (optional)v0_0_41_openapi_partition_resp_partitions_inner_maximums_cpus_per_node \ncpus_per_socket (optional)v0_0_41_openapi_partition_resp_partitions_inner_maximums_cpus_per_socket \nmemory_per_cpu (optional)Long MaxMemPerCPU or MaxMemPerNode format: int64\npartition_memory_per_cpu (optional)v0_0_41_openapi_partition_resp_partitions_inner_maximums_partition_memory_per_cpu \npartition_memory_per_node (optional)v0_0_41_openapi_partition_resp_partitions_inner_maximums_partition_memory_per_node \nnodes (optional)v0_0_41_openapi_partition_resp_partitions_inner_maximums_nodes \nshares (optional)Integer OverSubscribe format: int32\noversubscribe (optional)v0_0_41_openapi_partition_resp_partitions_inner_maximums_oversubscribe \ntime (optional)v0_0_41_openapi_partition_resp_partitions_inner_maximums_time \nover_time_limit (optional)v0_0_41_openapi_partition_resp_partitions_inner_maximums_over_time_limit \n \n\nv0_0_41_openapi_partition_resp_partitions_inner_maximums_cpus_per_node -  Up\nMaxCPUsPerNode\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_partition_resp_partitions_inner_maximums_cpus_per_socket -  Up\nMaxCPUsPerSocket\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_partition_resp_partitions_inner_maximums_nodes -  Up\nMaxNodes\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_partition_resp_partitions_inner_maximums_over_time_limit -  Up\nOverTimeLimit\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_partition_resp_partitions_inner_maximums_oversubscribe -  Up\n\n\njobs (optional)Integer Maximum number of jobs allowed to oversubscribe resources format: int32\nflags (optional)array[String] Flags applicable to the OverSubscribe setting \nEnum:\n \n\nv0_0_41_openapi_partition_resp_partitions_inner_maximums_partition_memory_per_cpu -  Up\nMaxMemPerCPU\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_partition_resp_partitions_inner_maximums_partition_memory_per_node -  Up\nMaxMemPerNode\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_partition_resp_partitions_inner_maximums_time -  Up\nMaxTime\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_partition_resp_partitions_inner_minimums -  Up\n\n\nnodes (optional)Integer MinNodes format: int32\n \n\nv0_0_41_openapi_partition_resp_partitions_inner_nodes -  Up\n\n\nallowed_allocation (optional)String AllocNodes \nconfigured (optional)String Nodes \ntotal (optional)Integer TotalNodes format: int32\n \n\nv0_0_41_openapi_partition_resp_partitions_inner_partition -  Up\n\n\nstate (optional)array[String] Current state(s) \nEnum:\n \n\nv0_0_41_openapi_partition_resp_partitions_inner_priority -  Up\n\n\njob_factor (optional)Integer PriorityJobFactor format: int32\ntier (optional)Integer PriorityTier format: int32\n \n\nv0_0_41_openapi_partition_resp_partitions_inner_qos -  Up\n\n\nallowed (optional)String AllowQOS \ndeny (optional)String DenyQOS \nassigned (optional)String QOS \n \n\nv0_0_41_openapi_partition_resp_partitions_inner_suspend_time -  Up\nSuspendTime (GLOBAL if both set and infinite are false)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_partition_resp_partitions_inner_timeouts -  Up\n\n\nresume (optional)v0_0_41_openapi_partition_resp_partitions_inner_timeouts_resume \nsuspend (optional)v0_0_41_openapi_partition_resp_partitions_inner_timeouts_suspend \n \n\nv0_0_41_openapi_partition_resp_partitions_inner_timeouts_resume -  Up\nResumeTimeout (GLOBAL if both set and infinite are false)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_partition_resp_partitions_inner_timeouts_suspend -  Up\nSuspendTimeout (GLOBAL if both set and infinite are false)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_partition_resp_partitions_inner_tres -  Up\n\n\nbilling_weights (optional)String TRESBillingWeights \nconfigured (optional)String TRES \n \n\nv0_0_41_openapi_ping_array_resp_pings_inner -  Up\n\n\nhostname (optional)String Target for ping \npinged (optional)String Ping result \nlatency (optional)Long Number of microseconds it took to successfully ping or timeout format: int64\nmode (optional)String The operating mode of the responding slurmctld \n \n\nv0_0_41_openapi_reservation_resp_last_update -  Up\nTime of last reservation change (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_reservation_resp_reservations_inner -  Up\n\n\naccounts (optional)String Comma separated list of permitted accounts \nburst_buffer (optional)String BurstBuffer \ncore_count (optional)Integer CoreCnt format: int32\ncore_specializations (optional)array[v0_0_41_openapi_reservation_resp_reservations_inner_core_specializations_inner] Reserved cores specification \nend_time (optional)v0_0_41_openapi_reservation_resp_reservations_inner_end_time \nfeatures (optional)String Features \nflags (optional)array[String] Flags associated with the reservation \nEnum:\ngroups (optional)String Groups \nlicenses (optional)String Licenses \nmax_start_delay (optional)Integer MaxStartDelay in seconds format: int32\nname (optional)String ReservationName \nnode_count (optional)Integer NodeCnt format: int32\nnode_list (optional)String Nodes \npartition (optional)String PartitionName \npurge_completed (optional)v0_0_41_openapi_reservation_resp_reservations_inner_purge_completed \nstart_time (optional)v0_0_41_openapi_reservation_resp_reservations_inner_start_time \nwatts (optional)v0_0_41_openapi_reservation_resp_reservations_inner_watts \ntres (optional)String Comma separated list of required TRES \nusers (optional)String Comma separated list of permitted users \n \n\nv0_0_41_openapi_reservation_resp_reservations_inner_core_specializations_inner -  Up\n\n\nnode (optional)String Name of reserved node \ncore (optional)String IDs of reserved cores \n \n\nv0_0_41_openapi_reservation_resp_reservations_inner_end_time -  Up\nEndTime (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_reservation_resp_reservations_inner_purge_completed -  Up\n\n\ntime (optional)v0_0_41_openapi_reservation_resp_reservations_inner_purge_completed_time \n \n\nv0_0_41_openapi_reservation_resp_reservations_inner_purge_completed_time -  Up\nIf PURGE_COMP flag is set, the number of seconds this reservation will sit idle until it is revoked\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_reservation_resp_reservations_inner_start_time -  Up\nStartTime (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_reservation_resp_reservations_inner_watts -  Up\n32 bit integer number with flags\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_shares_resp_shares -  Up\nfairshare info\n\nshares (optional)array[v0_0_41_openapi_shares_resp_shares_shares_inner] Association shares \ntotal_shares (optional)Long Total number of shares format: int64\n \n\nv0_0_41_openapi_shares_resp_shares_shares_inner -  Up\n\n\nid (optional)Integer Association ID format: int32\ncluster (optional)String Cluster name \nname (optional)String Share name \nparent (optional)String Parent name \npartition (optional)String Partition name \nshares_normalized (optional)v0_0_41_openapi_shares_resp_shares_shares_inner_shares_normalized \nshares (optional)v0_0_41_openapi_shares_resp_shares_shares_inner_shares \ntres (optional)v0_0_41_openapi_shares_resp_shares_shares_inner_tres \neffective_usage (optional)Double Effective, normalized usage format: double\nusage_normalized (optional)v0_0_41_openapi_shares_resp_shares_shares_inner_usage_normalized \nusage (optional)Long Measure of tresbillableunits usage format: int64\nfairshare (optional)v0_0_41_openapi_shares_resp_shares_shares_inner_fairshare \ntype (optional)array[String] User or account association \nEnum:\n \n\nv0_0_41_openapi_shares_resp_shares_shares_inner_fairshare -  Up\n\n\nfactor (optional)Double Fairshare factor format: double\nlevel (optional)Double Fairshare factor at this level; stored on an assoc as a long double, but that is not needed for display in sshare format: double\n \n\nv0_0_41_openapi_shares_resp_shares_shares_inner_shares -  Up\nNumber of shares allocated\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_shares_resp_shares_shares_inner_shares_normalized -  Up\nNormalized shares\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Double If \"set\" is True the number will be set with value; otherwise ignore number contents format: double\n \n\nv0_0_41_openapi_shares_resp_shares_shares_inner_tres -  Up\n\n\nrun_seconds (optional)array[v0_0_41_openapi_shares_resp_shares_shares_inner_tres_run_seconds_inner] Currently running tres-secs = grp_used_tres_run_secs \ngroup_minutes (optional)array[v0_0_41_openapi_shares_resp_shares_shares_inner_tres_run_seconds_inner] TRES-minute limit \nusage (optional)array[v0_0_41_openapi_shares_resp_shares_shares_inner_tres_usage_inner] Measure of each TRES usage \n \n\nv0_0_41_openapi_shares_resp_shares_shares_inner_tres_run_seconds_inner -  Up\n\n\nname (optional)String TRES name \nvalue (optional)v0_0_41_openapi_shares_resp_shares_shares_inner_tres_run_seconds_inner_value \n \n\nv0_0_41_openapi_shares_resp_shares_shares_inner_tres_run_seconds_inner_value -  Up\nTRES value\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_shares_resp_shares_shares_inner_tres_usage_inner -  Up\n\n\nname (optional)String TRES name \nvalue (optional)BigDecimal TRES value \n \n\nv0_0_41_openapi_shares_resp_shares_shares_inner_usage_normalized -  Up\nNormalized usage\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Double If \"set\" is True the number will be set with value; otherwise ignore number contents format: double\n \n\nv0_0_41_openapi_slurmdbd_config_resp_accounts_inner -  Up\n\n\nassociations (optional)array[v0_0_41_openapi_slurmdbd_config_resp_accounts_inner_associations_inner] Associations involving this account (only populated if requested) \ncoordinators (optional)array[v0_0_41_openapi_slurmdbd_config_resp_accounts_inner_coordinators_inner] List of users that are a coordinator of this account (only populated if requested) \ndescription String Arbitrary string describing the account \nname String Account name \norganization String Organization to which the account belongs \nflags (optional)array[String] Flags associated with the account \nEnum:\n \n\nv0_0_41_openapi_slurmdbd_config_resp_accounts_inner_associations_inner -  Up\n\n\naccount (optional)String Account \ncluster (optional)String Cluster \npartition (optional)String Partition \nuser String User name \nid (optional)Integer Numeric association ID format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_accounts_inner_coordinators_inner -  Up\n\n\nname String User name \ndirect (optional)Boolean Indicates whether the coordinator was directly assigned to this account \n \n\nv0_0_41_openapi_slurmdbd_config_resp_associations_inner -  Up\n\n\naccounting (optional)array[v0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner_accounting_inner] Accounting records containing related resource usage \naccount (optional)String Account \ncluster (optional)String Cluster name \ncomment (optional)String Arbitrary comment \ndefault (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_default \nflags (optional)array[String] Flags on the association \nEnum:\nmax (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max \nid (optional)Integer Unique ID format: int32\nis_default (optional)Boolean Is default association for user \nlineage (optional)String Complete path up the hierarchy to the root association \nmin (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_min \nparent_account (optional)String Name of parent account \npartition (optional)String Partition name \npriority (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_priority \nqos (optional)array[String] List of available QOS names \nshares_raw (optional)Integer Allocated shares used for fairshare calculation format: int32\nuser String User name \n \n\nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_default -  Up\n\n\nqos (optional)String Default QOS \n \n\nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max -  Up\n\n\njobs (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs \ntres (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres \nper (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_per \n \n\nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs -  Up\n\n\nper (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_per \nactive (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_active \naccruing (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_accruing \ntotal (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_total \n \n\nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_accruing -  Up\nMaxJobsAccrue\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_active -  Up\nMaxJobs\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_per -  Up\n\n\ncount (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_active_jobs_count \naccruing (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_active_jobs_accruing \nsubmitted (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_per_submitted \nwall_clock (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock_per_job \n \n\nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_per_submitted -  Up\nGrpSubmitJobs\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_total -  Up\nMaxSubmitJobs\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_per -  Up\n\n\naccount (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_per_account \n \n\nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_per_account -  Up\n\n\nwall_clock (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock_per_qos \n \n\nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres -  Up\n\n\ntotal (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] GrpTRES \ngroup (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres_group \nminutes (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres_minutes \nper (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres_per \n \n\nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres_group -  Up\n\n\nminutes (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] GrpTRESMins \nactive (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] GrpTRESRunMins \n \n\nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres_minutes -  Up\n\n\ntotal (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESMinsPerJob \nper (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres_minutes_per \n \n\nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres_minutes_per -  Up\n\n\njob (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESMinsPerJob \n \n\nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres_per -  Up\n\n\njob (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESPerJob \nnode (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESPerNode \n \n\nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_min -  Up\n\n\npriority_threshold (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min_priority_threshold \n \n\nv0_0_41_openapi_slurmdbd_config_resp_associations_inner_priority -  Up\nAssociation priority factor\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_clusters_inner -  Up\n\n\ncontroller (optional)v0_0_41_openapi_slurmdbd_config_resp_clusters_inner_controller \nflags (optional)array[String] Flags \nEnum:\nname (optional)String ClusterName \nnodes (optional)String Node names \nselect_plugin (optional)String \nassociations (optional)v0_0_41_openapi_slurmdbd_config_resp_clusters_inner_associations \nrpc_version (optional)Integer RPC version used in the cluster format: int32\ntres (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Trackable resources \n \n\nv0_0_41_openapi_slurmdbd_config_resp_clusters_inner_associations -  Up\n\n\nroot (optional)v0_0_41_openapi_slurmdbd_config_resp_clusters_inner_associations_root \n \n\nv0_0_41_openapi_slurmdbd_config_resp_clusters_inner_associations_root -  Up\nRoot association information\n\naccount (optional)String Account \ncluster (optional)String Cluster \npartition (optional)String Partition \nuser String User name \nid (optional)Integer Numeric association ID format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_clusters_inner_controller -  Up\n\n\nhost (optional)String ControlHost \nport (optional)Integer ControlPort format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_instances_inner -  Up\n\n\ncluster (optional)String Cluster name \nextra (optional)String Arbitrary string used for node filtering if extra constraints are enabled \ninstance_id (optional)String Cloud instance ID \ninstance_type (optional)String Cloud instance type \nnode_name (optional)String NodeName \ntime (optional)v0_0_41_openapi_slurmdbd_config_resp_instances_inner_time \n \n\nv0_0_41_openapi_slurmdbd_config_resp_instances_inner_time -  Up\n\n\ntime_end (optional)Long When the instance will end (UNIX timestamp) format: int64\ntime_start (optional)Long When the instance will start (UNIX timestamp) format: int64\n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner -  Up\n\n\ndescription (optional)String Arbitrary description \nflags (optional)array[String] Flags, to avoid modifying current values specify NOT_SET \nEnum:\nid (optional)Integer Unique ID format: int32\nlimits (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits \nname (optional)String Name \npreempt (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_preempt \npriority (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_priority \nusage_factor (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_usage_factor \nusage_threshold (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_usage_threshold \n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits -  Up\n\n\ngrace_time (optional)Integer GraceTime format: int32\nmax (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max \nfactor (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_factor \nmin (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min \n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_factor -  Up\nLimitFactor\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Double If \"set\" is True the number will be set with value; otherwise ignore number contents format: double\n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max -  Up\n\n\nactive_jobs (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_active_jobs \ntres (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_tres \nwall_clock (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock \njobs (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs \naccruing (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_accruing \n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_accruing -  Up\n\n\nper (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_accruing_per \n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_accruing_per -  Up\n\n\naccount (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_accruing_per_account \nuser (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_accruing_per_user \n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_accruing_per_account -  Up\nMaxJobsAccruePerAccount\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_accruing_per_user -  Up\nMaxJobsAccruePerUser\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_active_jobs -  Up\n\n\naccruing (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_active_jobs_accruing \ncount (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_active_jobs_count \n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_active_jobs_accruing -  Up\nGrpJobsAccrue\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_active_jobs_count -  Up\nGrpJobs\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs -  Up\n\n\nactive_jobs (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_active_jobs \nper (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_per \n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_active_jobs -  Up\n\n\nper (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_active_jobs_per \n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_active_jobs_per -  Up\n\n\naccount (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_active_jobs_per_account \nuser (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_active_jobs_per_user \n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_active_jobs_per_account -  Up\nMaxJobsPerAccount\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_active_jobs_per_user -  Up\nMaxJobsPerUser\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_per -  Up\n\n\naccount (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_per_account \nuser (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_per_user \n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_per_account -  Up\nMaxSubmitJobsPerAccount\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_per_user -  Up\nMaxSubmitJobsPerUser\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_tres -  Up\n\n\ntotal (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] GrpTRES \nminutes (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_tres_minutes \nper (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_tres_per \n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_tres_minutes -  Up\n\n\nper (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_tres_minutes_per \n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_tres_minutes_per -  Up\n\n\nqos (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] GrpTRESRunMins \njob (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESMinsPerJob \naccount (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESRunMinsPerAccount \nuser (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESRunMinsPerUser \n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_tres_per -  Up\n\n\naccount (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESPerAccount \njob (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESPerJob \nnode (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESPerNode \nuser (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESPerUser \n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock -  Up\n\n\nper (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock_per \n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock_per -  Up\n\n\nqos (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock_per_qos \njob (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock_per_job \n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock_per_job -  Up\nMaxWallDurationPerJob\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock_per_qos -  Up\nGrpWall\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min -  Up\n\n\npriority_threshold (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min_priority_threshold \ntres (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min_tres \n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min_priority_threshold -  Up\nMinPrioThreshold\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min_tres -  Up\n\n\nper (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min_tres_per \n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min_tres_per -  Up\n\n\njob (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MinTRES \n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_preempt -  Up\n\n\nlist (optional)array[String] Other QOS's this QOS can preempt \nmode (optional)array[String] PreemptMode \nEnum:\nexempt_time (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_preempt_exempt_time \n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_preempt_exempt_time -  Up\nPreemptExemptTime\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_priority -  Up\nPriority\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_usage_factor -  Up\nUsageFactor\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Double If \"set\" is True the number will be set with value; otherwise ignore number contents format: double\n \n\nv0_0_41_openapi_slurmdbd_config_resp_qos_inner_usage_threshold -  Up\nUsageThreshold\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Double If \"set\" is True the number will be set with value; otherwise ignore number contents format: double\n \n\nv0_0_41_openapi_slurmdbd_config_resp_users_inner -  Up\n\n\nadministrator_level (optional)array[String] AdminLevel granted to the user \nEnum:\nassociations (optional)array[v0_0_41_openapi_slurmdbd_config_resp_accounts_inner_associations_inner] Associations created for this user \ncoordinators (optional)array[v0_0_41_openapi_slurmdbd_config_resp_accounts_inner_coordinators_inner] Accounts this user is a coordinator for \ndefault (optional)v0_0_41_openapi_slurmdbd_config_resp_users_inner_default \nflags (optional)array[String] Flags associated with user \nEnum:\nname String User name \nold_name (optional)String Previous user name \nwckeys (optional)array[v0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner] List of available WCKeys \n \n\nv0_0_41_openapi_slurmdbd_config_resp_users_inner_default -  Up\n\n\naccount (optional)String Default Account \nwckey (optional)String Default WCKey \n \n\nv0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner -  Up\n\n\naccounting (optional)array[v0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner_accounting_inner] Accounting records containing related resource usage \ncluster String Cluster name \nid (optional)Integer Unique ID for this user-cluster-wckey combination format: int32\nname String WCKey name \nuser String User name \nflags (optional)array[String] Flags associated with the WCKey \nEnum:\n \n\nv0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner_accounting_inner -  Up\n\n\nallocated (optional)v0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner_accounting_inner_allocated \nid (optional)Integer Association ID or Workload characterization key ID format: int32\nstart (optional)Long When the record was started format: int64\nTRES (optional)v0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner_accounting_inner_TRES \n \n\nv0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner_accounting_inner_TRES -  Up\nTrackable resources\n\ntype String TRES type (CPU, MEM, etc) \nname (optional)String TRES name (if applicable) \nid (optional)Integer ID used in database format: int32\ncount (optional)Long TRES count (0 if listed generically) format: int64\n \n\nv0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner_accounting_inner_allocated -  Up\n\n\nseconds (optional)Long Number of cpu seconds allocated format: int64\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_errors_inner -  Up\n\n\ndescription (optional)String Long form error description \nerror_number (optional)Integer Slurm numeric error identifier format: int32\nerror (optional)String Short form error description \nsource (optional)String Source of error or where error was first detected \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner -  Up\n\n\naccount (optional)String Account the job ran under \ncomment (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_comment \nallocation_nodes (optional)Integer List of nodes allocated to the job format: int32\narray (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array \nassociation (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_association \nblock (optional)String The name of the block to be used (used with Blue Gene systems) \ncluster (optional)String Cluster name \nconstraints (optional)String Feature(s) the job requested as a constraint \ncontainer (optional)String Absolute path to OCI container bundle \nderived_exit_code (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code \ntime (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time \nexit_code (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_exit_code \nextra (optional)String Arbitrary string used for node filtering if extra constraints are enabled \nfailed_node (optional)String Name of node that caused job failure \nflags (optional)array[String] Flags associated with the job \nEnum:\ngroup (optional)String Group ID of the user that owns the job \nhet (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_het \njob_id (optional)Integer Job ID format: int32\nname (optional)String Job name \nlicenses (optional)String License(s) required by the job \nmcs (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_mcs \nnodes (optional)String Node(s) allocated to the job \npartition (optional)String Partition assigned to the job \nhold (optional)Boolean Hold (true) or release (false) job \npriority (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_priority \nqos (optional)String Quality of Service assigned to the job \nrequired (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required \nkill_request_user (optional)String User ID that requested termination of the job \nreservation (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_reservation \nscript (optional)String Job batch script; only the first component in a HetJob is populated or honored \nstdin_expanded (optional)String Job stdin with expanded fields \nstdout_expanded (optional)String Job stdout with expanded fields \nstderr_expanded (optional)String Job stderr with expanded fields \nstdout (optional)String Path to stdout file \nstderr (optional)String Path to stderr file \nstdin (optional)String Path to stdin file \nstate (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_state \nsteps (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner] Individual steps in the job \nsubmit_line (optional)String Command used to submit the job \ntres (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_tres \nused_gres (optional)String Generic resources used by job \nuser (optional)String User that owns the job \nwckey (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_wckey \nworking_directory (optional)String Path to current working directory \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array -  Up\n\n\njob_id (optional)Integer Job ID of job array, or 0 if N/A format: int32\nlimits (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_limits \ntask_id (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_task_id \ntask (optional)String String expression of task IDs in this record \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_limits -  Up\n\n\nmax (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_limits_max \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_limits_max -  Up\n\n\nrunning (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_limits_max_running \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_limits_max_running -  Up\n\n\ntasks (optional)Integer Maximum number of simultaneously running tasks, 0 if no limit format: int32\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_task_id -  Up\nTask ID of this task in job array\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_association -  Up\nUnique identifier for the association\n\naccount (optional)String Account \ncluster (optional)String Cluster \npartition (optional)String Partition \nuser String User name \nid (optional)Integer Numeric association ID format: int32\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_comment -  Up\n\n\nadministrator (optional)String Arbitrary comment made by administrator \njob (optional)String Arbitrary comment made by user \nsystem (optional)String Arbitrary comment from slurmctld \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code -  Up\nHighest exit code of all job steps\n\nstatus (optional)array[String] Status given by return code \nEnum:\nreturn_code (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_return_code \nsignal (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_signal \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_return_code -  Up\nProcess return code (numeric)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_signal -  Up\n\n\nid (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_signal_id \nname (optional)String Signal sent to process \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_signal_id -  Up\nSignal sent to process (numeric)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_exit_code -  Up\nExit code\n\nstatus (optional)array[String] Status given by return code \nEnum:\nreturn_code (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_return_code \nsignal (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_signal \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_het -  Up\n\n\njob_id (optional)Integer Heterogeneous job ID, if applicable format: int32\njob_offset (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_het_job_offset \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_het_job_offset -  Up\nUnique sequence number applied to this component of the heterogeneous job\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_mcs -  Up\n\n\nlabel (optional)String Multi-Category Security label on the job \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_priority -  Up\nRequest specific job priority\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required -  Up\n\n\nCPUs (optional)Integer Minimum number of CPUs required format: int32\nmemory_per_cpu (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required_memory_per_cpu \nmemory_per_node (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required_memory_per_node \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required_memory_per_cpu -  Up\nMinimum memory in megabytes per allocated CPU\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required_memory_per_node -  Up\nMinimum memory in megabytes per allocated node\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_reservation -  Up\n\n\nid (optional)Integer Unique identifier of requested reservation format: int32\nname (optional)String Name of reservation to use \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_state -  Up\n\n\ncurrent (optional)array[String] Current state \nEnum:\nreason (optional)String Reason for previous Pending or Failed state \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner -  Up\n\n\ntime (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time \nexit_code (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_exit_code \nnodes (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_nodes \ntasks (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tasks \npid (optional)String Process ID \nCPU (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_CPU \nkill_request_user (optional)String User ID that requested termination of the step \nstate (optional)array[String] Current state \nEnum:\nstatistics (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_statistics \nstep (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_step \ntask (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_task \ntres (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_CPU -  Up\n\n\nrequested_frequency (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_CPU_requested_frequency \ngovernor (optional)String Requested CPU frequency governor in kHz \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_CPU_requested_frequency -  Up\n\n\nmin (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_CPU_requested_frequency_min \nmax (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_CPU_requested_frequency_max \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_CPU_requested_frequency_max -  Up\nMaximum requested CPU frequency in kHz\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_CPU_requested_frequency_min -  Up\nMinimum requested CPU frequency in kHz\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_nodes -  Up\n\n\ncount (optional)Integer Number of nodes in the job step format: int32\nrange (optional)String Node(s) allocated to the job step \nlist (optional)array[String] List of nodes used by the step \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_statistics -  Up\n\n\nCPU (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_statistics_CPU \nenergy (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_statistics_energy \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_statistics_CPU -  Up\n\n\nactual_frequency (optional)Long Average weighted CPU frequency of all tasks in kHz format: int64\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_statistics_energy -  Up\n\n\nconsumed (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_statistics_energy_consumed \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_statistics_energy_consumed -  Up\nTotal energy consumed by all tasks in a job in joules\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_step -  Up\n\n\nid (optional)String Step ID \nname (optional)String Step name \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_task -  Up\n\n\ndistribution (optional)String The layout of the step was when it was running \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tasks -  Up\n\n\ncount (optional)Integer Total number of tasks format: int32\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time -  Up\n\n\nelapsed (optional)Integer Elapsed time in seconds format: int32\nend (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_end \nstart (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_start \nsuspended (optional)Integer Time in suspended state in seconds format: int32\nsystem (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_system \ntotal (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_total \nuser (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_user \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_end -  Up\nEnd time (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_start -  Up\nTime execution began (UNIX timestamp)\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_system -  Up\n\n\nseconds (optional)Long System CPU time used by the step in seconds format: int64\nmicroseconds (optional)Integer System CPU time used by the step in microseconds format: int32\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_total -  Up\n\n\nseconds (optional)Long Total CPU time used by the step in seconds format: int64\nmicroseconds (optional)Integer Total CPU time used by the step in microseconds format: int32\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_user -  Up\n\n\nseconds (optional)Long User CPU time used by the step in seconds format: int64\nmicroseconds (optional)Integer User CPU time used by the step in microseconds format: int32\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres -  Up\n\n\nrequested (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested \nconsumed (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_consumed \nallocated (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Trackable resources allocated to the step \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_consumed -  Up\n\n\nmax (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Maximum TRES usage consumed among all tasks \nmin (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Minimum TRES usage consumed among all tasks \naverage (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Average TRES usage consumed among all tasks \ntotal (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Total TRES usage consumed among all tasks \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested -  Up\n\n\nmax (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Maximum TRES usage requested among all tasks \nmin (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Minimum TRES usage requested among all tasks \naverage (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Average TRES usage requested among all tasks \ntotal (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Total TRES usage requested among all tasks \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner -  Up\n\n\ntype String TRES type (CPU, MEM, etc) \nname (optional)String TRES name (if applicable) \nid (optional)Integer ID used in database format: int32\ncount (optional)Long TRES count (0 if listed generically) format: int64\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time -  Up\n\n\nelapsed (optional)Integer Elapsed time in seconds format: int32\neligible (optional)Long Time when the job became eligible to run (UNIX timestamp) format: int64\nend (optional)Long End time (UNIX timestamp) format: int64\nplanned (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_planned \nstart (optional)Long Time execution began (UNIX timestamp) format: int64\nsubmission (optional)Long Time when the job was submitted (UNIX timestamp) format: int64\nsuspended (optional)Integer Total time in suspended state in seconds format: int32\nsystem (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_system \nlimit (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_limit \ntotal (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_total \nuser (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_user \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_limit -  Up\nMaximum run time in minutes\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_planned -  Up\nTime required to start job after becoming eligible to run in seconds\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_system -  Up\n\n\nseconds (optional)Long System CPU time used by the job in seconds format: int64\nmicroseconds (optional)Long System CPU time used by the job in microseconds format: int64\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_total -  Up\n\n\nseconds (optional)Long Sum of System and User CPU time used by the job in seconds format: int64\nmicroseconds (optional)Long Sum of System and User CPU time used by the job in microseconds format: int64\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_user -  Up\n\n\nseconds (optional)Long User CPU time used by the job in seconds format: int64\nmicroseconds (optional)Long User CPU time used by the job in microseconds format: int64\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_tres -  Up\n\n\nallocated (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Trackable resources allocated to the job \nrequested (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Trackable resources requested by job \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_wckey -  Up\nWorkload characterization key\n\nwckey String WCKey name \nflags array[String] Active flags \nEnum:\n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_meta -  Up\nSlurm meta values\n\nplugin (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta_plugin \nclient (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta_client \ncommand (optional)array[String] CLI command (if applicable) \nslurm (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta_slurm \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_meta_client -  Up\n\n\nsource (optional)String Client source description \nuser (optional)String Client user (if known) \ngroup (optional)String Client group (if known) \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_meta_plugin -  Up\n\n\ntype (optional)String Slurm plugin type (if applicable) \nname (optional)String Slurm plugin name (if applicable) \ndata_parser (optional)String Slurm data_parser plugin \naccounting_storage (optional)String Slurm accounting plugin \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_meta_slurm -  Up\n\n\nversion (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta_slurm_version \nrelease (optional)String Slurm release string \ncluster (optional)String Slurm cluster name \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_meta_slurm_version -  Up\n\n\nmajor (optional)String Slurm release major version \nmicro (optional)String Slurm release micro version \nminor (optional)String Slurm release minor version \n \n\nv0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner -  Up\n\n\ndescription (optional)String Long form warning description \nsource (optional)String Source of warning or where warning was first detected \n \n\nv0_0_41_openapi_slurmdbd_stats_resp_statistics -  Up\nstatistics\n\ntime_start (optional)Long When data collection started (UNIX timestamp) format: int64\nrollups (optional)v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups \nRPCs (optional)array[v0_0_41_openapi_slurmdbd_stats_resp_statistics_RPCs_inner] List of RPCs sent to the slurmdbd \nusers (optional)array[v0_0_41_openapi_slurmdbd_stats_resp_statistics_users_inner] List of users that issued RPCs \n \n\nv0_0_41_openapi_slurmdbd_stats_resp_statistics_RPCs_inner -  Up\n\n\nrpc (optional)String RPC type \ncount (optional)Integer Number of RPCs processed format: int32\ntime (optional)v0_0_41_openapi_slurmdbd_stats_resp_statistics_RPCs_inner_time \n \n\nv0_0_41_openapi_slurmdbd_stats_resp_statistics_RPCs_inner_time -  Up\n\n\naverage (optional)Long Average RPC processing time in microseconds format: int64\ntotal (optional)Long Total RPC processing time in microseconds format: int64\n \n\nv0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups -  Up\nRollup statistics\n\nhourly (optional)v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_hourly \ndaily (optional)v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_daily \nmonthly (optional)v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_monthly \n \n\nv0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_daily -  Up\n\n\ncount (optional)Integer Number of daily rollups since last_run format: int32\nlast_run (optional)Long Last time daily rollup ran (UNIX timestamp) format: int64\nduration (optional)v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_daily_duration \n \n\nv0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_daily_duration -  Up\n\n\nlast (optional)Long Total time spent doing daily daily rollup (seconds) format: int64\nmax (optional)Long Longest daily rollup time (seconds) format: int64\ntime (optional)Long Total time spent doing daily rollups (seconds) format: int64\n \n\nv0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_hourly -  Up\n\n\ncount (optional)Integer Number of hourly rollups since last_run format: int32\nlast_run (optional)Long Last time hourly rollup ran (UNIX timestamp) format: int64\nduration (optional)v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_hourly_duration \n \n\nv0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_hourly_duration -  Up\n\n\nlast (optional)Long Total time spent doing last daily rollup (seconds) format: int64\nmax (optional)Long Longest hourly rollup time (seconds) format: int64\ntime (optional)Long Total time spent doing hourly rollups (seconds) format: int64\n \n\nv0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_monthly -  Up\n\n\ncount (optional)Integer Number of monthly rollups since last_run format: int32\nlast_run (optional)Long Last time monthly rollup ran (UNIX timestamp) format: int64\nduration (optional)v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_monthly_duration \n \n\nv0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_monthly_duration -  Up\n\n\nlast (optional)Long Total time spent doing monthly daily rollup (seconds) format: int64\nmax (optional)Long Longest monthly rollup time (seconds) format: int64\ntime (optional)Long Total time spent doing monthly rollups (seconds) format: int64\n \n\nv0_0_41_openapi_slurmdbd_stats_resp_statistics_users_inner -  Up\n\n\nuser (optional)String User ID \ncount (optional)Integer Number of RPCs processed format: int32\ntime (optional)v0_0_41_openapi_slurmdbd_stats_resp_statistics_RPCs_inner_time \n \n\nv0_0_41_openapi_users_add_cond_resp_association_condition -  Up\nFilters to select associations for users\n\naccounts (optional)array[String] CSV accounts list \nassociation (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association \nclusters (optional)array[String] CSV clusters list \npartitions (optional)array[String] CSV partitions list \nusers array[String] CSV users list \nwckeys (optional)array[String] CSV WCKeys list \n \n\nv0_0_41_openapi_users_add_cond_resp_association_condition_association -  Up\nAssociation limits and options\n\ncomment (optional)String Arbitrary comment \ndefaultqos (optional)String Default QOS \ngrpjobs (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association_grpjobs \ngrpjobsaccrue (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association_grpjobsaccrue \ngrpsubmitjobs (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association_grpsubmitjobs \ngrptres (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Maximum number of TRES able to be allocated by running jobs in this association and its children \ngrptresmins (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Total number of TRES minutes that can possibly be used by past, present and future jobs in this association and its children \ngrptresrunmins (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Maximum number of TRES minutes able to be allocated by running jobs in this association and its children \ngrpwall (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association_grpwall \nmaxjobs (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association_maxjobs \nmaxjobsaccrue (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association_maxjobsaccrue \nmaxsubmitjobs (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association_maxsubmitjobs \nmaxtresminsperjob (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Maximum number of TRES minutes each job is able to use in this association \nmaxtresrunmins (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Maximum number of TRES minutes able to be allocated by running jobs in this association \nmaxtresperjob (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Maximum number of TRES each job is able to use in this association \nmaxtrespernode (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Maximum number of TRES each node is able to use \nmaxwalldurationperjob (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association_maxwalldurationperjob \nminpriothresh (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association_minpriothresh \nparent (optional)String Name of parent account \npriority (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_priority \nqoslevel (optional)array[String] List of available QOS names \nfairshare (optional)Integer Allocated shares used for fairshare calculation format: int32\n \n\nv0_0_41_openapi_users_add_cond_resp_association_condition_association_grpjobs -  Up\nMaximum number of running jobs in this association and its children\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_users_add_cond_resp_association_condition_association_grpjobsaccrue -  Up\nMaximum number of pending jobs able to accrue age priority in this association and its children\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_users_add_cond_resp_association_condition_association_grpsubmitjobs -  Up\nMaximum number of jobs which can be in a pending or running state at any time in this association and its children\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_users_add_cond_resp_association_condition_association_grpwall -  Up\nMaximum wall clock time in minutes able to be allocated by running jobs in this association and its children\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_users_add_cond_resp_association_condition_association_maxjobs -  Up\nMaximum number of running jobs per user in this association\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_users_add_cond_resp_association_condition_association_maxjobsaccrue -  Up\nMaximum number of pending jobs able to accrue age priority at any given time in this association\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_users_add_cond_resp_association_condition_association_maxsubmitjobs -  Up\nMaximum number of jobs which can be in a pending or running state at any time in this association\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_users_add_cond_resp_association_condition_association_maxwalldurationperjob -  Up\nMaximum wall clock time each job is able to use in this association\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_users_add_cond_resp_association_condition_association_minpriothresh -  Up\nMinimum priority required to reserve resources when scheduling\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_openapi_users_add_cond_resp_user -  Up\nAdmin level of user, DefaultAccount, DefaultWCKey\n\nadminlevel (optional)array[String] AdminLevel granted to the user \nEnum:\ndefaultaccount (optional)String Default account \ndefaultwckey (optional)String Default WCKey \n \n\nv0_0_41_update_node_msg_resume_after -  Up\nNumber of seconds after which to automatically resume DOWN or DRAINED node\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n\nv0_0_41_update_node_msg_weight -  Up\nWeight of the node for scheduling purposes\n\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n \n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Path parameters",
                "content": "\njob_id (required)\nPath Parameter \u2014 Job ID default: null \nQuery parameters\nsignal (optional)\nQuery Parameter \u2014 Signal to send to Job default: null flags (optional)\nQuery Parameter \u2014 Signalling flags default: null \nReturn type\nv0.0.41_openapi_resp\nExample dataContent-Type: application/json{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ]\n}Example dataContent-Type: application/x-yamlCustom MIME type example not yet supported: application/x-yamlProducesAcceptContent-Type\napplication/json\napplication/x-yaml\nResponses200v0.0.41_openapi_respdefaultv0.0.41_openapi_resp"
            },
            {
                "title": "Consumes",
                "content": "Content-Type\napplication/json\napplication/x-yaml\nRequest body\nv0.0.41_kill_jobs_msg v0.0.41_kill_jobs_msg (optional)\nBody Parameter \u2014  \nReturn type\nv0.0.41_openapi_kill_jobs_resp\nExample dataContent-Type: application/json{\n  \"meta\" : {\n    \"slurm\" : {\n      \"cluster\" : \"cluster\",\n      \"release\" : \"release\",\n      \"version\" : {\n        \"major\" : \"major\",\n        \"minor\" : \"minor\",\n        \"micro\" : \"micro\"\n      }\n    },\n    \"plugin\" : {\n      \"accounting_storage\" : \"accounting_storage\",\n      \"name\" : \"name\",\n      \"type\" : \"type\",\n      \"data_parser\" : \"data_parser\"\n    },\n    \"client\" : {\n      \"source\" : \"source\",\n      \"user\" : \"user\",\n      \"group\" : \"group\"\n    },\n    \"command\" : [ \"command\", \"command\" ]\n  },\n  \"warnings\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\"\n  } ],\n  \"errors\" : [ {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  }, {\n    \"description\" : \"description\",\n    \"source\" : \"source\",\n    \"error\" : \"error\",\n    \"error_number\" : 5\n  } ],\n  \"status\" : [ {\n    \"federation\" : {\n      \"sibling\" : \"sibling\"\n    },\n    \"job_id\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"error\" : {\n      \"code\" : 0,\n      \"string\" : \"string\",\n      \"message\" : \"message\"\n    },\n    \"step_id\" : \"step_id\"\n  }, {\n    \"federation\" : {\n      \"sibling\" : \"sibling\"\n    },\n    \"job_id\" : {\n      \"number\" : 6,\n      \"set\" : true,\n      \"infinite\" : true\n    },\n    \"error\" : {\n      \"code\" : 0,\n      \"string\" : \"string\",\n      \"message\" : \"message\"\n    },\n    \"step_id\" : \"step_id\"\n  } ]\n}Example dataContent-Type: application/x-yamlCustom MIME type example not yet supported: application/x-yamlProducesAcceptContent-Type\napplication/json\napplication/x-yaml\nResponses200v0.0.41_openapi_kill_jobs_respdefaultv0.0.41_openapi_kill_jobs_resp"
            },
            {
                "title": "v0.0.41_job_alloc_req -  Up",
                "content": "\nhetjob (optional)array[v0.0.41_job_desc_msg] HetJob description \njob (optional)v0.0.41_job_desc_msg \n"
            },
            {
                "title": "v0.0.41_job_desc_msg -  Up",
                "content": "\naccount (optional)String Account associated with the job \naccount_gather_frequency (optional)String Job accounting and profiling sampling intervals in seconds \nadmin_comment (optional)String Arbitrary comment made by administrator \nallocation_node_list (optional)String Local node making the resource allocation \nallocation_node_port (optional)Integer Port to send allocation confirmation to format: int32\nargv (optional)array[String] Arguments to the script \narray (optional)String Job array index value specification \nbatch_features (optional)String Features required for batch script's node \nbegin_time (optional)v0_0_41_job_desc_msg_begin_time \nflags (optional)array[String] Job flags \nEnum:\nburst_buffer (optional)String Burst buffer specifications \nclusters (optional)String Clusters that a federated job can run on \ncluster_constraint (optional)String Required features that a federated cluster must have to have a sibling job submitted to it \ncomment (optional)String Arbitrary comment made by user \ncontiguous (optional)Boolean True if job requires contiguous nodes \ncontainer (optional)String Absolute path to OCI container bundle \ncontainer_id (optional)String OCI container ID \ncore_specification (optional)Integer Specialized core count format: int32\nthread_specification (optional)Integer Specialized thread count format: int32\ncpu_binding (optional)String Method for binding tasks to allocated CPUs \ncpu_binding_flags (optional)array[String] Flags for CPU binding \nEnum:\ncpu_frequency (optional)String Requested CPU frequency range [-p2][:p3] \ncpus_per_tres (optional)String Semicolon delimited list of TRES=# values values indicating how many CPUs should be allocated for each specified TRES (currently only used for gres/gpu) \ncrontab (optional)v0_0_41_job_desc_msg_crontab \ndeadline (optional)Long Latest time that the job may start (UNIX timestamp) format: int64\ndelay_boot (optional)Integer Number of seconds after job eligible start that nodes will be rebooted to satisfy feature specification format: int32\ndependency (optional)String Other jobs that must meet certain criteria before this job can start \nend_time (optional)Long Expected end time (UNIX timestamp) format: int64\nenvironment (optional)array[String] Environment variables to be set for the job \nrlimits (optional)v0_0_41_job_desc_msg_rlimits \nexcluded_nodes (optional)array[String] Comma separated list of nodes that may not be used \nextra (optional)String Arbitrary string used for node filtering if extra constraints are enabled \nconstraints (optional)String Comma separated list of features that are required \ngroup_id (optional)String Group ID of the user that owns the job \nhetjob_group (optional)Integer Unique sequence number applied to this component of the heterogeneous job format: int32\nimmediate (optional)Boolean If true, exit if resources are not available within the time period specified \njob_id (optional)Integer Job ID format: int32\nkill_on_node_fail (optional)Boolean If true, kill job on node failure \nlicenses (optional)String License(s) required by the job \nmail_type (optional)array[String] Mail event type(s) \nEnum:\nmail_user (optional)String User to receive email notifications \nmcs_label (optional)String Multi-Category Security label on the job \nmemory_binding (optional)String Binding map for map/mask_cpu \nmemory_binding_type (optional)array[String] Method for binding tasks to memory \nEnum:\nmemory_per_tres (optional)String Semicolon delimited list of TRES=# values indicating how much memory in megabytes should be allocated for each specified TRES (currently only used for gres/gpu) \nname (optional)String Job name \nnetwork (optional)String Network specs for job step \nnice (optional)Integer Requested job priority change format: int32\ntasks (optional)Integer Number of tasks format: int32\nopen_mode (optional)array[String] Open mode used for stdout and stderr files \nEnum:\nreserve_ports (optional)Integer Port to send various notification msg to format: int32\novercommit (optional)Boolean Overcommit resources \npartition (optional)String Partition assigned to the job \ndistribution_plane_size (optional)v0_0_41_job_desc_msg_distribution_plane_size \npower_flags (optional)array[oas_any_type_not_mapped] \nprefer (optional)String Comma separated list of features that are preferred but not required \nhold (optional)Boolean Hold (true) or release (false) job \npriority (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_priority \nprofile (optional)array[String] Profile used by the acct_gather_profile plugin \nEnum:\nqos (optional)String Quality of Service assigned to the job \nreboot (optional)Boolean Node reboot requested before start \nrequired_nodes (optional)array[String] Comma separated list of required nodes \nrequeue (optional)Boolean Determines whether the job may be requeued \nreservation (optional)String Name of reservation to use \nresv_mpi_ports (optional)Integer Number of reserved communication ports; can only be used if slurmstepd step manager is enabled format: int32\nscript (optional)String Job batch script; only the first component in a HetJob is populated or honored \nshared (optional)array[String] How the job can share resources with other jobs, if at all \nEnum:\nexclusive (optional)array[String] \nEnum:\noversubscribe (optional)Boolean \nsite_factor (optional)Integer Site-specific priority factor format: int32\nspank_environment (optional)array[String] Environment variables for job prolog/epilog scripts as set by SPANK plugins \ndistribution (optional)String Layout \ntime_limit (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_limit \ntime_minimum (optional)v0_0_41_job_desc_msg_time_minimum \ntres_bind (optional)String Task to TRES binding directives \ntres_freq (optional)String TRES frequency directives \ntres_per_job (optional)String Comma separated list of TRES=# values to be allocated for every job \ntres_per_node (optional)String Comma separated list of TRES=# values to be allocated for every node \ntres_per_socket (optional)String Comma separated list of TRES=# values to be allocated for every socket \ntres_per_task (optional)String Comma separated list of TRES=# values to be allocated for every task \nuser_id (optional)String User ID that owns the job \nwait_all_nodes (optional)Boolean If true, wait to start until after all nodes have booted \nkill_warning_flags (optional)array[String] Flags related to job signals \nEnum:\nkill_warning_signal (optional)String Signal to send when approaching end time (e.g. \"10\" or \"USR1\") \nkill_warning_delay (optional)v0_0_41_job_desc_msg_kill_warning_delay \ncurrent_working_directory (optional)String Working directory to use for the job \ncpus_per_task (optional)Integer Number of CPUs required by each task format: int32\nminimum_cpus (optional)Integer Minimum number of CPUs required format: int32\nmaximum_cpus (optional)Integer Maximum number of CPUs required format: int32\nnodes (optional)String Node count range specification (e.g. 1-15:4) \nminimum_nodes (optional)Integer Minimum node count format: int32\nmaximum_nodes (optional)Integer Maximum node count format: int32\nminimum_boards_per_node (optional)Integer Boards per node required format: int32\nminimum_sockets_per_board (optional)Integer Sockets per board required format: int32\nsockets_per_node (optional)Integer Sockets per node required format: int32\nthreads_per_core (optional)Integer Threads per core required format: int32\ntasks_per_node (optional)Integer Number of tasks to invoke on each node format: int32\ntasks_per_socket (optional)Integer Number of tasks to invoke on each socket format: int32\ntasks_per_core (optional)Integer Number of tasks to invoke on each core format: int32\ntasks_per_board (optional)Integer Number of tasks to invoke on each board format: int32\nntasks_per_tres (optional)Integer Number of tasks that can access each GPU format: int32\nminimum_cpus_per_node (optional)Integer Minimum number of CPUs per node format: int32\nmemory_per_cpu (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required_memory_per_cpu \nmemory_per_node (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required_memory_per_cpu \ntemporary_disk_per_node (optional)Integer Minimum tmp disk space required per node format: int32\nselinux_context (optional)String SELinux context \nrequired_switches (optional)v0_0_41_job_desc_msg_required_switches \nsegment_size (optional)v0_0_41_job_desc_msg_segment_size \nstandard_error (optional)String Path to stderr file \nstandard_input (optional)String Path to stdin file \nstandard_output (optional)String Path to stdout file \nwait_for_switch (optional)Integer Maximum time to wait for switches in seconds format: int32\nwckey (optional)String Workload characterization key \nx11 (optional)array[String] X11 forwarding options \nEnum:\nx11_magic_cookie (optional)String Magic cookie for X11 forwarding \nx11_target_host (optional)String Hostname or UNIX socket if x11_target_port=0 \nx11_target_port (optional)Integer TCP port format: int32\n"
            },
            {
                "title": "v0.0.41_job_submit_req -  Up",
                "content": "\nscript (optional)String Deprecated; Populate script field in jobs[0] or job \njobs (optional)array[v0.0.41_job_desc_msg] HetJob description \njob (optional)v0.0.41_job_desc_msg \n"
            },
            {
                "title": "v0.0.41_kill_jobs_msg -  Up",
                "content": "\naccount (optional)String Filter jobs to a specific account \nflags (optional)array[String] Filter jobs according to flags \nEnum:\njob_name (optional)String Filter jobs to a specific name \njobs (optional)array[String] List of jobs to signal \npartition (optional)String Filter jobs to a specific partition \nqos (optional)String Filter jobs to a specific QOS \nreservation (optional)String Filter jobs to a specific reservation \nsignal (optional)String Signal to send to jobs \njob_state (optional)array[String] Filter jobs to a specific state \nEnum:\nuser_id (optional)String Filter jobs to a specific numeric user id \nuser_name (optional)String Filter jobs to a specific user name \nwckey (optional)String Filter jobs to a specific wckey \nnodes (optional)array[String] Filter jobs to a set of nodes \n"
            },
            {
                "title": "v0.0.41_openapi_accounts_add_cond_resp -  Up",
                "content": "\nassociation_condition (optional)v0_0_41_openapi_accounts_add_cond_resp_association_condition \naccount (optional)v0_0_41_openapi_accounts_add_cond_resp_account \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_accounts_add_cond_resp_str -  Up",
                "content": "\nadded_accounts String added_accounts \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_accounts_removed_resp -  Up",
                "content": "\nremoved_accounts array[String] removed_accounts \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_accounts_resp -  Up",
                "content": "\naccounts array[v0_0_41_openapi_slurmdbd_config_resp_accounts_inner] accounts \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_assocs_removed_resp -  Up",
                "content": "\nremoved_associations array[String] removed_associations \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_assocs_resp -  Up",
                "content": "\nassociations array[v0_0_41_openapi_slurmdbd_config_resp_associations_inner] associations \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_clusters_removed_resp -  Up",
                "content": "\ndeleted_clusters array[String] deleted_clusters \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_clusters_resp -  Up",
                "content": "\nclusters array[v0_0_41_openapi_slurmdbd_config_resp_clusters_inner] clusters \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_diag_resp -  Up",
                "content": "\nstatistics v0_0_41_openapi_diag_resp_statistics \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_instances_resp -  Up",
                "content": "\ninstances array[v0_0_41_openapi_slurmdbd_config_resp_instances_inner] instances \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_job_alloc_resp -  Up",
                "content": "\njob_id (optional)Integer Submitted Job ID format: int32\njob_submit_user_msg (optional)String Job submission user message \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_job_info_resp -  Up",
                "content": "\njobs array[v0_0_41_openapi_job_info_resp_jobs_inner] List of jobs \nlast_backfill v0_0_41_openapi_job_info_resp_last_backfill \nlast_update v0_0_41_openapi_job_info_resp_last_update \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_job_post_response -  Up",
                "content": "\nresults (optional)array[v0_0_41_openapi_job_post_response_results_inner] Job update results \njob_id (optional)String First updated Job ID - Use results instead \nstep_id (optional)String First updated Step ID - Use results instead \njob_submit_user_msg (optional)String First updated Job submission user message - Use results instead \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_job_submit_response -  Up",
                "content": "\nresult (optional)v0_0_41_openapi_job_submit_response_result \njob_id (optional)Integer Submitted Job ID format: int32\nstep_id (optional)String Submitted Step ID \njob_submit_user_msg (optional)String Job submission user message \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_kill_jobs_resp -  Up",
                "content": "\nstatus array[v0_0_41_openapi_kill_jobs_resp_status_inner] resultant status of signal request \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_licenses_resp -  Up",
                "content": "\nlicenses array[v0_0_41_openapi_licenses_resp_licenses_inner] List of licenses \nlast_update v0_0_41_openapi_licenses_resp_last_update \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_nodes_resp -  Up",
                "content": "\nnodes array[v0_0_41_openapi_nodes_resp_nodes_inner] List of nodes \nlast_update v0_0_41_openapi_nodes_resp_last_update \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_partition_resp -  Up",
                "content": "\npartitions array[v0_0_41_openapi_partition_resp_partitions_inner] List of partitions \nlast_update v0_0_41_openapi_partition_resp_last_update \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_ping_array_resp -  Up",
                "content": "\npings array[v0_0_41_openapi_ping_array_resp_pings_inner] pings \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_reservation_resp -  Up",
                "content": "\nreservations array[v0_0_41_openapi_reservation_resp_reservations_inner] List of reservations \nlast_update v0_0_41_openapi_reservation_resp_last_update \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_resp -  Up",
                "content": "\nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_shares_resp -  Up",
                "content": "\nshares v0_0_41_openapi_shares_resp_shares \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_slurmdbd_config_resp -  Up",
                "content": "\nclusters (optional)array[v0_0_41_openapi_slurmdbd_config_resp_clusters_inner] Clusters \ntres (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] TRES \naccounts (optional)array[v0_0_41_openapi_slurmdbd_config_resp_accounts_inner] Accounts \nusers (optional)array[v0_0_41_openapi_slurmdbd_config_resp_users_inner] Users \nqos (optional)array[v0_0_41_openapi_slurmdbd_config_resp_qos_inner] QOS \nwckeys (optional)array[v0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner] WCKeys \nassociations (optional)array[v0_0_41_openapi_slurmdbd_config_resp_associations_inner] Associations \ninstances (optional)array[v0_0_41_openapi_slurmdbd_config_resp_instances_inner] Instances \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_slurmdbd_jobs_resp -  Up",
                "content": "\njobs array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner] jobs \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_slurmdbd_qos_removed_resp -  Up",
                "content": "\nremoved_qos array[String] removed QOS \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_slurmdbd_qos_resp -  Up",
                "content": "\nqos array[v0_0_41_openapi_slurmdbd_config_resp_qos_inner] List of QOS \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_slurmdbd_stats_resp -  Up",
                "content": "\nstatistics v0_0_41_openapi_slurmdbd_stats_resp_statistics \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_tres_resp -  Up",
                "content": "\nTRES array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] TRES \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_users_add_cond_resp -  Up",
                "content": "\nassociation_condition v0_0_41_openapi_users_add_cond_resp_association_condition \nuser v0_0_41_openapi_users_add_cond_resp_user \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_users_add_cond_resp_str -  Up",
                "content": "\nadded_users String added_users \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_users_resp -  Up",
                "content": "\nusers array[v0_0_41_openapi_slurmdbd_config_resp_users_inner] users \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_wckey_removed_resp -  Up",
                "content": "\ndeleted_wckeys array[String] deleted wckeys \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_openapi_wckey_resp -  Up",
                "content": "\nwckeys array[v0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner] wckeys \nmeta (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta \nerrors (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner] Query errors \nwarnings (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner] Query warnings \n"
            },
            {
                "title": "v0.0.41_update_node_msg -  Up",
                "content": "\ncomment (optional)String Arbitrary comment \ncpu_bind (optional)Integer Default method for binding tasks to allocated CPUs format: int32\nextra (optional)String Arbitrary string used for node filtering if extra constraints are enabled \nfeatures (optional)array[String] Available features \nfeatures_act (optional)array[String] Currently active features \ngres (optional)String Generic resources \naddress (optional)array[String] NodeAddr, used to establish a communication path \nhostname (optional)array[String] NodeHostname \nname (optional)array[String] NodeName \nstate (optional)array[String] New state to assign to the node \nEnum:\nreason (optional)String Reason for node being DOWN or DRAINING \nreason_uid (optional)String User ID to associate with the reason (needed if user root is sending message) \nresume_after (optional)v0_0_41_update_node_msg_resume_after \nweight (optional)v0_0_41_update_node_msg_weight \n"
            },
            {
                "title": "v0_0_41_job_desc_msg_begin_time -  Up",
                "content": "Defer the allocation of the job until the specified time (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_job_desc_msg_crontab -  Up",
                "content": "Specification for scrontab job\nflags (optional)array[String] Flags \nEnum:\nminute (optional)String Ranged string specifying eligible minute values (e.g. 0-10,50) \nhour (optional)String Ranged string specifying eligible hour values (e.g. 0-5,23) \nday_of_month (optional)String Ranged string specifying eligible day of month values (e.g. 0-10,29) \nmonth (optional)String Ranged string specifying eligible month values (e.g. 0-5,12) \nday_of_week (optional)String Ranged string specifying eligible day of week values (e.g.0-3,7) \nspecification (optional)String Time specification (* means valid for all allowed values) - minute hour day_of_month month day_of_week \ncommand (optional)String Command to run \nline (optional)v0_0_41_job_desc_msg_crontab_line \n"
            },
            {
                "title": "v0_0_41_job_desc_msg_crontab_line -  Up",
                "content": "\nstart (optional)Integer Start of this entry in file format: int32\nend (optional)Integer End of this entry in file format: int32\n"
            },
            {
                "title": "v0_0_41_job_desc_msg_distribution_plane_size -  Up",
                "content": "Plane size specification when distribution specifies plane\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_job_desc_msg_kill_warning_delay -  Up",
                "content": "Number of seconds before end time to send the warning signal\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_job_desc_msg_required_switches -  Up",
                "content": "Maximum number of switches\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_job_desc_msg_rlimits -  Up",
                "content": "\ncpu (optional)v0_0_41_job_desc_msg_rlimits_cpu \nfsize (optional)v0_0_41_job_desc_msg_rlimits_fsize \ndata (optional)v0_0_41_job_desc_msg_rlimits_data \nstack (optional)v0_0_41_job_desc_msg_rlimits_stack \ncore (optional)v0_0_41_job_desc_msg_rlimits_core \nrss (optional)v0_0_41_job_desc_msg_rlimits_rss \nnproc (optional)v0_0_41_job_desc_msg_rlimits_nproc \nnofile (optional)v0_0_41_job_desc_msg_rlimits_nofile \nmemlock (optional)v0_0_41_job_desc_msg_rlimits_memlock \nas (optional)v0_0_41_job_desc_msg_rlimits_as \n"
            },
            {
                "title": "v0_0_41_job_desc_msg_rlimits_as -  Up",
                "content": "Address space limit.\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_job_desc_msg_rlimits_core -  Up",
                "content": "Largest core file that can be created, in bytes.\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_job_desc_msg_rlimits_cpu -  Up",
                "content": "Per-process CPU limit, in seconds.\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_job_desc_msg_rlimits_data -  Up",
                "content": "Maximum size of data segment, in bytes.\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_job_desc_msg_rlimits_fsize -  Up",
                "content": "Largest file that can be created, in bytes.\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_job_desc_msg_rlimits_memlock -  Up",
                "content": "Locked-in-memory address space\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_job_desc_msg_rlimits_nofile -  Up",
                "content": "Number of open files.\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_job_desc_msg_rlimits_nproc -  Up",
                "content": "Number of processes.\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_job_desc_msg_rlimits_rss -  Up",
                "content": "Largest resident set size, in bytes. This affects swapping; processes that are exceeding their resident set size will be more likely to have physical memory taken from them.\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_job_desc_msg_rlimits_stack -  Up",
                "content": "Maximum size of stack segment, in bytes.\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_job_desc_msg_segment_size -  Up",
                "content": "Segment size for topology/block\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_job_desc_msg_time_minimum -  Up",
                "content": "Minimum run time in minutes\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_accounts_add_cond_resp_account -  Up",
                "content": "Account organization and description\ndescription (optional)String Arbitrary string describing the account \norganization (optional)String Organization to which the account belongs \n"
            },
            {
                "title": "v0_0_41_openapi_accounts_add_cond_resp_association_condition -  Up",
                "content": "CSV list of accounts, association limits and options, CSV list of clusters\naccounts array[String] CSV accounts list \nassociation (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association \nclusters (optional)array[String] CSV clusters list \n"
            },
            {
                "title": "v0_0_41_openapi_diag_resp_statistics -  Up",
                "content": "statistics\nparts_packed (optional)Integer Zero if only RPC statistic included format: int32\nreq_time (optional)v0_0_41_openapi_diag_resp_statistics_req_time \nreq_time_start (optional)v0_0_41_openapi_diag_resp_statistics_req_time_start \nserver_thread_count (optional)Integer Number of current active slurmctld threads format: int32\nagent_queue_size (optional)Integer Number of enqueued outgoing RPC requests in an internal retry list format: int32\nagent_count (optional)Integer Number of agent threads format: int32\nagent_thread_count (optional)Integer Total number of active threads created by all agent threads format: int32\ndbd_agent_queue_size (optional)Integer Number of messages for SlurmDBD that are queued format: int32\ngettimeofday_latency (optional)Integer Latency of 1000 calls to the gettimeofday() syscall in microseconds, as measured at controller startup format: int32\nschedule_cycle_max (optional)Integer Max time of any scheduling cycle in microseconds since last reset format: int32\nschedule_cycle_last (optional)Integer Time in microseconds for last scheduling cycle format: int32\nschedule_cycle_sum (optional)Integer Total run time in microseconds for all scheduling cycles since last reset format: int32\nschedule_cycle_total (optional)Integer Number of scheduling cycles since last reset format: int32\nschedule_cycle_mean (optional)Long Mean time in microseconds for all scheduling cycles since last reset format: int64\nschedule_cycle_mean_depth (optional)Long Mean of the number of jobs processed in a scheduling cycle format: int64\nschedule_cycle_per_minute (optional)Long Number of scheduling executions per minute format: int64\nschedule_cycle_depth (optional)Integer Total number of jobs processed in scheduling cycles format: int32\nschedule_exit (optional)v0_0_41_openapi_diag_resp_statistics_schedule_exit \nschedule_queue_length (optional)Integer Number of jobs pending in queue format: int32\njobs_submitted (optional)Integer Number of jobs submitted since last reset format: int32\njobs_started (optional)Integer Number of jobs started since last reset format: int32\njobs_completed (optional)Integer Number of jobs completed since last reset format: int32\njobs_canceled (optional)Integer Number of jobs canceled since the last reset format: int32\njobs_failed (optional)Integer Number of jobs failed due to slurmd or other internal issues since last reset format: int32\njobs_pending (optional)Integer Number of jobs pending at the time of listed in job_state_ts format: int32\njobs_running (optional)Integer Number of jobs running at the time of listed in job_state_ts format: int32\njob_states_ts (optional)v0_0_41_openapi_diag_resp_statistics_job_states_ts \nbf_backfilled_jobs (optional)Integer Number of jobs started through backfilling since last slurm start format: int32\nbf_last_backfilled_jobs (optional)Integer Number of jobs started through backfilling since last reset format: int32\nbf_backfilled_het_jobs (optional)Integer Number of heterogeneous job components started through backfilling since last Slurm start format: int32\nbf_cycle_counter (optional)Integer Number of backfill scheduling cycles since last reset format: int32\nbf_cycle_mean (optional)Long Mean time in microseconds of backfilling scheduling cycles since last reset format: int64\nbf_depth_mean (optional)Long Mean number of eligible to run jobs processed during all backfilling scheduling cycles since last reset format: int64\nbf_depth_mean_try (optional)Long The subset of Depth Mean that the backfill scheduler attempted to schedule format: int64\nbf_cycle_sum (optional)Long Total time in microseconds of backfilling scheduling cycles since last reset format: int64\nbf_cycle_last (optional)Integer Execution time in microseconds of last backfill scheduling cycle format: int32\nbf_cycle_max (optional)Integer Execution time in microseconds of longest backfill scheduling cycle format: int32\nbf_exit (optional)v0_0_41_openapi_diag_resp_statistics_bf_exit \nbf_last_depth (optional)Integer Number of processed jobs during last backfilling scheduling cycle format: int32\nbf_last_depth_try (optional)Integer Number of processed jobs during last backfilling scheduling cycle that had a chance to start using available resources format: int32\nbf_depth_sum (optional)Integer Total number of jobs processed during all backfilling scheduling cycles since last reset format: int32\nbf_depth_try_sum (optional)Integer Subset of bf_depth_sum that the backfill scheduler attempted to schedule format: int32\nbf_queue_len (optional)Integer Number of jobs pending to be processed by backfilling algorithm format: int32\nbf_queue_len_mean (optional)Long Mean number of jobs pending to be processed by backfilling algorithm format: int64\nbf_queue_len_sum (optional)Integer Total number of jobs pending to be processed by backfilling algorithm since last reset format: int32\nbf_table_size (optional)Integer Number of different time slots tested by the backfill scheduler in its last iteration format: int32\nbf_table_size_sum (optional)Integer Total number of different time slots tested by the backfill scheduler format: int32\nbf_table_size_mean (optional)Long Mean number of different time slots tested by the backfill scheduler format: int64\nbf_when_last_cycle (optional)v0_0_41_openapi_diag_resp_statistics_bf_when_last_cycle \nbf_active (optional)Boolean Backfill scheduler currently running \nrpcs_by_message_type (optional)array[v0_0_41_openapi_diag_resp_statistics_rpcs_by_message_type_inner] Most frequently issued remote procedure calls (RPCs) \nrpcs_by_user (optional)array[v0_0_41_openapi_diag_resp_statistics_rpcs_by_user_inner] RPCs issued by user ID \npending_rpcs (optional)array[v0_0_41_openapi_diag_resp_statistics_pending_rpcs_inner] Pending RPC statistics \npending_rpcs_by_hostlist (optional)array[v0_0_41_openapi_diag_resp_statistics_pending_rpcs_by_hostlist_inner] Pending RPCs hostlists \n"
            },
            {
                "title": "v0_0_41_openapi_diag_resp_statistics_bf_exit -  Up",
                "content": "Reasons for which the backfill scheduling cycle exited since last reset\nend_job_queue (optional)Integer Reached end of queue format: int32\nbf_max_job_start (optional)Integer Reached number of jobs allowed to start format: int32\nbf_max_job_test (optional)Integer Reached number of jobs allowed to be tested format: int32\nbf_max_time (optional)Integer Reached maximum allowed scheduler time format: int32\nbf_node_space_size (optional)Integer Reached table size limit format: int32\nstate_changed (optional)Integer System state changed format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_diag_resp_statistics_bf_when_last_cycle -  Up",
                "content": "When the last backfill scheduling cycle happened (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_diag_resp_statistics_job_states_ts -  Up",
                "content": "When the job state counts were gathered (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_diag_resp_statistics_pending_rpcs_by_hostlist_inner -  Up",
                "content": "Pending RPCs by hostlist\ntype_id Integer Message type as integer format: int32\nmessage_type String Message type as string \ncount array[String] Number of RPCs received \n"
            },
            {
                "title": "v0_0_41_openapi_diag_resp_statistics_pending_rpcs_inner -  Up",
                "content": "Pending RPCs\ntype_id Integer Message type as integer format: int32\nmessage_type String Message type as string \ncount Integer Number of pending RPCs queued format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_diag_resp_statistics_req_time -  Up",
                "content": "When the request was made (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_diag_resp_statistics_req_time_start -  Up",
                "content": "When the data in the report started (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_diag_resp_statistics_rpcs_by_message_type_inner -  Up",
                "content": "RPCs by type\ntype_id Integer Message type as integer format: int32\nmessage_type String Message type as string \ncount Integer Number of RPCs received format: int32\nqueued Integer Number of RPCs queued format: int32\ndropped Long Number of RPCs dropped format: int64\ncycle_last Integer Number of RPCs processed within the last RPC queue cycle format: int32\ncycle_max Integer Maximum number of RPCs processed within a RPC queue cycle since start format: int32\ntotal_time Long Total time spent processing RPC in seconds format: int64\naverage_time v0_0_41_openapi_diag_resp_statistics_rpcs_by_message_type_inner_average_time \n"
            },
            {
                "title": "v0_0_41_openapi_diag_resp_statistics_rpcs_by_message_type_inner_average_time -  Up",
                "content": "Average time spent processing RPC in seconds\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_diag_resp_statistics_rpcs_by_user_inner -  Up",
                "content": "RPCs by user\nuser_id Integer User ID (numeric) format: int32\nuser String User name \ncount Integer Number of RPCs received format: int32\ntotal_time Long Total time spent processing RPC in seconds format: int64\naverage_time v0_0_41_openapi_diag_resp_statistics_rpcs_by_message_type_inner_average_time \n"
            },
            {
                "title": "v0_0_41_openapi_diag_resp_statistics_schedule_exit -  Up",
                "content": "Reasons for which the scheduling cycle exited since last reset\nend_job_queue (optional)Integer Reached end of queue format: int32\ndefault_queue_depth (optional)Integer Reached number of jobs allowed to be tested format: int32\nmax_job_start (optional)Integer Reached number of jobs allowed to start format: int32\nmax_rpc_cnt (optional)Integer Reached RPC limit format: int32\nmax_sched_time (optional)Integer Reached maximum allowed scheduler time format: int32\nlicenses (optional)Integer Blocked on licenses format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner -  Up",
                "content": "\naccount (optional)String Account associated with the job \naccrue_time (optional)v0_0_41_openapi_job_info_resp_jobs_inner_accrue_time \nadmin_comment (optional)String Arbitrary comment made by administrator \nallocating_node (optional)String Local node making the resource allocation \narray_job_id (optional)v0_0_41_openapi_job_info_resp_jobs_inner_array_job_id \narray_task_id (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_task_id \narray_max_tasks (optional)v0_0_41_openapi_job_info_resp_jobs_inner_array_max_tasks \narray_task_string (optional)String String expression of task IDs in this record \nassociation_id (optional)Integer Unique identifier for the association format: int32\nbatch_features (optional)String Features required for batch script's node \nbatch_flag (optional)Boolean True if batch job \nbatch_host (optional)String Name of host running batch script \nflags (optional)array[String] Job flags \nEnum:\nburst_buffer (optional)String Burst buffer specifications \nburst_buffer_state (optional)String Burst buffer state details \ncluster (optional)String Cluster name \ncluster_features (optional)String List of required cluster features \ncommand (optional)String Executed command \ncomment (optional)String Arbitrary comment \ncontainer (optional)String Absolute path to OCI container bundle \ncontainer_id (optional)String OCI container ID \ncontiguous (optional)Boolean True if job requires contiguous nodes \ncore_spec (optional)Integer Specialized core count format: int32\nthread_spec (optional)Integer Specialized thread count format: int32\ncores_per_socket (optional)v0_0_41_openapi_job_info_resp_jobs_inner_cores_per_socket \nbillable_tres (optional)v0_0_41_openapi_job_info_resp_jobs_inner_billable_tres \ncpus_per_task (optional)v0_0_41_openapi_job_info_resp_jobs_inner_cpus_per_task \ncpu_frequency_minimum (optional)v0_0_41_openapi_job_info_resp_jobs_inner_cpu_frequency_minimum \ncpu_frequency_maximum (optional)v0_0_41_openapi_job_info_resp_jobs_inner_cpu_frequency_maximum \ncpu_frequency_governor (optional)v0_0_41_openapi_job_info_resp_jobs_inner_cpu_frequency_governor \ncpus_per_tres (optional)String Semicolon delimited list of TRES=# values indicating how many CPUs should be allocated for each specified TRES (currently only used for gres/gpu) \ncron (optional)String Time specification for scrontab job \ndeadline (optional)v0_0_41_openapi_job_info_resp_jobs_inner_deadline \ndelay_boot (optional)v0_0_41_openapi_job_info_resp_jobs_inner_delay_boot \ndependency (optional)String Other jobs that must meet certain criteria before this job can start \nderived_exit_code (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code \neligible_time (optional)v0_0_41_openapi_job_info_resp_jobs_inner_eligible_time \nend_time (optional)v0_0_41_openapi_job_info_resp_jobs_inner_end_time \nexcluded_nodes (optional)String Comma separated list of nodes that may not be used \nexit_code (optional)v0_0_41_openapi_job_info_resp_jobs_inner_exit_code \nextra (optional)String Arbitrary string used for node filtering if extra constraints are enabled \nfailed_node (optional)String Name of node that caused job failure \nfeatures (optional)String Comma separated list of features that are required \nfederation_origin (optional)String Origin cluster's name (when using federation) \nfederation_siblings_active (optional)String Active sibling job names \nfederation_siblings_viable (optional)String Viable sibling job names \ngres_detail (optional)array[String] List of GRES index and counts allocated per node \ngroup_id (optional)Integer Group ID of the user that owns the job format: int32\ngroup_name (optional)String Group name of the user that owns the job \nhet_job_id (optional)v0_0_41_openapi_job_info_resp_jobs_inner_het_job_id \nhet_job_id_set (optional)String Job ID range for all heterogeneous job components \nhet_job_offset (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_het_job_offset \njob_id (optional)Integer Job ID format: int32\njob_resources (optional)v0_0_41_openapi_job_info_resp_jobs_inner_job_resources \njob_size_str (optional)array[String] Number of nodes (in a range) required for this job \njob_state (optional)array[String] Current state \nEnum:\nlast_sched_evaluation (optional)v0_0_41_openapi_job_info_resp_jobs_inner_last_sched_evaluation \nlicenses (optional)String License(s) required by the job \nmail_type (optional)array[String] Mail event type(s) \nEnum:\nmail_user (optional)String User to receive email notifications \nmax_cpus (optional)v0_0_41_openapi_job_info_resp_jobs_inner_max_cpus \nmax_nodes (optional)v0_0_41_openapi_job_info_resp_jobs_inner_max_nodes \nmcs_label (optional)String Multi-Category Security label on the job \nmemory_per_tres (optional)String Semicolon delimited list of TRES=# values indicating how much memory in megabytes should be allocated for each specified TRES (currently only used for gres/gpu) \nname (optional)String Job name \nnetwork (optional)String Network specs for the job \nnodes (optional)String Node(s) allocated to the job \nnice (optional)Integer Requested job priority change format: int32\ntasks_per_core (optional)v0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_core \ntasks_per_tres (optional)v0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_tres \ntasks_per_node (optional)v0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_node \ntasks_per_socket (optional)v0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_socket \ntasks_per_board (optional)v0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_board \ncpus (optional)v0_0_41_openapi_job_info_resp_jobs_inner_cpus \nnode_count (optional)v0_0_41_openapi_job_info_resp_jobs_inner_node_count \ntasks (optional)v0_0_41_openapi_job_info_resp_jobs_inner_tasks \npartition (optional)String Partition assigned to the job \nprefer (optional)String Feature(s) the job requested but that are not required \nmemory_per_cpu (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required_memory_per_cpu \nmemory_per_node (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required_memory_per_node \nminimum_cpus_per_node (optional)v0_0_41_openapi_job_info_resp_jobs_inner_minimum_cpus_per_node \nminimum_tmp_disk_per_node (optional)v0_0_41_openapi_job_info_resp_jobs_inner_minimum_tmp_disk_per_node \npower (optional)v0_0_41_openapi_job_info_resp_jobs_inner_power \npreempt_time (optional)v0_0_41_openapi_job_info_resp_jobs_inner_preempt_time \npreemptable_time (optional)v0_0_41_openapi_job_info_resp_jobs_inner_preemptable_time \npre_sus_time (optional)v0_0_41_openapi_job_info_resp_jobs_inner_pre_sus_time \nhold (optional)Boolean Hold (true) or release (false) job \npriority (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_priority \nprofile (optional)array[String] Profile used by the acct_gather_profile plugin \nEnum:\nqos (optional)String Quality of Service assigned to the job \nreboot (optional)Boolean Node reboot requested before start \nrequired_nodes (optional)String Comma separated list of required nodes \nminimum_switches (optional)Integer Maximum number of switches (the 'minimum' in the key is incorrect) format: int32\nrequeue (optional)Boolean Determines whether the job may be requeued \nresize_time (optional)v0_0_41_openapi_job_info_resp_jobs_inner_resize_time \nrestart_cnt (optional)Integer Number of job restarts format: int32\nresv_name (optional)String Name of reservation to use \nscheduled_nodes (optional)String List of nodes scheduled to be used for the job \nselinux_context (optional)String SELinux context \nshared (optional)array[String] How the job can share resources with other jobs, if at all \nEnum:\nexclusive (optional)array[String] \nEnum:\noversubscribe (optional)Boolean \nshow_flags (optional)array[String] Job details shown in this response \nEnum:\nsockets_per_board (optional)Integer Number of sockets per board required format: int32\nsockets_per_node (optional)v0_0_41_openapi_job_info_resp_jobs_inner_sockets_per_node \nstart_time (optional)v0_0_41_openapi_job_info_resp_jobs_inner_start_time \nstate_description (optional)String Optional details for state_reason \nstate_reason (optional)String Reason for current Pending or Failed state \nstandard_error (optional)String Path to stderr file \nstandard_input (optional)String Path to stdin file \nstandard_output (optional)String Path to stdout file \nsubmit_time (optional)v0_0_41_openapi_job_info_resp_jobs_inner_submit_time \nsuspend_time (optional)v0_0_41_openapi_job_info_resp_jobs_inner_suspend_time \nsystem_comment (optional)String Arbitrary comment from slurmctld \ntime_limit (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_limit \ntime_minimum (optional)v0_0_41_job_desc_msg_time_minimum \nthreads_per_core (optional)v0_0_41_openapi_job_info_resp_jobs_inner_threads_per_core \ntres_bind (optional)String Task to TRES binding directives \ntres_freq (optional)String TRES frequency directives \ntres_per_job (optional)String Comma separated list of TRES=# values to be allocated per job \ntres_per_node (optional)String Comma separated list of TRES=# values to be allocated per node \ntres_per_socket (optional)String Comma separated list of TRES=# values to be allocated per socket \ntres_per_task (optional)String Comma separated list of TRES=# values to be allocated per task \ntres_req_str (optional)String TRES requested by the job \ntres_alloc_str (optional)String TRES used by the job \nuser_id (optional)Integer User ID that owns the job format: int32\nuser_name (optional)String User name that owns the job \nmaximum_switch_wait_time (optional)Integer Maximum time to wait for switches in seconds format: int32\nwckey (optional)String Workload characterization key \ncurrent_working_directory (optional)String Working directory to use for the job \n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_accrue_time -  Up",
                "content": "When the job started accruing age priority (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_array_job_id -  Up",
                "content": "Job ID of job array, or 0 if N/A\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_array_max_tasks -  Up",
                "content": "Maximum number of simultaneously running array tasks, 0 if no limit\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_billable_tres -  Up",
                "content": "Billable TRES\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Double If \"set\" is True the number will be set with value; otherwise ignore number contents format: double\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_cores_per_socket -  Up",
                "content": "Cores per socket required\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_cpu_frequency_governor -  Up",
                "content": "CPU frequency governor\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_cpu_frequency_maximum -  Up",
                "content": "Maximum CPU frequency\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_cpu_frequency_minimum -  Up",
                "content": "Minimum CPU frequency\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_cpus -  Up",
                "content": "Minimum number of CPUs required\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_cpus_per_task -  Up",
                "content": "Number of CPUs required by each task\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_deadline -  Up",
                "content": "Latest time that the job may start (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_delay_boot -  Up",
                "content": "Number of seconds after job eligible start that nodes will be rebooted to satisfy feature specification\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_eligible_time -  Up",
                "content": "Time when the job became eligible to run (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_end_time -  Up",
                "content": "End time, real or expected (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_exit_code -  Up",
                "content": "Exit code of the job\nstatus (optional)array[String] Status given by return code \nEnum:\nreturn_code (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_return_code \nsignal (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_signal \n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_het_job_id -  Up",
                "content": "Heterogeneous job ID, if applicable\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_job_resources -  Up",
                "content": "Resources used by the job\nselect_type array[String] Scheduler consumable resource selection type \nEnum:\nnodes (optional)v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes \ncpus Integer Number of allocated CPUs format: int32\nthreads_per_core v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_threads_per_core \n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes -  Up",
                "content": "\ncount (optional)Integer Number of allocated nodes format: int32\nselect_type (optional)array[String] Node scheduling selection method \nEnum:\nlist (optional)String Node(s) allocated to the job \nwhole (optional)Boolean Whether whole nodes were allocated \nallocation (optional)array[v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner] Allocated node resources \n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner -  Up",
                "content": "Job resources for a node\nindex Integer Node index format: int32\nname String Node name \ncpus (optional)v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner_cpus \nmemory (optional)v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner_memory \nsockets array[v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner_sockets_inner] Socket allocations in node \n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner_cpus -  Up",
                "content": "\ncount (optional)Integer Total number of CPUs assigned to job format: int32\nused (optional)Integer Total number of CPUs used by job format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner_memory -  Up",
                "content": "\nused (optional)Long Total memory (MiB) used by job format: int64\nallocated (optional)Long Total memory (MiB) allocated to job format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner_sockets_inner -  Up",
                "content": "\nindex Integer Core index format: int32\ncores array[v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner_sockets_inner_cores_inner] Core in socket \n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_nodes_allocation_inner_sockets_inner_cores_inner -  Up",
                "content": "\nindex Integer Core index format: int32\nstatus array[String] Core status \nEnum:\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_job_resources_threads_per_core -  Up",
                "content": "Number of processor threads per CPU core\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_last_sched_evaluation -  Up",
                "content": "Last time job was evaluated for scheduling (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_max_cpus -  Up",
                "content": "Maximum number of CPUs usable by the job\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_max_nodes -  Up",
                "content": "Maximum number of nodes usable by the job\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_minimum_cpus_per_node -  Up",
                "content": "Minimum number of CPUs per node\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_minimum_tmp_disk_per_node -  Up",
                "content": "Minimum tmp disk space required per node\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_node_count -  Up",
                "content": "Minimum number of nodes required\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_power -  Up",
                "content": "\nflags (optional)array[oas_any_type_not_mapped] \n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_pre_sus_time -  Up",
                "content": "Total run time prior to last suspend in seconds\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_preempt_time -  Up",
                "content": "Time job received preemption signal (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_preemptable_time -  Up",
                "content": "Time job becomes eligible for preemption (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_resize_time -  Up",
                "content": "Time of last size change (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_sockets_per_node -  Up",
                "content": "Number of sockets per node required\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_start_time -  Up",
                "content": "Time execution began, or is expected to begin (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_submit_time -  Up",
                "content": "Time when the job was submitted (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_suspend_time -  Up",
                "content": "Time the job was last suspended or resumed (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_tasks -  Up",
                "content": "Number of tasks\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_board -  Up",
                "content": "Number of tasks invoked on each board\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_core -  Up",
                "content": "Number of tasks invoked on each core\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_node -  Up",
                "content": "Number of tasks invoked on each node\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_socket -  Up",
                "content": "Number of tasks invoked on each socket\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_tasks_per_tres -  Up",
                "content": "Number of tasks that can assess each GPU\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_jobs_inner_threads_per_core -  Up",
                "content": "Number of processor threads per CPU core required\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_last_backfill -  Up",
                "content": "Time of last backfill scheduler run (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_job_info_resp_last_update -  Up",
                "content": "Time of last job change (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_job_post_response_results_inner -  Up",
                "content": "\njob_id (optional)Integer Job ID for updated job format: int32\nstep_id (optional)String Step ID for updated job \nerror (optional)String Verbose update status or error \nerror_code (optional)Integer Verbose update status or error format: int32\nwhy (optional)String Update response message \n"
            },
            {
                "title": "v0_0_41_openapi_job_submit_response_result -  Up",
                "content": "Job submission\njob_id (optional)Integer New job ID format: int32\nstep_id (optional)String New job step ID \nerror_code (optional)Integer Error code format: int32\nerror (optional)String Error message \njob_submit_user_msg (optional)String Message to user from job_submit plugin \n"
            },
            {
                "title": "v0_0_41_openapi_kill_jobs_resp_status_inner -  Up",
                "content": "List of jobs signal responses\nerror (optional)v0_0_41_openapi_kill_jobs_resp_status_inner_error \nstep_id String Job or Step ID that signaling failed \njob_id v0_0_41_openapi_kill_jobs_resp_status_inner_job_id \nfederation (optional)v0_0_41_openapi_kill_jobs_resp_status_inner_federation \n"
            },
            {
                "title": "v0_0_41_openapi_kill_jobs_resp_status_inner_error -  Up",
                "content": "\nstring (optional)String String error encountered signaling job \ncode (optional)Integer Numeric error encountered signaling job format: int32\nmessage (optional)String Error message why signaling job failed \n"
            },
            {
                "title": "v0_0_41_openapi_kill_jobs_resp_status_inner_federation -  Up",
                "content": "\nsibling (optional)String Name of federation sibling (may be empty for non-federation) \n"
            },
            {
                "title": "v0_0_41_openapi_kill_jobs_resp_status_inner_job_id -  Up",
                "content": "Job ID that signaling failed\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_licenses_resp_last_update -  Up",
                "content": "Time of last licenses change (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_licenses_resp_licenses_inner -  Up",
                "content": "\nLicenseName (optional)String Name of the license \nTotal (optional)Integer Total number of licenses present format: int32\nUsed (optional)Integer Number of licenses in use format: int32\nFree (optional)Integer Number of licenses currently available format: int32\nRemote (optional)Boolean Indicates whether licenses are served by the database \nReserved (optional)Integer Number of licenses reserved format: int32\nLastConsumed (optional)Integer Last known number of licenses that were consumed in the license manager (Remote Only) format: int32\nLastDeficit (optional)Integer Number of \"missing licenses\" from the cluster's perspective format: int32\nLastUpdate (optional)Long When the license information was last updated (UNIX Timestamp) format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_nodes_resp_last_update -  Up",
                "content": "Time of last node change (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_nodes_resp_nodes_inner -  Up",
                "content": "\narchitecture (optional)String Computer architecture \nburstbuffer_network_address (optional)String Alternate network path to be used for sbcast network traffic \nboards (optional)Integer Number of Baseboards in nodes with a baseboard controller format: int32\nboot_time (optional)v0_0_41_openapi_nodes_resp_nodes_inner_boot_time \ncluster_name (optional)String Cluster name (only set in federated environments) \ncores (optional)Integer Number of cores in a single physical processor socket format: int32\nspecialized_cores (optional)Integer Number of cores reserved for system use format: int32\ncpu_binding (optional)Integer Default method for binding tasks to allocated CPUs format: int32\ncpu_load (optional)Integer CPU load as reported by the OS format: int32\nfree_mem (optional)v0_0_41_openapi_nodes_resp_nodes_inner_free_mem \ncpus (optional)Integer Total CPUs, including cores and threads format: int32\neffective_cpus (optional)Integer Number of effective CPUs (excluding specialized CPUs) format: int32\nspecialized_cpus (optional)String Abstract CPU IDs on this node reserved for exclusive use by slurmd and slurmstepd \nenergy (optional)v0_0_41_openapi_nodes_resp_nodes_inner_energy \nexternal_sensors (optional)Object \nextra (optional)String Arbitrary string used for node filtering if extra constraints are enabled \npower (optional)Object \nfeatures (optional)array[String] Available features \nactive_features (optional)array[String] Currently active features \ngpu_spec (optional)String CPU cores reserved for jobs that also use a GPU \ngres (optional)String Generic resources \ngres_drained (optional)String Drained generic resources \ngres_used (optional)String Generic resources currently in use \ninstance_id (optional)String Cloud instance ID \ninstance_type (optional)String Cloud instance type \nlast_busy (optional)v0_0_41_openapi_nodes_resp_nodes_inner_last_busy \nmcs_label (optional)String Multi-Category Security label \nspecialized_memory (optional)Long Combined memory limit, in MB, for Slurm compute node daemons format: int64\nname (optional)String NodeName \nnext_state_after_reboot (optional)array[String] The state the node will be assigned after rebooting \nEnum:\naddress (optional)String NodeAddr, used to establish a communication path \nhostname (optional)String NodeHostname \nstate (optional)array[String] Node state(s) applicable to this node \nEnum:\noperating_system (optional)String Operating system reported by the node \nowner (optional)String User allowed to run jobs on this node (unset if no restriction) \npartitions (optional)array[String] Partitions containing this node \nport (optional)Integer TCP port number of the slurmd format: int32\nreal_memory (optional)Long Total memory in MB on the node format: int64\nres_cores_per_gpu (optional)Integer Number of CPU cores per GPU restricted to GPU jobs format: int32\ncomment (optional)String Arbitrary comment \nreason (optional)String Describes why the node is in a \"DOWN\", \"DRAINED\", \"DRAINING\", \"FAILING\" or \"FAIL\" state \nreason_changed_at (optional)v0_0_41_openapi_nodes_resp_nodes_inner_reason_changed_at \nreason_set_by_user (optional)String User who set the reason \nresume_after (optional)v0_0_41_openapi_nodes_resp_nodes_inner_resume_after \nreservation (optional)String Name of reservation containing this node \nalloc_memory (optional)Long Total memory in MB currently allocated for jobs format: int64\nalloc_cpus (optional)Integer Total number of CPUs currently allocated for jobs format: int32\nalloc_idle_cpus (optional)Integer Total number of idle CPUs format: int32\ntres_used (optional)String Trackable resources currently allocated for jobs \ntres_weighted (optional)Double Weighted number of billable trackable resources allocated format: double\nslurmd_start_time (optional)v0_0_41_openapi_nodes_resp_nodes_inner_slurmd_start_time \nsockets (optional)Integer Number of physical processor sockets/chips on the node format: int32\nthreads (optional)Integer Number of logical threads in a single physical core format: int32\ntemporary_disk (optional)Integer Total size in MB of temporary disk storage in TmpFS format: int32\nweight (optional)Integer Weight of the node for scheduling purposes format: int32\ntres (optional)String Configured trackable resources \nversion (optional)String Slurmd version \n"
            },
            {
                "title": "v0_0_41_openapi_nodes_resp_nodes_inner_boot_time -  Up",
                "content": "Time when the node booted (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_nodes_resp_nodes_inner_energy -  Up",
                "content": "Energy usage data\naverage_watts (optional)Integer Average power consumption, in watts format: int32\nbase_consumed_energy (optional)Long The energy consumed between when the node was powered on and the last time it was registered by slurmd, in joules format: int64\nconsumed_energy (optional)Long The energy consumed between the last time the node was registered by the slurmd daemon and the last node energy accounting sample, in joules format: int64\ncurrent_watts (optional)v0_0_41_openapi_nodes_resp_nodes_inner_energy_current_watts \nprevious_consumed_energy (optional)Long Previous value of consumed_energy format: int64\nlast_collected (optional)Long Time when energy data was last retrieved (UNIX timestamp) format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_nodes_resp_nodes_inner_energy_current_watts -  Up",
                "content": "The instantaneous power consumption at the time of the last node energy accounting sample, in watts\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_nodes_resp_nodes_inner_free_mem -  Up",
                "content": "Total memory in MB currently free as reported by the OS\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_nodes_resp_nodes_inner_last_busy -  Up",
                "content": "Time when the node was last busy (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_nodes_resp_nodes_inner_reason_changed_at -  Up",
                "content": "When the reason changed (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_nodes_resp_nodes_inner_resume_after -  Up",
                "content": "Number of seconds after the node's state is updated to \"DOWN\" or \"DRAIN\" before scheduling a node state resume\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_nodes_resp_nodes_inner_slurmd_start_time -  Up",
                "content": "Time when the slurmd started (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_last_update -  Up",
                "content": "Time of last partition change (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner -  Up",
                "content": "\nnodes (optional)v0_0_41_openapi_partition_resp_partitions_inner_nodes \naccounts (optional)v0_0_41_openapi_partition_resp_partitions_inner_accounts \ngroups (optional)v0_0_41_openapi_partition_resp_partitions_inner_groups \nqos (optional)v0_0_41_openapi_partition_resp_partitions_inner_qos \nalternate (optional)String Alternate \ntres (optional)v0_0_41_openapi_partition_resp_partitions_inner_tres \ncluster (optional)String Cluster name \nselect_type (optional)array[String] Scheduler consumable resource selection type \nEnum:\ncpus (optional)v0_0_41_openapi_partition_resp_partitions_inner_cpus \ndefaults (optional)v0_0_41_openapi_partition_resp_partitions_inner_defaults \ngrace_time (optional)Integer GraceTime format: int32\nmaximums (optional)v0_0_41_openapi_partition_resp_partitions_inner_maximums \nminimums (optional)v0_0_41_openapi_partition_resp_partitions_inner_minimums \nname (optional)String PartitionName \nnode_sets (optional)String NodeSets \npriority (optional)v0_0_41_openapi_partition_resp_partitions_inner_priority \ntimeouts (optional)v0_0_41_openapi_partition_resp_partitions_inner_timeouts \npartition (optional)v0_0_41_openapi_partition_resp_partitions_inner_partition \nsuspend_time (optional)v0_0_41_openapi_partition_resp_partitions_inner_suspend_time \n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_accounts -  Up",
                "content": "\nallowed (optional)String AllowAccounts \ndeny (optional)String DenyAccounts \n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_cpus -  Up",
                "content": "\ntask_binding (optional)Integer CpuBind format: int32\ntotal (optional)Integer TotalCPUs format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_defaults -  Up",
                "content": "\nmemory_per_cpu (optional)Long DefMemPerCPU or DefMemPerNode format: int64\npartition_memory_per_cpu (optional)v0_0_41_openapi_partition_resp_partitions_inner_defaults_partition_memory_per_cpu \npartition_memory_per_node (optional)v0_0_41_openapi_partition_resp_partitions_inner_defaults_partition_memory_per_node \ntime (optional)v0_0_41_openapi_partition_resp_partitions_inner_defaults_time \njob (optional)String JobDefaults \n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_defaults_partition_memory_per_cpu -  Up",
                "content": "DefMemPerCPU\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_defaults_partition_memory_per_node -  Up",
                "content": "DefMemPerNode\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_defaults_time -  Up",
                "content": "DefaultTime in minutes\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_groups -  Up",
                "content": "\nallowed (optional)String AllowGroups \n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_maximums -  Up",
                "content": "\ncpus_per_node (optional)v0_0_41_openapi_partition_resp_partitions_inner_maximums_cpus_per_node \ncpus_per_socket (optional)v0_0_41_openapi_partition_resp_partitions_inner_maximums_cpus_per_socket \nmemory_per_cpu (optional)Long MaxMemPerCPU or MaxMemPerNode format: int64\npartition_memory_per_cpu (optional)v0_0_41_openapi_partition_resp_partitions_inner_maximums_partition_memory_per_cpu \npartition_memory_per_node (optional)v0_0_41_openapi_partition_resp_partitions_inner_maximums_partition_memory_per_node \nnodes (optional)v0_0_41_openapi_partition_resp_partitions_inner_maximums_nodes \nshares (optional)Integer OverSubscribe format: int32\noversubscribe (optional)v0_0_41_openapi_partition_resp_partitions_inner_maximums_oversubscribe \ntime (optional)v0_0_41_openapi_partition_resp_partitions_inner_maximums_time \nover_time_limit (optional)v0_0_41_openapi_partition_resp_partitions_inner_maximums_over_time_limit \n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_maximums_cpus_per_node -  Up",
                "content": "MaxCPUsPerNode\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_maximums_cpus_per_socket -  Up",
                "content": "MaxCPUsPerSocket\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_maximums_nodes -  Up",
                "content": "MaxNodes\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_maximums_over_time_limit -  Up",
                "content": "OverTimeLimit\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_maximums_oversubscribe -  Up",
                "content": "\njobs (optional)Integer Maximum number of jobs allowed to oversubscribe resources format: int32\nflags (optional)array[String] Flags applicable to the OverSubscribe setting \nEnum:\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_maximums_partition_memory_per_cpu -  Up",
                "content": "MaxMemPerCPU\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_maximums_partition_memory_per_node -  Up",
                "content": "MaxMemPerNode\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_maximums_time -  Up",
                "content": "MaxTime\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_minimums -  Up",
                "content": "\nnodes (optional)Integer MinNodes format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_nodes -  Up",
                "content": "\nallowed_allocation (optional)String AllocNodes \nconfigured (optional)String Nodes \ntotal (optional)Integer TotalNodes format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_partition -  Up",
                "content": "\nstate (optional)array[String] Current state(s) \nEnum:\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_priority -  Up",
                "content": "\njob_factor (optional)Integer PriorityJobFactor format: int32\ntier (optional)Integer PriorityTier format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_qos -  Up",
                "content": "\nallowed (optional)String AllowQOS \ndeny (optional)String DenyQOS \nassigned (optional)String QOS \n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_suspend_time -  Up",
                "content": "SuspendTime (GLOBAL if both set and infinite are false)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_timeouts -  Up",
                "content": "\nresume (optional)v0_0_41_openapi_partition_resp_partitions_inner_timeouts_resume \nsuspend (optional)v0_0_41_openapi_partition_resp_partitions_inner_timeouts_suspend \n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_timeouts_resume -  Up",
                "content": "ResumeTimeout (GLOBAL if both set and infinite are false)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_timeouts_suspend -  Up",
                "content": "SuspendTimeout (GLOBAL if both set and infinite are false)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_partition_resp_partitions_inner_tres -  Up",
                "content": "\nbilling_weights (optional)String TRESBillingWeights \nconfigured (optional)String TRES \n"
            },
            {
                "title": "v0_0_41_openapi_ping_array_resp_pings_inner -  Up",
                "content": "\nhostname (optional)String Target for ping \npinged (optional)String Ping result \nlatency (optional)Long Number of microseconds it took to successfully ping or timeout format: int64\nmode (optional)String The operating mode of the responding slurmctld \n"
            },
            {
                "title": "v0_0_41_openapi_reservation_resp_last_update -  Up",
                "content": "Time of last reservation change (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_reservation_resp_reservations_inner -  Up",
                "content": "\naccounts (optional)String Comma separated list of permitted accounts \nburst_buffer (optional)String BurstBuffer \ncore_count (optional)Integer CoreCnt format: int32\ncore_specializations (optional)array[v0_0_41_openapi_reservation_resp_reservations_inner_core_specializations_inner] Reserved cores specification \nend_time (optional)v0_0_41_openapi_reservation_resp_reservations_inner_end_time \nfeatures (optional)String Features \nflags (optional)array[String] Flags associated with the reservation \nEnum:\ngroups (optional)String Groups \nlicenses (optional)String Licenses \nmax_start_delay (optional)Integer MaxStartDelay in seconds format: int32\nname (optional)String ReservationName \nnode_count (optional)Integer NodeCnt format: int32\nnode_list (optional)String Nodes \npartition (optional)String PartitionName \npurge_completed (optional)v0_0_41_openapi_reservation_resp_reservations_inner_purge_completed \nstart_time (optional)v0_0_41_openapi_reservation_resp_reservations_inner_start_time \nwatts (optional)v0_0_41_openapi_reservation_resp_reservations_inner_watts \ntres (optional)String Comma separated list of required TRES \nusers (optional)String Comma separated list of permitted users \n"
            },
            {
                "title": "v0_0_41_openapi_reservation_resp_reservations_inner_core_specializations_inner -  Up",
                "content": "\nnode (optional)String Name of reserved node \ncore (optional)String IDs of reserved cores \n"
            },
            {
                "title": "v0_0_41_openapi_reservation_resp_reservations_inner_end_time -  Up",
                "content": "EndTime (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_reservation_resp_reservations_inner_purge_completed -  Up",
                "content": "\ntime (optional)v0_0_41_openapi_reservation_resp_reservations_inner_purge_completed_time \n"
            },
            {
                "title": "v0_0_41_openapi_reservation_resp_reservations_inner_purge_completed_time -  Up",
                "content": "If PURGE_COMP flag is set, the number of seconds this reservation will sit idle until it is revoked\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_reservation_resp_reservations_inner_start_time -  Up",
                "content": "StartTime (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_reservation_resp_reservations_inner_watts -  Up",
                "content": "32 bit integer number with flags\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_shares_resp_shares -  Up",
                "content": "fairshare info\nshares (optional)array[v0_0_41_openapi_shares_resp_shares_shares_inner] Association shares \ntotal_shares (optional)Long Total number of shares format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_shares_resp_shares_shares_inner -  Up",
                "content": "\nid (optional)Integer Association ID format: int32\ncluster (optional)String Cluster name \nname (optional)String Share name \nparent (optional)String Parent name \npartition (optional)String Partition name \nshares_normalized (optional)v0_0_41_openapi_shares_resp_shares_shares_inner_shares_normalized \nshares (optional)v0_0_41_openapi_shares_resp_shares_shares_inner_shares \ntres (optional)v0_0_41_openapi_shares_resp_shares_shares_inner_tres \neffective_usage (optional)Double Effective, normalized usage format: double\nusage_normalized (optional)v0_0_41_openapi_shares_resp_shares_shares_inner_usage_normalized \nusage (optional)Long Measure of tresbillableunits usage format: int64\nfairshare (optional)v0_0_41_openapi_shares_resp_shares_shares_inner_fairshare \ntype (optional)array[String] User or account association \nEnum:\n"
            },
            {
                "title": "v0_0_41_openapi_shares_resp_shares_shares_inner_fairshare -  Up",
                "content": "\nfactor (optional)Double Fairshare factor format: double\nlevel (optional)Double Fairshare factor at this level; stored on an assoc as a long double, but that is not needed for display in sshare format: double\n"
            },
            {
                "title": "v0_0_41_openapi_shares_resp_shares_shares_inner_shares -  Up",
                "content": "Number of shares allocated\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_shares_resp_shares_shares_inner_shares_normalized -  Up",
                "content": "Normalized shares\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Double If \"set\" is True the number will be set with value; otherwise ignore number contents format: double\n"
            },
            {
                "title": "v0_0_41_openapi_shares_resp_shares_shares_inner_tres -  Up",
                "content": "\nrun_seconds (optional)array[v0_0_41_openapi_shares_resp_shares_shares_inner_tres_run_seconds_inner] Currently running tres-secs = grp_used_tres_run_secs \ngroup_minutes (optional)array[v0_0_41_openapi_shares_resp_shares_shares_inner_tres_run_seconds_inner] TRES-minute limit \nusage (optional)array[v0_0_41_openapi_shares_resp_shares_shares_inner_tres_usage_inner] Measure of each TRES usage \n"
            },
            {
                "title": "v0_0_41_openapi_shares_resp_shares_shares_inner_tres_run_seconds_inner -  Up",
                "content": "\nname (optional)String TRES name \nvalue (optional)v0_0_41_openapi_shares_resp_shares_shares_inner_tres_run_seconds_inner_value \n"
            },
            {
                "title": "v0_0_41_openapi_shares_resp_shares_shares_inner_tres_run_seconds_inner_value -  Up",
                "content": "TRES value\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_shares_resp_shares_shares_inner_tres_usage_inner -  Up",
                "content": "\nname (optional)String TRES name \nvalue (optional)BigDecimal TRES value \n"
            },
            {
                "title": "v0_0_41_openapi_shares_resp_shares_shares_inner_usage_normalized -  Up",
                "content": "Normalized usage\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Double If \"set\" is True the number will be set with value; otherwise ignore number contents format: double\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_accounts_inner -  Up",
                "content": "\nassociations (optional)array[v0_0_41_openapi_slurmdbd_config_resp_accounts_inner_associations_inner] Associations involving this account (only populated if requested) \ncoordinators (optional)array[v0_0_41_openapi_slurmdbd_config_resp_accounts_inner_coordinators_inner] List of users that are a coordinator of this account (only populated if requested) \ndescription String Arbitrary string describing the account \nname String Account name \norganization String Organization to which the account belongs \nflags (optional)array[String] Flags associated with the account \nEnum:\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_accounts_inner_associations_inner -  Up",
                "content": "\naccount (optional)String Account \ncluster (optional)String Cluster \npartition (optional)String Partition \nuser String User name \nid (optional)Integer Numeric association ID format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_accounts_inner_coordinators_inner -  Up",
                "content": "\nname String User name \ndirect (optional)Boolean Indicates whether the coordinator was directly assigned to this account \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_associations_inner -  Up",
                "content": "\naccounting (optional)array[v0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner_accounting_inner] Accounting records containing related resource usage \naccount (optional)String Account \ncluster (optional)String Cluster name \ncomment (optional)String Arbitrary comment \ndefault (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_default \nflags (optional)array[String] Flags on the association \nEnum:\nmax (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max \nid (optional)Integer Unique ID format: int32\nis_default (optional)Boolean Is default association for user \nlineage (optional)String Complete path up the hierarchy to the root association \nmin (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_min \nparent_account (optional)String Name of parent account \npartition (optional)String Partition name \npriority (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_priority \nqos (optional)array[String] List of available QOS names \nshares_raw (optional)Integer Allocated shares used for fairshare calculation format: int32\nuser String User name \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_associations_inner_default -  Up",
                "content": "\nqos (optional)String Default QOS \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max -  Up",
                "content": "\njobs (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs \ntres (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres \nper (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_per \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs -  Up",
                "content": "\nper (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_per \nactive (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_active \naccruing (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_accruing \ntotal (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_total \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_accruing -  Up",
                "content": "MaxJobsAccrue\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_active -  Up",
                "content": "MaxJobs\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_per -  Up",
                "content": "\ncount (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_active_jobs_count \naccruing (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_active_jobs_accruing \nsubmitted (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_per_submitted \nwall_clock (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock_per_job \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_per_submitted -  Up",
                "content": "GrpSubmitJobs\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_jobs_total -  Up",
                "content": "MaxSubmitJobs\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_per -  Up",
                "content": "\naccount (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_per_account \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_per_account -  Up",
                "content": "\nwall_clock (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock_per_qos \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres -  Up",
                "content": "\ntotal (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] GrpTRES \ngroup (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres_group \nminutes (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres_minutes \nper (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres_per \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres_group -  Up",
                "content": "\nminutes (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] GrpTRESMins \nactive (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] GrpTRESRunMins \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres_minutes -  Up",
                "content": "\ntotal (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESMinsPerJob \nper (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres_minutes_per \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres_minutes_per -  Up",
                "content": "\njob (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESMinsPerJob \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_associations_inner_max_tres_per -  Up",
                "content": "\njob (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESPerJob \nnode (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESPerNode \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_associations_inner_min -  Up",
                "content": "\npriority_threshold (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min_priority_threshold \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_associations_inner_priority -  Up",
                "content": "Association priority factor\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_clusters_inner -  Up",
                "content": "\ncontroller (optional)v0_0_41_openapi_slurmdbd_config_resp_clusters_inner_controller \nflags (optional)array[String] Flags \nEnum:\nname (optional)String ClusterName \nnodes (optional)String Node names \nselect_plugin (optional)String \nassociations (optional)v0_0_41_openapi_slurmdbd_config_resp_clusters_inner_associations \nrpc_version (optional)Integer RPC version used in the cluster format: int32\ntres (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Trackable resources \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_clusters_inner_associations -  Up",
                "content": "\nroot (optional)v0_0_41_openapi_slurmdbd_config_resp_clusters_inner_associations_root \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_clusters_inner_associations_root -  Up",
                "content": "Root association information\naccount (optional)String Account \ncluster (optional)String Cluster \npartition (optional)String Partition \nuser String User name \nid (optional)Integer Numeric association ID format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_clusters_inner_controller -  Up",
                "content": "\nhost (optional)String ControlHost \nport (optional)Integer ControlPort format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_instances_inner -  Up",
                "content": "\ncluster (optional)String Cluster name \nextra (optional)String Arbitrary string used for node filtering if extra constraints are enabled \ninstance_id (optional)String Cloud instance ID \ninstance_type (optional)String Cloud instance type \nnode_name (optional)String NodeName \ntime (optional)v0_0_41_openapi_slurmdbd_config_resp_instances_inner_time \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_instances_inner_time -  Up",
                "content": "\ntime_end (optional)Long When the instance will end (UNIX timestamp) format: int64\ntime_start (optional)Long When the instance will start (UNIX timestamp) format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner -  Up",
                "content": "\ndescription (optional)String Arbitrary description \nflags (optional)array[String] Flags, to avoid modifying current values specify NOT_SET \nEnum:\nid (optional)Integer Unique ID format: int32\nlimits (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits \nname (optional)String Name \npreempt (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_preempt \npriority (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_priority \nusage_factor (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_usage_factor \nusage_threshold (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_usage_threshold \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits -  Up",
                "content": "\ngrace_time (optional)Integer GraceTime format: int32\nmax (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max \nfactor (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_factor \nmin (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_factor -  Up",
                "content": "LimitFactor\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Double If \"set\" is True the number will be set with value; otherwise ignore number contents format: double\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max -  Up",
                "content": "\nactive_jobs (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_active_jobs \ntres (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_tres \nwall_clock (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock \njobs (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs \naccruing (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_accruing \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_accruing -  Up",
                "content": "\nper (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_accruing_per \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_accruing_per -  Up",
                "content": "\naccount (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_accruing_per_account \nuser (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_accruing_per_user \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_accruing_per_account -  Up",
                "content": "MaxJobsAccruePerAccount\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_accruing_per_user -  Up",
                "content": "MaxJobsAccruePerUser\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_active_jobs -  Up",
                "content": "\naccruing (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_active_jobs_accruing \ncount (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_active_jobs_count \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_active_jobs_accruing -  Up",
                "content": "GrpJobsAccrue\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_active_jobs_count -  Up",
                "content": "GrpJobs\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs -  Up",
                "content": "\nactive_jobs (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_active_jobs \nper (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_per \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_active_jobs -  Up",
                "content": "\nper (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_active_jobs_per \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_active_jobs_per -  Up",
                "content": "\naccount (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_active_jobs_per_account \nuser (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_active_jobs_per_user \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_active_jobs_per_account -  Up",
                "content": "MaxJobsPerAccount\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_active_jobs_per_user -  Up",
                "content": "MaxJobsPerUser\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_per -  Up",
                "content": "\naccount (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_per_account \nuser (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_per_user \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_per_account -  Up",
                "content": "MaxSubmitJobsPerAccount\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_jobs_per_user -  Up",
                "content": "MaxSubmitJobsPerUser\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_tres -  Up",
                "content": "\ntotal (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] GrpTRES \nminutes (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_tres_minutes \nper (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_tres_per \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_tres_minutes -  Up",
                "content": "\nper (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_tres_minutes_per \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_tres_minutes_per -  Up",
                "content": "\nqos (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] GrpTRESRunMins \njob (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESMinsPerJob \naccount (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESRunMinsPerAccount \nuser (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESRunMinsPerUser \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_tres_per -  Up",
                "content": "\naccount (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESPerAccount \njob (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESPerJob \nnode (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESPerNode \nuser (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MaxTRESPerUser \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock -  Up",
                "content": "\nper (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock_per \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock_per -  Up",
                "content": "\nqos (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock_per_qos \njob (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock_per_job \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock_per_job -  Up",
                "content": "MaxWallDurationPerJob\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_max_wall_clock_per_qos -  Up",
                "content": "GrpWall\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min -  Up",
                "content": "\npriority_threshold (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min_priority_threshold \ntres (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min_tres \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min_priority_threshold -  Up",
                "content": "MinPrioThreshold\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min_tres -  Up",
                "content": "\nper (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min_tres_per \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_limits_min_tres_per -  Up",
                "content": "\njob (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] MinTRES \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_preempt -  Up",
                "content": "\nlist (optional)array[String] Other QOS's this QOS can preempt \nmode (optional)array[String] PreemptMode \nEnum:\nexempt_time (optional)v0_0_41_openapi_slurmdbd_config_resp_qos_inner_preempt_exempt_time \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_preempt_exempt_time -  Up",
                "content": "PreemptExemptTime\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_priority -  Up",
                "content": "Priority\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_usage_factor -  Up",
                "content": "UsageFactor\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Double If \"set\" is True the number will be set with value; otherwise ignore number contents format: double\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_qos_inner_usage_threshold -  Up",
                "content": "UsageThreshold\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Double If \"set\" is True the number will be set with value; otherwise ignore number contents format: double\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_users_inner -  Up",
                "content": "\nadministrator_level (optional)array[String] AdminLevel granted to the user \nEnum:\nassociations (optional)array[v0_0_41_openapi_slurmdbd_config_resp_accounts_inner_associations_inner] Associations created for this user \ncoordinators (optional)array[v0_0_41_openapi_slurmdbd_config_resp_accounts_inner_coordinators_inner] Accounts this user is a coordinator for \ndefault (optional)v0_0_41_openapi_slurmdbd_config_resp_users_inner_default \nflags (optional)array[String] Flags associated with user \nEnum:\nname String User name \nold_name (optional)String Previous user name \nwckeys (optional)array[v0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner] List of available WCKeys \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_users_inner_default -  Up",
                "content": "\naccount (optional)String Default Account \nwckey (optional)String Default WCKey \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner -  Up",
                "content": "\naccounting (optional)array[v0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner_accounting_inner] Accounting records containing related resource usage \ncluster String Cluster name \nid (optional)Integer Unique ID for this user-cluster-wckey combination format: int32\nname String WCKey name \nuser String User name \nflags (optional)array[String] Flags associated with the WCKey \nEnum:\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner_accounting_inner -  Up",
                "content": "\nallocated (optional)v0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner_accounting_inner_allocated \nid (optional)Integer Association ID or Workload characterization key ID format: int32\nstart (optional)Long When the record was started format: int64\nTRES (optional)v0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner_accounting_inner_TRES \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner_accounting_inner_TRES -  Up",
                "content": "Trackable resources\ntype String TRES type (CPU, MEM, etc) \nname (optional)String TRES name (if applicable) \nid (optional)Integer ID used in database format: int32\ncount (optional)Long TRES count (0 if listed generically) format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_config_resp_users_inner_wckeys_inner_accounting_inner_allocated -  Up",
                "content": "\nseconds (optional)Long Number of cpu seconds allocated format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_errors_inner -  Up",
                "content": "\ndescription (optional)String Long form error description \nerror_number (optional)Integer Slurm numeric error identifier format: int32\nerror (optional)String Short form error description \nsource (optional)String Source of error or where error was first detected \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner -  Up",
                "content": "\naccount (optional)String Account the job ran under \ncomment (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_comment \nallocation_nodes (optional)Integer List of nodes allocated to the job format: int32\narray (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array \nassociation (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_association \nblock (optional)String The name of the block to be used (used with Blue Gene systems) \ncluster (optional)String Cluster name \nconstraints (optional)String Feature(s) the job requested as a constraint \ncontainer (optional)String Absolute path to OCI container bundle \nderived_exit_code (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code \ntime (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time \nexit_code (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_exit_code \nextra (optional)String Arbitrary string used for node filtering if extra constraints are enabled \nfailed_node (optional)String Name of node that caused job failure \nflags (optional)array[String] Flags associated with the job \nEnum:\ngroup (optional)String Group ID of the user that owns the job \nhet (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_het \njob_id (optional)Integer Job ID format: int32\nname (optional)String Job name \nlicenses (optional)String License(s) required by the job \nmcs (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_mcs \nnodes (optional)String Node(s) allocated to the job \npartition (optional)String Partition assigned to the job \nhold (optional)Boolean Hold (true) or release (false) job \npriority (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_priority \nqos (optional)String Quality of Service assigned to the job \nrequired (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required \nkill_request_user (optional)String User ID that requested termination of the job \nreservation (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_reservation \nscript (optional)String Job batch script; only the first component in a HetJob is populated or honored \nstdin_expanded (optional)String Job stdin with expanded fields \nstdout_expanded (optional)String Job stdout with expanded fields \nstderr_expanded (optional)String Job stderr with expanded fields \nstdout (optional)String Path to stdout file \nstderr (optional)String Path to stderr file \nstdin (optional)String Path to stdin file \nstate (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_state \nsteps (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner] Individual steps in the job \nsubmit_line (optional)String Command used to submit the job \ntres (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_tres \nused_gres (optional)String Generic resources used by job \nuser (optional)String User that owns the job \nwckey (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_wckey \nworking_directory (optional)String Path to current working directory \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array -  Up",
                "content": "\njob_id (optional)Integer Job ID of job array, or 0 if N/A format: int32\nlimits (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_limits \ntask_id (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_task_id \ntask (optional)String String expression of task IDs in this record \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_limits -  Up",
                "content": "\nmax (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_limits_max \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_limits_max -  Up",
                "content": "\nrunning (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_limits_max_running \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_limits_max_running -  Up",
                "content": "\ntasks (optional)Integer Maximum number of simultaneously running tasks, 0 if no limit format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_array_task_id -  Up",
                "content": "Task ID of this task in job array\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_association -  Up",
                "content": "Unique identifier for the association\naccount (optional)String Account \ncluster (optional)String Cluster \npartition (optional)String Partition \nuser String User name \nid (optional)Integer Numeric association ID format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_comment -  Up",
                "content": "\nadministrator (optional)String Arbitrary comment made by administrator \njob (optional)String Arbitrary comment made by user \nsystem (optional)String Arbitrary comment from slurmctld \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code -  Up",
                "content": "Highest exit code of all job steps\nstatus (optional)array[String] Status given by return code \nEnum:\nreturn_code (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_return_code \nsignal (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_signal \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_return_code -  Up",
                "content": "Process return code (numeric)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_signal -  Up",
                "content": "\nid (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_signal_id \nname (optional)String Signal sent to process \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_signal_id -  Up",
                "content": "Signal sent to process (numeric)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_exit_code -  Up",
                "content": "Exit code\nstatus (optional)array[String] Status given by return code \nEnum:\nreturn_code (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_return_code \nsignal (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_derived_exit_code_signal \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_het -  Up",
                "content": "\njob_id (optional)Integer Heterogeneous job ID, if applicable format: int32\njob_offset (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_het_job_offset \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_het_job_offset -  Up",
                "content": "Unique sequence number applied to this component of the heterogeneous job\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_mcs -  Up",
                "content": "\nlabel (optional)String Multi-Category Security label on the job \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_priority -  Up",
                "content": "Request specific job priority\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required -  Up",
                "content": "\nCPUs (optional)Integer Minimum number of CPUs required format: int32\nmemory_per_cpu (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required_memory_per_cpu \nmemory_per_node (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required_memory_per_node \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required_memory_per_cpu -  Up",
                "content": "Minimum memory in megabytes per allocated CPU\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_required_memory_per_node -  Up",
                "content": "Minimum memory in megabytes per allocated node\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_reservation -  Up",
                "content": "\nid (optional)Integer Unique identifier of requested reservation format: int32\nname (optional)String Name of reservation to use \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_state -  Up",
                "content": "\ncurrent (optional)array[String] Current state \nEnum:\nreason (optional)String Reason for previous Pending or Failed state \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner -  Up",
                "content": "\ntime (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time \nexit_code (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_exit_code \nnodes (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_nodes \ntasks (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tasks \npid (optional)String Process ID \nCPU (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_CPU \nkill_request_user (optional)String User ID that requested termination of the step \nstate (optional)array[String] Current state \nEnum:\nstatistics (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_statistics \nstep (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_step \ntask (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_task \ntres (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_CPU -  Up",
                "content": "\nrequested_frequency (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_CPU_requested_frequency \ngovernor (optional)String Requested CPU frequency governor in kHz \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_CPU_requested_frequency -  Up",
                "content": "\nmin (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_CPU_requested_frequency_min \nmax (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_CPU_requested_frequency_max \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_CPU_requested_frequency_max -  Up",
                "content": "Maximum requested CPU frequency in kHz\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_CPU_requested_frequency_min -  Up",
                "content": "Minimum requested CPU frequency in kHz\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_nodes -  Up",
                "content": "\ncount (optional)Integer Number of nodes in the job step format: int32\nrange (optional)String Node(s) allocated to the job step \nlist (optional)array[String] List of nodes used by the step \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_statistics -  Up",
                "content": "\nCPU (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_statistics_CPU \nenergy (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_statistics_energy \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_statistics_CPU -  Up",
                "content": "\nactual_frequency (optional)Long Average weighted CPU frequency of all tasks in kHz format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_statistics_energy -  Up",
                "content": "\nconsumed (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_statistics_energy_consumed \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_statistics_energy_consumed -  Up",
                "content": "Total energy consumed by all tasks in a job in joules\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_step -  Up",
                "content": "\nid (optional)String Step ID \nname (optional)String Step name \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_task -  Up",
                "content": "\ndistribution (optional)String The layout of the step was when it was running \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tasks -  Up",
                "content": "\ncount (optional)Integer Total number of tasks format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time -  Up",
                "content": "\nelapsed (optional)Integer Elapsed time in seconds format: int32\nend (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_end \nstart (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_start \nsuspended (optional)Integer Time in suspended state in seconds format: int32\nsystem (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_system \ntotal (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_total \nuser (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_user \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_end -  Up",
                "content": "End time (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_start -  Up",
                "content": "Time execution began (UNIX timestamp)\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_system -  Up",
                "content": "\nseconds (optional)Long System CPU time used by the step in seconds format: int64\nmicroseconds (optional)Integer System CPU time used by the step in microseconds format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_total -  Up",
                "content": "\nseconds (optional)Long Total CPU time used by the step in seconds format: int64\nmicroseconds (optional)Integer Total CPU time used by the step in microseconds format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_time_user -  Up",
                "content": "\nseconds (optional)Long User CPU time used by the step in seconds format: int64\nmicroseconds (optional)Integer User CPU time used by the step in microseconds format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres -  Up",
                "content": "\nrequested (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested \nconsumed (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_consumed \nallocated (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Trackable resources allocated to the step \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_consumed -  Up",
                "content": "\nmax (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Maximum TRES usage consumed among all tasks \nmin (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Minimum TRES usage consumed among all tasks \naverage (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Average TRES usage consumed among all tasks \ntotal (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Total TRES usage consumed among all tasks \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested -  Up",
                "content": "\nmax (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Maximum TRES usage requested among all tasks \nmin (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Minimum TRES usage requested among all tasks \naverage (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Average TRES usage requested among all tasks \ntotal (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Total TRES usage requested among all tasks \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner -  Up",
                "content": "\ntype String TRES type (CPU, MEM, etc) \nname (optional)String TRES name (if applicable) \nid (optional)Integer ID used in database format: int32\ncount (optional)Long TRES count (0 if listed generically) format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time -  Up",
                "content": "\nelapsed (optional)Integer Elapsed time in seconds format: int32\neligible (optional)Long Time when the job became eligible to run (UNIX timestamp) format: int64\nend (optional)Long End time (UNIX timestamp) format: int64\nplanned (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_planned \nstart (optional)Long Time execution began (UNIX timestamp) format: int64\nsubmission (optional)Long Time when the job was submitted (UNIX timestamp) format: int64\nsuspended (optional)Integer Total time in suspended state in seconds format: int32\nsystem (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_system \nlimit (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_limit \ntotal (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_total \nuser (optional)v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_user \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_limit -  Up",
                "content": "Maximum run time in minutes\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_planned -  Up",
                "content": "Time required to start job after becoming eligible to run in seconds\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Long If \"set\" is True the number will be set with value; otherwise ignore number contents format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_system -  Up",
                "content": "\nseconds (optional)Long System CPU time used by the job in seconds format: int64\nmicroseconds (optional)Long System CPU time used by the job in microseconds format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_total -  Up",
                "content": "\nseconds (optional)Long Sum of System and User CPU time used by the job in seconds format: int64\nmicroseconds (optional)Long Sum of System and User CPU time used by the job in microseconds format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_time_user -  Up",
                "content": "\nseconds (optional)Long User CPU time used by the job in seconds format: int64\nmicroseconds (optional)Long User CPU time used by the job in microseconds format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_tres -  Up",
                "content": "\nallocated (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Trackable resources allocated to the job \nrequested (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Trackable resources requested by job \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_wckey -  Up",
                "content": "Workload characterization key\nwckey String WCKey name \nflags array[String] Active flags \nEnum:\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_meta -  Up",
                "content": "Slurm meta values\nplugin (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta_plugin \nclient (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta_client \ncommand (optional)array[String] CLI command (if applicable) \nslurm (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta_slurm \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_meta_client -  Up",
                "content": "\nsource (optional)String Client source description \nuser (optional)String Client user (if known) \ngroup (optional)String Client group (if known) \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_meta_plugin -  Up",
                "content": "\ntype (optional)String Slurm plugin type (if applicable) \nname (optional)String Slurm plugin name (if applicable) \ndata_parser (optional)String Slurm data_parser plugin \naccounting_storage (optional)String Slurm accounting plugin \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_meta_slurm -  Up",
                "content": "\nversion (optional)v0_0_41_openapi_slurmdbd_jobs_resp_meta_slurm_version \nrelease (optional)String Slurm release string \ncluster (optional)String Slurm cluster name \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_meta_slurm_version -  Up",
                "content": "\nmajor (optional)String Slurm release major version \nmicro (optional)String Slurm release micro version \nminor (optional)String Slurm release minor version \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_jobs_resp_warnings_inner -  Up",
                "content": "\ndescription (optional)String Long form warning description \nsource (optional)String Source of warning or where warning was first detected \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_stats_resp_statistics -  Up",
                "content": "statistics\ntime_start (optional)Long When data collection started (UNIX timestamp) format: int64\nrollups (optional)v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups \nRPCs (optional)array[v0_0_41_openapi_slurmdbd_stats_resp_statistics_RPCs_inner] List of RPCs sent to the slurmdbd \nusers (optional)array[v0_0_41_openapi_slurmdbd_stats_resp_statistics_users_inner] List of users that issued RPCs \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_stats_resp_statistics_RPCs_inner -  Up",
                "content": "\nrpc (optional)String RPC type \ncount (optional)Integer Number of RPCs processed format: int32\ntime (optional)v0_0_41_openapi_slurmdbd_stats_resp_statistics_RPCs_inner_time \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_stats_resp_statistics_RPCs_inner_time -  Up",
                "content": "\naverage (optional)Long Average RPC processing time in microseconds format: int64\ntotal (optional)Long Total RPC processing time in microseconds format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups -  Up",
                "content": "Rollup statistics\nhourly (optional)v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_hourly \ndaily (optional)v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_daily \nmonthly (optional)v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_monthly \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_daily -  Up",
                "content": "\ncount (optional)Integer Number of daily rollups since last_run format: int32\nlast_run (optional)Long Last time daily rollup ran (UNIX timestamp) format: int64\nduration (optional)v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_daily_duration \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_daily_duration -  Up",
                "content": "\nlast (optional)Long Total time spent doing daily daily rollup (seconds) format: int64\nmax (optional)Long Longest daily rollup time (seconds) format: int64\ntime (optional)Long Total time spent doing daily rollups (seconds) format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_hourly -  Up",
                "content": "\ncount (optional)Integer Number of hourly rollups since last_run format: int32\nlast_run (optional)Long Last time hourly rollup ran (UNIX timestamp) format: int64\nduration (optional)v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_hourly_duration \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_hourly_duration -  Up",
                "content": "\nlast (optional)Long Total time spent doing last daily rollup (seconds) format: int64\nmax (optional)Long Longest hourly rollup time (seconds) format: int64\ntime (optional)Long Total time spent doing hourly rollups (seconds) format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_monthly -  Up",
                "content": "\ncount (optional)Integer Number of monthly rollups since last_run format: int32\nlast_run (optional)Long Last time monthly rollup ran (UNIX timestamp) format: int64\nduration (optional)v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_monthly_duration \n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_stats_resp_statistics_rollups_monthly_duration -  Up",
                "content": "\nlast (optional)Long Total time spent doing monthly daily rollup (seconds) format: int64\nmax (optional)Long Longest monthly rollup time (seconds) format: int64\ntime (optional)Long Total time spent doing monthly rollups (seconds) format: int64\n"
            },
            {
                "title": "v0_0_41_openapi_slurmdbd_stats_resp_statistics_users_inner -  Up",
                "content": "\nuser (optional)String User ID \ncount (optional)Integer Number of RPCs processed format: int32\ntime (optional)v0_0_41_openapi_slurmdbd_stats_resp_statistics_RPCs_inner_time \n"
            },
            {
                "title": "v0_0_41_openapi_users_add_cond_resp_association_condition -  Up",
                "content": "Filters to select associations for users\naccounts (optional)array[String] CSV accounts list \nassociation (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association \nclusters (optional)array[String] CSV clusters list \npartitions (optional)array[String] CSV partitions list \nusers array[String] CSV users list \nwckeys (optional)array[String] CSV WCKeys list \n"
            },
            {
                "title": "v0_0_41_openapi_users_add_cond_resp_association_condition_association -  Up",
                "content": "Association limits and options\ncomment (optional)String Arbitrary comment \ndefaultqos (optional)String Default QOS \ngrpjobs (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association_grpjobs \ngrpjobsaccrue (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association_grpjobsaccrue \ngrpsubmitjobs (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association_grpsubmitjobs \ngrptres (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Maximum number of TRES able to be allocated by running jobs in this association and its children \ngrptresmins (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Total number of TRES minutes that can possibly be used by past, present and future jobs in this association and its children \ngrptresrunmins (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Maximum number of TRES minutes able to be allocated by running jobs in this association and its children \ngrpwall (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association_grpwall \nmaxjobs (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association_maxjobs \nmaxjobsaccrue (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association_maxjobsaccrue \nmaxsubmitjobs (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association_maxsubmitjobs \nmaxtresminsperjob (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Maximum number of TRES minutes each job is able to use in this association \nmaxtresrunmins (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Maximum number of TRES minutes able to be allocated by running jobs in this association \nmaxtresperjob (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Maximum number of TRES each job is able to use in this association \nmaxtrespernode (optional)array[v0_0_41_openapi_slurmdbd_jobs_resp_jobs_inner_steps_inner_tres_requested_max_inner] Maximum number of TRES each node is able to use \nmaxwalldurationperjob (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association_maxwalldurationperjob \nminpriothresh (optional)v0_0_41_openapi_users_add_cond_resp_association_condition_association_minpriothresh \nparent (optional)String Name of parent account \npriority (optional)v0_0_41_openapi_slurmdbd_config_resp_associations_inner_priority \nqoslevel (optional)array[String] List of available QOS names \nfairshare (optional)Integer Allocated shares used for fairshare calculation format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_users_add_cond_resp_association_condition_association_grpjobs -  Up",
                "content": "Maximum number of running jobs in this association and its children\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_users_add_cond_resp_association_condition_association_grpjobsaccrue -  Up",
                "content": "Maximum number of pending jobs able to accrue age priority in this association and its children\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_users_add_cond_resp_association_condition_association_grpsubmitjobs -  Up",
                "content": "Maximum number of jobs which can be in a pending or running state at any time in this association and its children\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_users_add_cond_resp_association_condition_association_grpwall -  Up",
                "content": "Maximum wall clock time in minutes able to be allocated by running jobs in this association and its children\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_users_add_cond_resp_association_condition_association_maxjobs -  Up",
                "content": "Maximum number of running jobs per user in this association\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_users_add_cond_resp_association_condition_association_maxjobsaccrue -  Up",
                "content": "Maximum number of pending jobs able to accrue age priority at any given time in this association\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_users_add_cond_resp_association_condition_association_maxsubmitjobs -  Up",
                "content": "Maximum number of jobs which can be in a pending or running state at any time in this association\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_users_add_cond_resp_association_condition_association_maxwalldurationperjob -  Up",
                "content": "Maximum wall clock time each job is able to use in this association\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_users_add_cond_resp_association_condition_association_minpriothresh -  Up",
                "content": "Minimum priority required to reserve resources when scheduling\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_openapi_users_add_cond_resp_user -  Up",
                "content": "Admin level of user, DefaultAccount, DefaultWCKey\nadminlevel (optional)array[String] AdminLevel granted to the user \nEnum:\ndefaultaccount (optional)String Default account \ndefaultwckey (optional)String Default WCKey \n"
            },
            {
                "title": "v0_0_41_update_node_msg_resume_after -  Up",
                "content": "Number of seconds after which to automatically resume DOWN or DRAINED node\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            },
            {
                "title": "v0_0_41_update_node_msg_weight -  Up",
                "content": "Weight of the node for scheduling purposes\nset (optional)Boolean True if number has been set; False if number is unset \ninfinite (optional)Boolean True if number has been set to infinite; \"set\" and \"number\" will be ignored \nnumber (optional)Integer If \"set\" is True the number will be set with value; otherwise ignore number contents format: int32\n"
            }
        ]
    },
    {
        "url": "https://www.schedmd.com/downloads/extras/Slurm_ParallelCluster_AWS.pdf",
        "sections": []
    },
    {
        "url": "https://slurm.schedmd.com/dynamic_nodes.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Dynamic Nodes",
                "content": "OverviewStarting in Slurm 22.05, nodes can be dynamically added and removed from\nSlurm.\nDynamic Node Communications\n\n\nFor regular, non-dynamically created nodes, Slurm knows how to communicate with\nnodes by reading in the slurm.conf. This is why it is important for a\nnon-dynamic setup that the slurm.conf is synchronized across the cluster. For\ndynamically created nodes, The controller automatically grabs the node's\nNodeAddr and NodeHostname for dynamic slurmd registrations. The\ncontroller then passes the node addresses to the clients so that they\ncommunicate, and even fanout, to other nodes.\nSlurm Configuration\n\n\n\nMaxNodeCount=#\n\nSet to the number of possible nodes that can be active in a system at a time.\nSee the slurm.conf man page for\nmore details.\n\nSelectType=select/cons_tres\nDynamic nodes are only supported with cons_tres.\n\nPartition Assignment\n\n\nDynamic nodes can be automatically assigned to partitions at creation by using\nthe partition's nodes ALL keyword or\nNodeSets and\nspecifying a feature on the nodes.\n\ne.g.\n\nNodeset=ns1 Feature=f1\nNodeset=ns2 Feature=f2\n\nPartitionName=all  Nodes=ALL Default=yes\nPartitionName=dyn1 Nodes=ns1\nPartitionName=dyn2 Nodes=ns2\nPartitionName=dyn3 Nodes=ns1,ns2\n\n\nCreating Nodes\n\n\n\nNodes can be created two ways:\n\n\n\nDynamic slurmd registration\n\nUsing the slurmd -Z and\n--conf options a slurmd\nwill register with the controller and will automatically be added to the system.\n\n\ne.g.\n\nslurmd -Z --conf \"RealMemory=80000 Gres=gpu:2 Feature=f1\"\n\n\n\n\n\n\nscontrol create NodeName= ...\n\nCreate nodes using scontrol by specifying the same NodeName\nline that you would define in the slurm.conf. See slurm.conf\nman page for node\noptions. Only State=CLOUD and State=FUTURE are supported. The\nnode configuration should match what the slurmd will register with\n(e.g. slurmd -C) plus any additional attributes.\n\n\ne.g.\n\nscontrol create NodeName=d[1-100] CPUs=16 Boards=1 SocketsPerBoard=1 CoresPerSocket=8 ThreadsPerCore=2 RealMemory=31848 Gres=gpu:2 Feature=f1 State=cloud\n\nDeleting Nodes\n\n\nNodes can be deleted using scontrol delete nodename=<nodelist>.\nOnly dynamic nodes that have no running jobs and that are not part of a\nreservation can be deleted.\nLimitations\n\n\n\n\nDynamic nodes are not sorted internally and when added to Slurm they will\npotentially be alphabetically out of order internally \u2014 leading to\nsuboptimal job allocations if node names represent topology of the nodes.\n\n\nLast modified 13 January 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Creating Nodes\n\n",
                "content": "\nNodes can be created two ways:\n\n\n\nDynamic slurmd registration\n\nUsing the slurmd -Z and\n--conf options a slurmd\nwill register with the controller and will automatically be added to the system.\n\n\ne.g.\n\nslurmd -Z --conf \"RealMemory=80000 Gres=gpu:2 Feature=f1\"\n\n\n\n\n\n\nscontrol create NodeName= ...\n\nCreate nodes using scontrol by specifying the same NodeName\nline that you would define in the slurm.conf. See slurm.conf\nman page for node\noptions. Only State=CLOUD and State=FUTURE are supported. The\nnode configuration should match what the slurmd will register with\n(e.g. slurmd -C) plus any additional attributes.\n\n\nscontrol create NodeName=d[1-100] CPUs=16 Boards=1 SocketsPerBoard=1 CoresPerSocket=8 ThreadsPerCore=2 RealMemory=31848 Gres=gpu:2 Feature=f1 State=cloud\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/overview.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Overview",
                "content": "Slurm is an open source,\nfault-tolerant, and highly scalable cluster management and job scheduling system\nfor large and small Linux clusters. Slurm requires no kernel modifications for\nits operation and is relatively self-contained. As a cluster workload manager,\nSlurm has three key functions. First, it allocates exclusive and/or non-exclusive\naccess to resources (compute nodes) to users for some duration of time so they\ncan perform work. Second, it provides a framework for starting, executing, and\nmonitoring work (normally a parallel job) on the set of allocated nodes.\nFinally, it arbitrates contention for resources by managing a queue of\npending work.\nOptional plugins can be used for\naccounting,\nadvanced reservation,\ngang scheduling (time sharing for\nparallel jobs), backfill scheduling,\ntopology optimized resource selection,\nresource limits by user or bank account,\nand sophisticated  multifactor job\nprioritization algorithms.\n\nArchitecture\n\n\nSlurm has a centralized manager, slurmctld, to monitor resources and\nwork. There may also be a backup manager to assume those responsibilities in the\nevent of failure. Each compute server (node) has a slurmd daemon, which\ncan be compared to a remote shell: it waits for work, executes that work, returns\nstatus, and waits for more work.\nThe slurmd daemons provide fault-tolerant hierarchical communications.\nThere is an optional slurmdbd (Slurm DataBase Daemon) which can be used\nto record accounting information for multiple Slurm-managed clusters in a\nsingle database.\nThere is an optional\nslurmrestd (Slurm REST API Daemon)\nwhich can be used to interact with Slurm through its\n\nREST API.\nUser tools include srun to initiate jobs,\nscancel to terminate queued or running jobs,\nsinfo to report system status,\nsqueue to report the status of jobs, and\nsacct to get information about jobs and job steps that are running or have completed.\nThe sview commands graphically reports system and\njob status including network topology.\nThere is an administrative tool scontrol available to monitor\nand/or modify configuration and state information on the cluster.\nThe administrative tool used to manage the database is sacctmgr.\nIt can be used to identify the clusters, valid users, valid bank accounts, etc.\nAPIs are available for all functions.\n\n\n  Figure 1. Slurm components\n\nSlurm has a general-purpose plugin mechanism available to easily support various\ninfrastructures. This permits a wide variety of Slurm configurations using a\nbuilding block approach. These plugins presently include:\n\nAccounting Storage:\n  Primarily Used to store historical data about jobs.  When used with\n  SlurmDBD (Slurm Database Daemon), it can also supply a\n  limits based system along with historical system status.\n\nAccount Gather Energy:\n  Gather energy consumption data per job or nodes in the system.\n  This plugin is integrated with the\n  Accounting Storage and Job Account Gather plugins.\n\nAuthentication of communications:\n  Provides authentication mechanism between various components of Slurm.\n\nContainers:\n  HPC workload container support and implementations.\n\nCredential (Digital Signature Generation):\n  Mechanism used to generate a digital signature, which is used to validate\n  that job step is authorized to execute on specific nodes.\n  This is distinct from the plugin used for\n  Authentication since the job step\n  request is sent from the user's srun command rather than directly from the\n  slurmctld daemon, which generates the job step credential and its\n  digital signature.\n\nGeneric Resources: Provide interface to\n  control generic resources, including Graphical Processing Units (GPUs).\n\nJob Submit:\n  Custom plugin to allow site specific control over job requirements at\n  submission and update.\n\nJob Accounting Gather:\n  Gather job step resource utilization data.\n\nJob Completion Logging:\n  Log a job's termination data. This is typically a subset of data stored by\n  an Accounting Storage Plugin.\n\nLaunchers:\n  Controls the mechanism used by the 'srun' command\n  to launch the tasks.\n\nMPI:\n  Provides different hooks for the various MPI implementations.\n  For example, this can set MPI specific environment variables.\n\nPreempt:\n  Determines which jobs can preempt other jobs and the preemption mechanism\n  to be used.\n\nPriority:\n  Assigns priorities to jobs upon submission and on an ongoing basis\n  (e.g. as they age).\n\nProcess tracking (for signaling):\n  Provides a mechanism for identifying the processes associated with each job.\n  Used for job accounting and signaling.\n\nScheduler:\n  Plugin determines how and when Slurm schedules jobs.\n\nNode selection:\n  Plugin used to determine the resources used for a job allocation.\n\nSite Factor (Priority):\n  Assigns a specific site_factor component of a job's multifactor priority to\n  jobs upon submission and on an ongoing basis (e.g. as they age).\n\nSwitch or interconnect:\n  Plugin to interface with a switch or interconnect.\n  For most systems (Ethernet or InfiniBand) this is not needed.\n\nTask Affinity:\n  Provides mechanism to bind a job and its individual tasks to specific\n  processors.\n\nNetwork Topology:\n  Optimizes resource selection based upon the network topology.\n  Used for both job allocations and advanced reservation.\n\n\nThe entities managed by these Slurm daemons, shown in Figure 2, include nodes,\nthe compute resource in Slurm, partitions, which group nodes into logical\nsets, jobs, or allocations of resources assigned to a user for\na specified amount of time, and job steps, which are sets of (possibly\nparallel) tasks within a job.\nThe partitions can be considered job queues, each of which has an assortment of\nconstraints such as job size limit, job time limit, users permitted to use it, etc.\nPriority-ordered jobs are allocated nodes within a partition until the resources\n(nodes, processors, memory, etc.) within that partition are exhausted. Once\na job is assigned a set of nodes, the user is able to initiate parallel work in\nthe form of job steps in any configuration within the allocation. For instance,\na single job step may be started that utilizes all nodes allocated to the job,\nor several job steps may independently use a portion of the allocation.\nSlurm provides resource management for the processors allocated to a job,\nso that multiple job steps can be simultaneously submitted and queued until\nthere are available resources within the job's allocation.\n\n\n  Figure 2. Slurm entities\n\nConfigurability\n\n\nNode state monitored include: count of processors, size of real memory, size\nof temporary disk space, and state (UP, DOWN, etc.). Additional node information\nincludes weight (preference in being allocated work) and features (arbitrary information\nsuch as processor speed or type).\nNodes are grouped into partitions, which may contain overlapping nodes so they are\nbest thought of as job queues.\nPartition information includes: name, list of associated nodes, state (UP or DOWN),\nmaximum job time limit, maximum node count per job, group access list,\npriority (important if nodes are in multiple partitions) and shared node access policy\nwith optional over-subscription level for gang scheduling (e.g. YES, NO or FORCE:2).\nBit maps are used to represent nodes and scheduling\ndecisions can be made by performing a small number of comparisons and a series\nof fast bit map manipulations. A sample (partial. Slurm configuration file follows.\n\n#\n# Sample /etc/slurm.conf\n#\nSlurmctldHost=linux0001  # Primary server\nSlurmctldHost=linux0002  # Backup server\n#\nAuthType=auth/munge\nEpilog=/usr/local/slurm/sbin/epilog\nPluginDir=/usr/local/slurm/lib\nProlog=/usr/local/slurm/sbin/prolog\nSlurmctldPort=7002\nSlurmctldTimeout=120\nSlurmdPort=7003\nSlurmdSpoolDir=/var/tmp/slurmd.spool\nSlurmdTimeout=120\nStateSaveLocation=/usr/local/slurm/slurm.state\nTmpFS=/tmp\n#\n# Node Configurations\n#\nNodeName=DEFAULT CPUs=4 TmpDisk=16384 State=IDLE\nNodeName=lx[0001-0002] State=DRAINED\nNodeName=lx[0003-8000] RealMemory=2048 Weight=2\nNodeName=lx[8001-9999] RealMemory=4096 Weight=6 Feature=video\n#\n# Partition Configurations\n#\nPartitionName=DEFAULT MaxTime=30 MaxNodes=2\nPartitionName=login Nodes=lx[0001-0002] State=DOWN\nPartitionName=debug Nodes=lx[0003-0030] State=UP Default=YES\nPartitionName=class Nodes=lx[0031-0040] AllowGroups=students\nPartitionName=DEFAULT MaxTime=UNLIMITED MaxNodes=4096\nPartitionName=batch Nodes=lx[0041-9999]\n\nLast modified 6 August 2021\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Architecture\n\n",
                "content": "Slurm has a centralized manager, slurmctld, to monitor resources and\nwork. There may also be a backup manager to assume those responsibilities in the\nevent of failure. Each compute server (node) has a slurmd daemon, which\ncan be compared to a remote shell: it waits for work, executes that work, returns\nstatus, and waits for more work.\nThe slurmd daemons provide fault-tolerant hierarchical communications.\nThere is an optional slurmdbd (Slurm DataBase Daemon) which can be used\nto record accounting information for multiple Slurm-managed clusters in a\nsingle database.\nThere is an optional\nslurmrestd (Slurm REST API Daemon)\nwhich can be used to interact with Slurm through its\n\nREST API.\nUser tools include srun to initiate jobs,\nscancel to terminate queued or running jobs,\nsinfo to report system status,\nsqueue to report the status of jobs, and\nsacct to get information about jobs and job steps that are running or have completed.\nThe sview commands graphically reports system and\njob status including network topology.\nThere is an administrative tool scontrol available to monitor\nand/or modify configuration and state information on the cluster.\nThe administrative tool used to manage the database is sacctmgr.\nIt can be used to identify the clusters, valid users, valid bank accounts, etc.\nAPIs are available for all functions.\n\n  Figure 1. Slurm components\nSlurm has a general-purpose plugin mechanism available to easily support various\ninfrastructures. This permits a wide variety of Slurm configurations using a\nbuilding block approach. These plugins presently include:\n\nAccounting Storage:\n  Primarily Used to store historical data about jobs.  When used with\n  SlurmDBD (Slurm Database Daemon), it can also supply a\n  limits based system along with historical system status.\n\nAccount Gather Energy:\n  Gather energy consumption data per job or nodes in the system.\n  This plugin is integrated with the\n  Accounting Storage and Job Account Gather plugins.\n\nAuthentication of communications:\n  Provides authentication mechanism between various components of Slurm.\n\nContainers:\n  HPC workload container support and implementations.\n\nCredential (Digital Signature Generation):\n  Mechanism used to generate a digital signature, which is used to validate\n  that job step is authorized to execute on specific nodes.\n  This is distinct from the plugin used for\n  Authentication since the job step\n  request is sent from the user's srun command rather than directly from the\n  slurmctld daemon, which generates the job step credential and its\n  digital signature.\n\nGeneric Resources: Provide interface to\n  control generic resources, including Graphical Processing Units (GPUs).\n\nJob Submit:\n  Custom plugin to allow site specific control over job requirements at\n  submission and update.\n\nJob Accounting Gather:\n  Gather job step resource utilization data.\n\nJob Completion Logging:\n  Log a job's termination data. This is typically a subset of data stored by\n  an Accounting Storage Plugin.\n\nLaunchers:\n  Controls the mechanism used by the 'srun' command\n  to launch the tasks.\n\nMPI:\n  Provides different hooks for the various MPI implementations.\n  For example, this can set MPI specific environment variables.\n\nPreempt:\n  Determines which jobs can preempt other jobs and the preemption mechanism\n  to be used.\n\nPriority:\n  Assigns priorities to jobs upon submission and on an ongoing basis\n  (e.g. as they age).\n\nProcess tracking (for signaling):\n  Provides a mechanism for identifying the processes associated with each job.\n  Used for job accounting and signaling.\n\nScheduler:\n  Plugin determines how and when Slurm schedules jobs.\n\nNode selection:\n  Plugin used to determine the resources used for a job allocation.\n\nSite Factor (Priority):\n  Assigns a specific site_factor component of a job's multifactor priority to\n  jobs upon submission and on an ongoing basis (e.g. as they age).\n\nSwitch or interconnect:\n  Plugin to interface with a switch or interconnect.\n  For most systems (Ethernet or InfiniBand) this is not needed.\n\nTask Affinity:\n  Provides mechanism to bind a job and its individual tasks to specific\n  processors.\n\nNetwork Topology:\n  Optimizes resource selection based upon the network topology.\n  Used for both job allocations and advanced reservation.\n\n\nThe entities managed by these Slurm daemons, shown in Figure 2, include nodes,\nthe compute resource in Slurm, partitions, which group nodes into logical\nsets, jobs, or allocations of resources assigned to a user for\na specified amount of time, and job steps, which are sets of (possibly\nparallel) tasks within a job.\nThe partitions can be considered job queues, each of which has an assortment of\nconstraints such as job size limit, job time limit, users permitted to use it, etc.\nPriority-ordered jobs are allocated nodes within a partition until the resources\n(nodes, processors, memory, etc.) within that partition are exhausted. Once\na job is assigned a set of nodes, the user is able to initiate parallel work in\nthe form of job steps in any configuration within the allocation. For instance,\na single job step may be started that utilizes all nodes allocated to the job,\nor several job steps may independently use a portion of the allocation.\nSlurm provides resource management for the processors allocated to a job,\nso that multiple job steps can be simultaneously submitted and queued until\nthere are available resources within the job's allocation.\n\n\n  Figure 2. Slurm entities\n\nConfigurability\n\n\nNode state monitored include: count of processors, size of real memory, size\nof temporary disk space, and state (UP, DOWN, etc.). Additional node information\nincludes weight (preference in being allocated work) and features (arbitrary information\nsuch as processor speed or type).\nNodes are grouped into partitions, which may contain overlapping nodes so they are\nbest thought of as job queues.\nPartition information includes: name, list of associated nodes, state (UP or DOWN),\nmaximum job time limit, maximum node count per job, group access list,\npriority (important if nodes are in multiple partitions) and shared node access policy\nwith optional over-subscription level for gang scheduling (e.g. YES, NO or FORCE:2).\nBit maps are used to represent nodes and scheduling\ndecisions can be made by performing a small number of comparisons and a series\nof fast bit map manipulations. A sample (partial. Slurm configuration file follows.\n\n#\n# Sample /etc/slurm.conf\n#\nSlurmctldHost=linux0001  # Primary server\nSlurmctldHost=linux0002  # Backup server\n#\nAuthType=auth/munge\nEpilog=/usr/local/slurm/sbin/epilog\nPluginDir=/usr/local/slurm/lib\nProlog=/usr/local/slurm/sbin/prolog\nSlurmctldPort=7002\nSlurmctldTimeout=120\nSlurmdPort=7003\nSlurmdSpoolDir=/var/tmp/slurmd.spool\nSlurmdTimeout=120\nStateSaveLocation=/usr/local/slurm/slurm.state\nTmpFS=/tmp\n#\n# Node Configurations\n#\nNodeName=DEFAULT CPUs=4 TmpDisk=16384 State=IDLE\nNodeName=lx[0001-0002] State=DRAINED\nNodeName=lx[0003-8000] RealMemory=2048 Weight=2\nNodeName=lx[8001-9999] RealMemory=4096 Weight=6 Feature=video\n#\n# Partition Configurations\n#\nPartitionName=DEFAULT MaxTime=30 MaxNodes=2\nPartitionName=login Nodes=lx[0001-0002] State=DOWN\nPartitionName=debug Nodes=lx[0003-0030] State=UP Default=YES\nPartitionName=class Nodes=lx[0031-0040] AllowGroups=students\nPartitionName=DEFAULT MaxTime=UNLIMITED MaxNodes=4096\nPartitionName=batch Nodes=lx[0041-9999]\n\nLast modified 6 August 2021\n"
            },
            {
                "title": "Configurability\n\n",
                "content": "Node state monitored include: count of processors, size of real memory, size\nof temporary disk space, and state (UP, DOWN, etc.). Additional node information\nincludes weight (preference in being allocated work) and features (arbitrary information\nsuch as processor speed or type).\nNodes are grouped into partitions, which may contain overlapping nodes so they are\nbest thought of as job queues.\nPartition information includes: name, list of associated nodes, state (UP or DOWN),\nmaximum job time limit, maximum node count per job, group access list,\npriority (important if nodes are in multiple partitions) and shared node access policy\nwith optional over-subscription level for gang scheduling (e.g. YES, NO or FORCE:2).\nBit maps are used to represent nodes and scheduling\ndecisions can be made by performing a small number of comparisons and a series\nof fast bit map manipulations. A sample (partial. Slurm configuration file follows.\n#\n# Sample /etc/slurm.conf\n#\nSlurmctldHost=linux0001  # Primary server\nSlurmctldHost=linux0002  # Backup server\n#\nAuthType=auth/munge\nEpilog=/usr/local/slurm/sbin/epilog\nPluginDir=/usr/local/slurm/lib\nProlog=/usr/local/slurm/sbin/prolog\nSlurmctldPort=7002\nSlurmctldTimeout=120\nSlurmdPort=7003\nSlurmdSpoolDir=/var/tmp/slurmd.spool\nSlurmdTimeout=120\nStateSaveLocation=/usr/local/slurm/slurm.state\nTmpFS=/tmp\n#\n# Node Configurations\n#\nNodeName=DEFAULT CPUs=4 TmpDisk=16384 State=IDLE\nNodeName=lx[0001-0002] State=DRAINED\nNodeName=lx[0003-8000] RealMemory=2048 Weight=2\nNodeName=lx[8001-9999] RealMemory=4096 Weight=6 Feature=video\n#\n# Partition Configurations\n#\nPartitionName=DEFAULT MaxTime=30 MaxNodes=2\nPartitionName=login Nodes=lx[0001-0002] State=DOWN\nPartitionName=debug Nodes=lx[0003-0030] State=UP Default=YES\nPartitionName=class Nodes=lx[0031-0040] AllowGroups=students\nPartitionName=DEFAULT MaxTime=UNLIMITED MaxNodes=4096\nPartitionName=batch Nodes=lx[0041-9999]\nLast modified 6 August 2021"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/core_spec.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Core Specialization",
                "content": "Core specialization is a feature designed to isolate system overhead\n(system interrupts, etc.) to designated cores on a compute node.\nThis can reduce context switching in applications to improve completion time.\nThe job processes will not be able to directly use the specialized cores.Command OptionsAll job allocation commands (salloc, sbatch and srun)\naccept the -S or --core-spec option with a core count value\nargument (e.g. \"-S 1\" or \"--core-spec=2\").\nThe count identifies the number of cores to be reserved for system overhead on\neach allocated compute node. Note that the --core-spec option will be\nignored if AllowSpecResourcesUsage is not enabled in your slurm.conf.\nEach job's core specialization count can be viewed using the scontrol,\nsview or squeue command.\nSpecification of a core specialization count for a job step is ignored\n(i.e. for the srun command within a job allocation created using the\nsalloc or sbatch command).\nUse the squeue command with the \"%X\" format option to see the count\n(it is not reported in the default output format).\nThe scontrol and sview commands can also be used to modify\nthe count for pending jobs.Explicitly setting a job's specialized core value implicitly sets its\n--exclusive option, reserving entire nodes for the job.\nThe job will be charged for all non-specialized CPUs on the node and the job's\nNumCPUs value reported by the scontrol, sview and squeue\ncommands will reflect all non-specialized CPUS on all allocated nodes as will\nthe job's accounting.Note that, due to the implicit --exclusive, if the requested specialized\ncore/thread count is lower than the number of cores in the CoreSpecCount or\nin the CpuSpecList of the allocated node, then the step will have access to\nall of the non-specialized cores as well as the specialized cores freed for\nthis job.For example, suppose a node has AllowSpecResourcesUsage=yes and\nCoreSpecCount=2 configured in the slurm.conf for a node\nwith a total of 16 Cores. If a job specified --core-spec=1, the implicit\n--exclusive would lead to an exclusive allocation of the node, leaving\n15 cores for use by the job, and keeping 1 core for system use.In sacct, the step's allocated CPUs\nwill include the specialized cores or threads that it has access to. However,\nthe job's allocated CPU count never includes specialized cores or threads to\nensure that utilization reports are accurate.Here is an example configuration, setting cores 0 and 1 as\nspecialized:\nAllowSpecResourcesUsage=yes\nNodename=n0 Port=10100 CoresPerSocket=16 ThreadsPerCore=1 CpuSpecList=0-1\nSubmit a job requesting a core spec count of 1 (freeing up core\nnumber 1 for job use).\n$ salloc --core-spec=1\nsalloc: Granted job allocation 4152\n$ srun bash -c 'cat /proc/self/status |grep Cpus_'\nCpus_allowed:        fffe\nCpus_allowed_list:   1-15\nNotice the job CPU count vs the step CPU count.\n$ sacct -j 4152 -ojobid%20,alloccpus\n               JobID  AllocCPUS\n-------------------- ----------\n                4152         14\n    4152.interactive         15\n              4152.0         15\nCore SelectionThe specific resources to be used for specialization may be identified using\nthe CPUSpecList configuration parameter associated with each node in\nthe slurm.conf file.\nIf CoreSpecCount is configured, but not CPUSpecList, the cores\nselected for specialization will follow the assignment algorithm\ndescribed below .\nThe first core selected will be the highest numbered core on the highest\nnumbered socket.\nSubsequent cores selected will be the highest numbered core on lower\nnumbered sockets.\nIf additional cores are required, they will come from the next highest numbered\ncores on each socket.\nBy way of example, consider a node with two sockets, each with four cores.\nThe specialized cores will be selected in the following order:\nsocket: 1 core: 3\nsocket: 0 core: 3\nsocket: 1 core: 2\nsocket: 0 core: 2\nsocket: 1 core: 1\nsocket: 0 core: 1\nsocket: 1 core: 0\nsocket: 0 core: 0\nSlurm can be configured to specialize the first, rather than the last cores\nby configuring SchedulerParameters=spec_cores_first. In that case,\nthe first core selected will be the lowest numbered core on the lowest\nnumbered socket.\nSubsequent cores selected will be the lowest numbered core on higher\nnumbered sockets.\nIf additional cores are required, they well come from the next lowest numbered\ncores on each socket.Note that core specialization reservation may impact the use of some\njob allocation request options, especially --cores-per-socket.System Configuration\n\nCore specialization requires SelectType=cons_tres and the\ntask/cgroup TaskPlugin.\nSpecialized resources should be configured in slurm.conf on the\nnode specification line using the CoreSpecCount or CPUSpecList\noptions to identify the CPUs to reserve.\nThe MemSpecLimit option can be used to reserve memory.\nThese resources will be reserved using Linux cgroups.\nUsers wanting a different number of specialized cores should use the\n--core-spec option as described above.A job's core specialization option will be silently cleared on other\nconfigurations.\nIn addition, each compute node's core count must be configured or the CPUs\ncount must be configured to the node's core count.\nIf the core count is not configured and the CPUs value is configured to the\ncount of hyperthreads, then hyperthreads rather than cores will be reserved for\nsystem use.If users are to be granted the right to control the number of specialized\ncores for their job, the configuration parameter AllowSpecResourcesUsage\nmust be set to a value of 1.Last modified 16 January 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/contributor.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Contributor Agreement",
                "content": "In order to help ensure the legal integrity of Slurm and provide protection\nfor all contributors and users a contributor agreement is required for all\nsubstantial contributions.\nEach individual making contributions should complete the\nSlurm Workload Manager Individual Contributor License Agreement.\nIn addition the individual's organization should complete the\nSlurm Workload Manager Entity Contributor License Agreement.Signed forms or questions should be directed to\ncla@schedmd.com or posted to\nSchedMD LLC\n905 North 100 East\nLehi, UT 84043\nUSAContacting SchedMD before starting any substantial development effort is\nrecommended to coordinate development and avoid duplication of effort.\nSchedMD can provide design review, code review, integration and long-term\nsupport at reasonable cost. SchedMD can also perform custom development work\nto satisfy your requirements.Last modified 30 December 2022"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/pam_slurm_adopt.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "pam_slurm_adopt",
                "content": "The purpose of this module is to prevent users from sshing into nodes that\nthey do not have a running job on, and to track the ssh connection and any\nother spawned processes for accounting and to ensure complete job cleanup when\nthe job is completed. This module does this by determining the job which\noriginated the ssh connection. The user's connection is \"adopted\" into the\n\"external\" step of the job. When access is denied, the user will receive a\nrelevant error message.Contents\nInstallation\nSlurm Configuration\nSSH Configuration\nPAM Configuration\nAdministrative Access Configuration\npam_slurm_adopt Module Options\nFirewalls, IP Addresses, etc.\nSELinux\nLimitations\nInstallation\n\nSource:In your Slurm source directory, navigate to ./contribs/pam_slurm_adopt/\nand runmake && make installas root. This will place pam_slurm_adopt.a, pam_slurm_adopt.la,\nand pam_slurm_adopt.so in /lib/security/ (on Debian systems) or\n/lib64/security/ (on RedHat/SuSE systems).RPM:The included slurm.spec will build a slurm-pam_slurm RPM which will install\npam_slurm_adopt. Refer to the\nQuick Start\nAdministrator Guide for instructions on managing an RPM-based install.DEB:The included debian packaging scripts will build the\nslurm-smd-libpam-slurm-adopt package which will install pam_slurm_adopt.\nQuick Start\nAdministrator Guide for instructions on managing an DEB-based install.Slurm Configuration\n\nPrologFlags=contain must be set in the slurm.conf. This sets up the\n\"extern\" step into which ssh-launched processes will be adopted. You must also\nenable the task/cgroup plugin in slurm.conf. See the\nSlurm cgroups guide.\nCAUTION This option must be in place before using this module.\nThe module bases its checks on local steps that have already been launched. Jobs\nlaunched without this option do not have an extern step, so pam_slurm_adopt will\nnot have access to those jobs.LaunchParameters=ulimit_pam_adopt will set RLIMIT_RSS in processes\nadopted by the external step, similar to tasks running in regular steps.The UsePAM option in slurm.conf is not related to pam_slurm_adopt.SSH Configuration\n\nVerify that UsePAM is set to On in /etc/ssh/sshd_config (it\nshould be on by default).PAM Configuration\n\nAdd the following line to the appropriate file in /etc/pam.d, such as\nsystem-auth or sshd (you may use either the \"required\" or \"sufficient\" PAM\ncontrol flag):\naccount    required      pam_slurm_adopt.so\n The order of plugins is very important. pam_slurm_adopt.so should be the\nlast PAM module in the account stack. Included files such as common-account\nshould normally be included before pam_slurm_adopt.\n\nYou might have the following account stack in sshd:\naccount    required      pam_nologin.so\naccount    include       password-auth\n...\n-account    required      pam_slurm_adopt.so\nNote the \"-\" before the account entry for pam_slurm_adopt. It allows\nPAM to fail gracefully if the pam_slurm_adopt.so file is not found. If Slurm\nis on a shared filesystem, such as NFS, then this is suggested to avoid being\nlocked out of a node while the shared filesystem is mounting or down.pam_slurm_adopt must be used with the task/cgroup task plugin and the\nproctrack/cgroup proctrack plugin.\nThe pam_systemd module will conflict with pam_slurm_adopt, so you need to\ndisable it in all files that are included in sshd or system-auth (e.g.\npassword-auth, common-session, etc.).If you need the user management features from pam_systemd, such as\nhandling user runtime directory /run/user/$UID, you can have the prolog script\nrun 'loginctl enable-linger $SLURM_JOB_USER' and the epilog script disable\nit again (after making sure there are no other jobs from this user on the node)\nby running 'loginctl disable-linger $SLURM_JOB_USER'. You will also need to\nexport the XDG_* environment variables if your software requires them.\nYou can see an example of prolog and epilog scripts here:\nloginctl enable-linger $SLURM_JOB_USER\nexit 0\n\necho \"export XDG_RUNTIME_DIR=/run/user/$SLURM_JOB_UID\"\necho \"export XDG_SESSION_ID=$(</proc/self/sessionid)\"\necho \"export XDG_SESSION_TYPE=tty\"\necho \"export XDG_SESSION_CLASS=user\"\n\n#Only disable linger if this is the last job running for this user.\nO_P=0\nfor pid in $(scontrol listpids | awk -v jid=$SLURM_JOB_ID 'NR!=1 { if ($2 != jid && $1 != \"-1\"){print $1} }'); do\n        ps --noheader -o euser p $pid | grep -q $SLURM_JOB_USER && O_P=1\ndone\nif [ $O_P -eq 0 ]; then\n        loginctl disable-linger $SLURM_JOB_USER\nfi\nexit 0\nYou must also make sure a different PAM\nmodule isn't short-circuiting the account stack before it gets to\npam_slurm_adopt.so. From the example above, the following two lines have been\ncommented out in the included password-auth file:\n#account    sufficient    pam_localuser.so\n#-session   optional      pam_systemd.so\nNote: This may involve editing a file that is auto-generated.\nDo not run the config script that generates the file or your\nchanges will be erased.Administrative Access Configuration\n\n\npam_slurm_adopt will always allow the root user access, and will do so even\nbefore checking for slurm configuration. If you wish to also allow other admins\nto the system with their own user accounts, this can be accomplished by stacking\nother modules along with pam_slurm_adopt.\n\n\nStacking pam_access with pam_slurm_adopt is one way to permit administrative\naccess, with two possible implementations with subtle differences in behavior.\nBoth will require editing the pam_access configuration file\n(/etc/security/access.conf). In the following example, the access.conf file will\nallow members of the group \"wheel\" to log in.\n\n\n+:(wheel):ALL\n-:ALL:ALL\n\n\nThen you will need to stack the modules in the /etc/pam.d/sshd file.  The order\nhere matters, and each ordering has different implications. In the example\nbelow, pam_slurm adopt is listed first as \"sufficient\", followed by pam_access.\nIn this configuration when the admin has a job running, their ssh session\nwill be adopted into the job. If they do not, access will be permitted by\npam_access, but note that pam_slurm_adopt will still emit the \"access denied\"\nmessage.\n\n\naccount    sufficient    pam_slurm_adopt.so\naccount    required      pam_access.so\n\n\nFlipping this order, with pam_access(sufficient) before\npam_slurm_adopt(required), members of the administrative group will bypass\npam_slurm_adopt entirely.\n\n\naccount    sufficient    pam_access.so\naccount    required      pam_slurm_adopt.so\n\n\nThe pam_listfile module is another module that can be stacked with\npam_slurm_adopt and achieve similar results.  In the following example, it will\nallow all users in the specified file to log in, skipping the pam_slurm_adopt\nmodule.  This can also be flipped similar to pam_access, with the same\nimplications.\n\n\naccount    sufficient    pam_listfile.so item=user sense=allow onerr=fail file=/path/to/allowed_users_file\naccount    required      pam_slurm_adopt.so\n\n\nMore information about the capabilities and configuration options for pam_access\nand pam_listfile can be found in their respective man pages.\n\npam_slurm_adopt Module Options\n\n\n\nThis module is configurable. Add these options to the end of the pam_slurm_adopt\nline in the appropriate file in /etc/pam.d/ (e.g., sshd or system-auth):\n\n\naccount sufficient pam_slurm_adopt.so optionname=optionvalue\n\nThis module has the following options:\n\n\naction_no_jobs\n\n\n\nThe action to perform if the user has no jobs on the node. Configurable\nvalues are:\n\n\n\n\n\n\nignore\n\n\nDo nothing. Fall through to the next pam module.\n\ndeny (default)\n  \n\nDeny the connection.\n\n\n\n\n\naction_unknown\n\n\n\nThe action to perform when the user has multiple jobs on the node and\nthe RPC does not locate the source job. If the RPC mechanism works properly in\nyour environment, this option will likely be relevant only when\nconnecting from a login node. Configurable values are:\n\n\n\n\n\n\nnewest (default)\n  \n\nOn systems with cgroup/v1 pick the newest job on the node.\nThe \"newest\" job is chosen based on the mtime of the job's step_extern cgroup;\nasking Slurm would require an RPC to the controller. Thus, the memory cgroup\nmust be in use so that the code can check mtimes of cgroup directories. The user\ncan ssh in but may be adopted into a job that exits earlier than the\njob they intended to check on. The ssh connection will at least be\nsubject to appropriate limits and the user can be informed of better\nways to accomplish their objectives if this becomes a problem.\nNOTE: If the module fails to retrieve the cgroup mtime, then the picked\njob may not be the newest one.\nOn systems with cgroup/v2 the newest is just the job with the greatest\nid, and thus this does not ensure that it is really the newest job.\n\n\nallow\n\n\nLet the connection through without adoption.\n\ndeny\n\n\nDeny the connection.\n\n\n\n\n\naction_adopt_failure\n\n\nThe action to perform if the process is unable to be adopted into any\njob for whatever reason. If the process cannot be adopted into the job\nidentified by the callerid RPC, it will fall through to the action_unknown\ncode and try to adopt there. A failure at that point or if there is only\none job will result in this action being taken. Configurable values are:\n\n\n\n\n\n\nallow (default)\n  \n\nLet the connection through without adoption. WARNING: This value is\ninsecure and is recommended for testing purposes only. We recommend using\n\"deny.\"\n\ndeny\n\n\nDeny the connection.\n\n\n\n\n\naction_generic_failure\n\n\nThe action to perform if there are certain failures such as the\ninability to talk to the local slurmd or if the kernel doesn't offer\nthe correct facilities. Configurable values are:\n\n\n\n\n\n\nignore (default)\n  \n\nDo nothing. Fall through to the next pam module. WARNING: This value is\ninsecure and is recommended for testing purposes only. We recommend using\n\"deny.\"\n\nallow\n\n\nLet the connection through without adoption.\n\ndeny\n\n\nDeny the connection.\n\n\n\n\n\ndisable_x11\n\n\nTurn off Slurm built-in X11 forwarding support. Configurable values are:\n\n\n\n\n\n\n0 (default)\n  \n\nIf the job the connection is adopted into has Slurm's X11 forwarding\nenabled, the DISPLAY variable will be overwritten with the X11 tunnel\nendpoint details.\n\n1\n\n\nDo not check for Slurm's X11 forwarding support, and do not alter the\nDISPLAY variable.\n\n\n\n\n\njoin_container\n\n\nControl the interaction with the job_container/tmpfs plugin.\nConfigurable values are:\n\n\n\n\n\ntrue (default)\n  \n\nAttempt to join a container created by the job_container/tmpfs plugin.\n\nfalse\n\n\nDo not attempt to join a container.\n\n\n\n\n\nlog_level\n\n\nSee \nSlurmdDebug in slurm.conf for available options.\nThe default log_level is info.\n\n\nnodename\n\n\nIf the NodeName defined in slurm.conf is different than this node's\nhostname (as reported by hostname -s), then this must be set to the\nNodeName in slurm.conf that this host operates as.\n\n\nservice\n\n\nThe pam service name for which this module should run. By default\nit only runs for sshd for which it was designed for. A\ndifferent service name can be specified like \"login\" or \"*\" to\nallow the module to in any service context. For local pam logins\nthis module could cause unexpected behavior or even security\nissues. Therefore if the service name does not match then this\nmodule will not perform the adoption logic and returns\nPAM_IGNORE immediately.\n\n\nFirewalls, IP Addresses, etc.\n\n\nslurmd should be accessible on any IP address from which a user might\nlaunch ssh. The RPC to determine the source job must be able to reach the\nslurmd port on that particular IP address. If there is no slurmd\non the source node, such as on a login node, it is better to have the RPC be\nrejected rather than silently dropped. This will allow better responsiveness to\nthe RPC initiator.\nSELinux\nSELinux may conflict with pam_slurm_adopt, but it is generally possible for\nthem to work side by side. This is an example type enforcement file that was\nused on a fairly stock Debian system. It is provided to give some direction\nand to show what is required to get this working but may require additional\nmodification.\n\nmodule pam_slurm_adopt 1.0;\n\nrequire {\n\ttype sshd_t;\n\ttype var_spool_t;\n\ttype unconfined_t;\n\ttype initrc_var_run_t;\n\tclass sock_file write;\n\tclass dir { read search };\n\tclass unix_stream_socket connectto;\n}\n\n#============= sshd_t ==============\nallow sshd_t initrc_var_run_t:dir search;\nallow sshd_t initrc_var_run_t:sock_file write;\nallow sshd_t unconfined_t:unix_stream_socket connectto;\nallow sshd_t var_spool_t:dir read;\nallow sshd_t var_spool_t:sock_file write;\n\nIt is possible for some plugins to require more permissions than this.\nNotably, job_container/tmpfs will require something more like this:\n\nmodule pam_slurm_adopt 1.0;\n\nrequire {\n\ttype nsfs_t;\n\ttype var_spool_t;\n\ttype initrc_var_run_t;\n\ttype unconfined_t;\n\ttype sshd_t;\n\tclass sock_file write;\n\tclass dir { read search };\n\tclass unix_stream_socket connectto;\n\tclass fd use;\n\tclass file read;\n\tclass capability sys_admin;\n}\n\n#============= sshd_t ==============\nallow sshd_t initrc_var_run_t:dir search;\nallow sshd_t initrc_var_run_t:sock_file write;\nallow sshd_t nsfs_t:file read;\nallow sshd_t unconfined_t:fd use;\nallow sshd_t unconfined_t:unix_stream_socket connectto;\nallow sshd_t var_spool_t:dir read;\nallow sshd_t var_spool_t:sock_file write;\nallow sshd_t self:capability sys_admin;\n\nLimitations\n\n\nAlternate authentication methods such as multi-factor authentication may\nbreak process adoption with pam_slurm_adopt.\nWhen using SELinux support in Slurm, the session started via pam_slurm_adopt\nwon't necessarily be in the same context as the job it is associated with.\nLast modified 23 July 2024\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "pam_slurm_adopt Module Options\n\n",
                "content": "\nThis module is configurable. Add these options to the end of the pam_slurm_adopt\nline in the appropriate file in /etc/pam.d/ (e.g., sshd or system-auth):\n\naccount sufficient pam_slurm_adopt.so optionname=optionvalue\nThis module has the following options:\n\naction_no_jobs\n\n\n\nThe action to perform if the user has no jobs on the node. Configurable\nvalues are:\n\n\n\n\n\n\nignore\n\n\nDo nothing. Fall through to the next pam module.\n\ndeny (default)\n  \n\nDeny the connection.\n\n\n\n\n\naction_unknown\n\n\n\nThe action to perform when the user has multiple jobs on the node and\nthe RPC does not locate the source job. If the RPC mechanism works properly in\nyour environment, this option will likely be relevant only when\nconnecting from a login node. Configurable values are:\n\n\n\n\n\n\nnewest (default)\n  \n\nOn systems with cgroup/v1 pick the newest job on the node.\nThe \"newest\" job is chosen based on the mtime of the job's step_extern cgroup;\nasking Slurm would require an RPC to the controller. Thus, the memory cgroup\nmust be in use so that the code can check mtimes of cgroup directories. The user\ncan ssh in but may be adopted into a job that exits earlier than the\njob they intended to check on. The ssh connection will at least be\nsubject to appropriate limits and the user can be informed of better\nways to accomplish their objectives if this becomes a problem.\nNOTE: If the module fails to retrieve the cgroup mtime, then the picked\njob may not be the newest one.\nOn systems with cgroup/v2 the newest is just the job with the greatest\nid, and thus this does not ensure that it is really the newest job.\n\n\nallow\n\n\nLet the connection through without adoption.\n\ndeny\n\n\nDeny the connection.\n\n\n\n\n\naction_adopt_failure\n\n\nThe action to perform if the process is unable to be adopted into any\njob for whatever reason. If the process cannot be adopted into the job\nidentified by the callerid RPC, it will fall through to the action_unknown\ncode and try to adopt there. A failure at that point or if there is only\none job will result in this action being taken. Configurable values are:\n\n\n\n\n\n\nallow (default)\n  \n\nLet the connection through without adoption. WARNING: This value is\ninsecure and is recommended for testing purposes only. We recommend using\n\"deny.\"\n\ndeny\n\n\nDeny the connection.\n\n\n\n\n\naction_generic_failure\n\n\nThe action to perform if there are certain failures such as the\ninability to talk to the local slurmd or if the kernel doesn't offer\nthe correct facilities. Configurable values are:\n\n\n\n\n\n\nignore (default)\n  \n\nDo nothing. Fall through to the next pam module. WARNING: This value is\ninsecure and is recommended for testing purposes only. We recommend using\n\"deny.\"\n\nallow\n\n\nLet the connection through without adoption.\n\ndeny\n\n\nDeny the connection.\n\n\n\n\n\ndisable_x11\n\n\nTurn off Slurm built-in X11 forwarding support. Configurable values are:\n\n\n\n\n\n\n0 (default)\n  \n\nIf the job the connection is adopted into has Slurm's X11 forwarding\nenabled, the DISPLAY variable will be overwritten with the X11 tunnel\nendpoint details.\n\n1\n\n\nDo not check for Slurm's X11 forwarding support, and do not alter the\nDISPLAY variable.\n\n\n\n\n\njoin_container\n\n\nControl the interaction with the job_container/tmpfs plugin.\nConfigurable values are:\n\n\n\n\n\ntrue (default)\n  \n\nAttempt to join a container created by the job_container/tmpfs plugin.\n\nfalse\n\n\nDo not attempt to join a container.\n\n\n\n\n\nlog_level\n\n\nSee \nSlurmdDebug in slurm.conf for available options.\nThe default log_level is info.\n\n\nnodename\n\n\nIf the NodeName defined in slurm.conf is different than this node's\nhostname (as reported by hostname -s), then this must be set to the\nNodeName in slurm.conf that this host operates as.\n\n\nservice\n\n\nThe pam service name for which this module should run. By default\nit only runs for sshd for which it was designed for. A\ndifferent service name can be specified like \"login\" or \"*\" to\nallow the module to in any service context. For local pam logins\nthis module could cause unexpected behavior or even security\nissues. Therefore if the service name does not match then this\nmodule will not perform the adoption logic and returns\nPAM_IGNORE immediately.\n\nFirewalls, IP Addresses, etc.\n\nslurmd should be accessible on any IP address from which a user might\nlaunch ssh. The RPC to determine the source job must be able to reach the\nslurmd port on that particular IP address. If there is no slurmd\non the source node, such as on a login node, it is better to have the RPC be\nrejected rather than silently dropped. This will allow better responsiveness to\nthe RPC initiator.SELinuxSELinux may conflict with pam_slurm_adopt, but it is generally possible for\nthem to work side by side. This is an example type enforcement file that was\nused on a fairly stock Debian system. It is provided to give some direction\nand to show what is required to get this working but may require additional\nmodification.\nmodule pam_slurm_adopt 1.0;\n\nrequire {\n\ttype sshd_t;\n\ttype var_spool_t;\n\ttype unconfined_t;\n\ttype initrc_var_run_t;\n\tclass sock_file write;\n\tclass dir { read search };\n\tclass unix_stream_socket connectto;\n}\n\n#============= sshd_t ==============\nallow sshd_t initrc_var_run_t:dir search;\nallow sshd_t initrc_var_run_t:sock_file write;\nallow sshd_t unconfined_t:unix_stream_socket connectto;\nallow sshd_t var_spool_t:dir read;\nallow sshd_t var_spool_t:sock_file write;\nIt is possible for some plugins to require more permissions than this.\nNotably, job_container/tmpfs will require something more like this:\nmodule pam_slurm_adopt 1.0;\n\nrequire {\n\ttype nsfs_t;\n\ttype var_spool_t;\n\ttype initrc_var_run_t;\n\ttype unconfined_t;\n\ttype sshd_t;\n\tclass sock_file write;\n\tclass dir { read search };\n\tclass unix_stream_socket connectto;\n\tclass fd use;\n\tclass file read;\n\tclass capability sys_admin;\n}\n\n#============= sshd_t ==============\nallow sshd_t initrc_var_run_t:dir search;\nallow sshd_t initrc_var_run_t:sock_file write;\nallow sshd_t nsfs_t:file read;\nallow sshd_t unconfined_t:fd use;\nallow sshd_t unconfined_t:unix_stream_socket connectto;\nallow sshd_t var_spool_t:dir read;\nallow sshd_t var_spool_t:sock_file write;\nallow sshd_t self:capability sys_admin;\nLimitations\n\nAlternate authentication methods such as multi-factor authentication may\nbreak process adoption with pam_slurm_adopt.When using SELinux support in Slurm, the session started via pam_slurm_adopt\nwon't necessarily be in the same context as the job it is associated with.Last modified 23 July 2024"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/release_notes.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Release Notes",
                "content": "\nThe following are the contents of the RELEASE_NOTES file as distributed\nwith the Slurm source code for this release. Please refer to the NEWS include\nalongside the source as well for more detailed descriptions of the associated\nchanges, and for bugs fixed within each maintenance release.\n\nRELEASE NOTES FOR SLURM VERSION 24.05\n\nIMPORTANT NOTES:\nIf using the slurmdbd (Slurm DataBase Daemon) you must update this first.\n\nNOTE: If using a backup DBD you must start the primary first to do any\ndatabase conversion, the backup will not start until this has happened.\n\nThe 24.05 slurmdbd will work with Slurm daemons of version 23.02 and above.\nYou will not need to update all clusters at the same time, but it is very\nimportant to update slurmdbd first and having it running before updating\nany other clusters making use of it.\n\nSlurm can be upgraded from version 23.02 or 23.11 to version 24.05 without loss\nof jobs or other state information. Upgrading directly from an earlier version\nof Slurm will result in loss of state information.\n\nAll SPANK plugins must be recompiled when upgrading from any Slurm version\nprior to 24.05.\n\nHIGHLIGHTS\n==========\n -- Remove support for Cray XC (\"cray_aries\") systems.\n -- Federation - allow client command operation when slurmdbd is unavailable.\n -- burst_buffer/lua - Added two new hooks: slurm_bb_test_data_in and\n    slurm_bb_test_data_out. The syntax and use of the new hooks are documented\n    in etc/burst_buffer.lua.example. These are required to exist. slurmctld now\n    checks on startup if the burst_buffer.lua script loads and contains all\n    required hooks; slurmctld will exit with a fatal error if this is not\n    successful. Added PollInterval to burst_buffer.conf. Removed the arbitrary\n    limit of 512 copies of the script running simultaneously.\n -- Add QOS limit MaxTRESRunMinsPerAccount.\n -- Add QOS limit MaxTRESRunMinsPerUser.\n -- Add ELIGIBLE environment variable to jobcomp/script plugin.\n -- Always use the QOS name for SLURM_JOB_QOS environment variables.\n    Previously the batch environment would use the description field,\n    which was usually equivalent to the name.\n -- cgroup/v2 - Require dbus-1 version >= 1.11.16.\n -- Allow NodeSet names to be used in SuspendExcNodes.\n -- SuspendExcNodes=:N now counts allocated nodes in N. The first N\n    powered up nodes in  are protected from being suspended.\n -- Store job output, input and error paths in SlurmDBD.\n -- Add USER_DELETE reservation flag to allow users with access to a reservation\n    to delete it.\n -- Add SlurmctldParameters=enable_stepmgr to enable step management through\n    the slurmstepd instead of the controller.\n -- Added PrologFlags=RunInJob to make prolog and epilog run inside the job\n    extern step to include it in the job's cgroup.\n -- Add ability to reserve MPI ports at the job level for stepmgr jobs and\n    subdivide them at the step level.\n -- slurmrestd - Add --generate-openapi-spec argument.\n\nCONFIGURATION FILE CHANGES (see appropriate man page for details)\n=====================================================================\n -- CoreSpecPlugin has been removed.\n -- Removed TopologyPlugin tree and dragonfly support from select/linear.\n    If those topology plugins are desired please switch to select/cons_tres.\n -- Changed the default value for UnkillableStepTimeout to 60 seconds or five\n    times the value of MessageTimeout, whichever is greater.\n -- An error log has been added if JobAcctGatherParams 'UsePss' or 'NoShare' are\n    configured with a plugin other than jobacct_gather/linux. In such case these\n    parameters are ignored.\n -- helpers.conf - Added Flags=rebootless parameter allowing feature changes\n    without rebooting compute nodes.\n -- topology/block - Replaced the BlockLevels with BlockSizes in topology.conf.\n -- Add contain_spank option to SlurmdParameters. When set, spank_user_init(),\n    spank_task_post_fork(), and spank_task_exit() will execute within the\n    job_container/tmpfs plugin namespace.\n -- Add SlurmctldParameters=max_powered_nodes=N, which prevents powering up\n    nodes after the max is reached.\n -- Add ExclusiveTopo to a partition definition in slurm.conf.\n -- Add AccountingStorageParameters=max_step_records to limit how many steps\n    are recorded in the database for each job -- excluding batch, extern, and\n    interactive steps.\n\nCOMMAND CHANGES (see man pages for details)\n===========================================\n -- Add support for \"elevenses\" as an additional time specification.\n -- Add support for sbcast --preserve when job_container/tmpfs configured\n    (previously documented as unsupported).\n -- scontrol - Add new subcommand 'power' for node power control.\n -- squeue - Adjust StdErr, StdOut, and StdIn output formats. These will now\n    consistently print \"(null)\" if a value is unavailable. StdErr will no\n    longer display StdOut if it is not distinctly set. StdOut will now\n    correctly display the default filename pattern for job arrays, and no\n    longer show it for non-batch jobs. However, the expansion patterns will\n    no longer be substituted by default.\n -- Add --segment to job allocation to be used in topology/block.\n -- Add --exclusive=topo for use with topology/block.\n -- squeue - Add --expand-patterns option to expand StdErr, StdOut, StdIn\n    filename patterns as best as possible.\n -- sacct - Add --expand-patterns option to expand StdErr, StdOut, StdIn\n    filename patterns as best as possible.\n -- sreport - Requesting format=Planned will now return the expected Planned\n    time as documented, instead of PlannedDown. To request Planned Down,\n    one must use now format=PLNDDown or format=PlannedDown explicitly. The\n    abbreviations \"Pl\" or \"Pla\" will now make reference to Planned instead of\n    PlannedDown.\n\nAPI CHANGES\n===========\n -- Removed ListIterator type from .\n -- Removed slurm_xlate_job_id() from \n\nSLURMRESTD CHANGES\n==================\n -- openapi/dbv0.0.38 and openapi/v0.0.38 plugins have been removed.\n -- openapi/dbv0.0.39 and openapi/v0.0.39 plugins have been tagged as\n    deprecated to warn of their removal in the next release.\n -- Changed slurmrestd.service to only listen on TCP socket by default.\n    Environments with existing drop-in units for the service may need\n    further adjustments to work after upgrading.\n -- slurmrestd - Tagged `script` field as deprecated in\n    'POST /slurm/v0.0.41/job/submit' in anticipation of removal in future\n    OpenAPI plugin versions. Job submissions should set the `job.script` (or\n    `jobs[0].script` for HetJobs) fields instead.\n -- slurmrestd - Attempt to automatically convert enumerated string arrays with\n    incoming non-string values into strings. Add warning when incoming value for\n    enumerated string arrays can not be converted to string and silently ignore\n    instead of rejecting entire request. This change affects any endpoint that\n    uses an enunmerated string as given in the OpenAPI specification. An\n    example of this conversion would be to 'POST /slurm/v0.0.41/job/submit' with\n    '.job.exclusive = true'. While the JSON (boolean) true value matches a\n    possible enumeration, it is not the expected \"true\" string. This change\n    automatically converts the (boolean) true to (string) \"true\" avoiding a\n    parsing failure.\n -- slurmrestd - Add 'POST /slurm/v0.0.41/job/allocate' endpoint. This endpoint\n    will create a new job allocation without any steps. The allocation will need\n    to be ended via signaling the job or it will run to the timelimit.\n -- slurmrestd - Allow startup when slurmdbd is not configured and avoid loading\n    slurmdbd specific plugins.\n\nMPI/PMI2 CHANGES\n================\n -- Jobs submitted with the SLURM_HOSTFILE environment variable set implies\n    using an arbitrary distribution. Nevertheless, the logic used in PMI2 when\n    generating their associated PMI_process_mapping values has been changed and\n    will now be the same used for the plane distribution, as if \"-m plane\" were\n    used. This has been changed because the original arbitrary distribution\n    implementation did not account for multiple instances of the same host being\n    present in SLURM_HOSTFILE, providing an incorrect process mapping in such\n    case. This change also enables distributing tasks in blocks when using\n    arbitrary distribution, which was not the case before. This only affects\n    mpi/pmi2 plugin.\n\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/classic_fair_share.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Classic Fairshare Algorithm",
                "content": "Overview\nAs of the 19.05 release, the Fair Tree algorithm is now the default, and the\nclassic fair share algorithm is only available if\nPriorityFlags=NO_FAIR_TREE has been explicitly configured.\nNormalized Shares\n\nThe fair-share hierarchy represents the portion of the computing resources\nthat have been allocated to different projects. These allocations are assigned\nto an account. There can be multiple levels of allocations made as allocations\nof a given account are further divided to sub-accounts:\n\n  Figure 1. Machine Allocation\nThe chart above shows the resources of the machine allocated to four\naccounts: A, B, C and D. Furthermore, account A's shares are allocated to\nsub-accounts A1 through A4. Users are granted permission (through sacctmgr) to\nsubmit jobs against specific accounts. If there are 10 users given equal shares\nin Account A3, they will each be allocated 1% of the machine.A user's normalized shares are simply:\nS =\t(Suser / Ssiblings) *\n\t(Saccount / Ssibling-accounts) *\n\t(Sparent / Sparent-siblings) * ...\n\n S\n is the user's normalized share, between zero and one\n Suser\n are the number of shares of the account allocated to the user\n Ssiblings\n are the total number of shares allocated to all users permitted to charge the account (including Suser)\n Saccount\n are the number of shares of the parent account allocated to the account\n Ssibling-accounts\n are the total number of shares allocated to all sub-accounts of the parent account\n Sparent\n are the number of shares of the grandparent account allocated to the parent\n Sparent-siblings\n are the total number of shares allocated to all sub-accounts of the grandparent account\nNormalized Usage\n\n The processor*seconds allocated to every job are tracked in real-time.  If one only considered usage over a fixed time period, then calculating a user's normalized usage would be a simple quotient:\n\tUN = Uuser / Utotal\n\n UN\n is normalized usage, between zero and one\n Uuser\n is the processor*seconds consumed by all of a user's jobs in a given account for over a fixed time period\n Utotal\n is the total number of processor*seconds utilized across the cluster during that same time period\n However, significant real-world usage quantities span multiple time periods.  Rather than treating usage over a number of weeks or months with equal importance, Slurm's fair-share priority calculation places more importance on the most recent resource usage and less importance on usage from the distant past. The Slurm usage metric is based off a half-life formula that favors the most recent usage statistics.  Usage statistics from the past decrease in importance based on a single decay factor, D:\n\tUH = Ucurrent_period +\n\t     ( D * Ulast_period) + (D * D * Uperiod-2) + ...\n\n UH\n is the historical usage subject to the half-life decay\n Ucurrent_period\n is the usage charged over the current measurement period\n Ulast_period\n is the usage charged over the last measurement period\n Uperiod-2\n is the usage charged over the second last measurement period\n D\n is a decay factor between zero and one that delivers the\n  half-life decay based off the PriorityDecayHalfLife setting\n  in the slurm.conf file.  Without accruing additional usage, a user's\n  UH usage will decay to half its original value after a time period\n  of PriorityDecayHalfLife seconds.\n In practice, the PriorityDecayHalfLife could be a matter of\nseconds or days as appropriate for each site.  The decay is\nrecalculated every PriorityCalcPeriod minutes, or 5 minutes by\ndefault.  The decay factor, D, is assigned the value that will achieve\nthe half-life decay rate specified by the PriorityDecayHalfLife\nparameter. The total number of processor*seconds utilized can be similarly aggregated with the same decay factor:\n\tRH = Rcurrent_period +\n\t    ( D * Rlast_period) + (D * D * Rperiod-2) + ...\n\n RH\n is the total historical usage subject to the same half-life decay as the usage formula.\n Rcurrent_period\n is the total usage charged over the current measurement period\n Rlast_period\n is the total usage charged over the last measurement period\n Rperiod-2\n is the total usage charged over the second last measurement period\n D\n is the decay factor between zero and one\n A user's normalized usage that spans multiple time periods then becomes:\n\tU = UH / RH\nSimplified Fair-Share Formula\n\n The simplified formula for calculating the fair-share factor for usage that spans multiple time periods and subject to a half-life decay is:\n\tF = 2**(-U/S/d)\n\n F\n is the fair-share factor\n S\n is the normalized shares\n U\n is the normalized usage factoring in half-life decay\n d\n is the FairShareDampeningFactor (a configuration parameter, default value of 1)\n The fair-share factor will therefore range from zero to one, where one represents the highest priority for a job.  A fair-share factor of 0.5 indicates that the user's jobs have used exactly the portion of the machine that they have been allocated.  A fair-share factor of above 0.5 indicates that the user's jobs have consumed less than their allocated share while a fair-share factor below 0.5 indicates that the user's jobs have consumed more than their allocated share of the computing resources.The Fair-share Factor Under An Account Hierarchy\n\n The method described above presents a system whereby the priority of a user's job is calculated based on the portion of the machine allocated to the user and the historical usage of all the jobs run by that user under a specific account. Another layer of \"fairness\" is necessary however, one that factors in the usage of other users drawing from the same account.  This allows a job's fair-share factor to be influenced by the computing resources delivered to jobs of other users drawing from the same account. If there are two members of a given account, and if one of those users has run many jobs under that account, the job priority of a job submitted by the user who has not run any jobs will be negatively affected.  This ensures that the combined usage charged to an account matches the portion of the machine that is allocated to that account. In the example below, when user 3 submits their first job using account C, they will want their job's priority to reflect all the resources delivered to account B.  They do not care that user 1 has been using up a significant portion of the cycles allocated to account B and user 2 has yet to run a job out of account B.  If user 2 submits a job using account B and user 3 submits a job using account C, user 3 expects their job to be scheduled before the job from user 2.\n\n  Figure 2. Usage Example\nThe Slurm Fair-Share Formula\n\n The Slurm fair-share formula has been designed to provide fair scheduling to users based on the allocation and usage of every account. The actual formula used is a refinement of the formula presented above:\n\tF = 2**(-UE/S)\n The difference is that the usage term is effective usage, which is defined as:\n\tUE = UAchild +\n\t\t  ((UEparent - UAchild) * Schild/Sall_siblings)\n\n UE\n is the effective usage of the child user or child account\n UAchild\n is the actual usage of the child user or child account\n UEparent\n is the effective usage of the parent account\n Schild\n is the shares allocated to the child user or child account\n Sall_siblings\n is the shares allocated to all the children of the parent account\n This formula only applies with the second tier of accounts below root.  For the tier of accounts just under root, their effective usage equals their actual usage. Because the formula for effective usage includes a term of the effective usage of the parent, the calculation for each account in the tree must start at the second tier of accounts and proceed downward:  to the children accounts, then grandchildren, etc.  The effective usage of the users will be the last to be calculated. Plugging in the effective usage into the fair-share formula above yields a fair-share factor that reflects the aggregated usage charged to each of the accounts in the fair-share hierarchy.FairShare=parentIt is possible to disable the fairshare at certain levels of the fair share\nhierarchy by using the FairShare=parent option of sacctmgr.\nFor users and accounts with FairShare=parent the normalized shares\nand effective usage values from the parent in the hierarchy will be used when\ncalculating fairshare priories.If all users in an account are configured with FairShare=parent\nthe result is that all the jobs drawing from that account will get the same\nfairshare priority, based on the accounts total usage. No additional fairness\nis added based on a user's individual usage.Example The following example demonstrates the effective usage calculations and resultant fair-share factors. (See Figure 3 below.) The machine's computing resources are allocated to accounts A and D with 40 and 60 shares respectively.  Account A is further divided into two children accounts, B with 30 shares and C with 10 shares.  Account D is further divided into two children accounts, E with 25 shares and F with 35 shares. Note:  the shares at any given tier in the Account hierarchy do not need to total up to 100 shares.  This example shows them totaling up to 100 to make the arithmetic easier to follow in your head. User 1 is granted permission to submit jobs against the B account.  Users 2 and 3 are granted one share each in the C account.  User 4 is the sole member of the E account and User 5 is the sole member of the F account. Note:  accounts A and D do not have any user members in this example, though users could have been assigned.The shares assigned to each account make it easy to determine normalized\nshares of the machine's complete resources. Account A has .4 normalized shares,\nB has .3 normalized shares, etc. Users who are sole members of an account have\nthe same number of normalized shares as the account. (E.g., User 1 has .3\nnormalized shares). Users who share accounts have a portion of the normalized\nshares based on their shares. For example, if user 2 had been allocated 4\nshares instead of 1, user 2 would have had .08 normalized shares. With users 2\nand 3 each holding 1 share, they each have a normalized share of 0.05. Users 1, 2, and 4 have run jobs that have consumed the machine's computing resources.  User 1's actual usage is 0.2 of the machine; user 2 is 0.25,  and user 4 is 0.25. The actual usage charged to each account is represented by the solid arrows.  The actual usage charged to each account is summed as one goes up the tree.  Account C's usage is the sum of the usage of Users 2 and 3; account A's actual usage is the sum of its children, accounts B and C.\n\n  Figure 3. Fair-share Example\n\n User 1 normalized share: 0.3\n User 2 normalized share: 0.05\n User 3 normalized share: 0.05\n User 4 normalized share: 0.25\n User 5 normalized share: 0.35\n As stated above, the effective usage is computed from the formula:\n\tUE = UAchild +\n\t\t  ((UEparent - UAchild) * Schild/Sall_siblings)\n The effective usage for all accounts at the first tier under the root allocation is always equal to the actual usage:\n Account B effective usage: 0.2 + ((0.45 - 0.2) * 30 / 40) = 0.3875\n Account C effective usage: 0.25 + ((0.45 - 0.25) * 10 / 40) = 0.3\n Account E effective usage: 0.25 + ((0.25 - 0.25) * 25 / 60) = 0.25\n Account F effective usage: 0.0 + ((0.25 - 0.0) * 35 / 60) = 0.1458\n The effective usage of each user is calculated using the same formula:\n User 1 effective usage: 0.2 + ((0.3875 - 0.2) * 1 / 1) = 0.3875\n User 2 effective usage: 0.25 + ((0.3 - 0.25) * 1 / 2) =  0.275\n User 3 effective usage: 0.0 + ((0.3 - 0.0) * 1 / 2) =  0.15\n User 4 effective usage: 0.25 + ((0.25 - 0.25) * 1 / 1) = 0.25\n User 5 effective usage: 0.0 + ((.1458 - 0.0) * 1 / 1) =  0.1458\n Using the Slurm fair-share formula,\n\tF = 2**(-UE/S)\n the fair-share factor for each user is:\n User 1 fair-share factor: 2**(-.3875 / .3) =  0.408479\n User 2 fair-share factor: 2**(-.275 / .05) =  0.022097\n User 3 fair-share factor: 2**(-.15 / .05) =   0.125000\n User 4 fair-share factor: 2**(-.25 / .25) =   0.500000\n User 5 fair-share factor: 2**(-.1458 / .35) = 0.749154\n From this example, once can see that users 1,2, and 3 are over-serviced while user 5 is under-serviced.  Even though user 3 has yet to submit a job, his/her fair-share factor is negatively influenced by the jobs users 1 and 2 have run. Based on the fair-share factor alone, if all 5 users were to submit a job charging their respective accounts, user 5's job would be granted the highest scheduling priority.Last modified 11 June 2019"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/hdf5_profile_user_guide.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Profiling Using HDF5 User Guide",
                "content": "Contents\nOverview\nAdministration\nProfiling Jobs\nHDF5\nData Structure\nOverviewThe acct_gather_profile/hdf5 plugin allows Slurm to coordinate collecting\ndata on jobs it runs on a cluster that is more detailed than is practical to\ninclude in its database. The data comes from periodically sampling various\nperformance data either collected by Slurm, the operating system, or\ncomponent software. The plugin will record the data from each source\nas a Time Series and also accumulate totals for each statistic for\nthe job.Time Series are energy data collected by an acct_gather_energy plugin,\nI/O data from a network interface collected by an acct_gather_interconnect\nplugin, I/O data from parallel file systems such as Lustre collected by an\nacct_gather_filesystem plugin, and task performance data such as local disk I/O,\ncpu consumption, and memory use from a jobacct_gather plugin.\nData from other sources may be added in the future.The data is collected into a file on a shared file system for each step on\neach allocated node of a job and then merged into an HDF5 file.\nIndividual files on a shared file system was chosen because it is possible\nthat the data is voluminous so solutions that pass data to the Slurm control\ndaemon via RPC may not scale to very large clusters or jobs with\nmany allocated nodes.Administration\n\nShared File System\nThe HDF5 Profile Plugin requires a common shared file system on all\nthe compute nodes. While a job is running, the plugin writes a\nfile into this file system for each step of the job on each node. When\nthe job ends, the merge process is launched and the node-step files\nare combined into one HDF5 file for the job.\nThe root of the directory structure is declared in the ProfileHDF5Dir\noption in the acct_gather.conf file. The directory will be created by\nSlurm if it doesn't exist.  Each user will have\ntheir own directory created in the ProfileHDF5Dir which contains\nthe HDF5 files.  All the directories and files are created by the\nSlurmdUser which is usually root. The user specific directories, as well\nas the files inside, are chowned to the user running the job so they\ncan access the files.  Since user root is usually creating these\nfiles/directories a root squashed file system will not work for\nthe ProfileHDF5Dir.\nEach user that creates a profile will have a subdirectory in the profile\ndirectory that has read/write permission only for the user.\nConfiguration parameters\n\n\nThe profile plugin is enabled in the\nslurm.conf file and it is internally\nconfigured in the\nacct_gather.conf file.\n\n\n\nslurm.conf parameters\n\n\n\n\nAcctGatherProfileType=acct_gather_profile/hdf5\nEnables the HDF5 plugin.\nJobAcctGatherFrequency=<seconds>\nSets the sampling frequency for data types.\n\n\n\n\n\nacct_gather.conf parameters\n\n\n\nThese parameters are directly used by the HDF5 Profile Plugin.\n\nProfileHDF5Dir=<path>\nThis parameter is the path to the shared folder into which the\nacct_gather_profile plugin will write detailed data as an HDF5 file.\nThe directory is assumed to be on a file system shared by the controller and\nall compute nodes. This is a required parameter.\nProfileHDF5Default=[options]\nA comma-delimited list of data types to be collected for each job\nsubmission. Use this option with caution. A node-step file will be created on\nevery node for every step of every job. They will not automatically be merged\ninto job files. (Even job files for large numbers of small jobs would fill the\nfile system.) This option is intended for test environments where you\nmight want to profile a series of jobs but do not want to have to\nadd the --profile option to the launch scripts.\nThe options are described below and in the man pages for acct_gather.conf,\nsrun, salloc and sbatch commands.\n\n\n\n\n\nTime Series Control Parameters\n\n\n\nOther plugins add time series data to the HDF5 collection. They typically\nhave a default polling frequency specified in slurm.conf in the\nJobAcctGatherFrequency parameter. The polling frequency can be overridden\nusing the --acctg-freq\nsrun parameter.\nThey are both of the form task=sec,energy=sec,filesystem=sec,network=sec.\nThe IPMI energy plugin also needs the EnergyIPMIFrequency value set\nin the acct_gather.conf file. This sets the rate at which the plugin samples\nthe external sensors. This value should be the same as the energy=sec in\neither JobAcctGatherFrequency or --acctg-freq.\nNote that the IPMI and profile sampling are not synchronous.\nThe profile sample simply takes the last available IPMI sample value.\nIf the profile energy sample is more frequent than the IPMI sample rate,\nthe IPMI value will be repeated. If the profile energy sample is greater\nthan the IPMI rate, IPMI values will be lost.\nAlso note that smallest effective IPMI (EnergyIPMIFrequency) sample rate\nfor 2013 era Intel processors is 3 seconds.\n\n\nProfiling Jobs\n\n\nData Collection\n\n\nThe --profile option on salloc|sbatch|srun  controls whether data is\ncollected and what type of data is collected. If --profile is not specified\nno data collected unless the ProfileHDF5Default\noption is used in acct_gather.conf. --profile on the command line overrides\nany value specified in the configuration file.\n\n\n--profile=<all|none|[energy[,|task[,|filesystem[,|network]]]]>\n\nEnables detailed data collection by the acct_gather_profile plugin.\nDetailed data are typically time-series that are stored in a HDF5 file for\nthe job.\n\n\nAll\nAll data types are collected. (Cannot be combined with other values.)\n\nNone\nNo data types are collected. This is the default. (Cannot be\ncombined with other values.)\n\nEnergy\nEnergy data is collected.\nFilesystem\nFilesystem data is collected. Currently only\nLustre filesystem is supported.\nNetwork\nNetwork (InfiniBand) data is collected.\nTask\nTask (I/O, Memory, ...) data is collected.\n\n\n\n\nData Consolidation\n\n\nThe node-step files are merged into one HDF5 file for the job using the\nsh5util.\nIf the job is started with sbatch, the command line may added to the normal\nlaunch script,  For example:\n\nsbatch -n1 -d$SLURM_JOB_ID --wrap=\"sh5util -j $SLURM_JOB_ID\"\n\nData Extraction\n\n\nThe sh5util program can also be used to extract\nspecific data from the HDF5 file and write it in comma separated value (csv)\nform for importation into other analysis tools such as spreadsheets.\nHDF5\nHDF5 is a well known structured data set that allows heterogeneous but\nrelated data to be stored in one file.\n(.i.e. sections for energy statistics, network I/O, Task data, etc.)\nIts internal structure resembles a\nfile system with groups being similar to directories and\ndata sets being similar to files. It also allows attributes\nto be attached to groups to store application defined properties.\nThere are commodity programs, notably\n\nHDFView, for viewing and manipulating these files.\n\nBelow is a screen shot from HDFView expanding the job tree and showing the\nattributes for a specific task.\n\n\nData Structure\n\n\n\n\n\n\n\nIn the job file, there will be a group for each step of the job.\nWithin each step, there will be a group for nodes, and a group for tasks.\n\n\n\nThe nodes group will have a group for each node in the step allocation.\nFor each node group, there is a sub-group for Time Series and another\nfor Totals.\n\n\nThe Time Series group\ncontains a group/dataset containing the time series for each collector.\n\n\nThe Totals group contains a group/dataset that has corresponding\nMinimum, Average, Maximum, and Sum Total for each item in the time series.\n\n\n\nThe Tasks group will only contain a subgroup for each task.\nIt primarily contains an attribute stating the node on which the task was\nexecuted. This set of groups is essentially a cross reference table.\n\n\n\n\nEnergy Data\nAcctGatherEnergyType=acct_gather_energy/ipmi\nis required in slurm.conf to collect energy data.\nAppropriately set energy=freq in either JobAcctGatherFrequency in slurm.conf\nor in --acctg-freq on the command line.\nAlso appropriately set EnergyIPMIFrequency in acct_gather.conf.\nEach data sample in the Energy Time Series contains the following data items.\n\n\nDate Time\nTime of day at which the data sample was taken. This can be used to\ncorrelate activity with other sources such as logs.\nTime\nElapsed time since the beginning of the step.\nPower\nPower consumption during the interval.\nCPU Frequency\nCPU Frequency at time of sample in kilohertz.\n\nFilesystem Data\n\n\nAcctGatherFilesystemType=acct_gather_filesystem/lustre\nis required in slurm.conf to collect task data.\nAppropriately set Filesystem=freq in either JobAcctGatherFrequency in slurm.conf\nor in --acctg-freq on the command line.\nEach data sample in the Filesystem Time Series contains the following data items.\n\n\nDate Time\nTime of day at which the data sample was taken. This can be used to\ncorrelate activity with other sources such as logs.\nTime\nElapsed time since the beginning of the step.\nReads\nNumber of read operations.\nMegabytes Read\nNumber of megabytes read.\nWrites\nNumber of write operations.\nMegabytes Write\nNumber of megabytes written.\n\nNetwork (Infiniband Data)\n\n\nAcctGatherInterconnectType=acct_gather_interconnect/ofed\nis required in slurm.conf to collect task data.\nAppropriately set network=freq in either JobAcctGatherFrequency in slurm.conf\nor in --acctg-freq on the command line.\nEach data sample in the Network Time Series contains the following\ndata items.\n\nDate Time\nTime of day at which the data sample was taken. This can be used to\ncorrelate activity with other sources such as logs.\nTime\nElapsed time since the beginning of the step.\nPackets In\nNumber of packets coming in.\nMegabytes Read\nNumber of megabytes coming in through the interface.\nPackets Out\nNumber of packets going out.\nMegabytes Write\nNumber of megabytes going out through the interface.\n\nTask Data\nJobAcctGatherType=jobacct_gather/linux\nis required in slurm.conf to collect task data.\nAppropriately set task=freq in either JobAcctGatherFrequency in slurm.conf\nor in --acctg-freq on the command line.\nEach data sample in the Task Time Series contains the following data\nitems.\n\nDate Time\nTime of day at which the data sample was taken. This can be used to\ncorrelate activity with other sources such as logs.\nTime\nElapsed time since the beginning of the step.\nCPU Frequency\nCPU Frequency at time of sample.\nCPU Time\nSeconds of CPU time used during the sample.\nCPU Utilization\nCPU Utilization during the interval.\nRSS\nValue of RSS at time of sample.\nVM Size\nValue of VM Size at time of sample.\nPages\nPages used in sample.\nRead Megabytes\nNumber of megabytes read from local disk.\nWrite Megabytes\nNumber of megabytes written to local disk.\n\nLast modified 17 October 2022\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Profiling Jobs\n\n",
                "content": "Data Collection\n\nThe --profile option on salloc|sbatch|srun  controls whether data is\ncollected and what type of data is collected. If --profile is not specified\nno data collected unless the ProfileHDF5Default\noption is used in acct_gather.conf. --profile on the command line overrides\nany value specified in the configuration file.\n\n--profile=<all|none|[energy[,|task[,|filesystem[,|network]]]]>\n\nEnables detailed data collection by the acct_gather_profile plugin.\nDetailed data are typically time-series that are stored in a HDF5 file for\nthe job.\n\n\nAll\nAll data types are collected. (Cannot be combined with other values.)\n\nNone\nNo data types are collected. This is the default. (Cannot be\ncombined with other values.)\n\nEnergy\nEnergy data is collected.\nFilesystem\nFilesystem data is collected. Currently only\nLustre filesystem is supported.\nNetwork\nNetwork (InfiniBand) data is collected.\nTask\nTask (I/O, Memory, ...) data is collected.\n\n\n\nData Consolidation\n\nThe node-step files are merged into one HDF5 file for the job using the\nsh5util.If the job is started with sbatch, the command line may added to the normal\nlaunch script,  For example:\nsbatch -n1 -d$SLURM_JOB_ID --wrap=\"sh5util -j $SLURM_JOB_ID\"\nData Extraction\n\nThe sh5util program can also be used to extract\nspecific data from the HDF5 file and write it in comma separated value (csv)\nform for importation into other analysis tools such as spreadsheets.HDF5HDF5 is a well known structured data set that allows heterogeneous but\nrelated data to be stored in one file.\n(.i.e. sections for energy statistics, network I/O, Task data, etc.)\nIts internal structure resembles a\nfile system with groups being similar to directories and\ndata sets being similar to files. It also allows attributes\nto be attached to groups to store application defined properties.There are commodity programs, notably\n\nHDFView, for viewing and manipulating these files.\n\nBelow is a screen shot from HDFView expanding the job tree and showing the\nattributes for a specific task.\n\n\nData Structure\n\n\n\n\n\n\n\nIn the job file, there will be a group for each step of the job.\nWithin each step, there will be a group for nodes, and a group for tasks.\n\n\n\nThe nodes group will have a group for each node in the step allocation.\nFor each node group, there is a sub-group for Time Series and another\nfor Totals.\n\n\nThe Time Series group\ncontains a group/dataset containing the time series for each collector.\n\n\nThe Totals group contains a group/dataset that has corresponding\nMinimum, Average, Maximum, and Sum Total for each item in the time series.\n\n\n\nThe Tasks group will only contain a subgroup for each task.\nIt primarily contains an attribute stating the node on which the task was\nexecuted. This set of groups is essentially a cross reference table.\n\n\n\n\nEnergy Data\nAcctGatherEnergyType=acct_gather_energy/ipmi\nis required in slurm.conf to collect energy data.\nAppropriately set energy=freq in either JobAcctGatherFrequency in slurm.conf\nor in --acctg-freq on the command line.\nAlso appropriately set EnergyIPMIFrequency in acct_gather.conf.\nEach data sample in the Energy Time Series contains the following data items.\n\n\nDate Time\nTime of day at which the data sample was taken. This can be used to\ncorrelate activity with other sources such as logs.\nTime\nElapsed time since the beginning of the step.\nPower\nPower consumption during the interval.\nCPU Frequency\nCPU Frequency at time of sample in kilohertz.\n\nFilesystem Data\n\n\nAcctGatherFilesystemType=acct_gather_filesystem/lustre\nis required in slurm.conf to collect task data.\nAppropriately set Filesystem=freq in either JobAcctGatherFrequency in slurm.conf\nor in --acctg-freq on the command line.\nEach data sample in the Filesystem Time Series contains the following data items.\n\n\nDate Time\nTime of day at which the data sample was taken. This can be used to\ncorrelate activity with other sources such as logs.\nTime\nElapsed time since the beginning of the step.\nReads\nNumber of read operations.\nMegabytes Read\nNumber of megabytes read.\nWrites\nNumber of write operations.\nMegabytes Write\nNumber of megabytes written.\n\nNetwork (Infiniband Data)\n\n\nAcctGatherInterconnectType=acct_gather_interconnect/ofed\nis required in slurm.conf to collect task data.\nAppropriately set network=freq in either JobAcctGatherFrequency in slurm.conf\nor in --acctg-freq on the command line.\nEach data sample in the Network Time Series contains the following\ndata items.\n\nDate Time\nTime of day at which the data sample was taken. This can be used to\ncorrelate activity with other sources such as logs.\nTime\nElapsed time since the beginning of the step.\nPackets In\nNumber of packets coming in.\nMegabytes Read\nNumber of megabytes coming in through the interface.\nPackets Out\nNumber of packets going out.\nMegabytes Write\nNumber of megabytes going out through the interface.\n\nTask Data\nJobAcctGatherType=jobacct_gather/linux\nis required in slurm.conf to collect task data.\nAppropriately set task=freq in either JobAcctGatherFrequency in slurm.conf\nor in --acctg-freq on the command line.\nEach data sample in the Task Time Series contains the following data\nitems.\n\nDate Time\nTime of day at which the data sample was taken. This can be used to\ncorrelate activity with other sources such as logs.\nTime\nElapsed time since the beginning of the step.\nCPU Frequency\nCPU Frequency at time of sample.\nCPU Time\nSeconds of CPU time used during the sample.\nCPU Utilization\nCPU Utilization during the interval.\nRSS\nValue of RSS at time of sample.\nVM Size\nValue of VM Size at time of sample.\nPages\nPages used in sample.\nRead Megabytes\nNumber of megabytes read from local disk.\nWrite Megabytes\nNumber of megabytes written to local disk.\n\nLast modified 17 October 2022\n"
            },
            {
                "title": "Data Structure\n\n",
                "content": "\n\n\n\n\nIn the job file, there will be a group for each step of the job.\nWithin each step, there will be a group for nodes, and a group for tasks.\n\n\n\nThe nodes group will have a group for each node in the step allocation.\nFor each node group, there is a sub-group for Time Series and another\nfor Totals.\n\n\nThe Time Series group\ncontains a group/dataset containing the time series for each collector.\n\n\nThe Totals group contains a group/dataset that has corresponding\nMinimum, Average, Maximum, and Sum Total for each item in the time series.\n\n\n\nThe Tasks group will only contain a subgroup for each task.\nIt primarily contains an attribute stating the node on which the task was\nexecuted. This set of groups is essentially a cross reference table.\n\n\n\nEnergy DataAcctGatherEnergyType=acct_gather_energy/ipmi\nis required in slurm.conf to collect energy data.\nAppropriately set energy=freq in either JobAcctGatherFrequency in slurm.conf\nor in --acctg-freq on the command line.\nAlso appropriately set EnergyIPMIFrequency in acct_gather.conf.\nEach data sample in the Energy Time Series contains the following data items.\n\n\nDate Time\nTime of day at which the data sample was taken. This can be used to\ncorrelate activity with other sources such as logs.\nTime\nElapsed time since the beginning of the step.\nPower\nPower consumption during the interval.\nCPU Frequency\nCPU Frequency at time of sample in kilohertz.\n\nFilesystem Data\n\n\nAcctGatherFilesystemType=acct_gather_filesystem/lustre\nis required in slurm.conf to collect task data.\nAppropriately set Filesystem=freq in either JobAcctGatherFrequency in slurm.conf\nor in --acctg-freq on the command line.\nEach data sample in the Filesystem Time Series contains the following data items.\n\n\nDate Time\nTime of day at which the data sample was taken. This can be used to\ncorrelate activity with other sources such as logs.\nTime\nElapsed time since the beginning of the step.\nReads\nNumber of read operations.\nMegabytes Read\nNumber of megabytes read.\nWrites\nNumber of write operations.\nMegabytes Write\nNumber of megabytes written.\n\nNetwork (Infiniband Data)\n\n\nAcctGatherInterconnectType=acct_gather_interconnect/ofed\nis required in slurm.conf to collect task data.\nAppropriately set network=freq in either JobAcctGatherFrequency in slurm.conf\nor in --acctg-freq on the command line.\nEach data sample in the Network Time Series contains the following\ndata items.\n\nDate Time\nTime of day at which the data sample was taken. This can be used to\ncorrelate activity with other sources such as logs.\nTime\nElapsed time since the beginning of the step.\nPackets In\nNumber of packets coming in.\nMegabytes Read\nNumber of megabytes coming in through the interface.\nPackets Out\nNumber of packets going out.\nMegabytes Write\nNumber of megabytes going out through the interface.\n\nTask Data\nJobAcctGatherType=jobacct_gather/linux\nis required in slurm.conf to collect task data.\nAppropriately set task=freq in either JobAcctGatherFrequency in slurm.conf\nor in --acctg-freq on the command line.\nEach data sample in the Task Time Series contains the following data\nitems.\n\nDate Time\nTime of day at which the data sample was taken. This can be used to\ncorrelate activity with other sources such as logs.\nTime\nElapsed time since the beginning of the step.\nCPU Frequency\nCPU Frequency at time of sample.\nCPU Time\nSeconds of CPU time used during the sample.\nCPU Utilization\nCPU Utilization during the interval.\nRSS\nValue of RSS at time of sample.\nVM Size\nValue of VM Size at time of sample.\nPages\nPages used in sample.\nRead Megabytes\nNumber of megabytes read from local disk.\nWrite Megabytes\nNumber of megabytes written to local disk.\n\nLast modified 17 October 2022\n"
            },
            {
                "title": "slurm.conf parameters\n\n",
                "content": "\n\nAcctGatherProfileType=acct_gather_profile/hdf5\nEnables the HDF5 plugin.\nJobAcctGatherFrequency=<seconds>\nSets the sampling frequency for data types.\n\n"
            },
            {
                "title": "acct_gather.conf parameters\n\n",
                "content": "\nThese parameters are directly used by the HDF5 Profile Plugin.\n\nProfileHDF5Dir=<path>\nThis parameter is the path to the shared folder into which the\nacct_gather_profile plugin will write detailed data as an HDF5 file.\nThe directory is assumed to be on a file system shared by the controller and\nall compute nodes. This is a required parameter.\nProfileHDF5Default=[options]\nA comma-delimited list of data types to be collected for each job\nsubmission. Use this option with caution. A node-step file will be created on\nevery node for every step of every job. They will not automatically be merged\ninto job files. (Even job files for large numbers of small jobs would fill the\nfile system.) This option is intended for test environments where you\nmight want to profile a series of jobs but do not want to have to\nadd the --profile option to the launch scripts.\nThe options are described below and in the man pages for acct_gather.conf,\nsrun, salloc and sbatch commands.\n\n"
            },
            {
                "title": "Time Series Control Parameters\n\n",
                "content": "\nOther plugins add time series data to the HDF5 collection. They typically\nhave a default polling frequency specified in slurm.conf in the\nJobAcctGatherFrequency parameter. The polling frequency can be overridden\nusing the --acctg-freq\nsrun parameter.\nThey are both of the form task=sec,energy=sec,filesystem=sec,network=sec.\nThe IPMI energy plugin also needs the EnergyIPMIFrequency value set\nin the acct_gather.conf file. This sets the rate at which the plugin samples\nthe external sensors. This value should be the same as the energy=sec in\neither JobAcctGatherFrequency or --acctg-freq.\nNote that the IPMI and profile sampling are not synchronous.\nThe profile sample simply takes the last available IPMI sample value.\nIf the profile energy sample is more frequent than the IPMI sample rate,\nthe IPMI value will be repeated. If the profile energy sample is greater\nthan the IPMI rate, IPMI values will be lost.\nAlso note that smallest effective IPMI (EnergyIPMIFrequency) sample rate\nfor 2013 era Intel processors is 3 seconds.\n"
            },
            {
                "title": "Filesystem Data\n\n",
                "content": "AcctGatherFilesystemType=acct_gather_filesystem/lustre\nis required in slurm.conf to collect task data.\nAppropriately set Filesystem=freq in either JobAcctGatherFrequency in slurm.conf\nor in --acctg-freq on the command line.\nEach data sample in the Filesystem Time Series contains the following data items.\n\n\nDate Time\nTime of day at which the data sample was taken. This can be used to\ncorrelate activity with other sources such as logs.\nTime\nElapsed time since the beginning of the step.\nReads\nNumber of read operations.\nMegabytes Read\nNumber of megabytes read.\nWrites\nNumber of write operations.\nMegabytes Write\nNumber of megabytes written.\n\nNetwork (Infiniband Data)\n\n\nAcctGatherInterconnectType=acct_gather_interconnect/ofed\nis required in slurm.conf to collect task data.\nAppropriately set network=freq in either JobAcctGatherFrequency in slurm.conf\nor in --acctg-freq on the command line.\nEach data sample in the Network Time Series contains the following\ndata items.\n\nDate Time\nTime of day at which the data sample was taken. This can be used to\ncorrelate activity with other sources such as logs.\nTime\nElapsed time since the beginning of the step.\nPackets In\nNumber of packets coming in.\nMegabytes Read\nNumber of megabytes coming in through the interface.\nPackets Out\nNumber of packets going out.\nMegabytes Write\nNumber of megabytes going out through the interface.\n\nTask Data\nJobAcctGatherType=jobacct_gather/linux\nis required in slurm.conf to collect task data.\nAppropriately set task=freq in either JobAcctGatherFrequency in slurm.conf\nor in --acctg-freq on the command line.\nEach data sample in the Task Time Series contains the following data\nitems.\n\nDate Time\nTime of day at which the data sample was taken. This can be used to\ncorrelate activity with other sources such as logs.\nTime\nElapsed time since the beginning of the step.\nCPU Frequency\nCPU Frequency at time of sample.\nCPU Time\nSeconds of CPU time used during the sample.\nCPU Utilization\nCPU Utilization during the interval.\nRSS\nValue of RSS at time of sample.\nVM Size\nValue of VM Size at time of sample.\nPages\nPages used in sample.\nRead Megabytes\nNumber of megabytes read from local disk.\nWrite Megabytes\nNumber of megabytes written to local disk.\n\nLast modified 17 October 2022\n"
            },
            {
                "title": "Network (Infiniband Data)\n\n",
                "content": "AcctGatherInterconnectType=acct_gather_interconnect/ofed\nis required in slurm.conf to collect task data.\nAppropriately set network=freq in either JobAcctGatherFrequency in slurm.conf\nor in --acctg-freq on the command line.\nEach data sample in the Network Time Series contains the following\ndata items.\n\nDate Time\nTime of day at which the data sample was taken. This can be used to\ncorrelate activity with other sources such as logs.\nTime\nElapsed time since the beginning of the step.\nPackets In\nNumber of packets coming in.\nMegabytes Read\nNumber of megabytes coming in through the interface.\nPackets Out\nNumber of packets going out.\nMegabytes Write\nNumber of megabytes going out through the interface.\n\nTask Data\nJobAcctGatherType=jobacct_gather/linux\nis required in slurm.conf to collect task data.\nAppropriately set task=freq in either JobAcctGatherFrequency in slurm.conf\nor in --acctg-freq on the command line.\nEach data sample in the Task Time Series contains the following data\nitems.\n\nDate Time\nTime of day at which the data sample was taken. This can be used to\ncorrelate activity with other sources such as logs.\nTime\nElapsed time since the beginning of the step.\nCPU Frequency\nCPU Frequency at time of sample.\nCPU Time\nSeconds of CPU time used during the sample.\nCPU Utilization\nCPU Utilization during the interval.\nRSS\nValue of RSS at time of sample.\nVM Size\nValue of VM Size at time of sample.\nPages\nPages used in sample.\nRead Megabytes\nNumber of megabytes read from local disk.\nWrite Megabytes\nNumber of megabytes written to local disk.\n\nLast modified 17 October 2022\n"
            },
            {
                "title": "Task Data",
                "content": "JobAcctGatherType=jobacct_gather/linux\nis required in slurm.conf to collect task data.\nAppropriately set task=freq in either JobAcctGatherFrequency in slurm.conf\nor in --acctg-freq on the command line.\nEach data sample in the Task Time Series contains the following data\nitems.\n\nDate Time\nTime of day at which the data sample was taken. This can be used to\ncorrelate activity with other sources such as logs.\nTime\nElapsed time since the beginning of the step.\nCPU Frequency\nCPU Frequency at time of sample.\nCPU Time\nSeconds of CPU time used during the sample.\nCPU Utilization\nCPU Utilization during the interval.\nRSS\nValue of RSS at time of sample.\nVM Size\nValue of VM Size at time of sample.\nPages\nPages used in sample.\nRead Megabytes\nNumber of megabytes read from local disk.\nWrite Megabytes\nNumber of megabytes written to local disk.\n\nLast modified 17 October 2022\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/programmer_guide.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Slurm Programmer's Guide",
                "content": "Contents\nOverview\nPlugins\nDirectory Structure\nDocumentation\nSource Code\nSource Code Management\nAdding New Modules\nCompiling\nConfiguration\nTest Suite\nAdding Files and Directories\nTricks of the Trade\nOverviewSlurm is an open source, fault-tolerant,\nand highly scalable cluster management and job scheduling system for large and\nsmall Linux clusters. Components include machine status, partition management,\njob management, scheduling, and stream copy modules. Slurm requires no kernel\nmodifications for it operation and is relatively self-contained.\n\nSlurm is written in the C language and uses a GNU autoconf configuration\nengine. While initially written for Linux, other UNIX-like operating systems should\nbe easy porting targets. Code should adhere to the \nLinux kernel coding style. (Some components of Slurm have been taken from\nvarious sources. Some of these components do not conform to the Linux kernel\ncoding style. However, new code written for Slurm should follow these standards.)\nPlugins\nTo make the use of different infrastructures possible, Slurm uses a general\npurpose plugin mechanism. A Slurm plugin is a dynamically linked code object that\nis loaded explicitly at run time by the Slurm libraries. It provides a customized\nimplementation of a well-defined API connected to tasks such as authentication,\ninterconnect fabric, task scheduling, etc. A set of functions is defined for use\nby all of the different infrastructures of a particular variety. When a Slurm\ndaemon is initiated, it reads the configuration file to determine which of the\navailable plugins should be used. A plugin developer's\nguide is available with general information about plugins.\n\n\nDirectory Structure\n\n\nThe contents of the Slurm directory structure will be described below in increasing\ndetail as the structure is descended. The top level directory contains the scripts\nand tools required to build the entire Slurm system. It also contains a variety\nof subdirectories for each type of file.\nGeneral build tools/files include: acinclude.m4,\nconfigure.ac, Makefile.am, Make-rpm.mk, META, README,\nslurm.spec.in, and the contents of the auxdir directory. autoconf\nand make commands are used to build and install\nSlurm in an automated fashion.\nNOTE: autoconf\nversion 2.52 or higher is required to build Slurm. Execute\nautoconf -V to check your version number.\nThe build process is described in the README file.\n\nCopyright and disclaimer information are in the files COPYING and DISCLAIMER.\nAll of the top-level subdirectories are described below.\nauxdir \u2014 Used for building Slurm.\ncontribs \u2014 Various contributed tools.\ndoc \u2014 Documentation including man pages. \netc \u2014 Sample configuration files.\nslurm \u2014 Header files for API use. These files must be installed. Placing\nthese header files in this location makes for better code portability.\nsrc \u2014 Contains all source code and header files not in the \"slurm\" subdirectory\ndescribed above.\ntestsuite \u2014 Check, Expect and Pytest tests are here.\nDocumentation\n\n\nAll of the documentation is in the subdirectory doc.\nTwo directories are of particular interest:\n\ndoc/man \u2014 contains the man pages for the APIs,\nconfiguration file, commands, and daemons.\ndoc/html \u2014 contains the web pages.\nSource Code\n\n\nFunctions are divided into several categories, each in its own subdirectory.\nThe details of each directory's contents are provided below. The directories are\nas follows: \n\napi \u2014 Application Program Interfaces into\nthe Slurm code. Used to send and get Slurm information from the central manager.\nThese are the functions user applications might utilize.\ncommon \u2014 General purpose functions for widespread use throughout\nSlurm.\ndatabase \u2014 Various database files that support the accounting\n storage plugin.\nplugins \u2014 Plugin functions for various infrastructures or optional\nbehavior. A separate subdirectory is used for each plugin class:\n\naccounting_storage for specifying the type of storage for accounting,\nauth for user authentication,\ncred for job credential functions,\njobacct_gather for job accounting,\njobcomp for job completion logging,\nmpi for MPI support,\npriority calculates job priority based on a number of factors\nincluding fair-share,\nproctrack for process tracking,\nsched for job scheduler,\nselect for a job's node selection,\nswitch for switch (interconnect) specific functions,\ntask for task affinity to processors,\ntopology methods for assigning nodes to jobs based on node\ntopology.\n\n\nsacct \u2014 User command to view accounting information about jobs.\nsacctmgr \u2014 User and administrator tool to manage accounting.\nsalloc \u2014 User command to allocate resources for a job.\nsattach \u2014 User command to attach standard input, output and error\nfiles to a running job or job step.\nsbatch \u2014 User command to submit a batch job (script for later execution).\nsbcast \u2014 User command to broadcast a file to all nodes associated\nwith an existing Slurm job.\nscancel \u2014 User command to cancel (or signal) a job or job step.\nscontrol \u2014 Administrator tool to manage Slurm.\nsinfo \u2014 User command to get information on Slurm nodes and partitions.\nslurmctld \u2014 Slurm central manager daemon code.\nslurmd \u2014 Slurm daemon code to manage the compute server nodes including\nthe execution of user applications.\nslurmdbd \u2014 Slurm database daemon managing access to the accounting\nstorage database.\nsprio \u2014 User command to see the breakdown of a job's priority\ncalculation when the Multifactor Job Priority plugin is installed.\nsqueue \u2014 User command to get information on Slurm jobs and job steps.\nsreport \u2014 User command to view various reports about past\nusage across the enterprise.\nsrun \u2014 User command to submit a job, get an allocation, and/or\ninitiation a parallel job step.\nsshare \u2014 User command to view shares and usage when the Multifactor\nJob Priority plugin is installed.\nsstat \u2014 User command to view detailed statistics about running\njobs when a Job Accounting Gather plugin is installed.\nstrigger \u2014 User and administrator tool to manage event triggers.\nsview \u2014 User command to view and update node, partition, and\njob state information.\nSource Code Management\n\n\nThe latest code is in github:\nhttps://github.com/SchedMD/slurm.\nCreating your own branch will make it easier to keep it synchronized\nwith our work.\nAdding New Modules\n\n\nAdd the new file name to the Makefile.am file in the appropriate directory.\nThen execute autoreconf (at the top level of the Slurm source directory).\nNote that a relatively current version of automake is required.\nThe autoreconf program will build Makefile.in files from the Makefile.am files.\nIf any new files need to be installed, update the slurm.spec file to identify\nthe RPM in which the new files should be placed\nIf new directories need to be added, add to the configure.ac file the path\nto the Makefile to be built in the new directory. In summary:\nautoreconf translates .am files into .in files\nconfigure translates .in files, adding paths and version numbers.\nCompiling\nSending the standard output of \"make\" to a file makes it easier to see any\nwarning or error messages:\n\"make -j install >make.out\"\nConfiguration\n\n\nSample configuration files are included in the etc subdirectory.\nThe slurm.conf can be built using a configuration tool.\nSee doc/man/man5/slurm.conf.5 and the man pages for other configuration files\nfor more details.\ninit.d.slurm is a script that determines which\nSlurm daemon(s) should execute on any node based upon the configuration file contents.\nIt will also manage these daemons: starting, signaling, restarting, and stopping them.\nTest Suite\nThe testsuite files use Check, Expect and Pytest for testing Slurm in\ndifferents ways.\nThe Check tests are designed to unit test C code. Only with\nmake check, without Slurm installed, they will validate that key C\nfunctions work correctly.\nWe also have a set of Expect Slurm tests available under the testsuite/expect\ndirectory.  These tests are executed after Slurm has been installed\nand the daemons initiated. These tests exercise all Slurm commands\nand options including stress tests.  The file testsuite/expect/globals\ncontains the Expect test framewrok for all of the individual tests.  At\nthe very least, you will need to set the slurm_dir variable to the correct\nvalue.  To avoid conflicts with other developers, you can override variable settings\nin a separate file named testsuite/expect/globals.local.\nSet your working directory to testsuite/expect before\nstarting these tests.  Tests may be executed individually by name\n(e.g.  test1.1)\nor the full test suite may be executed with the single command\nregression.py.\nSee testsuite/expect/README for more information.\nSlurm also has a Pytest environment that can work like the Expect one, but it\nalso works together with an external QA framework to improve the overall QA\nof Slurm.\nAdding Files and Directories\n\n\nIf you are adding files and directories to Slurm, it will be necessary to\nre-build configuration files before executing the configure command.\nUpdate Makefile.am files as needed then execute\nautoreconf before executing configure.\n\nTricks of the Trade\n\n\nHAVE_FRONT_END\n\n\nYou can make a single node appear to Slurm as a Linux cluster by running\nconfigure with the --enable-front-end option. This\ndefines HAVE_FRONT_END with a non-zero value in the file config.h.\nAll (fake) nodes should be defined in the slurm.conf file.\nThese nodes should be configured with a single NodeAddr value\nindicating the node on which single slurmd daemon\nexecutes.  Initiate one slurmd and one\nslurmctld daemon. Do not initiate too many\nsimultaneous job steps to avoid overloading the\nslurmd daemon executing them all.\nMultiple slurmd support\n\n\nIt is possible to run multiple slurmd daemons on a single node, each using\na different port number and NodeName alias.  This is very useful for testing\nnetworking and protocol changes, or anytime you want to simulate a larger\ncluster than you really have.  The author uses this on his desktop to simulate\nmultiple nodes.  However, it is important to note that not all slurm functions\nwill work with multiple slurmd support enabled (e.g. many switch plugins will\nnot work, it is best not to use any).\nMultiple support is enabled at configure-time with the\n\"--enable-multiple-slurmd\" parameter.  This enables a new parameter in the\nslurm.conf file on the NodeName line, \"Port=\", and adds a new\ncommand line parameter to slurmd, \"-N\".\nEach slurmd needs to have its own NodeName, and its own TCP port number. Here\nis an example of the NodeName lines for running three slurmd daemons on each\nof ten nodes:\n\nNodeName=foo[1-10] NodeHostname=host[1-10]  Port=17001\nNodeName=foo[11-20] NodeHostname=host[1-10] Port=17002\nNodeName=foo[21-30] NodeHostname=host[1-10] Port=17003\n\n\nIt is likely that you will also want to use the \"%n\" symbol in any slurmd\nrelated paths in the slurm.conf file, for instance SlurmdLogFile,\nSlurmdPidFile, and especially SlurmdSpoolDir.  Each slurmd replaces the \"%n\"\nwith its own NodeName.  Here is an example:\n\nSlurmdLogFile=/var/log/slurm/slurmd.%n.log\nSlurmdPidFile=/var/run/slurmd.%n.pid\nSlurmdSpoolDir=/var/spool/slurmd.%n\n\n\nYou can manually start each slurmd daemon with the proper NodeName.\nFor example, to start the slurmd daemons for host1 from the\nabove slurm.conf example:\n\nhost1> slurmd -N foo1\nhost1> slurmd -N foo11\nhost1> slurmd -N foo21\n\n\nIf you have SysV init scripts, slurmd daemons will automatically be started by\nwhenever MULTIPLE_SLURMD is set to yes in /etc/sysconfig/slurm.\nIf your distribution uses systemd, you may want to use templating feature to\ndefine one slurmd.service file and registering each of your virtual nodes\nwithin it, for example:\n\n[Unit]\nDescription=Slurm node daemon\nAfter=network.target munge.service remote-fs.target\nConditionPathExists=/etc/slurm.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmd\nExecStart=/usr/sbin/slurmd -N%i $SLURMD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurmd-%i.pid\nKillMode=process\nLimitNOFILE=51200\nLimitMEMLOCK=infinity\nLimitSTACK=infinity\nDelegate=yes\n\n[Install]\nWantedBy=multi-user.target\n\nThen, enabling/managing a service like this (what is %i in the file\nwill be replaced by what is after the @ in the command line):\n\nsystemctl enable slurmd@nodeXYZ\nsystemctl start/stop/restart slurmd@nodeXYZ\n\n\nLast modified 6 August 2021\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Plugins",
                "content": "To make the use of different infrastructures possible, Slurm uses a general\npurpose plugin mechanism. A Slurm plugin is a dynamically linked code object that\nis loaded explicitly at run time by the Slurm libraries. It provides a customized\nimplementation of a well-defined API connected to tasks such as authentication,\ninterconnect fabric, task scheduling, etc. A set of functions is defined for use\nby all of the different infrastructures of a particular variety. When a Slurm\ndaemon is initiated, it reads the configuration file to determine which of the\navailable plugins should be used. A plugin developer's\nguide is available with general information about plugins.\n\n\nDirectory Structure\n\n\nThe contents of the Slurm directory structure will be described below in increasing\ndetail as the structure is descended. The top level directory contains the scripts\nand tools required to build the entire Slurm system. It also contains a variety\nof subdirectories for each type of file.\nGeneral build tools/files include: acinclude.m4,\nconfigure.ac, Makefile.am, Make-rpm.mk, META, README,\nslurm.spec.in, and the contents of the auxdir directory. autoconf\nand make commands are used to build and install\nSlurm in an automated fashion.\nNOTE: autoconf\nversion 2.52 or higher is required to build Slurm. Execute\nautoconf -V to check your version number.\nThe build process is described in the README file.\n\nCopyright and disclaimer information are in the files COPYING and DISCLAIMER.\nAll of the top-level subdirectories are described below.\nauxdir \u2014 Used for building Slurm.\ncontribs \u2014 Various contributed tools.\ndoc \u2014 Documentation including man pages. \netc \u2014 Sample configuration files.\nslurm \u2014 Header files for API use. These files must be installed. Placing\nthese header files in this location makes for better code portability.\nsrc \u2014 Contains all source code and header files not in the \"slurm\" subdirectory\ndescribed above.\ntestsuite \u2014 Check, Expect and Pytest tests are here.\nDocumentation\n\n\nAll of the documentation is in the subdirectory doc.\nTwo directories are of particular interest:\n\ndoc/man \u2014 contains the man pages for the APIs,\nconfiguration file, commands, and daemons.\ndoc/html \u2014 contains the web pages.\nSource Code\n\n\nFunctions are divided into several categories, each in its own subdirectory.\nThe details of each directory's contents are provided below. The directories are\nas follows: \n\napi \u2014 Application Program Interfaces into\nthe Slurm code. Used to send and get Slurm information from the central manager.\nThese are the functions user applications might utilize.\ncommon \u2014 General purpose functions for widespread use throughout\nSlurm.\ndatabase \u2014 Various database files that support the accounting\n storage plugin.\nplugins \u2014 Plugin functions for various infrastructures or optional\nbehavior. A separate subdirectory is used for each plugin class:\n\naccounting_storage for specifying the type of storage for accounting,\nauth for user authentication,\ncred for job credential functions,\njobacct_gather for job accounting,\njobcomp for job completion logging,\nmpi for MPI support,\npriority calculates job priority based on a number of factors\nincluding fair-share,\nproctrack for process tracking,\nsched for job scheduler,\nselect for a job's node selection,\nswitch for switch (interconnect) specific functions,\ntask for task affinity to processors,\ntopology methods for assigning nodes to jobs based on node\ntopology.\n\n\nsacct \u2014 User command to view accounting information about jobs.\nsacctmgr \u2014 User and administrator tool to manage accounting.\nsalloc \u2014 User command to allocate resources for a job.\nsattach \u2014 User command to attach standard input, output and error\nfiles to a running job or job step.\nsbatch \u2014 User command to submit a batch job (script for later execution).\nsbcast \u2014 User command to broadcast a file to all nodes associated\nwith an existing Slurm job.\nscancel \u2014 User command to cancel (or signal) a job or job step.\nscontrol \u2014 Administrator tool to manage Slurm.\nsinfo \u2014 User command to get information on Slurm nodes and partitions.\nslurmctld \u2014 Slurm central manager daemon code.\nslurmd \u2014 Slurm daemon code to manage the compute server nodes including\nthe execution of user applications.\nslurmdbd \u2014 Slurm database daemon managing access to the accounting\nstorage database.\nsprio \u2014 User command to see the breakdown of a job's priority\ncalculation when the Multifactor Job Priority plugin is installed.\nsqueue \u2014 User command to get information on Slurm jobs and job steps.\nsreport \u2014 User command to view various reports about past\nusage across the enterprise.\nsrun \u2014 User command to submit a job, get an allocation, and/or\ninitiation a parallel job step.\nsshare \u2014 User command to view shares and usage when the Multifactor\nJob Priority plugin is installed.\nsstat \u2014 User command to view detailed statistics about running\njobs when a Job Accounting Gather plugin is installed.\nstrigger \u2014 User and administrator tool to manage event triggers.\nsview \u2014 User command to view and update node, partition, and\njob state information.\nSource Code Management\n\n\nThe latest code is in github:\nhttps://github.com/SchedMD/slurm.\nCreating your own branch will make it easier to keep it synchronized\nwith our work.\nAdding New Modules\n\n\nAdd the new file name to the Makefile.am file in the appropriate directory.\nThen execute autoreconf (at the top level of the Slurm source directory).\nNote that a relatively current version of automake is required.\nThe autoreconf program will build Makefile.in files from the Makefile.am files.\nIf any new files need to be installed, update the slurm.spec file to identify\nthe RPM in which the new files should be placed\nIf new directories need to be added, add to the configure.ac file the path\nto the Makefile to be built in the new directory. In summary:\nautoreconf translates .am files into .in files\nconfigure translates .in files, adding paths and version numbers.\nCompiling\nSending the standard output of \"make\" to a file makes it easier to see any\nwarning or error messages:\n\"make -j install >make.out\"\nConfiguration\n\n\nSample configuration files are included in the etc subdirectory.\nThe slurm.conf can be built using a configuration tool.\nSee doc/man/man5/slurm.conf.5 and the man pages for other configuration files\nfor more details.\ninit.d.slurm is a script that determines which\nSlurm daemon(s) should execute on any node based upon the configuration file contents.\nIt will also manage these daemons: starting, signaling, restarting, and stopping them.\nTest Suite\nThe testsuite files use Check, Expect and Pytest for testing Slurm in\ndifferents ways.\nThe Check tests are designed to unit test C code. Only with\nmake check, without Slurm installed, they will validate that key C\nfunctions work correctly.\nWe also have a set of Expect Slurm tests available under the testsuite/expect\ndirectory.  These tests are executed after Slurm has been installed\nand the daemons initiated. These tests exercise all Slurm commands\nand options including stress tests.  The file testsuite/expect/globals\ncontains the Expect test framewrok for all of the individual tests.  At\nthe very least, you will need to set the slurm_dir variable to the correct\nvalue.  To avoid conflicts with other developers, you can override variable settings\nin a separate file named testsuite/expect/globals.local.\nSet your working directory to testsuite/expect before\nstarting these tests.  Tests may be executed individually by name\n(e.g.  test1.1)\nor the full test suite may be executed with the single command\nregression.py.\nSee testsuite/expect/README for more information.\nSlurm also has a Pytest environment that can work like the Expect one, but it\nalso works together with an external QA framework to improve the overall QA\nof Slurm.\nAdding Files and Directories\n\n\nIf you are adding files and directories to Slurm, it will be necessary to\nre-build configuration files before executing the configure command.\nUpdate Makefile.am files as needed then execute\nautoreconf before executing configure.\n\nTricks of the Trade\n\n\nHAVE_FRONT_END\n\n\nYou can make a single node appear to Slurm as a Linux cluster by running\nconfigure with the --enable-front-end option. This\ndefines HAVE_FRONT_END with a non-zero value in the file config.h.\nAll (fake) nodes should be defined in the slurm.conf file.\nThese nodes should be configured with a single NodeAddr value\nindicating the node on which single slurmd daemon\nexecutes.  Initiate one slurmd and one\nslurmctld daemon. Do not initiate too many\nsimultaneous job steps to avoid overloading the\nslurmd daemon executing them all.\nMultiple slurmd support\n\n\nIt is possible to run multiple slurmd daemons on a single node, each using\na different port number and NodeName alias.  This is very useful for testing\nnetworking and protocol changes, or anytime you want to simulate a larger\ncluster than you really have.  The author uses this on his desktop to simulate\nmultiple nodes.  However, it is important to note that not all slurm functions\nwill work with multiple slurmd support enabled (e.g. many switch plugins will\nnot work, it is best not to use any).\nMultiple support is enabled at configure-time with the\n\"--enable-multiple-slurmd\" parameter.  This enables a new parameter in the\nslurm.conf file on the NodeName line, \"Port=\", and adds a new\ncommand line parameter to slurmd, \"-N\".\nEach slurmd needs to have its own NodeName, and its own TCP port number. Here\nis an example of the NodeName lines for running three slurmd daemons on each\nof ten nodes:\n\nNodeName=foo[1-10] NodeHostname=host[1-10]  Port=17001\nNodeName=foo[11-20] NodeHostname=host[1-10] Port=17002\nNodeName=foo[21-30] NodeHostname=host[1-10] Port=17003\n\n\nIt is likely that you will also want to use the \"%n\" symbol in any slurmd\nrelated paths in the slurm.conf file, for instance SlurmdLogFile,\nSlurmdPidFile, and especially SlurmdSpoolDir.  Each slurmd replaces the \"%n\"\nwith its own NodeName.  Here is an example:\n\nSlurmdLogFile=/var/log/slurm/slurmd.%n.log\nSlurmdPidFile=/var/run/slurmd.%n.pid\nSlurmdSpoolDir=/var/spool/slurmd.%n\n\n\nYou can manually start each slurmd daemon with the proper NodeName.\nFor example, to start the slurmd daemons for host1 from the\nabove slurm.conf example:\n\nhost1> slurmd -N foo1\nhost1> slurmd -N foo11\nhost1> slurmd -N foo21\n\n\nIf you have SysV init scripts, slurmd daemons will automatically be started by\nwhenever MULTIPLE_SLURMD is set to yes in /etc/sysconfig/slurm.\nIf your distribution uses systemd, you may want to use templating feature to\ndefine one slurmd.service file and registering each of your virtual nodes\nwithin it, for example:\n\n[Unit]\nDescription=Slurm node daemon\nAfter=network.target munge.service remote-fs.target\nConditionPathExists=/etc/slurm.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmd\nExecStart=/usr/sbin/slurmd -N%i $SLURMD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurmd-%i.pid\nKillMode=process\nLimitNOFILE=51200\nLimitMEMLOCK=infinity\nLimitSTACK=infinity\nDelegate=yes\n\n[Install]\nWantedBy=multi-user.target\n\nThen, enabling/managing a service like this (what is %i in the file\nwill be replaced by what is after the @ in the command line):\n\nsystemctl enable slurmd@nodeXYZ\nsystemctl start/stop/restart slurmd@nodeXYZ\n\n\nLast modified 6 August 2021\n"
            },
            {
                "title": "Directory Structure\n\n",
                "content": "The contents of the Slurm directory structure will be described below in increasing\ndetail as the structure is descended. The top level directory contains the scripts\nand tools required to build the entire Slurm system. It also contains a variety\nof subdirectories for each type of file.General build tools/files include: acinclude.m4,\nconfigure.ac, Makefile.am, Make-rpm.mk, META, README,\nslurm.spec.in, and the contents of the auxdir directory. autoconf\nand make commands are used to build and install\nSlurm in an automated fashion.\nNOTE: autoconf\nversion 2.52 or higher is required to build Slurm. Execute\nautoconf -V to check your version number.\nThe build process is described in the README file.\n\nCopyright and disclaimer information are in the files COPYING and DISCLAIMER.\nAll of the top-level subdirectories are described below.\nauxdir \u2014 Used for building Slurm.\ncontribs \u2014 Various contributed tools.\ndoc \u2014 Documentation including man pages. \netc \u2014 Sample configuration files.\nslurm \u2014 Header files for API use. These files must be installed. Placing\nthese header files in this location makes for better code portability.\nsrc \u2014 Contains all source code and header files not in the \"slurm\" subdirectory\ndescribed above.\ntestsuite \u2014 Check, Expect and Pytest tests are here.\nDocumentation\n\n\nAll of the documentation is in the subdirectory doc.\nTwo directories are of particular interest:\n\ndoc/man \u2014 contains the man pages for the APIs,\nconfiguration file, commands, and daemons.\ndoc/html \u2014 contains the web pages.\nSource Code\n\n\nFunctions are divided into several categories, each in its own subdirectory.\nThe details of each directory's contents are provided below. The directories are\nas follows: \n\napi \u2014 Application Program Interfaces into\nthe Slurm code. Used to send and get Slurm information from the central manager.\nThese are the functions user applications might utilize.\ncommon \u2014 General purpose functions for widespread use throughout\nSlurm.\ndatabase \u2014 Various database files that support the accounting\n storage plugin.\nplugins \u2014 Plugin functions for various infrastructures or optional\nbehavior. A separate subdirectory is used for each plugin class:\n\naccounting_storage for specifying the type of storage for accounting,\nauth for user authentication,\ncred for job credential functions,\njobacct_gather for job accounting,\njobcomp for job completion logging,\nmpi for MPI support,\npriority calculates job priority based on a number of factors\nincluding fair-share,\nproctrack for process tracking,\nsched for job scheduler,\nselect for a job's node selection,\nswitch for switch (interconnect) specific functions,\ntask for task affinity to processors,\ntopology methods for assigning nodes to jobs based on node\ntopology.\n\n\nsacct \u2014 User command to view accounting information about jobs.\nsacctmgr \u2014 User and administrator tool to manage accounting.\nsalloc \u2014 User command to allocate resources for a job.\nsattach \u2014 User command to attach standard input, output and error\nfiles to a running job or job step.\nsbatch \u2014 User command to submit a batch job (script for later execution).\nsbcast \u2014 User command to broadcast a file to all nodes associated\nwith an existing Slurm job.\nscancel \u2014 User command to cancel (or signal) a job or job step.\nscontrol \u2014 Administrator tool to manage Slurm.\nsinfo \u2014 User command to get information on Slurm nodes and partitions.\nslurmctld \u2014 Slurm central manager daemon code.\nslurmd \u2014 Slurm daemon code to manage the compute server nodes including\nthe execution of user applications.\nslurmdbd \u2014 Slurm database daemon managing access to the accounting\nstorage database.\nsprio \u2014 User command to see the breakdown of a job's priority\ncalculation when the Multifactor Job Priority plugin is installed.\nsqueue \u2014 User command to get information on Slurm jobs and job steps.\nsreport \u2014 User command to view various reports about past\nusage across the enterprise.\nsrun \u2014 User command to submit a job, get an allocation, and/or\ninitiation a parallel job step.\nsshare \u2014 User command to view shares and usage when the Multifactor\nJob Priority plugin is installed.\nsstat \u2014 User command to view detailed statistics about running\njobs when a Job Accounting Gather plugin is installed.\nstrigger \u2014 User and administrator tool to manage event triggers.\nsview \u2014 User command to view and update node, partition, and\njob state information.\nSource Code Management\n\n\nThe latest code is in github:\nhttps://github.com/SchedMD/slurm.\nCreating your own branch will make it easier to keep it synchronized\nwith our work.\nAdding New Modules\n\n\nAdd the new file name to the Makefile.am file in the appropriate directory.\nThen execute autoreconf (at the top level of the Slurm source directory).\nNote that a relatively current version of automake is required.\nThe autoreconf program will build Makefile.in files from the Makefile.am files.\nIf any new files need to be installed, update the slurm.spec file to identify\nthe RPM in which the new files should be placed\nIf new directories need to be added, add to the configure.ac file the path\nto the Makefile to be built in the new directory. In summary:\nautoreconf translates .am files into .in files\nconfigure translates .in files, adding paths and version numbers.\nCompiling\nSending the standard output of \"make\" to a file makes it easier to see any\nwarning or error messages:\n\"make -j install >make.out\"\nConfiguration\n\n\nSample configuration files are included in the etc subdirectory.\nThe slurm.conf can be built using a configuration tool.\nSee doc/man/man5/slurm.conf.5 and the man pages for other configuration files\nfor more details.\ninit.d.slurm is a script that determines which\nSlurm daemon(s) should execute on any node based upon the configuration file contents.\nIt will also manage these daemons: starting, signaling, restarting, and stopping them.\nTest Suite\nThe testsuite files use Check, Expect and Pytest for testing Slurm in\ndifferents ways.\nThe Check tests are designed to unit test C code. Only with\nmake check, without Slurm installed, they will validate that key C\nfunctions work correctly.\nWe also have a set of Expect Slurm tests available under the testsuite/expect\ndirectory.  These tests are executed after Slurm has been installed\nand the daemons initiated. These tests exercise all Slurm commands\nand options including stress tests.  The file testsuite/expect/globals\ncontains the Expect test framewrok for all of the individual tests.  At\nthe very least, you will need to set the slurm_dir variable to the correct\nvalue.  To avoid conflicts with other developers, you can override variable settings\nin a separate file named testsuite/expect/globals.local.\nSet your working directory to testsuite/expect before\nstarting these tests.  Tests may be executed individually by name\n(e.g.  test1.1)\nor the full test suite may be executed with the single command\nregression.py.\nSee testsuite/expect/README for more information.\nSlurm also has a Pytest environment that can work like the Expect one, but it\nalso works together with an external QA framework to improve the overall QA\nof Slurm.\nAdding Files and Directories\n\n\nIf you are adding files and directories to Slurm, it will be necessary to\nre-build configuration files before executing the configure command.\nUpdate Makefile.am files as needed then execute\nautoreconf before executing configure.\n\nTricks of the Trade\n\n\nHAVE_FRONT_END\n\n\nYou can make a single node appear to Slurm as a Linux cluster by running\nconfigure with the --enable-front-end option. This\ndefines HAVE_FRONT_END with a non-zero value in the file config.h.\nAll (fake) nodes should be defined in the slurm.conf file.\nThese nodes should be configured with a single NodeAddr value\nindicating the node on which single slurmd daemon\nexecutes.  Initiate one slurmd and one\nslurmctld daemon. Do not initiate too many\nsimultaneous job steps to avoid overloading the\nslurmd daemon executing them all.\nMultiple slurmd support\n\n\nIt is possible to run multiple slurmd daemons on a single node, each using\na different port number and NodeName alias.  This is very useful for testing\nnetworking and protocol changes, or anytime you want to simulate a larger\ncluster than you really have.  The author uses this on his desktop to simulate\nmultiple nodes.  However, it is important to note that not all slurm functions\nwill work with multiple slurmd support enabled (e.g. many switch plugins will\nnot work, it is best not to use any).\nMultiple support is enabled at configure-time with the\n\"--enable-multiple-slurmd\" parameter.  This enables a new parameter in the\nslurm.conf file on the NodeName line, \"Port=\", and adds a new\ncommand line parameter to slurmd, \"-N\".\nEach slurmd needs to have its own NodeName, and its own TCP port number. Here\nis an example of the NodeName lines for running three slurmd daemons on each\nof ten nodes:\n\nNodeName=foo[1-10] NodeHostname=host[1-10]  Port=17001\nNodeName=foo[11-20] NodeHostname=host[1-10] Port=17002\nNodeName=foo[21-30] NodeHostname=host[1-10] Port=17003\n\n\nIt is likely that you will also want to use the \"%n\" symbol in any slurmd\nrelated paths in the slurm.conf file, for instance SlurmdLogFile,\nSlurmdPidFile, and especially SlurmdSpoolDir.  Each slurmd replaces the \"%n\"\nwith its own NodeName.  Here is an example:\n\nSlurmdLogFile=/var/log/slurm/slurmd.%n.log\nSlurmdPidFile=/var/run/slurmd.%n.pid\nSlurmdSpoolDir=/var/spool/slurmd.%n\n\n\nYou can manually start each slurmd daemon with the proper NodeName.\nFor example, to start the slurmd daemons for host1 from the\nabove slurm.conf example:\n\nhost1> slurmd -N foo1\nhost1> slurmd -N foo11\nhost1> slurmd -N foo21\n\n\nIf you have SysV init scripts, slurmd daemons will automatically be started by\nwhenever MULTIPLE_SLURMD is set to yes in /etc/sysconfig/slurm.\nIf your distribution uses systemd, you may want to use templating feature to\ndefine one slurmd.service file and registering each of your virtual nodes\nwithin it, for example:\n\n[Unit]\nDescription=Slurm node daemon\nAfter=network.target munge.service remote-fs.target\nConditionPathExists=/etc/slurm.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmd\nExecStart=/usr/sbin/slurmd -N%i $SLURMD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurmd-%i.pid\nKillMode=process\nLimitNOFILE=51200\nLimitMEMLOCK=infinity\nLimitSTACK=infinity\nDelegate=yes\n\n[Install]\nWantedBy=multi-user.target\n\nThen, enabling/managing a service like this (what is %i in the file\nwill be replaced by what is after the @ in the command line):\n\nsystemctl enable slurmd@nodeXYZ\nsystemctl start/stop/restart slurmd@nodeXYZ\n\n\nLast modified 6 August 2021\n"
            },
            {
                "title": "Documentation\n\n",
                "content": "All of the documentation is in the subdirectory doc.\nTwo directories are of particular interest:\ndoc/man \u2014 contains the man pages for the APIs,\nconfiguration file, commands, and daemons.\ndoc/html \u2014 contains the web pages.Source Code\n\nFunctions are divided into several categories, each in its own subdirectory.\nThe details of each directory's contents are provided below. The directories are\nas follows: \napi \u2014 Application Program Interfaces into\nthe Slurm code. Used to send and get Slurm information from the central manager.\nThese are the functions user applications might utilize.\ncommon \u2014 General purpose functions for widespread use throughout\nSlurm.\ndatabase \u2014 Various database files that support the accounting\n storage plugin.\nplugins \u2014 Plugin functions for various infrastructures or optional\nbehavior. A separate subdirectory is used for each plugin class:\n\naccounting_storage for specifying the type of storage for accounting,\nauth for user authentication,\ncred for job credential functions,\njobacct_gather for job accounting,\njobcomp for job completion logging,\nmpi for MPI support,\npriority calculates job priority based on a number of factors\nincluding fair-share,\nproctrack for process tracking,\nsched for job scheduler,\nselect for a job's node selection,\nswitch for switch (interconnect) specific functions,\ntask for task affinity to processors,\ntopology methods for assigning nodes to jobs based on node\ntopology.\n\n\nsacct \u2014 User command to view accounting information about jobs.\nsacctmgr \u2014 User and administrator tool to manage accounting.\nsalloc \u2014 User command to allocate resources for a job.\nsattach \u2014 User command to attach standard input, output and error\nfiles to a running job or job step.\nsbatch \u2014 User command to submit a batch job (script for later execution).\nsbcast \u2014 User command to broadcast a file to all nodes associated\nwith an existing Slurm job.\nscancel \u2014 User command to cancel (or signal) a job or job step.\nscontrol \u2014 Administrator tool to manage Slurm.\nsinfo \u2014 User command to get information on Slurm nodes and partitions.\nslurmctld \u2014 Slurm central manager daemon code.\nslurmd \u2014 Slurm daemon code to manage the compute server nodes including\nthe execution of user applications.\nslurmdbd \u2014 Slurm database daemon managing access to the accounting\nstorage database.\nsprio \u2014 User command to see the breakdown of a job's priority\ncalculation when the Multifactor Job Priority plugin is installed.\nsqueue \u2014 User command to get information on Slurm jobs and job steps.\nsreport \u2014 User command to view various reports about past\nusage across the enterprise.\nsrun \u2014 User command to submit a job, get an allocation, and/or\ninitiation a parallel job step.\nsshare \u2014 User command to view shares and usage when the Multifactor\nJob Priority plugin is installed.\nsstat \u2014 User command to view detailed statistics about running\njobs when a Job Accounting Gather plugin is installed.\nstrigger \u2014 User and administrator tool to manage event triggers.\nsview \u2014 User command to view and update node, partition, and\njob state information.\nSource Code Management\n\n\nThe latest code is in github:\nhttps://github.com/SchedMD/slurm.\nCreating your own branch will make it easier to keep it synchronized\nwith our work.\nAdding New Modules\n\n\nAdd the new file name to the Makefile.am file in the appropriate directory.\nThen execute autoreconf (at the top level of the Slurm source directory).\nNote that a relatively current version of automake is required.\nThe autoreconf program will build Makefile.in files from the Makefile.am files.\nIf any new files need to be installed, update the slurm.spec file to identify\nthe RPM in which the new files should be placed\nIf new directories need to be added, add to the configure.ac file the path\nto the Makefile to be built in the new directory. In summary:\nautoreconf translates .am files into .in files\nconfigure translates .in files, adding paths and version numbers.\nCompiling\nSending the standard output of \"make\" to a file makes it easier to see any\nwarning or error messages:\n\"make -j install >make.out\"\nConfiguration\n\n\nSample configuration files are included in the etc subdirectory.\nThe slurm.conf can be built using a configuration tool.\nSee doc/man/man5/slurm.conf.5 and the man pages for other configuration files\nfor more details.\ninit.d.slurm is a script that determines which\nSlurm daemon(s) should execute on any node based upon the configuration file contents.\nIt will also manage these daemons: starting, signaling, restarting, and stopping them.\nTest Suite\nThe testsuite files use Check, Expect and Pytest for testing Slurm in\ndifferents ways.\nThe Check tests are designed to unit test C code. Only with\nmake check, without Slurm installed, they will validate that key C\nfunctions work correctly.\nWe also have a set of Expect Slurm tests available under the testsuite/expect\ndirectory.  These tests are executed after Slurm has been installed\nand the daemons initiated. These tests exercise all Slurm commands\nand options including stress tests.  The file testsuite/expect/globals\ncontains the Expect test framewrok for all of the individual tests.  At\nthe very least, you will need to set the slurm_dir variable to the correct\nvalue.  To avoid conflicts with other developers, you can override variable settings\nin a separate file named testsuite/expect/globals.local.\nSet your working directory to testsuite/expect before\nstarting these tests.  Tests may be executed individually by name\n(e.g.  test1.1)\nor the full test suite may be executed with the single command\nregression.py.\nSee testsuite/expect/README for more information.\nSlurm also has a Pytest environment that can work like the Expect one, but it\nalso works together with an external QA framework to improve the overall QA\nof Slurm.\nAdding Files and Directories\n\n\nIf you are adding files and directories to Slurm, it will be necessary to\nre-build configuration files before executing the configure command.\nUpdate Makefile.am files as needed then execute\nautoreconf before executing configure.\n\nTricks of the Trade\n\n\nHAVE_FRONT_END\n\n\nYou can make a single node appear to Slurm as a Linux cluster by running\nconfigure with the --enable-front-end option. This\ndefines HAVE_FRONT_END with a non-zero value in the file config.h.\nAll (fake) nodes should be defined in the slurm.conf file.\nThese nodes should be configured with a single NodeAddr value\nindicating the node on which single slurmd daemon\nexecutes.  Initiate one slurmd and one\nslurmctld daemon. Do not initiate too many\nsimultaneous job steps to avoid overloading the\nslurmd daemon executing them all.\nMultiple slurmd support\n\n\nIt is possible to run multiple slurmd daemons on a single node, each using\na different port number and NodeName alias.  This is very useful for testing\nnetworking and protocol changes, or anytime you want to simulate a larger\ncluster than you really have.  The author uses this on his desktop to simulate\nmultiple nodes.  However, it is important to note that not all slurm functions\nwill work with multiple slurmd support enabled (e.g. many switch plugins will\nnot work, it is best not to use any).\nMultiple support is enabled at configure-time with the\n\"--enable-multiple-slurmd\" parameter.  This enables a new parameter in the\nslurm.conf file on the NodeName line, \"Port=\", and adds a new\ncommand line parameter to slurmd, \"-N\".\nEach slurmd needs to have its own NodeName, and its own TCP port number. Here\nis an example of the NodeName lines for running three slurmd daemons on each\nof ten nodes:\n\nNodeName=foo[1-10] NodeHostname=host[1-10]  Port=17001\nNodeName=foo[11-20] NodeHostname=host[1-10] Port=17002\nNodeName=foo[21-30] NodeHostname=host[1-10] Port=17003\n\n\nIt is likely that you will also want to use the \"%n\" symbol in any slurmd\nrelated paths in the slurm.conf file, for instance SlurmdLogFile,\nSlurmdPidFile, and especially SlurmdSpoolDir.  Each slurmd replaces the \"%n\"\nwith its own NodeName.  Here is an example:\n\nSlurmdLogFile=/var/log/slurm/slurmd.%n.log\nSlurmdPidFile=/var/run/slurmd.%n.pid\nSlurmdSpoolDir=/var/spool/slurmd.%n\n\n\nYou can manually start each slurmd daemon with the proper NodeName.\nFor example, to start the slurmd daemons for host1 from the\nabove slurm.conf example:\n\nhost1> slurmd -N foo1\nhost1> slurmd -N foo11\nhost1> slurmd -N foo21\n\n\nIf you have SysV init scripts, slurmd daemons will automatically be started by\nwhenever MULTIPLE_SLURMD is set to yes in /etc/sysconfig/slurm.\nIf your distribution uses systemd, you may want to use templating feature to\ndefine one slurmd.service file and registering each of your virtual nodes\nwithin it, for example:\n\n[Unit]\nDescription=Slurm node daemon\nAfter=network.target munge.service remote-fs.target\nConditionPathExists=/etc/slurm.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmd\nExecStart=/usr/sbin/slurmd -N%i $SLURMD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurmd-%i.pid\nKillMode=process\nLimitNOFILE=51200\nLimitMEMLOCK=infinity\nLimitSTACK=infinity\nDelegate=yes\n\n[Install]\nWantedBy=multi-user.target\n\nThen, enabling/managing a service like this (what is %i in the file\nwill be replaced by what is after the @ in the command line):\n\nsystemctl enable slurmd@nodeXYZ\nsystemctl start/stop/restart slurmd@nodeXYZ\n\n\nLast modified 6 August 2021\n"
            },
            {
                "title": "Source Code Management\n\n",
                "content": "The latest code is in github:\nhttps://github.com/SchedMD/slurm.\nCreating your own branch will make it easier to keep it synchronized\nwith our work.Adding New Modules\n\nAdd the new file name to the Makefile.am file in the appropriate directory.\nThen execute autoreconf (at the top level of the Slurm source directory).\nNote that a relatively current version of automake is required.\nThe autoreconf program will build Makefile.in files from the Makefile.am files.\nIf any new files need to be installed, update the slurm.spec file to identify\nthe RPM in which the new files should be placedIf new directories need to be added, add to the configure.ac file the path\nto the Makefile to be built in the new directory. In summary:\nautoreconf translates .am files into .in files\nconfigure translates .in files, adding paths and version numbers.CompilingSending the standard output of \"make\" to a file makes it easier to see any\nwarning or error messages:\n\"make -j install >make.out\"Configuration\n\nSample configuration files are included in the etc subdirectory.\nThe slurm.conf can be built using a configuration tool.\nSee doc/man/man5/slurm.conf.5 and the man pages for other configuration files\nfor more details.\ninit.d.slurm is a script that determines which\nSlurm daemon(s) should execute on any node based upon the configuration file contents.\nIt will also manage these daemons: starting, signaling, restarting, and stopping them.Test SuiteThe testsuite files use Check, Expect and Pytest for testing Slurm in\ndifferents ways.The Check tests are designed to unit test C code. Only with\nmake check, without Slurm installed, they will validate that key C\nfunctions work correctly.We also have a set of Expect Slurm tests available under the testsuite/expect\ndirectory.  These tests are executed after Slurm has been installed\nand the daemons initiated. These tests exercise all Slurm commands\nand options including stress tests.  The file testsuite/expect/globals\ncontains the Expect test framewrok for all of the individual tests.  At\nthe very least, you will need to set the slurm_dir variable to the correct\nvalue.  To avoid conflicts with other developers, you can override variable settings\nin a separate file named testsuite/expect/globals.local.Set your working directory to testsuite/expect before\nstarting these tests.  Tests may be executed individually by name\n(e.g.  test1.1)\nor the full test suite may be executed with the single command\nregression.py.\nSee testsuite/expect/README for more information.Slurm also has a Pytest environment that can work like the Expect one, but it\nalso works together with an external QA framework to improve the overall QA\nof Slurm.Adding Files and Directories\n\nIf you are adding files and directories to Slurm, it will be necessary to\nre-build configuration files before executing the configure command.\nUpdate Makefile.am files as needed then execute\nautoreconf before executing configure.\n\nTricks of the Trade\n\n\nHAVE_FRONT_END\n\n\nYou can make a single node appear to Slurm as a Linux cluster by running\nconfigure with the --enable-front-end option. This\ndefines HAVE_FRONT_END with a non-zero value in the file config.h.\nAll (fake) nodes should be defined in the slurm.conf file.\nThese nodes should be configured with a single NodeAddr value\nindicating the node on which single slurmd daemon\nexecutes.  Initiate one slurmd and one\nslurmctld daemon. Do not initiate too many\nsimultaneous job steps to avoid overloading the\nslurmd daemon executing them all.\nMultiple slurmd support\n\n\nIt is possible to run multiple slurmd daemons on a single node, each using\na different port number and NodeName alias.  This is very useful for testing\nnetworking and protocol changes, or anytime you want to simulate a larger\ncluster than you really have.  The author uses this on his desktop to simulate\nmultiple nodes.  However, it is important to note that not all slurm functions\nwill work with multiple slurmd support enabled (e.g. many switch plugins will\nnot work, it is best not to use any).\nMultiple support is enabled at configure-time with the\n\"--enable-multiple-slurmd\" parameter.  This enables a new parameter in the\nslurm.conf file on the NodeName line, \"Port=\", and adds a new\ncommand line parameter to slurmd, \"-N\".\nEach slurmd needs to have its own NodeName, and its own TCP port number. Here\nis an example of the NodeName lines for running three slurmd daemons on each\nof ten nodes:\n\nNodeName=foo[1-10] NodeHostname=host[1-10]  Port=17001\nNodeName=foo[11-20] NodeHostname=host[1-10] Port=17002\nNodeName=foo[21-30] NodeHostname=host[1-10] Port=17003\n\n\nIt is likely that you will also want to use the \"%n\" symbol in any slurmd\nrelated paths in the slurm.conf file, for instance SlurmdLogFile,\nSlurmdPidFile, and especially SlurmdSpoolDir.  Each slurmd replaces the \"%n\"\nwith its own NodeName.  Here is an example:\n\nSlurmdLogFile=/var/log/slurm/slurmd.%n.log\nSlurmdPidFile=/var/run/slurmd.%n.pid\nSlurmdSpoolDir=/var/spool/slurmd.%n\n\n\nYou can manually start each slurmd daemon with the proper NodeName.\nFor example, to start the slurmd daemons for host1 from the\nabove slurm.conf example:\n\nhost1> slurmd -N foo1\nhost1> slurmd -N foo11\nhost1> slurmd -N foo21\n\n\nIf you have SysV init scripts, slurmd daemons will automatically be started by\nwhenever MULTIPLE_SLURMD is set to yes in /etc/sysconfig/slurm.\nIf your distribution uses systemd, you may want to use templating feature to\ndefine one slurmd.service file and registering each of your virtual nodes\nwithin it, for example:\n\n[Unit]\nDescription=Slurm node daemon\nAfter=network.target munge.service remote-fs.target\nConditionPathExists=/etc/slurm.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmd\nExecStart=/usr/sbin/slurmd -N%i $SLURMD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurmd-%i.pid\nKillMode=process\nLimitNOFILE=51200\nLimitMEMLOCK=infinity\nLimitSTACK=infinity\nDelegate=yes\n\n[Install]\nWantedBy=multi-user.target\n\nThen, enabling/managing a service like this (what is %i in the file\nwill be replaced by what is after the @ in the command line):\n\nsystemctl enable slurmd@nodeXYZ\nsystemctl start/stop/restart slurmd@nodeXYZ\n\nLast modified 6 August 2021"
            },
            {
                "title": "Tricks of the Trade\n\n",
                "content": "HAVE_FRONT_END\n\nYou can make a single node appear to Slurm as a Linux cluster by running\nconfigure with the --enable-front-end option. This\ndefines HAVE_FRONT_END with a non-zero value in the file config.h.\nAll (fake) nodes should be defined in the slurm.conf file.\nThese nodes should be configured with a single NodeAddr value\nindicating the node on which single slurmd daemon\nexecutes.  Initiate one slurmd and one\nslurmctld daemon. Do not initiate too many\nsimultaneous job steps to avoid overloading the\nslurmd daemon executing them all.Multiple slurmd support\n\nIt is possible to run multiple slurmd daemons on a single node, each using\na different port number and NodeName alias.  This is very useful for testing\nnetworking and protocol changes, or anytime you want to simulate a larger\ncluster than you really have.  The author uses this on his desktop to simulate\nmultiple nodes.  However, it is important to note that not all slurm functions\nwill work with multiple slurmd support enabled (e.g. many switch plugins will\nnot work, it is best not to use any).Multiple support is enabled at configure-time with the\n\"--enable-multiple-slurmd\" parameter.  This enables a new parameter in the\nslurm.conf file on the NodeName line, \"Port=\", and adds a new\ncommand line parameter to slurmd, \"-N\".Each slurmd needs to have its own NodeName, and its own TCP port number. Here\nis an example of the NodeName lines for running three slurmd daemons on each\nof ten nodes:\nNodeName=foo[1-10] NodeHostname=host[1-10]  Port=17001\nNodeName=foo[11-20] NodeHostname=host[1-10] Port=17002\nNodeName=foo[21-30] NodeHostname=host[1-10] Port=17003\n\nIt is likely that you will also want to use the \"%n\" symbol in any slurmd\nrelated paths in the slurm.conf file, for instance SlurmdLogFile,\nSlurmdPidFile, and especially SlurmdSpoolDir.  Each slurmd replaces the \"%n\"\nwith its own NodeName.  Here is an example:\nSlurmdLogFile=/var/log/slurm/slurmd.%n.log\nSlurmdPidFile=/var/run/slurmd.%n.pid\nSlurmdSpoolDir=/var/spool/slurmd.%n\n\nYou can manually start each slurmd daemon with the proper NodeName.\nFor example, to start the slurmd daemons for host1 from the\nabove slurm.conf example:\nhost1> slurmd -N foo1\nhost1> slurmd -N foo11\nhost1> slurmd -N foo21\n\nIf you have SysV init scripts, slurmd daemons will automatically be started by\nwhenever MULTIPLE_SLURMD is set to yes in /etc/sysconfig/slurm.\nIf your distribution uses systemd, you may want to use templating feature to\ndefine one slurmd.service file and registering each of your virtual nodes\nwithin it, for example:\n[Unit]\nDescription=Slurm node daemon\nAfter=network.target munge.service remote-fs.target\nConditionPathExists=/etc/slurm.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmd\nExecStart=/usr/sbin/slurmd -N%i $SLURMD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurmd-%i.pid\nKillMode=process\nLimitNOFILE=51200\nLimitMEMLOCK=infinity\nLimitSTACK=infinity\nDelegate=yes\n\n[Install]\nWantedBy=multi-user.target\nThen, enabling/managing a service like this (what is %i in the file\nwill be replaced by what is after the @ in the command line):\nsystemctl enable slurmd@nodeXYZ\nsystemctl start/stop/restart slurmd@nodeXYZ\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/quickstart_admin.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Quick Start Administrator Guide",
                "content": "Contents\nOverview\nSuper Quick Start\n\nBuilding and Installing Slurm\n\nInstalling Prerequisites\nBuilding RPMs\nInstalling RPMs\nBuilding Debian Packages\nInstalling Debian Packages\nBuilding Manually\n\n\nDaemons\nInfrastructure\nConfiguration\nSecurity\nStarting the Daemons\nAdministration Examples\nUpgrades\nFreeBSD\nOverviewPlease see the Quick Start User Guide for a\ngeneral overview.Also see Platforms for a list of supported\ncomputer platforms.For information on performing an upgrade, please see the\nUpgrade Guide.Super Quick Start\n\n\nMake sure the clocks, users and groups (UIDs and GIDs) are synchronized\nacross the cluster.\nInstall MUNGE for\nauthentication. Make sure that all nodes in your cluster have the\nsame munge.key. Make sure the MUNGE daemon, munged,\nis started before you start the Slurm daemons.\nDownload the latest\nversion of Slurm.\nInstall Slurm using one of the following methods:\n\nBuild RPM or DEB packages\n(recommended for production)\nBuild Manually from source\n(for developers or advanced users)\nNOTE: Some Linux distributions may have unofficial\nSlurm packages available in software repositories. SchedMD does not maintain\nor recommend these packages.\n\n\nBuild a configuration file using your favorite web browser and the\nSlurm Configuration Tool.\nNOTE: The SlurmUser must exist prior to starting Slurm\nand must exist on all nodes of the cluster.\nNOTE: The parent directories for Slurm's log files, process ID files,\nstate save directories, etc. are not created by Slurm.\nThey must be created and made writable by SlurmUser as needed prior to\nstarting Slurm daemons.\nNOTE: If any parent directories are created during the installation\nprocess (for the executable files, libraries, etc.),\nthose directories will have access rights equal to read/write/execute for\neveryone minus the umask value (e.g. umask=0022 generates directories with\npermissions of \"drwxr-r-x\" and mask=0000 generates directories with\npermissions of \"drwxrwrwx\" which is a security problem).\nInstall the configuration file in <sysconfdir>/slurm.conf.\nNOTE: You will need to install this configuration file on all nodes of\nthe cluster.\nsystemd (optional): enable the appropriate services on each system:\n\nController: systemctl enable slurmctld\nDatabase: systemctl enable slurmdbd\nCompute Nodes: systemctl enable slurmd\n\nStart the slurmctld and slurmd daemons.\nFreeBSD administrators should see the FreeBSD section below.Building and Installing Slurm\n\nInstalling Prerequisites\n\nBefore building Slurm, consider which plugins you will need for your\ninstallation.  Which plugins are built can vary based on the libraries that\nare available when running configure.  Refer to the below list of possible\nplugins and what is required to build them.\n auth/Slurm The auth/slurm plugin will be built if the jwt\n\t\tdevelopment library is installed. This is an alternative to the\n\t\ttraditional MUNGE authentication mechanism.\n AMD GPU Support Autodetection of AMD GPUs will be available\n\t\tif the ROCm development library is installed.\n\t\t\n cgroup Task Constraining The task/cgroup plugin will be built\n\t\tif the hwloc development library is present. cgroup/v2\n\t\tsupport also requires the bpf and dbus development\n\t\tlibraries.\n HDF5 Job Profiling The acct_gather_profile/hdf5 job profiling\n\t\tplugin will be built if the hdf5 development library is\n\t\tpresent.\n HTML Man Pages HTML versions of the man pages will be generated if\n\t\tthe man2html command is present.\n HPE Slingshot The switch/hpe_slingshot plugin will be built\n\t\tif the cray-libcxi, curl, and json-c\n\t\tdevelopment libraries are present.\n InfiniBand Accounting The acct_gather_interconnect/ofed\n\t\tInfiniBand accounting plugin will be built if the\n\t\tlibibmad and libibumad development libraries are\n\t\tpresent.\n Intel GPU Support Autodetection of Intel GPUs will be available\n\t\tif the libvpl development library is installed.\n\t\t\n IPMI Energy Consumption The acct_gather_energy/ipmi\n\t\taccounting plugin will be built if the freeipmi\n\t\tdevelopment library is present. When building the RPM,\n\t\trpmbuild ... --with freeipmi can be\n\t\tspecified to explicitly check for these dependencies.\n Lua Support The lua API will be available in various plugins if the\n\t\tlua development library is present.\n MUNGE The auth/munge plugin will be built if the MUNGE\n\t\tauthentication development library is installed. MUNGE is used\n\t\tas the default authentication mechanism.\n MySQL MySQL support for accounting will be built if the\n\t\tMySQL or MariaDB development library is present.\n\t\tA currently supported version of MySQL or MariaDB should be\n\t\tused.\n NUMA Affinity NUMA support in the task/affinity plugin will be\n\t\tavailable if the numa development library is installed.\n\t\t\n NVIDIA GPU Support Autodetection of NVIDIA GPUs will be available\n\t\tif the libnvidia-ml development library is installed.\n\t\t\n PAM Support PAM support will be added if the PAM development\n\t\tlibrary is installed.\n PMIx PMIx support will be added if the pmix development\n\t\tlibrary is installed and the --with-pmix flag is\n\t\tprovided at build time.\n Readline Support Readline support in scontrol and sacctmgr's\n\t\tinteractive modes will be available if the readline\n\t\tdevelopment library is present.\n REST API Support for Slurm's REST API will be built if the\n\t\thttp-parser and json-c development libraries\n\t\tare installed. Additional functionality will be included\n\t\tif the optional yaml and jwt development\n\t\tlibraries are installed.\n sview The sview command will be built only if gtk+-2.0\n\t\tis installed.\nPlease see the Related Software page for\nreferences to required software to build these plugins.If required libraries or header files are in non-standard locations, set\nCFLAGS and LDFLAGS environment variables accordingly.\nBuilding RPMsTo build RPMs directly, copy the distributed tarball into a directory\nand execute (substituting the appropriate Slurm version\nnumber):rpmbuild -ta slurm-23.02.7.tar.bz2$(HOME)/rpmbuildYou can control some aspects of the RPM built with a .rpmmacros\nfile in your home directory. Special macro definitions will likely\nonly be required if files are installed in unconventional locations.\nA full list of rpmbuild options can be found near the top of the\nslurm.spec file.\nSome macro definitions that may be used in building Slurm include:\n\n_enable_debug\nSpecify if debugging logic within Slurm is to be enabled\n_prefix\nPathname of directory to contain the Slurm files\n_slurm_sysconfdir\nPathname of directory containing the slurm.conf configuration file (default\n/etc/slurm)\nwith_munge\nSpecifies the MUNGE (authentication library) installation location\n\nAn example .rpmmacros file:\n\n# .rpmmacros\n# Override some RPM macros from /usr/lib/rpm/macros\n# Set Slurm-specific macros for unconventional file locations\n#\n%_enable_debug     \"--with-debug\"\n%_prefix           /opt/slurm\n%_slurm_sysconfdir %{_prefix}/etc/slurm\n%_defaultdocdir    %{_prefix}/doc\n%with_munge        \"--with-munge=/opt/munge\"\n\nRPMs Installed\nThe RPMs needed on the head node, compute nodes, and slurmdbd node can vary\nby configuration, but here is a suggested starting point:\n\nHead Node (where the slurmctld daemon runs),\n    Compute and Login Nodes\n\t\nslurm\nslurm-perlapi\nslurm-slurmctld (only on the head node)\nslurm-slurmd (only on the compute nodes)\n\n\nSlurmDBD Node\n\t\nslurm\nslurm-slurmdbd\n\n\n\nBuilding Debian Packages\n\n\nBeginning with Slurm 23.11.0, Slurm includes the files required to build\nDebian packages. These packages conflict with the packages shipped with Debian\nbased distributions, and are named distinctly to differentiate them. After\ndownloading the desired version of Slurm, the following can be done to build\nthe packages:\n\nInstall basic Debian package build requirements:\napt-get install build-essential fakeroot devscripts equivs\n\nUnpack the distributed tarball:\ntar -xaf slurm*tar.bz2\n\ncd to the directory containing the Slurm source\nInstall the Slurm package dependencies:\nmk-build-deps -i debian/control\n\nBuild the Slurm packages:\ndebuild -b -uc -us\n\n\nThe packages will be in the parent directory after debuild completes.\nInstalling Debian Packages\n\n\nThe packages needed on the head node, compute nodes, and slurmdbd node can\nvary site to site, but this is a good starting point:\n\nSlurmDBD Node\n\t\nslurm-smd\nslurm-smd-slurmdbd\n\n\nHead Node (slurmctld node)\n\t\nslurm-smd\nslurm-smd-slurmctld\nslurm-smd-client\n\n\nCompute Nodes (slurmd node)\n\t\nslurm-smd\nslurm-smd-slurmd\nslurm-smd-client\n\n\nLogin Nodes\n\t\nslurm-smd\nslurm-smd-client\n\n\n\nBuilding Manually\n\n\nInstructions to build and install Slurm manually are shown below.\nThis is significantly more complicated to manage than the RPM and DEB build\nprocedures, so this approach is only recommended for developers or\nadvanced users who are looking for a more customized install.\nSee the README and INSTALL files in the source distribution for more details.\n\n\nUnpack the distributed tarball:\ntar -xaf slurm*tar.bz2\ncd to the directory containing the Slurm source and type\n./configure with appropriate options (see below).\nType make install to compile and install the programs,\ndocumentation, libraries, header files, etc.\nType ldconfig -n <library_location> so that the Slurm\nlibraries can be found by applications that intend to use Slurm APIs directly.\nThe library location will be a subdirectory of PREFIX (described below) and\ndepend upon the system type and configuration, typically lib or lib64.\nFor example, if PREFIX is \"/usr\" and the subdirectory is \"lib64\" then you would\nfind that a file named \"/usr/lib64/libslurm.so\" was installed and the command\nldconfig -n /usr/lib64 should be executed.\n\nA full list of configure options will be returned by the\ncommand configure --help. The most commonly used arguments\nto the configure command include:\n--enable-debug\nEnable additional debugging logic within Slurm.\n--prefix=PREFIX\nInstall architecture-independent files in PREFIX; default value is /usr/local.\n--sysconfdir=DIR\nSpecify location of Slurm configuration file. The default value is PREFIX/etc\nDaemons\nslurmctld is sometimes called the \"controller\".\nIt orchestrates Slurm activities, including queuing of jobs,\nmonitoring node states, and allocating resources to jobs. There is an\noptional backup controller that automatically assumes control in the\nevent the primary controller fails (see the High\nAvailability section below).  The primary controller resumes\ncontrol whenever it is restored to service. The controller saves its\nstate to disk whenever there is a change in state (see\n\"StateSaveLocation\" in Configuration\nsection below).  This state can be recovered by the controller at\nstartup time.  State changes are saved so that jobs and other state\ninformation can be preserved when the controller moves (to or from a\nbackup controller) or is restarted.\nWe recommend that you create a Unix user slurm for use by\nslurmctld. This user name will also be specified using the\nSlurmUser in the slurm.conf configuration file.\nThis user must exist on all nodes of the cluster.\nNote that files and directories used by slurmctld will need to be\nreadable or writable by the user SlurmUser (the Slurm configuration\nfiles must be readable; the log file directory and state save directory\nmust be writable).\nThe slurmd daemon executes on every compute node. It resembles a\nremote shell daemon to export control to Slurm. Because slurmd initiates and\nmanages user jobs, it must execute as the user root.\nIf you want to archive job accounting records to a database, the\nslurmdbd (Slurm DataBase Daemon) should be used. We recommend that\nyou defer adding accounting support until after basic Slurm functionality is\nestablished on your system. An Accounting web\npage contains more information.\nslurmctld and/or slurmd should be initiated at node startup\ntime per the Slurm configuration.\nThe slurmrestd daemon was introduced in version 20.02 and allows\nclients to communicate with Slurm via the REST API. This is installed by\ndefault, assuming the prerequisites are met.\nIt has two run modes,\nallowing you to have it run as a traditional Unix service and always listen\nfor TCP connections, or you can have it run as an Inet service and only have\nit active when in use.\nHigh Availability\nMultiple SlurmctldHost entries can be configured, with any entry beyond the\nfirst being treated as a backup host. Any backup hosts configured should be on\na different node than the node hosting the primary slurmctld. However, all\nhosts should mount a common file system containing the state information (see\n\"StateSaveLocation\" in the Configuration\nsection below).\nIf more than one host is specified, when the primary fails the second listed\nSlurmctldHost will take over for it. When the primary returns to service, it\nnotifies the backup.  The backup then saves the state and returns to backup\nmode. The primary reads the saved state and resumes normal operation. Likewise,\nif both of the first two listed hosts fail the third SlurmctldHost will take\nover until the primary returns to service. Other than a brief period of non-\nresponsiveness, the transition back and forth should go undetected.\nPrior to 18.08, Slurm used the \n\"BackupAddr\" and \n\"BackupController\" parameters for High Availability. These\nparameters have been deprecated and are replaced by\n\"SlurmctldHost\".\nAlso see \"\nSlurmctldPrimaryOnProg\" and\n\"\nSlurmctldPrimaryOffProg\" to adjust the actions taken when machines\ntransition between being the primary controller.\nAny time the slurmctld daemon or hardware fails before state information\nreaches disk can result in lost state.\nSlurmctld writes state frequently (every five seconds by default), but with\nlarge numbers of jobs, the formatting and writing of records can take seconds\nand recent changes might not be written to disk.\nAnother example is if the state information is written to file, but that\ninformation is cached in memory rather than written to disk when the node fails.\nThe interval between state saves being written to disk can be configured at\nbuild time by defining SAVE_MAX_WAIT to a different value than five.\nA backup instance of slurmdbd can also be configured by specifying\n\nAccountingStorageBackupHost in slurm.conf, as well as\nDbdBackupHost in\nslurmdbd.conf. The backup host should be on a different machine than the one\nhosting the primary instance of slurmdbd. Both instances of slurmdbd should\nhave access to the same database. The\nnetwork page has a visual representation\nof how this might look.\nInfrastructure\n\n\nUser and Group Identification\n\n\nThere must be a uniform user and group name space (including\nUIDs and GIDs) across the cluster.\nIt is not necessary to permit user logins to the control hosts\n(SlurmctldHost), but the\nusers and groups must be resolvable on those hosts.\nAuthentication of Slurm communications\n\n\nAll communications between Slurm components are authenticated. The\nauthentication infrastructure is provided by a dynamically loaded\nplugin chosen at runtime via the AuthType keyword in the Slurm\nconfiguration file. Until 23.11.0, the only supported authentication type was\nmunge, which requires the\ninstallation of the MUNGE package.\nWhen using MUNGE, all nodes in the cluster must be configured with the\nsame munge.key file. The MUNGE daemon, munged, must also be\nstarted before Slurm daemons. Note that MUNGE does require clocks to be\nsynchronized throughout the cluster, usually done by NTP.\nAs of 23.11.0, AuthType can also be set to\nslurm, an internal authentication\nplugin. This plugin has similar requirements to MUNGE, requiring a key file\nshared to all Slurm daemons. The auth/slurm plugin requires installation of the\njwt package.\nMUNGE is currently the default and recommended option.\nThe configure script in the top-level directory of this distribution will\ndetermine which authentication plugins may be built.\nThe configuration file specifies which of the available plugins will be\nutilized.\nMPI support\nSlurm supports many different MPI implementations.\nFor more information, see MPI.\n\nScheduler support\n\n\nSlurm can be configured with rather simple or quite sophisticated\nscheduling algorithms depending upon your needs and willingness to\nmanage the configuration (much of which requires a database).\nThe first configuration parameter of interest is PriorityType\nwith two options available: basic (first-in-first-out) and\nmultifactor.\nThe multifactor plugin will assign a priority to jobs based upon\na multitude of configuration parameters (age, size, fair-share allocation,\netc.) and its details are beyond the scope of this document.\nSee the Multifactor Job Priority Plugin\ndocument for details.\nThe SchedType configuration parameter controls how queued\njobs are scheduled and several options are available.\n\nbuiltin will initiate jobs strictly in their priority order,\ntypically (first-in-first-out) \nbackfill will initiate a lower-priority job if doing so does\nnot delay the expected initiation time of higher priority jobs; essentially\nusing smaller jobs to fill holes in the resource allocation plan. Effective\nbackfill scheduling does require users to specify job time limits.\ngang time-slices jobs in the same partition/queue and can be\nused to preempt jobs from lower-priority queues in order to execute\njobs in higher priority queues.\n\nFor more information about scheduling options see\nGang Scheduling,\nPreemption,\nResource Reservation Guide,\nResource Limits and\nSharing Consumable Resources.\nResource selection\n\n\nThe resource selection mechanism used by Slurm is controlled by the\nSelectType configuration parameter.\nIf you want to execute multiple jobs per node, but track and manage allocation\nof the processors, memory and other resources, the cons_tres (consumable\ntrackable resources) plugin is recommended.\nFor more information, please see\nConsumable Resources in Slurm.\nLogging\nSlurm uses syslog to record events if the SlurmctldLogFile and\nSlurmdLogFile locations are not set.\nAccounting\nSlurm supports accounting records being written to a simple text file,\ndirectly to a database (MySQL or MariaDB), or to a daemon securely\nmanaging accounting data for multiple clusters. For more information\nsee Accounting. \nCompute node access\n\n\nSlurm does not by itself limit access to allocated compute nodes,\nbut it does provide mechanisms to accomplish this.\nThere is a Pluggable Authentication Module (PAM) for restricting access\nto compute nodes available for download.\nWhen installed, the Slurm PAM module will prevent users from logging\ninto any node that has not be assigned to that user.\nOn job termination, any processes initiated by the user outside of\nSlurm's control may be killed using an Epilog script configured\nin slurm.conf.\nConfiguration\nThe Slurm configuration file includes a wide variety of parameters.\nThis configuration file must be available on each node of the cluster and\nmust have consistent contents. A full\ndescription of the parameters is included in the slurm.conf man page. Rather than\nduplicate that information, a minimal sample configuration file is shown below.\nYour slurm.conf file should define at least the configuration parameters defined\nin this sample and likely additional ones. Any text\nfollowing a \"#\" is considered a comment. The keywords in the file are\nnot case sensitive, although the argument typically is (e.g., \"SlurmUser=slurm\"\nmight be specified as \"slurmuser=slurm\"). The control machine, like\nall other machine specifications, can include both the host name and the name\nused for communications. In this case, the host's name is \"mcri\" and\nthe name \"emcri\" is used for communications.\nIn this case \"emcri\" is the private management network interface\nfor the host \"mcri\". Port numbers to be used for\ncommunications are specified as well as various timer values.\nThe SlurmUser must be created as needed prior to starting Slurm\nand must exist on all nodes in your cluster.\nThe parent directories for Slurm's log files, process ID files,\nstate save directories, etc. are not created by Slurm.\nThey must be created and made writable by SlurmUser as needed prior to\nstarting Slurm daemons.\nThe StateSaveLocation is used to store information about the current\nstate of the cluster, including information about queued, running and recently\ncompleted jobs. The directory used should be on a low-latency local disk to\nprevent file system delays from affecting Slurm performance. If using a backup\nhost, the StateSaveLocation should reside on a file system shared by the two\nhosts. We do not recommend using NFS to make the directory accessible to both\nhosts, but do recommend a shared mount that is accessible to the two\ncontrollers and allows low-latency reads and writes to the disk. If a\ncontroller comes up without access to the state information, queued and\nrunning jobs will be cancelled.\nA description of the nodes and their grouping into partitions is required.\nA simple node range expression may optionally be used to specify\nranges of nodes to avoid building a configuration file with large\nnumbers of entries. The node range expression can contain one\npair of square brackets with a sequence of comma separated\nnumbers and/or ranges of numbers separated by a \"-\"\n(e.g. \"linux[0-64,128]\", or \"lx[15,18,32-33]\").\nUp to two numeric ranges can be included in the expression\n(e.g. \"rack[0-63]_blade[0-41]\").\nIf one or more numeric expressions are included, one of them\nmust be at the end of the name (e.g. \"unit[0-31]rack\" is invalid),\nbut arbitrary names can always be used in a comma separated list.\nNode names can have up to three name specifications:\nNodeName is the name used by all Slurm tools when referring to the node,\nNodeAddr is the name or IP address Slurm uses to communicate with the node, and\nNodeHostname is the name returned by the command /bin/hostname -s.\nOnly NodeName is required (the others default to the same name),\nalthough supporting all three parameters provides complete control over\nnaming and addressing the nodes.  See the slurm.conf man page for\ndetails on all configuration parameters.\nNodes can be in more than one partition and each partition can have different\nconstraints (permitted users, time limits, job size limits, etc.).\nEach partition can thus be considered a separate queue.\nPartition and node specifications use node range expressions to identify\nnodes in a concise fashion. This configuration file defines a 1154-node cluster\nfor Slurm, but it might be used for a much larger cluster by just changing a few\nnode range expressions. Specify the minimum processor count (CPUs), real memory\nspace (RealMemory, megabytes), and temporary disk space (TmpDisk, megabytes) that\na node should have to be considered available for use. Any node lacking these\nminimum configuration values will be considered DOWN and not scheduled.\nNote that a more extensive sample configuration file is provided in\netc/slurm.conf.example. We also have a web-based\nconfiguration tool which can\nbe used to build a simple configuration file, which can then be\nmanually edited for more complex configurations.\n\n#\n# Sample /etc/slurm.conf for mcr.llnl.gov\n#\nSlurmctldHost=mcri(12.34.56.78)\nSlurmctldHost=mcrj(12.34.56.79)\n#\nAuthType=auth/munge\nEpilog=/usr/local/slurm/etc/epilog\nJobCompLoc=/var/tmp/jette/slurm.job.log\nJobCompType=jobcomp/filetxt\nPluginDir=/usr/local/slurm/lib/slurm\nProlog=/usr/local/slurm/etc/prolog\nSchedulerType=sched/backfill\nSelectType=select/linear\nSlurmUser=slurm\nSlurmctldPort=7002\nSlurmctldTimeout=300\nSlurmdPort=7003\nSlurmdSpoolDir=/var/spool/slurmd.spool\nSlurmdTimeout=300\nStateSaveLocation=/var/spool/slurm.state\nTreeWidth=16\n#\n# Node Configurations\n#\nNodeName=DEFAULT CPUs=2 RealMemory=2000 TmpDisk=64000 State=UNKNOWN\nNodeName=mcr[0-1151] NodeAddr=emcr[0-1151]\n#\n# Partition Configurations\n#\nPartitionName=DEFAULT State=UP\nPartitionName=pdebug Nodes=mcr[0-191] MaxTime=30 MaxNodes=32 Default=YES\nPartitionName=pbatch Nodes=mcr[192-1151]\n\nSecurity\nBesides authentication of Slurm communications based upon the value\nof the AuthType, digital signatures are used in job step\ncredentials.\nThis signature is used by slurmctld to construct a job step\ncredential, which is sent to srun and then forwarded to\nslurmd to initiate job steps.\nThis design offers improved performance by removing much of the\njob step initiation overhead from the  slurmctld  daemon.\nThe digital signature mechanism is specified by the CredType\nconfiguration parameter and the default mechanism is MUNGE. \nPluggable Authentication Module (PAM) support\n\n\nA PAM module (Pluggable Authentication Module) is available for Slurm that\ncan prevent a user from accessing a node which he has not been allocated,\nif that mode of operation is desired.\nStarting the Daemons\n\n\nFor testing purposes you may want to start by just running slurmctld and slurmd\non one node. By default, they execute in the background. Use the -D\noption for each daemon to execute them in the foreground and logging will be done\nto your terminal. The -v option will log events\nin more detail with more v's increasing the level of detail (e.g. -vvvvvv).\nYou can use one window to execute \"slurmctld -D -vvvvvv\",\na second window to execute \"slurmd -D -vvvvv\".\nYou may see errors such as \"Connection refused\" or \"Node X not responding\"\nwhile one daemon is operative and the other is being started, but the\ndaemons can be started in any order and proper communications will be\nestablished once both daemons complete initialization.\nYou can use a third window to execute commands such as\n\"srun -N1 /bin/hostname\" to confirm functionality.\nAnother important option for the daemons is \"-c\"\nto clear previous state information. Without the \"-c\"\noption, the daemons will restore any previously saved state information: node\nstate, job state, etc. With the \"-c\" option all\npreviously running jobs will be purged and node state will be restored to the\nvalues specified in the configuration file. This means that a node configured\ndown manually using the scontrol command will\nbe returned to service unless noted as being down in the configuration file.\nIn practice, Slurm consistently restarts with preservation.\nAdministration Examples\n\n\nscontrol can be used to print all system information\nand modify most of it. Only a few examples are shown below. Please see the scontrol\nman page for full details. The commands and options are all case insensitive.\nPrint detailed state of all jobs in the system.\n\nadev0: scontrol\nscontrol: show job\nJobId=475 UserId=bob(6885) Name=sleep JobState=COMPLETED\n   Priority=4294901286 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:53:41 EndTime=03/19-12:53:59\n   NodeList=adev8 NodeListIndecies=-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n\nJobId=476 UserId=bob(6885) Name=sleep JobState=RUNNING\n   Priority=4294901285 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:54:01 EndTime=NONE\n   NodeList=adev8 NodeListIndecies=8,8,-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n Print the detailed state of job 477 and change its priority to\nzero. A priority of zero prevents a job from being initiated (it is held in \"pending\"\nstate).\n\nadev0: scontrol\nscontrol: show job 477\nJobId=477 UserId=bob(6885) Name=sleep JobState=PENDING\n   Priority=4294901286 Partition=batch BatchFlag=0\n   more data removed....\nscontrol: update JobId=477 Priority=0\n\nPrint the state of node adev13 and drain it. To drain a node, specify a new\nstate of DRAIN, DRAINED, or DRAINING. Slurm will automatically set it to the appropriate\nvalue of either DRAINING or DRAINED depending on whether the node is allocated\nor not. Return it to service later.\n\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=ALLOCATED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=DRAIN\nscontrol: show node adev13\nNodeName=adev13 State=DRAINING CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: quit\nLater\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=DRAINED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=IDLE\n Reconfigure all Slurm daemons on all nodes. This should\nbe done after changing the Slurm configuration file.\n\nadev0: scontrol reconfig\n Print the current Slurm configuration. This also reports if the\nprimary and secondary controllers (slurmctld daemons) are responding. To just\nsee the state of the controllers, use the command ping.\n\nadev0: scontrol show config\nConfiguration data as of 2019-03-29T12:20:45\n...\nSlurmctldAddr           = eadevi\nSlurmctldDebug          = info\nSlurmctldHost[0]        = adevi\nSlurmctldHost[1]        = adevj\nSlurmctldLogFile        = /var/log/slurmctld.log\n...\n\nSlurmctld(primary) at adevi is UP\nSlurmctld(backup) at adevj is UP\n Shutdown all Slurm daemons on all nodes.\n\nadev0: scontrol shutdown\n\nUpgrades\nSlurm supports in-place upgrades between certain versions. Important details\nabout the steps necessary to perform an upgrade and the potential complications\nto prepare for are contained on this page:\nUpgrade Guide\nFreeBSD\nFreeBSD administrators can install the latest stable Slurm as a binary\npackage using:\n\npkg install slurm-wlm\n\nOr, it can be built and installed from source using:\n\ncd /usr/ports/sysutils/slurm-wlm && make install\n\nThe binary package installs a minimal Slurm configuration suitable for\ntypical compute nodes.  Installing from source allows the user to enable\noptions such as mysql and gui tools via a configuration menu.\nLast modified 16 August 2024\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Daemons",
                "content": "slurmctld is sometimes called the \"controller\".\nIt orchestrates Slurm activities, including queuing of jobs,\nmonitoring node states, and allocating resources to jobs. There is an\noptional backup controller that automatically assumes control in the\nevent the primary controller fails (see the High\nAvailability section below).  The primary controller resumes\ncontrol whenever it is restored to service. The controller saves its\nstate to disk whenever there is a change in state (see\n\"StateSaveLocation\" in Configuration\nsection below).  This state can be recovered by the controller at\nstartup time.  State changes are saved so that jobs and other state\ninformation can be preserved when the controller moves (to or from a\nbackup controller) or is restarted.We recommend that you create a Unix user slurm for use by\nslurmctld. This user name will also be specified using the\nSlurmUser in the slurm.conf configuration file.\nThis user must exist on all nodes of the cluster.\nNote that files and directories used by slurmctld will need to be\nreadable or writable by the user SlurmUser (the Slurm configuration\nfiles must be readable; the log file directory and state save directory\nmust be writable).The slurmd daemon executes on every compute node. It resembles a\nremote shell daemon to export control to Slurm. Because slurmd initiates and\nmanages user jobs, it must execute as the user root.If you want to archive job accounting records to a database, the\nslurmdbd (Slurm DataBase Daemon) should be used. We recommend that\nyou defer adding accounting support until after basic Slurm functionality is\nestablished on your system. An Accounting web\npage contains more information.slurmctld and/or slurmd should be initiated at node startup\ntime per the Slurm configuration.The slurmrestd daemon was introduced in version 20.02 and allows\nclients to communicate with Slurm via the REST API. This is installed by\ndefault, assuming the prerequisites are met.\nIt has two run modes,\nallowing you to have it run as a traditional Unix service and always listen\nfor TCP connections, or you can have it run as an Inet service and only have\nit active when in use.High AvailabilityMultiple SlurmctldHost entries can be configured, with any entry beyond the\nfirst being treated as a backup host. Any backup hosts configured should be on\na different node than the node hosting the primary slurmctld. However, all\nhosts should mount a common file system containing the state information (see\n\"StateSaveLocation\" in the Configuration\nsection below).If more than one host is specified, when the primary fails the second listed\nSlurmctldHost will take over for it. When the primary returns to service, it\nnotifies the backup.  The backup then saves the state and returns to backup\nmode. The primary reads the saved state and resumes normal operation. Likewise,\nif both of the first two listed hosts fail the third SlurmctldHost will take\nover until the primary returns to service. Other than a brief period of non-\nresponsiveness, the transition back and forth should go undetected.Prior to 18.08, Slurm used the \n\"BackupAddr\" and \n\"BackupController\" parameters for High Availability. These\nparameters have been deprecated and are replaced by\n\"SlurmctldHost\".\nAlso see \"\nSlurmctldPrimaryOnProg\" and\n\"\nSlurmctldPrimaryOffProg\" to adjust the actions taken when machines\ntransition between being the primary controller.Any time the slurmctld daemon or hardware fails before state information\nreaches disk can result in lost state.\nSlurmctld writes state frequently (every five seconds by default), but with\nlarge numbers of jobs, the formatting and writing of records can take seconds\nand recent changes might not be written to disk.\nAnother example is if the state information is written to file, but that\ninformation is cached in memory rather than written to disk when the node fails.\nThe interval between state saves being written to disk can be configured at\nbuild time by defining SAVE_MAX_WAIT to a different value than five.A backup instance of slurmdbd can also be configured by specifying\n\nAccountingStorageBackupHost in slurm.conf, as well as\nDbdBackupHost in\nslurmdbd.conf. The backup host should be on a different machine than the one\nhosting the primary instance of slurmdbd. Both instances of slurmdbd should\nhave access to the same database. The\nnetwork page has a visual representation\nof how this might look.Infrastructure\n\nUser and Group Identification\n\nThere must be a uniform user and group name space (including\nUIDs and GIDs) across the cluster.\nIt is not necessary to permit user logins to the control hosts\n(SlurmctldHost), but the\nusers and groups must be resolvable on those hosts.Authentication of Slurm communications\n\nAll communications between Slurm components are authenticated. The\nauthentication infrastructure is provided by a dynamically loaded\nplugin chosen at runtime via the AuthType keyword in the Slurm\nconfiguration file. Until 23.11.0, the only supported authentication type was\nmunge, which requires the\ninstallation of the MUNGE package.\nWhen using MUNGE, all nodes in the cluster must be configured with the\nsame munge.key file. The MUNGE daemon, munged, must also be\nstarted before Slurm daemons. Note that MUNGE does require clocks to be\nsynchronized throughout the cluster, usually done by NTP.As of 23.11.0, AuthType can also be set to\nslurm, an internal authentication\nplugin. This plugin has similar requirements to MUNGE, requiring a key file\nshared to all Slurm daemons. The auth/slurm plugin requires installation of the\njwt package.MUNGE is currently the default and recommended option.\nThe configure script in the top-level directory of this distribution will\ndetermine which authentication plugins may be built.\nThe configuration file specifies which of the available plugins will be\nutilized.MPI supportSlurm supports many different MPI implementations.\nFor more information, see MPI.\n\nScheduler support\n\n\nSlurm can be configured with rather simple or quite sophisticated\nscheduling algorithms depending upon your needs and willingness to\nmanage the configuration (much of which requires a database).\nThe first configuration parameter of interest is PriorityType\nwith two options available: basic (first-in-first-out) and\nmultifactor.\nThe multifactor plugin will assign a priority to jobs based upon\na multitude of configuration parameters (age, size, fair-share allocation,\netc.) and its details are beyond the scope of this document.\nSee the Multifactor Job Priority Plugin\ndocument for details.\nThe SchedType configuration parameter controls how queued\njobs are scheduled and several options are available.\n\nbuiltin will initiate jobs strictly in their priority order,\ntypically (first-in-first-out) \nbackfill will initiate a lower-priority job if doing so does\nnot delay the expected initiation time of higher priority jobs; essentially\nusing smaller jobs to fill holes in the resource allocation plan. Effective\nbackfill scheduling does require users to specify job time limits.\ngang time-slices jobs in the same partition/queue and can be\nused to preempt jobs from lower-priority queues in order to execute\njobs in higher priority queues.\n\nFor more information about scheduling options see\nGang Scheduling,\nPreemption,\nResource Reservation Guide,\nResource Limits and\nSharing Consumable Resources.\nResource selection\n\n\nThe resource selection mechanism used by Slurm is controlled by the\nSelectType configuration parameter.\nIf you want to execute multiple jobs per node, but track and manage allocation\nof the processors, memory and other resources, the cons_tres (consumable\ntrackable resources) plugin is recommended.\nFor more information, please see\nConsumable Resources in Slurm.\nLogging\nSlurm uses syslog to record events if the SlurmctldLogFile and\nSlurmdLogFile locations are not set.\nAccounting\nSlurm supports accounting records being written to a simple text file,\ndirectly to a database (MySQL or MariaDB), or to a daemon securely\nmanaging accounting data for multiple clusters. For more information\nsee Accounting. \nCompute node access\n\n\nSlurm does not by itself limit access to allocated compute nodes,\nbut it does provide mechanisms to accomplish this.\nThere is a Pluggable Authentication Module (PAM) for restricting access\nto compute nodes available for download.\nWhen installed, the Slurm PAM module will prevent users from logging\ninto any node that has not be assigned to that user.\nOn job termination, any processes initiated by the user outside of\nSlurm's control may be killed using an Epilog script configured\nin slurm.conf.\nConfiguration\nThe Slurm configuration file includes a wide variety of parameters.\nThis configuration file must be available on each node of the cluster and\nmust have consistent contents. A full\ndescription of the parameters is included in the slurm.conf man page. Rather than\nduplicate that information, a minimal sample configuration file is shown below.\nYour slurm.conf file should define at least the configuration parameters defined\nin this sample and likely additional ones. Any text\nfollowing a \"#\" is considered a comment. The keywords in the file are\nnot case sensitive, although the argument typically is (e.g., \"SlurmUser=slurm\"\nmight be specified as \"slurmuser=slurm\"). The control machine, like\nall other machine specifications, can include both the host name and the name\nused for communications. In this case, the host's name is \"mcri\" and\nthe name \"emcri\" is used for communications.\nIn this case \"emcri\" is the private management network interface\nfor the host \"mcri\". Port numbers to be used for\ncommunications are specified as well as various timer values.\nThe SlurmUser must be created as needed prior to starting Slurm\nand must exist on all nodes in your cluster.\nThe parent directories for Slurm's log files, process ID files,\nstate save directories, etc. are not created by Slurm.\nThey must be created and made writable by SlurmUser as needed prior to\nstarting Slurm daemons.\nThe StateSaveLocation is used to store information about the current\nstate of the cluster, including information about queued, running and recently\ncompleted jobs. The directory used should be on a low-latency local disk to\nprevent file system delays from affecting Slurm performance. If using a backup\nhost, the StateSaveLocation should reside on a file system shared by the two\nhosts. We do not recommend using NFS to make the directory accessible to both\nhosts, but do recommend a shared mount that is accessible to the two\ncontrollers and allows low-latency reads and writes to the disk. If a\ncontroller comes up without access to the state information, queued and\nrunning jobs will be cancelled.\nA description of the nodes and their grouping into partitions is required.\nA simple node range expression may optionally be used to specify\nranges of nodes to avoid building a configuration file with large\nnumbers of entries. The node range expression can contain one\npair of square brackets with a sequence of comma separated\nnumbers and/or ranges of numbers separated by a \"-\"\n(e.g. \"linux[0-64,128]\", or \"lx[15,18,32-33]\").\nUp to two numeric ranges can be included in the expression\n(e.g. \"rack[0-63]_blade[0-41]\").\nIf one or more numeric expressions are included, one of them\nmust be at the end of the name (e.g. \"unit[0-31]rack\" is invalid),\nbut arbitrary names can always be used in a comma separated list.\nNode names can have up to three name specifications:\nNodeName is the name used by all Slurm tools when referring to the node,\nNodeAddr is the name or IP address Slurm uses to communicate with the node, and\nNodeHostname is the name returned by the command /bin/hostname -s.\nOnly NodeName is required (the others default to the same name),\nalthough supporting all three parameters provides complete control over\nnaming and addressing the nodes.  See the slurm.conf man page for\ndetails on all configuration parameters.\nNodes can be in more than one partition and each partition can have different\nconstraints (permitted users, time limits, job size limits, etc.).\nEach partition can thus be considered a separate queue.\nPartition and node specifications use node range expressions to identify\nnodes in a concise fashion. This configuration file defines a 1154-node cluster\nfor Slurm, but it might be used for a much larger cluster by just changing a few\nnode range expressions. Specify the minimum processor count (CPUs), real memory\nspace (RealMemory, megabytes), and temporary disk space (TmpDisk, megabytes) that\na node should have to be considered available for use. Any node lacking these\nminimum configuration values will be considered DOWN and not scheduled.\nNote that a more extensive sample configuration file is provided in\netc/slurm.conf.example. We also have a web-based\nconfiguration tool which can\nbe used to build a simple configuration file, which can then be\nmanually edited for more complex configurations.\n\n#\n# Sample /etc/slurm.conf for mcr.llnl.gov\n#\nSlurmctldHost=mcri(12.34.56.78)\nSlurmctldHost=mcrj(12.34.56.79)\n#\nAuthType=auth/munge\nEpilog=/usr/local/slurm/etc/epilog\nJobCompLoc=/var/tmp/jette/slurm.job.log\nJobCompType=jobcomp/filetxt\nPluginDir=/usr/local/slurm/lib/slurm\nProlog=/usr/local/slurm/etc/prolog\nSchedulerType=sched/backfill\nSelectType=select/linear\nSlurmUser=slurm\nSlurmctldPort=7002\nSlurmctldTimeout=300\nSlurmdPort=7003\nSlurmdSpoolDir=/var/spool/slurmd.spool\nSlurmdTimeout=300\nStateSaveLocation=/var/spool/slurm.state\nTreeWidth=16\n#\n# Node Configurations\n#\nNodeName=DEFAULT CPUs=2 RealMemory=2000 TmpDisk=64000 State=UNKNOWN\nNodeName=mcr[0-1151] NodeAddr=emcr[0-1151]\n#\n# Partition Configurations\n#\nPartitionName=DEFAULT State=UP\nPartitionName=pdebug Nodes=mcr[0-191] MaxTime=30 MaxNodes=32 Default=YES\nPartitionName=pbatch Nodes=mcr[192-1151]\n\nSecurity\nBesides authentication of Slurm communications based upon the value\nof the AuthType, digital signatures are used in job step\ncredentials.\nThis signature is used by slurmctld to construct a job step\ncredential, which is sent to srun and then forwarded to\nslurmd to initiate job steps.\nThis design offers improved performance by removing much of the\njob step initiation overhead from the  slurmctld  daemon.\nThe digital signature mechanism is specified by the CredType\nconfiguration parameter and the default mechanism is MUNGE. \nPluggable Authentication Module (PAM) support\n\n\nA PAM module (Pluggable Authentication Module) is available for Slurm that\ncan prevent a user from accessing a node which he has not been allocated,\nif that mode of operation is desired.\nStarting the Daemons\n\n\nFor testing purposes you may want to start by just running slurmctld and slurmd\non one node. By default, they execute in the background. Use the -D\noption for each daemon to execute them in the foreground and logging will be done\nto your terminal. The -v option will log events\nin more detail with more v's increasing the level of detail (e.g. -vvvvvv).\nYou can use one window to execute \"slurmctld -D -vvvvvv\",\na second window to execute \"slurmd -D -vvvvv\".\nYou may see errors such as \"Connection refused\" or \"Node X not responding\"\nwhile one daemon is operative and the other is being started, but the\ndaemons can be started in any order and proper communications will be\nestablished once both daemons complete initialization.\nYou can use a third window to execute commands such as\n\"srun -N1 /bin/hostname\" to confirm functionality.\nAnother important option for the daemons is \"-c\"\nto clear previous state information. Without the \"-c\"\noption, the daemons will restore any previously saved state information: node\nstate, job state, etc. With the \"-c\" option all\npreviously running jobs will be purged and node state will be restored to the\nvalues specified in the configuration file. This means that a node configured\ndown manually using the scontrol command will\nbe returned to service unless noted as being down in the configuration file.\nIn practice, Slurm consistently restarts with preservation.\nAdministration Examples\n\n\nscontrol can be used to print all system information\nand modify most of it. Only a few examples are shown below. Please see the scontrol\nman page for full details. The commands and options are all case insensitive.\nPrint detailed state of all jobs in the system.\n\nadev0: scontrol\nscontrol: show job\nJobId=475 UserId=bob(6885) Name=sleep JobState=COMPLETED\n   Priority=4294901286 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:53:41 EndTime=03/19-12:53:59\n   NodeList=adev8 NodeListIndecies=-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n\nJobId=476 UserId=bob(6885) Name=sleep JobState=RUNNING\n   Priority=4294901285 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:54:01 EndTime=NONE\n   NodeList=adev8 NodeListIndecies=8,8,-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n Print the detailed state of job 477 and change its priority to\nzero. A priority of zero prevents a job from being initiated (it is held in \"pending\"\nstate).\n\nadev0: scontrol\nscontrol: show job 477\nJobId=477 UserId=bob(6885) Name=sleep JobState=PENDING\n   Priority=4294901286 Partition=batch BatchFlag=0\n   more data removed....\nscontrol: update JobId=477 Priority=0\n\nPrint the state of node adev13 and drain it. To drain a node, specify a new\nstate of DRAIN, DRAINED, or DRAINING. Slurm will automatically set it to the appropriate\nvalue of either DRAINING or DRAINED depending on whether the node is allocated\nor not. Return it to service later.\n\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=ALLOCATED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=DRAIN\nscontrol: show node adev13\nNodeName=adev13 State=DRAINING CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: quit\nLater\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=DRAINED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=IDLE\n Reconfigure all Slurm daemons on all nodes. This should\nbe done after changing the Slurm configuration file.\n\nadev0: scontrol reconfig\n Print the current Slurm configuration. This also reports if the\nprimary and secondary controllers (slurmctld daemons) are responding. To just\nsee the state of the controllers, use the command ping.\n\nadev0: scontrol show config\nConfiguration data as of 2019-03-29T12:20:45\n...\nSlurmctldAddr           = eadevi\nSlurmctldDebug          = info\nSlurmctldHost[0]        = adevi\nSlurmctldHost[1]        = adevj\nSlurmctldLogFile        = /var/log/slurmctld.log\n...\n\nSlurmctld(primary) at adevi is UP\nSlurmctld(backup) at adevj is UP\n Shutdown all Slurm daemons on all nodes.\n\nadev0: scontrol shutdown\n\nUpgrades\nSlurm supports in-place upgrades between certain versions. Important details\nabout the steps necessary to perform an upgrade and the potential complications\nto prepare for are contained on this page:\nUpgrade Guide\nFreeBSD\nFreeBSD administrators can install the latest stable Slurm as a binary\npackage using:\n\npkg install slurm-wlm\n\nOr, it can be built and installed from source using:\n\ncd /usr/ports/sysutils/slurm-wlm && make install\n\nThe binary package installs a minimal Slurm configuration suitable for\ntypical compute nodes.  Installing from source allows the user to enable\noptions such as mysql and gui tools via a configuration menu.\nLast modified 16 August 2024\n"
            },
            {
                "title": "Configuration",
                "content": "The Slurm configuration file includes a wide variety of parameters.\nThis configuration file must be available on each node of the cluster and\nmust have consistent contents. A full\ndescription of the parameters is included in the slurm.conf man page. Rather than\nduplicate that information, a minimal sample configuration file is shown below.\nYour slurm.conf file should define at least the configuration parameters defined\nin this sample and likely additional ones. Any text\nfollowing a \"#\" is considered a comment. The keywords in the file are\nnot case sensitive, although the argument typically is (e.g., \"SlurmUser=slurm\"\nmight be specified as \"slurmuser=slurm\"). The control machine, like\nall other machine specifications, can include both the host name and the name\nused for communications. In this case, the host's name is \"mcri\" and\nthe name \"emcri\" is used for communications.\nIn this case \"emcri\" is the private management network interface\nfor the host \"mcri\". Port numbers to be used for\ncommunications are specified as well as various timer values.The SlurmUser must be created as needed prior to starting Slurm\nand must exist on all nodes in your cluster.\nThe parent directories for Slurm's log files, process ID files,\nstate save directories, etc. are not created by Slurm.\nThey must be created and made writable by SlurmUser as needed prior to\nstarting Slurm daemons.The StateSaveLocation is used to store information about the current\nstate of the cluster, including information about queued, running and recently\ncompleted jobs. The directory used should be on a low-latency local disk to\nprevent file system delays from affecting Slurm performance. If using a backup\nhost, the StateSaveLocation should reside on a file system shared by the two\nhosts. We do not recommend using NFS to make the directory accessible to both\nhosts, but do recommend a shared mount that is accessible to the two\ncontrollers and allows low-latency reads and writes to the disk. If a\ncontroller comes up without access to the state information, queued and\nrunning jobs will be cancelled.A description of the nodes and their grouping into partitions is required.\nA simple node range expression may optionally be used to specify\nranges of nodes to avoid building a configuration file with large\nnumbers of entries. The node range expression can contain one\npair of square brackets with a sequence of comma separated\nnumbers and/or ranges of numbers separated by a \"-\"\n(e.g. \"linux[0-64,128]\", or \"lx[15,18,32-33]\").\nUp to two numeric ranges can be included in the expression\n(e.g. \"rack[0-63]_blade[0-41]\").\nIf one or more numeric expressions are included, one of them\nmust be at the end of the name (e.g. \"unit[0-31]rack\" is invalid),\nbut arbitrary names can always be used in a comma separated list.Node names can have up to three name specifications:\nNodeName is the name used by all Slurm tools when referring to the node,\nNodeAddr is the name or IP address Slurm uses to communicate with the node, and\nNodeHostname is the name returned by the command /bin/hostname -s.\nOnly NodeName is required (the others default to the same name),\nalthough supporting all three parameters provides complete control over\nnaming and addressing the nodes.  See the slurm.conf man page for\ndetails on all configuration parameters.Nodes can be in more than one partition and each partition can have different\nconstraints (permitted users, time limits, job size limits, etc.).\nEach partition can thus be considered a separate queue.\nPartition and node specifications use node range expressions to identify\nnodes in a concise fashion. This configuration file defines a 1154-node cluster\nfor Slurm, but it might be used for a much larger cluster by just changing a few\nnode range expressions. Specify the minimum processor count (CPUs), real memory\nspace (RealMemory, megabytes), and temporary disk space (TmpDisk, megabytes) that\na node should have to be considered available for use. Any node lacking these\nminimum configuration values will be considered DOWN and not scheduled.\nNote that a more extensive sample configuration file is provided in\netc/slurm.conf.example. We also have a web-based\nconfiguration tool which can\nbe used to build a simple configuration file, which can then be\nmanually edited for more complex configurations.\n#\n# Sample /etc/slurm.conf for mcr.llnl.gov\n#\nSlurmctldHost=mcri(12.34.56.78)\nSlurmctldHost=mcrj(12.34.56.79)\n#\nAuthType=auth/munge\nEpilog=/usr/local/slurm/etc/epilog\nJobCompLoc=/var/tmp/jette/slurm.job.log\nJobCompType=jobcomp/filetxt\nPluginDir=/usr/local/slurm/lib/slurm\nProlog=/usr/local/slurm/etc/prolog\nSchedulerType=sched/backfill\nSelectType=select/linear\nSlurmUser=slurm\nSlurmctldPort=7002\nSlurmctldTimeout=300\nSlurmdPort=7003\nSlurmdSpoolDir=/var/spool/slurmd.spool\nSlurmdTimeout=300\nStateSaveLocation=/var/spool/slurm.state\nTreeWidth=16\n#\n# Node Configurations\n#\nNodeName=DEFAULT CPUs=2 RealMemory=2000 TmpDisk=64000 State=UNKNOWN\nNodeName=mcr[0-1151] NodeAddr=emcr[0-1151]\n#\n# Partition Configurations\n#\nPartitionName=DEFAULT State=UP\nPartitionName=pdebug Nodes=mcr[0-191] MaxTime=30 MaxNodes=32 Default=YES\nPartitionName=pbatch Nodes=mcr[192-1151]\nSecurityBesides authentication of Slurm communications based upon the value\nof the AuthType, digital signatures are used in job step\ncredentials.\nThis signature is used by slurmctld to construct a job step\ncredential, which is sent to srun and then forwarded to\nslurmd to initiate job steps.\nThis design offers improved performance by removing much of the\njob step initiation overhead from the  slurmctld  daemon.\nThe digital signature mechanism is specified by the CredType\nconfiguration parameter and the default mechanism is MUNGE. Pluggable Authentication Module (PAM) support\n\nA PAM module (Pluggable Authentication Module) is available for Slurm that\ncan prevent a user from accessing a node which he has not been allocated,\nif that mode of operation is desired.Starting the Daemons\n\nFor testing purposes you may want to start by just running slurmctld and slurmd\non one node. By default, they execute in the background. Use the -D\noption for each daemon to execute them in the foreground and logging will be done\nto your terminal. The -v option will log events\nin more detail with more v's increasing the level of detail (e.g. -vvvvvv).\nYou can use one window to execute \"slurmctld -D -vvvvvv\",\na second window to execute \"slurmd -D -vvvvv\".\nYou may see errors such as \"Connection refused\" or \"Node X not responding\"\nwhile one daemon is operative and the other is being started, but the\ndaemons can be started in any order and proper communications will be\nestablished once both daemons complete initialization.\nYou can use a third window to execute commands such as\n\"srun -N1 /bin/hostname\" to confirm functionality.Another important option for the daemons is \"-c\"\nto clear previous state information. Without the \"-c\"\noption, the daemons will restore any previously saved state information: node\nstate, job state, etc. With the \"-c\" option all\npreviously running jobs will be purged and node state will be restored to the\nvalues specified in the configuration file. This means that a node configured\ndown manually using the scontrol command will\nbe returned to service unless noted as being down in the configuration file.\nIn practice, Slurm consistently restarts with preservation.Administration Examples\n\nscontrol can be used to print all system information\nand modify most of it. Only a few examples are shown below. Please see the scontrol\nman page for full details. The commands and options are all case insensitive.Print detailed state of all jobs in the system.\nadev0: scontrol\nscontrol: show job\nJobId=475 UserId=bob(6885) Name=sleep JobState=COMPLETED\n   Priority=4294901286 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:53:41 EndTime=03/19-12:53:59\n   NodeList=adev8 NodeListIndecies=-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n\nJobId=476 UserId=bob(6885) Name=sleep JobState=RUNNING\n   Priority=4294901285 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:54:01 EndTime=NONE\n   NodeList=adev8 NodeListIndecies=8,8,-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\nPrint the detailed state of job 477 and change its priority to\nzero. A priority of zero prevents a job from being initiated (it is held in \"pending\"\nstate).\nadev0: scontrol\nscontrol: show job 477\nJobId=477 UserId=bob(6885) Name=sleep JobState=PENDING\n   Priority=4294901286 Partition=batch BatchFlag=0\n   more data removed....\nscontrol: update JobId=477 Priority=0\nPrint the state of node adev13 and drain it. To drain a node, specify a new\nstate of DRAIN, DRAINED, or DRAINING. Slurm will automatically set it to the appropriate\nvalue of either DRAINING or DRAINED depending on whether the node is allocated\nor not. Return it to service later.\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=ALLOCATED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=DRAIN\nscontrol: show node adev13\nNodeName=adev13 State=DRAINING CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: quit\nLater\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=DRAINED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=IDLE\nReconfigure all Slurm daemons on all nodes. This should\nbe done after changing the Slurm configuration file.\nadev0: scontrol reconfig\nPrint the current Slurm configuration. This also reports if the\nprimary and secondary controllers (slurmctld daemons) are responding. To just\nsee the state of the controllers, use the command ping.\nadev0: scontrol show config\nConfiguration data as of 2019-03-29T12:20:45\n...\nSlurmctldAddr           = eadevi\nSlurmctldDebug          = info\nSlurmctldHost[0]        = adevi\nSlurmctldHost[1]        = adevj\nSlurmctldLogFile        = /var/log/slurmctld.log\n...\n\nSlurmctld(primary) at adevi is UP\nSlurmctld(backup) at adevj is UP\nShutdown all Slurm daemons on all nodes.\nadev0: scontrol shutdown\nUpgradesSlurm supports in-place upgrades between certain versions. Important details\nabout the steps necessary to perform an upgrade and the potential complications\nto prepare for are contained on this page:\nUpgrade GuideFreeBSDFreeBSD administrators can install the latest stable Slurm as a binary\npackage using:\npkg install slurm-wlm\nOr, it can be built and installed from source using:\ncd /usr/ports/sysutils/slurm-wlm && make install\nThe binary package installs a minimal Slurm configuration suitable for\ntypical compute nodes.  Installing from source allows the user to enable\noptions such as mysql and gui tools via a configuration menu.Last modified 16 August 2024"
            },
            {
                "title": "RPMs Installed",
                "content": "The RPMs needed on the head node, compute nodes, and slurmdbd node can vary\nby configuration, but here is a suggested starting point:\n\nHead Node (where the slurmctld daemon runs),\n    Compute and Login Nodes\n\t\nslurm\nslurm-perlapi\nslurm-slurmctld (only on the head node)\nslurm-slurmd (only on the compute nodes)\n\n\nSlurmDBD Node\n\t\nslurm\nslurm-slurmdbd\n\n\n\nBuilding Debian Packages\n\n\nBeginning with Slurm 23.11.0, Slurm includes the files required to build\nDebian packages. These packages conflict with the packages shipped with Debian\nbased distributions, and are named distinctly to differentiate them. After\ndownloading the desired version of Slurm, the following can be done to build\nthe packages:\n\nInstall basic Debian package build requirements:\napt-get install build-essential fakeroot devscripts equivs\n\nUnpack the distributed tarball:\ntar -xaf slurm*tar.bz2\n\ncd to the directory containing the Slurm source\nInstall the Slurm package dependencies:\nmk-build-deps -i debian/control\n\nBuild the Slurm packages:\ndebuild -b -uc -us\n\n\nThe packages will be in the parent directory after debuild completes.\nInstalling Debian Packages\n\n\nThe packages needed on the head node, compute nodes, and slurmdbd node can\nvary site to site, but this is a good starting point:\n\nSlurmDBD Node\n\t\nslurm-smd\nslurm-smd-slurmdbd\n\n\nHead Node (slurmctld node)\n\t\nslurm-smd\nslurm-smd-slurmctld\nslurm-smd-client\n\n\nCompute Nodes (slurmd node)\n\t\nslurm-smd\nslurm-smd-slurmd\nslurm-smd-client\n\n\nLogin Nodes\n\t\nslurm-smd\nslurm-smd-client\n\n\n\nBuilding Manually\n\n\nInstructions to build and install Slurm manually are shown below.\nThis is significantly more complicated to manage than the RPM and DEB build\nprocedures, so this approach is only recommended for developers or\nadvanced users who are looking for a more customized install.\nSee the README and INSTALL files in the source distribution for more details.\n\n\nUnpack the distributed tarball:\ntar -xaf slurm*tar.bz2\ncd to the directory containing the Slurm source and type\n./configure with appropriate options (see below).\nType make install to compile and install the programs,\ndocumentation, libraries, header files, etc.\nType ldconfig -n <library_location> so that the Slurm\nlibraries can be found by applications that intend to use Slurm APIs directly.\nThe library location will be a subdirectory of PREFIX (described below) and\ndepend upon the system type and configuration, typically lib or lib64.\nFor example, if PREFIX is \"/usr\" and the subdirectory is \"lib64\" then you would\nfind that a file named \"/usr/lib64/libslurm.so\" was installed and the command\nldconfig -n /usr/lib64 should be executed.\n\nA full list of configure options will be returned by the\ncommand configure --help. The most commonly used arguments\nto the configure command include:\n--enable-debug\nEnable additional debugging logic within Slurm.\n--prefix=PREFIX\nInstall architecture-independent files in PREFIX; default value is /usr/local.\n--sysconfdir=DIR\nSpecify location of Slurm configuration file. The default value is PREFIX/etc\nDaemons\nslurmctld is sometimes called the \"controller\".\nIt orchestrates Slurm activities, including queuing of jobs,\nmonitoring node states, and allocating resources to jobs. There is an\noptional backup controller that automatically assumes control in the\nevent the primary controller fails (see the High\nAvailability section below).  The primary controller resumes\ncontrol whenever it is restored to service. The controller saves its\nstate to disk whenever there is a change in state (see\n\"StateSaveLocation\" in Configuration\nsection below).  This state can be recovered by the controller at\nstartup time.  State changes are saved so that jobs and other state\ninformation can be preserved when the controller moves (to or from a\nbackup controller) or is restarted.\nWe recommend that you create a Unix user slurm for use by\nslurmctld. This user name will also be specified using the\nSlurmUser in the slurm.conf configuration file.\nThis user must exist on all nodes of the cluster.\nNote that files and directories used by slurmctld will need to be\nreadable or writable by the user SlurmUser (the Slurm configuration\nfiles must be readable; the log file directory and state save directory\nmust be writable).\nThe slurmd daemon executes on every compute node. It resembles a\nremote shell daemon to export control to Slurm. Because slurmd initiates and\nmanages user jobs, it must execute as the user root.\nIf you want to archive job accounting records to a database, the\nslurmdbd (Slurm DataBase Daemon) should be used. We recommend that\nyou defer adding accounting support until after basic Slurm functionality is\nestablished on your system. An Accounting web\npage contains more information.\nslurmctld and/or slurmd should be initiated at node startup\ntime per the Slurm configuration.\nThe slurmrestd daemon was introduced in version 20.02 and allows\nclients to communicate with Slurm via the REST API. This is installed by\ndefault, assuming the prerequisites are met.\nIt has two run modes,\nallowing you to have it run as a traditional Unix service and always listen\nfor TCP connections, or you can have it run as an Inet service and only have\nit active when in use.\nHigh Availability\nMultiple SlurmctldHost entries can be configured, with any entry beyond the\nfirst being treated as a backup host. Any backup hosts configured should be on\na different node than the node hosting the primary slurmctld. However, all\nhosts should mount a common file system containing the state information (see\n\"StateSaveLocation\" in the Configuration\nsection below).\nIf more than one host is specified, when the primary fails the second listed\nSlurmctldHost will take over for it. When the primary returns to service, it\nnotifies the backup.  The backup then saves the state and returns to backup\nmode. The primary reads the saved state and resumes normal operation. Likewise,\nif both of the first two listed hosts fail the third SlurmctldHost will take\nover until the primary returns to service. Other than a brief period of non-\nresponsiveness, the transition back and forth should go undetected.\nPrior to 18.08, Slurm used the \n\"BackupAddr\" and \n\"BackupController\" parameters for High Availability. These\nparameters have been deprecated and are replaced by\n\"SlurmctldHost\".\nAlso see \"\nSlurmctldPrimaryOnProg\" and\n\"\nSlurmctldPrimaryOffProg\" to adjust the actions taken when machines\ntransition between being the primary controller.\nAny time the slurmctld daemon or hardware fails before state information\nreaches disk can result in lost state.\nSlurmctld writes state frequently (every five seconds by default), but with\nlarge numbers of jobs, the formatting and writing of records can take seconds\nand recent changes might not be written to disk.\nAnother example is if the state information is written to file, but that\ninformation is cached in memory rather than written to disk when the node fails.\nThe interval between state saves being written to disk can be configured at\nbuild time by defining SAVE_MAX_WAIT to a different value than five.\nA backup instance of slurmdbd can also be configured by specifying\n\nAccountingStorageBackupHost in slurm.conf, as well as\nDbdBackupHost in\nslurmdbd.conf. The backup host should be on a different machine than the one\nhosting the primary instance of slurmdbd. Both instances of slurmdbd should\nhave access to the same database. The\nnetwork page has a visual representation\nof how this might look.\nInfrastructure\n\n\nUser and Group Identification\n\n\nThere must be a uniform user and group name space (including\nUIDs and GIDs) across the cluster.\nIt is not necessary to permit user logins to the control hosts\n(SlurmctldHost), but the\nusers and groups must be resolvable on those hosts.\nAuthentication of Slurm communications\n\n\nAll communications between Slurm components are authenticated. The\nauthentication infrastructure is provided by a dynamically loaded\nplugin chosen at runtime via the AuthType keyword in the Slurm\nconfiguration file. Until 23.11.0, the only supported authentication type was\nmunge, which requires the\ninstallation of the MUNGE package.\nWhen using MUNGE, all nodes in the cluster must be configured with the\nsame munge.key file. The MUNGE daemon, munged, must also be\nstarted before Slurm daemons. Note that MUNGE does require clocks to be\nsynchronized throughout the cluster, usually done by NTP.\nAs of 23.11.0, AuthType can also be set to\nslurm, an internal authentication\nplugin. This plugin has similar requirements to MUNGE, requiring a key file\nshared to all Slurm daemons. The auth/slurm plugin requires installation of the\njwt package.\nMUNGE is currently the default and recommended option.\nThe configure script in the top-level directory of this distribution will\ndetermine which authentication plugins may be built.\nThe configuration file specifies which of the available plugins will be\nutilized.\nMPI support\nSlurm supports many different MPI implementations.\nFor more information, see MPI.\n\nScheduler support\n\n\nSlurm can be configured with rather simple or quite sophisticated\nscheduling algorithms depending upon your needs and willingness to\nmanage the configuration (much of which requires a database).\nThe first configuration parameter of interest is PriorityType\nwith two options available: basic (first-in-first-out) and\nmultifactor.\nThe multifactor plugin will assign a priority to jobs based upon\na multitude of configuration parameters (age, size, fair-share allocation,\netc.) and its details are beyond the scope of this document.\nSee the Multifactor Job Priority Plugin\ndocument for details.\nThe SchedType configuration parameter controls how queued\njobs are scheduled and several options are available.\n\nbuiltin will initiate jobs strictly in their priority order,\ntypically (first-in-first-out) \nbackfill will initiate a lower-priority job if doing so does\nnot delay the expected initiation time of higher priority jobs; essentially\nusing smaller jobs to fill holes in the resource allocation plan. Effective\nbackfill scheduling does require users to specify job time limits.\ngang time-slices jobs in the same partition/queue and can be\nused to preempt jobs from lower-priority queues in order to execute\njobs in higher priority queues.\n\nFor more information about scheduling options see\nGang Scheduling,\nPreemption,\nResource Reservation Guide,\nResource Limits and\nSharing Consumable Resources.\nResource selection\n\n\nThe resource selection mechanism used by Slurm is controlled by the\nSelectType configuration parameter.\nIf you want to execute multiple jobs per node, but track and manage allocation\nof the processors, memory and other resources, the cons_tres (consumable\ntrackable resources) plugin is recommended.\nFor more information, please see\nConsumable Resources in Slurm.\nLogging\nSlurm uses syslog to record events if the SlurmctldLogFile and\nSlurmdLogFile locations are not set.\nAccounting\nSlurm supports accounting records being written to a simple text file,\ndirectly to a database (MySQL or MariaDB), or to a daemon securely\nmanaging accounting data for multiple clusters. For more information\nsee Accounting. \nCompute node access\n\n\nSlurm does not by itself limit access to allocated compute nodes,\nbut it does provide mechanisms to accomplish this.\nThere is a Pluggable Authentication Module (PAM) for restricting access\nto compute nodes available for download.\nWhen installed, the Slurm PAM module will prevent users from logging\ninto any node that has not be assigned to that user.\nOn job termination, any processes initiated by the user outside of\nSlurm's control may be killed using an Epilog script configured\nin slurm.conf.\nConfiguration\nThe Slurm configuration file includes a wide variety of parameters.\nThis configuration file must be available on each node of the cluster and\nmust have consistent contents. A full\ndescription of the parameters is included in the slurm.conf man page. Rather than\nduplicate that information, a minimal sample configuration file is shown below.\nYour slurm.conf file should define at least the configuration parameters defined\nin this sample and likely additional ones. Any text\nfollowing a \"#\" is considered a comment. The keywords in the file are\nnot case sensitive, although the argument typically is (e.g., \"SlurmUser=slurm\"\nmight be specified as \"slurmuser=slurm\"). The control machine, like\nall other machine specifications, can include both the host name and the name\nused for communications. In this case, the host's name is \"mcri\" and\nthe name \"emcri\" is used for communications.\nIn this case \"emcri\" is the private management network interface\nfor the host \"mcri\". Port numbers to be used for\ncommunications are specified as well as various timer values.\nThe SlurmUser must be created as needed prior to starting Slurm\nand must exist on all nodes in your cluster.\nThe parent directories for Slurm's log files, process ID files,\nstate save directories, etc. are not created by Slurm.\nThey must be created and made writable by SlurmUser as needed prior to\nstarting Slurm daemons.\nThe StateSaveLocation is used to store information about the current\nstate of the cluster, including information about queued, running and recently\ncompleted jobs. The directory used should be on a low-latency local disk to\nprevent file system delays from affecting Slurm performance. If using a backup\nhost, the StateSaveLocation should reside on a file system shared by the two\nhosts. We do not recommend using NFS to make the directory accessible to both\nhosts, but do recommend a shared mount that is accessible to the two\ncontrollers and allows low-latency reads and writes to the disk. If a\ncontroller comes up without access to the state information, queued and\nrunning jobs will be cancelled.\nA description of the nodes and their grouping into partitions is required.\nA simple node range expression may optionally be used to specify\nranges of nodes to avoid building a configuration file with large\nnumbers of entries. The node range expression can contain one\npair of square brackets with a sequence of comma separated\nnumbers and/or ranges of numbers separated by a \"-\"\n(e.g. \"linux[0-64,128]\", or \"lx[15,18,32-33]\").\nUp to two numeric ranges can be included in the expression\n(e.g. \"rack[0-63]_blade[0-41]\").\nIf one or more numeric expressions are included, one of them\nmust be at the end of the name (e.g. \"unit[0-31]rack\" is invalid),\nbut arbitrary names can always be used in a comma separated list.\nNode names can have up to three name specifications:\nNodeName is the name used by all Slurm tools when referring to the node,\nNodeAddr is the name or IP address Slurm uses to communicate with the node, and\nNodeHostname is the name returned by the command /bin/hostname -s.\nOnly NodeName is required (the others default to the same name),\nalthough supporting all three parameters provides complete control over\nnaming and addressing the nodes.  See the slurm.conf man page for\ndetails on all configuration parameters.\nNodes can be in more than one partition and each partition can have different\nconstraints (permitted users, time limits, job size limits, etc.).\nEach partition can thus be considered a separate queue.\nPartition and node specifications use node range expressions to identify\nnodes in a concise fashion. This configuration file defines a 1154-node cluster\nfor Slurm, but it might be used for a much larger cluster by just changing a few\nnode range expressions. Specify the minimum processor count (CPUs), real memory\nspace (RealMemory, megabytes), and temporary disk space (TmpDisk, megabytes) that\na node should have to be considered available for use. Any node lacking these\nminimum configuration values will be considered DOWN and not scheduled.\nNote that a more extensive sample configuration file is provided in\netc/slurm.conf.example. We also have a web-based\nconfiguration tool which can\nbe used to build a simple configuration file, which can then be\nmanually edited for more complex configurations.\n\n#\n# Sample /etc/slurm.conf for mcr.llnl.gov\n#\nSlurmctldHost=mcri(12.34.56.78)\nSlurmctldHost=mcrj(12.34.56.79)\n#\nAuthType=auth/munge\nEpilog=/usr/local/slurm/etc/epilog\nJobCompLoc=/var/tmp/jette/slurm.job.log\nJobCompType=jobcomp/filetxt\nPluginDir=/usr/local/slurm/lib/slurm\nProlog=/usr/local/slurm/etc/prolog\nSchedulerType=sched/backfill\nSelectType=select/linear\nSlurmUser=slurm\nSlurmctldPort=7002\nSlurmctldTimeout=300\nSlurmdPort=7003\nSlurmdSpoolDir=/var/spool/slurmd.spool\nSlurmdTimeout=300\nStateSaveLocation=/var/spool/slurm.state\nTreeWidth=16\n#\n# Node Configurations\n#\nNodeName=DEFAULT CPUs=2 RealMemory=2000 TmpDisk=64000 State=UNKNOWN\nNodeName=mcr[0-1151] NodeAddr=emcr[0-1151]\n#\n# Partition Configurations\n#\nPartitionName=DEFAULT State=UP\nPartitionName=pdebug Nodes=mcr[0-191] MaxTime=30 MaxNodes=32 Default=YES\nPartitionName=pbatch Nodes=mcr[192-1151]\n\nSecurity\nBesides authentication of Slurm communications based upon the value\nof the AuthType, digital signatures are used in job step\ncredentials.\nThis signature is used by slurmctld to construct a job step\ncredential, which is sent to srun and then forwarded to\nslurmd to initiate job steps.\nThis design offers improved performance by removing much of the\njob step initiation overhead from the  slurmctld  daemon.\nThe digital signature mechanism is specified by the CredType\nconfiguration parameter and the default mechanism is MUNGE. \nPluggable Authentication Module (PAM) support\n\n\nA PAM module (Pluggable Authentication Module) is available for Slurm that\ncan prevent a user from accessing a node which he has not been allocated,\nif that mode of operation is desired.\nStarting the Daemons\n\n\nFor testing purposes you may want to start by just running slurmctld and slurmd\non one node. By default, they execute in the background. Use the -D\noption for each daemon to execute them in the foreground and logging will be done\nto your terminal. The -v option will log events\nin more detail with more v's increasing the level of detail (e.g. -vvvvvv).\nYou can use one window to execute \"slurmctld -D -vvvvvv\",\na second window to execute \"slurmd -D -vvvvv\".\nYou may see errors such as \"Connection refused\" or \"Node X not responding\"\nwhile one daemon is operative and the other is being started, but the\ndaemons can be started in any order and proper communications will be\nestablished once both daemons complete initialization.\nYou can use a third window to execute commands such as\n\"srun -N1 /bin/hostname\" to confirm functionality.\nAnother important option for the daemons is \"-c\"\nto clear previous state information. Without the \"-c\"\noption, the daemons will restore any previously saved state information: node\nstate, job state, etc. With the \"-c\" option all\npreviously running jobs will be purged and node state will be restored to the\nvalues specified in the configuration file. This means that a node configured\ndown manually using the scontrol command will\nbe returned to service unless noted as being down in the configuration file.\nIn practice, Slurm consistently restarts with preservation.\nAdministration Examples\n\n\nscontrol can be used to print all system information\nand modify most of it. Only a few examples are shown below. Please see the scontrol\nman page for full details. The commands and options are all case insensitive.\nPrint detailed state of all jobs in the system.\n\nadev0: scontrol\nscontrol: show job\nJobId=475 UserId=bob(6885) Name=sleep JobState=COMPLETED\n   Priority=4294901286 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:53:41 EndTime=03/19-12:53:59\n   NodeList=adev8 NodeListIndecies=-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n\nJobId=476 UserId=bob(6885) Name=sleep JobState=RUNNING\n   Priority=4294901285 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:54:01 EndTime=NONE\n   NodeList=adev8 NodeListIndecies=8,8,-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n Print the detailed state of job 477 and change its priority to\nzero. A priority of zero prevents a job from being initiated (it is held in \"pending\"\nstate).\n\nadev0: scontrol\nscontrol: show job 477\nJobId=477 UserId=bob(6885) Name=sleep JobState=PENDING\n   Priority=4294901286 Partition=batch BatchFlag=0\n   more data removed....\nscontrol: update JobId=477 Priority=0\n\nPrint the state of node adev13 and drain it. To drain a node, specify a new\nstate of DRAIN, DRAINED, or DRAINING. Slurm will automatically set it to the appropriate\nvalue of either DRAINING or DRAINED depending on whether the node is allocated\nor not. Return it to service later.\n\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=ALLOCATED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=DRAIN\nscontrol: show node adev13\nNodeName=adev13 State=DRAINING CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: quit\nLater\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=DRAINED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=IDLE\n Reconfigure all Slurm daemons on all nodes. This should\nbe done after changing the Slurm configuration file.\n\nadev0: scontrol reconfig\n Print the current Slurm configuration. This also reports if the\nprimary and secondary controllers (slurmctld daemons) are responding. To just\nsee the state of the controllers, use the command ping.\n\nadev0: scontrol show config\nConfiguration data as of 2019-03-29T12:20:45\n...\nSlurmctldAddr           = eadevi\nSlurmctldDebug          = info\nSlurmctldHost[0]        = adevi\nSlurmctldHost[1]        = adevj\nSlurmctldLogFile        = /var/log/slurmctld.log\n...\n\nSlurmctld(primary) at adevi is UP\nSlurmctld(backup) at adevj is UP\n Shutdown all Slurm daemons on all nodes.\n\nadev0: scontrol shutdown\n\nUpgrades\nSlurm supports in-place upgrades between certain versions. Important details\nabout the steps necessary to perform an upgrade and the potential complications\nto prepare for are contained on this page:\nUpgrade Guide\nFreeBSD\nFreeBSD administrators can install the latest stable Slurm as a binary\npackage using:\n\npkg install slurm-wlm\n\nOr, it can be built and installed from source using:\n\ncd /usr/ports/sysutils/slurm-wlm && make install\n\nThe binary package installs a minimal Slurm configuration suitable for\ntypical compute nodes.  Installing from source allows the user to enable\noptions such as mysql and gui tools via a configuration menu.\nLast modified 16 August 2024\n"
            },
            {
                "title": "Building Debian Packages\n\n",
                "content": "Beginning with Slurm 23.11.0, Slurm includes the files required to build\nDebian packages. These packages conflict with the packages shipped with Debian\nbased distributions, and are named distinctly to differentiate them. After\ndownloading the desired version of Slurm, the following can be done to build\nthe packages:\nInstall basic Debian package build requirements:\napt-get install build-essential fakeroot devscripts equivs\n\nUnpack the distributed tarball:\ntar -xaf slurm*tar.bz2\n\ncd to the directory containing the Slurm source\nInstall the Slurm package dependencies:\nmk-build-deps -i debian/control\n\nBuild the Slurm packages:\ndebuild -b -uc -us\n\nThe packages will be in the parent directory after debuild completes.Installing Debian Packages\n\nThe packages needed on the head node, compute nodes, and slurmdbd node can\nvary site to site, but this is a good starting point:\nSlurmDBD Node\n\t\nslurm-smd\nslurm-smd-slurmdbd\n\n\nHead Node (slurmctld node)\n\t\nslurm-smd\nslurm-smd-slurmctld\nslurm-smd-client\n\n\nCompute Nodes (slurmd node)\n\t\nslurm-smd\nslurm-smd-slurmd\nslurm-smd-client\n\n\nLogin Nodes\n\t\nslurm-smd\nslurm-smd-client\n\n\nBuilding Manually\n\nInstructions to build and install Slurm manually are shown below.\nThis is significantly more complicated to manage than the RPM and DEB build\nprocedures, so this approach is only recommended for developers or\nadvanced users who are looking for a more customized install.\nSee the README and INSTALL files in the source distribution for more details.\n\nUnpack the distributed tarball:\ntar -xaf slurm*tar.bz2\ncd to the directory containing the Slurm source and type\n./configure with appropriate options (see below).\nType make install to compile and install the programs,\ndocumentation, libraries, header files, etc.\nType ldconfig -n <library_location> so that the Slurm\nlibraries can be found by applications that intend to use Slurm APIs directly.\nThe library location will be a subdirectory of PREFIX (described below) and\ndepend upon the system type and configuration, typically lib or lib64.\nFor example, if PREFIX is \"/usr\" and the subdirectory is \"lib64\" then you would\nfind that a file named \"/usr/lib64/libslurm.so\" was installed and the command\nldconfig -n /usr/lib64 should be executed.\nA full list of configure options will be returned by the\ncommand configure --help. The most commonly used arguments\nto the configure command include:--enable-debug\nEnable additional debugging logic within Slurm.--prefix=PREFIX\nInstall architecture-independent files in PREFIX; default value is /usr/local.--sysconfdir=DIR\nSpecify location of Slurm configuration file. The default value is PREFIX/etcDaemonsslurmctld is sometimes called the \"controller\".\nIt orchestrates Slurm activities, including queuing of jobs,\nmonitoring node states, and allocating resources to jobs. There is an\noptional backup controller that automatically assumes control in the\nevent the primary controller fails (see the High\nAvailability section below).  The primary controller resumes\ncontrol whenever it is restored to service. The controller saves its\nstate to disk whenever there is a change in state (see\n\"StateSaveLocation\" in Configuration\nsection below).  This state can be recovered by the controller at\nstartup time.  State changes are saved so that jobs and other state\ninformation can be preserved when the controller moves (to or from a\nbackup controller) or is restarted.We recommend that you create a Unix user slurm for use by\nslurmctld. This user name will also be specified using the\nSlurmUser in the slurm.conf configuration file.\nThis user must exist on all nodes of the cluster.\nNote that files and directories used by slurmctld will need to be\nreadable or writable by the user SlurmUser (the Slurm configuration\nfiles must be readable; the log file directory and state save directory\nmust be writable).The slurmd daemon executes on every compute node. It resembles a\nremote shell daemon to export control to Slurm. Because slurmd initiates and\nmanages user jobs, it must execute as the user root.If you want to archive job accounting records to a database, the\nslurmdbd (Slurm DataBase Daemon) should be used. We recommend that\nyou defer adding accounting support until after basic Slurm functionality is\nestablished on your system. An Accounting web\npage contains more information.slurmctld and/or slurmd should be initiated at node startup\ntime per the Slurm configuration.The slurmrestd daemon was introduced in version 20.02 and allows\nclients to communicate with Slurm via the REST API. This is installed by\ndefault, assuming the prerequisites are met.\nIt has two run modes,\nallowing you to have it run as a traditional Unix service and always listen\nfor TCP connections, or you can have it run as an Inet service and only have\nit active when in use.High AvailabilityMultiple SlurmctldHost entries can be configured, with any entry beyond the\nfirst being treated as a backup host. Any backup hosts configured should be on\na different node than the node hosting the primary slurmctld. However, all\nhosts should mount a common file system containing the state information (see\n\"StateSaveLocation\" in the Configuration\nsection below).If more than one host is specified, when the primary fails the second listed\nSlurmctldHost will take over for it. When the primary returns to service, it\nnotifies the backup.  The backup then saves the state and returns to backup\nmode. The primary reads the saved state and resumes normal operation. Likewise,\nif both of the first two listed hosts fail the third SlurmctldHost will take\nover until the primary returns to service. Other than a brief period of non-\nresponsiveness, the transition back and forth should go undetected.Prior to 18.08, Slurm used the \n\"BackupAddr\" and \n\"BackupController\" parameters for High Availability. These\nparameters have been deprecated and are replaced by\n\"SlurmctldHost\".\nAlso see \"\nSlurmctldPrimaryOnProg\" and\n\"\nSlurmctldPrimaryOffProg\" to adjust the actions taken when machines\ntransition between being the primary controller.Any time the slurmctld daemon or hardware fails before state information\nreaches disk can result in lost state.\nSlurmctld writes state frequently (every five seconds by default), but with\nlarge numbers of jobs, the formatting and writing of records can take seconds\nand recent changes might not be written to disk.\nAnother example is if the state information is written to file, but that\ninformation is cached in memory rather than written to disk when the node fails.\nThe interval between state saves being written to disk can be configured at\nbuild time by defining SAVE_MAX_WAIT to a different value than five.A backup instance of slurmdbd can also be configured by specifying\n\nAccountingStorageBackupHost in slurm.conf, as well as\nDbdBackupHost in\nslurmdbd.conf. The backup host should be on a different machine than the one\nhosting the primary instance of slurmdbd. Both instances of slurmdbd should\nhave access to the same database. The\nnetwork page has a visual representation\nof how this might look.Infrastructure\n\nUser and Group Identification\n\nThere must be a uniform user and group name space (including\nUIDs and GIDs) across the cluster.\nIt is not necessary to permit user logins to the control hosts\n(SlurmctldHost), but the\nusers and groups must be resolvable on those hosts.Authentication of Slurm communications\n\nAll communications between Slurm components are authenticated. The\nauthentication infrastructure is provided by a dynamically loaded\nplugin chosen at runtime via the AuthType keyword in the Slurm\nconfiguration file. Until 23.11.0, the only supported authentication type was\nmunge, which requires the\ninstallation of the MUNGE package.\nWhen using MUNGE, all nodes in the cluster must be configured with the\nsame munge.key file. The MUNGE daemon, munged, must also be\nstarted before Slurm daemons. Note that MUNGE does require clocks to be\nsynchronized throughout the cluster, usually done by NTP.As of 23.11.0, AuthType can also be set to\nslurm, an internal authentication\nplugin. This plugin has similar requirements to MUNGE, requiring a key file\nshared to all Slurm daemons. The auth/slurm plugin requires installation of the\njwt package.MUNGE is currently the default and recommended option.\nThe configure script in the top-level directory of this distribution will\ndetermine which authentication plugins may be built.\nThe configuration file specifies which of the available plugins will be\nutilized.MPI supportSlurm supports many different MPI implementations.\nFor more information, see MPI.\n\nScheduler support\n\n\nSlurm can be configured with rather simple or quite sophisticated\nscheduling algorithms depending upon your needs and willingness to\nmanage the configuration (much of which requires a database).\nThe first configuration parameter of interest is PriorityType\nwith two options available: basic (first-in-first-out) and\nmultifactor.\nThe multifactor plugin will assign a priority to jobs based upon\na multitude of configuration parameters (age, size, fair-share allocation,\netc.) and its details are beyond the scope of this document.\nSee the Multifactor Job Priority Plugin\ndocument for details.\nThe SchedType configuration parameter controls how queued\njobs are scheduled and several options are available.\n\nbuiltin will initiate jobs strictly in their priority order,\ntypically (first-in-first-out) \nbackfill will initiate a lower-priority job if doing so does\nnot delay the expected initiation time of higher priority jobs; essentially\nusing smaller jobs to fill holes in the resource allocation plan. Effective\nbackfill scheduling does require users to specify job time limits.\ngang time-slices jobs in the same partition/queue and can be\nused to preempt jobs from lower-priority queues in order to execute\njobs in higher priority queues.\n\nFor more information about scheduling options see\nGang Scheduling,\nPreemption,\nResource Reservation Guide,\nResource Limits and\nSharing Consumable Resources.\nResource selection\n\n\nThe resource selection mechanism used by Slurm is controlled by the\nSelectType configuration parameter.\nIf you want to execute multiple jobs per node, but track and manage allocation\nof the processors, memory and other resources, the cons_tres (consumable\ntrackable resources) plugin is recommended.\nFor more information, please see\nConsumable Resources in Slurm.\nLogging\nSlurm uses syslog to record events if the SlurmctldLogFile and\nSlurmdLogFile locations are not set.\nAccounting\nSlurm supports accounting records being written to a simple text file,\ndirectly to a database (MySQL or MariaDB), or to a daemon securely\nmanaging accounting data for multiple clusters. For more information\nsee Accounting. \nCompute node access\n\n\nSlurm does not by itself limit access to allocated compute nodes,\nbut it does provide mechanisms to accomplish this.\nThere is a Pluggable Authentication Module (PAM) for restricting access\nto compute nodes available for download.\nWhen installed, the Slurm PAM module will prevent users from logging\ninto any node that has not be assigned to that user.\nOn job termination, any processes initiated by the user outside of\nSlurm's control may be killed using an Epilog script configured\nin slurm.conf.\nConfiguration\nThe Slurm configuration file includes a wide variety of parameters.\nThis configuration file must be available on each node of the cluster and\nmust have consistent contents. A full\ndescription of the parameters is included in the slurm.conf man page. Rather than\nduplicate that information, a minimal sample configuration file is shown below.\nYour slurm.conf file should define at least the configuration parameters defined\nin this sample and likely additional ones. Any text\nfollowing a \"#\" is considered a comment. The keywords in the file are\nnot case sensitive, although the argument typically is (e.g., \"SlurmUser=slurm\"\nmight be specified as \"slurmuser=slurm\"). The control machine, like\nall other machine specifications, can include both the host name and the name\nused for communications. In this case, the host's name is \"mcri\" and\nthe name \"emcri\" is used for communications.\nIn this case \"emcri\" is the private management network interface\nfor the host \"mcri\". Port numbers to be used for\ncommunications are specified as well as various timer values.\nThe SlurmUser must be created as needed prior to starting Slurm\nand must exist on all nodes in your cluster.\nThe parent directories for Slurm's log files, process ID files,\nstate save directories, etc. are not created by Slurm.\nThey must be created and made writable by SlurmUser as needed prior to\nstarting Slurm daemons.\nThe StateSaveLocation is used to store information about the current\nstate of the cluster, including information about queued, running and recently\ncompleted jobs. The directory used should be on a low-latency local disk to\nprevent file system delays from affecting Slurm performance. If using a backup\nhost, the StateSaveLocation should reside on a file system shared by the two\nhosts. We do not recommend using NFS to make the directory accessible to both\nhosts, but do recommend a shared mount that is accessible to the two\ncontrollers and allows low-latency reads and writes to the disk. If a\ncontroller comes up without access to the state information, queued and\nrunning jobs will be cancelled.\nA description of the nodes and their grouping into partitions is required.\nA simple node range expression may optionally be used to specify\nranges of nodes to avoid building a configuration file with large\nnumbers of entries. The node range expression can contain one\npair of square brackets with a sequence of comma separated\nnumbers and/or ranges of numbers separated by a \"-\"\n(e.g. \"linux[0-64,128]\", or \"lx[15,18,32-33]\").\nUp to two numeric ranges can be included in the expression\n(e.g. \"rack[0-63]_blade[0-41]\").\nIf one or more numeric expressions are included, one of them\nmust be at the end of the name (e.g. \"unit[0-31]rack\" is invalid),\nbut arbitrary names can always be used in a comma separated list.\nNode names can have up to three name specifications:\nNodeName is the name used by all Slurm tools when referring to the node,\nNodeAddr is the name or IP address Slurm uses to communicate with the node, and\nNodeHostname is the name returned by the command /bin/hostname -s.\nOnly NodeName is required (the others default to the same name),\nalthough supporting all three parameters provides complete control over\nnaming and addressing the nodes.  See the slurm.conf man page for\ndetails on all configuration parameters.\nNodes can be in more than one partition and each partition can have different\nconstraints (permitted users, time limits, job size limits, etc.).\nEach partition can thus be considered a separate queue.\nPartition and node specifications use node range expressions to identify\nnodes in a concise fashion. This configuration file defines a 1154-node cluster\nfor Slurm, but it might be used for a much larger cluster by just changing a few\nnode range expressions. Specify the minimum processor count (CPUs), real memory\nspace (RealMemory, megabytes), and temporary disk space (TmpDisk, megabytes) that\na node should have to be considered available for use. Any node lacking these\nminimum configuration values will be considered DOWN and not scheduled.\nNote that a more extensive sample configuration file is provided in\netc/slurm.conf.example. We also have a web-based\nconfiguration tool which can\nbe used to build a simple configuration file, which can then be\nmanually edited for more complex configurations.\n\n#\n# Sample /etc/slurm.conf for mcr.llnl.gov\n#\nSlurmctldHost=mcri(12.34.56.78)\nSlurmctldHost=mcrj(12.34.56.79)\n#\nAuthType=auth/munge\nEpilog=/usr/local/slurm/etc/epilog\nJobCompLoc=/var/tmp/jette/slurm.job.log\nJobCompType=jobcomp/filetxt\nPluginDir=/usr/local/slurm/lib/slurm\nProlog=/usr/local/slurm/etc/prolog\nSchedulerType=sched/backfill\nSelectType=select/linear\nSlurmUser=slurm\nSlurmctldPort=7002\nSlurmctldTimeout=300\nSlurmdPort=7003\nSlurmdSpoolDir=/var/spool/slurmd.spool\nSlurmdTimeout=300\nStateSaveLocation=/var/spool/slurm.state\nTreeWidth=16\n#\n# Node Configurations\n#\nNodeName=DEFAULT CPUs=2 RealMemory=2000 TmpDisk=64000 State=UNKNOWN\nNodeName=mcr[0-1151] NodeAddr=emcr[0-1151]\n#\n# Partition Configurations\n#\nPartitionName=DEFAULT State=UP\nPartitionName=pdebug Nodes=mcr[0-191] MaxTime=30 MaxNodes=32 Default=YES\nPartitionName=pbatch Nodes=mcr[192-1151]\n\nSecurity\nBesides authentication of Slurm communications based upon the value\nof the AuthType, digital signatures are used in job step\ncredentials.\nThis signature is used by slurmctld to construct a job step\ncredential, which is sent to srun and then forwarded to\nslurmd to initiate job steps.\nThis design offers improved performance by removing much of the\njob step initiation overhead from the  slurmctld  daemon.\nThe digital signature mechanism is specified by the CredType\nconfiguration parameter and the default mechanism is MUNGE. \nPluggable Authentication Module (PAM) support\n\n\nA PAM module (Pluggable Authentication Module) is available for Slurm that\ncan prevent a user from accessing a node which he has not been allocated,\nif that mode of operation is desired.\nStarting the Daemons\n\n\nFor testing purposes you may want to start by just running slurmctld and slurmd\non one node. By default, they execute in the background. Use the -D\noption for each daemon to execute them in the foreground and logging will be done\nto your terminal. The -v option will log events\nin more detail with more v's increasing the level of detail (e.g. -vvvvvv).\nYou can use one window to execute \"slurmctld -D -vvvvvv\",\na second window to execute \"slurmd -D -vvvvv\".\nYou may see errors such as \"Connection refused\" or \"Node X not responding\"\nwhile one daemon is operative and the other is being started, but the\ndaemons can be started in any order and proper communications will be\nestablished once both daemons complete initialization.\nYou can use a third window to execute commands such as\n\"srun -N1 /bin/hostname\" to confirm functionality.\nAnother important option for the daemons is \"-c\"\nto clear previous state information. Without the \"-c\"\noption, the daemons will restore any previously saved state information: node\nstate, job state, etc. With the \"-c\" option all\npreviously running jobs will be purged and node state will be restored to the\nvalues specified in the configuration file. This means that a node configured\ndown manually using the scontrol command will\nbe returned to service unless noted as being down in the configuration file.\nIn practice, Slurm consistently restarts with preservation.\nAdministration Examples\n\n\nscontrol can be used to print all system information\nand modify most of it. Only a few examples are shown below. Please see the scontrol\nman page for full details. The commands and options are all case insensitive.\nPrint detailed state of all jobs in the system.\n\nadev0: scontrol\nscontrol: show job\nJobId=475 UserId=bob(6885) Name=sleep JobState=COMPLETED\n   Priority=4294901286 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:53:41 EndTime=03/19-12:53:59\n   NodeList=adev8 NodeListIndecies=-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n\nJobId=476 UserId=bob(6885) Name=sleep JobState=RUNNING\n   Priority=4294901285 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:54:01 EndTime=NONE\n   NodeList=adev8 NodeListIndecies=8,8,-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n Print the detailed state of job 477 and change its priority to\nzero. A priority of zero prevents a job from being initiated (it is held in \"pending\"\nstate).\n\nadev0: scontrol\nscontrol: show job 477\nJobId=477 UserId=bob(6885) Name=sleep JobState=PENDING\n   Priority=4294901286 Partition=batch BatchFlag=0\n   more data removed....\nscontrol: update JobId=477 Priority=0\n\nPrint the state of node adev13 and drain it. To drain a node, specify a new\nstate of DRAIN, DRAINED, or DRAINING. Slurm will automatically set it to the appropriate\nvalue of either DRAINING or DRAINED depending on whether the node is allocated\nor not. Return it to service later.\n\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=ALLOCATED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=DRAIN\nscontrol: show node adev13\nNodeName=adev13 State=DRAINING CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: quit\nLater\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=DRAINED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=IDLE\n Reconfigure all Slurm daemons on all nodes. This should\nbe done after changing the Slurm configuration file.\n\nadev0: scontrol reconfig\n Print the current Slurm configuration. This also reports if the\nprimary and secondary controllers (slurmctld daemons) are responding. To just\nsee the state of the controllers, use the command ping.\n\nadev0: scontrol show config\nConfiguration data as of 2019-03-29T12:20:45\n...\nSlurmctldAddr           = eadevi\nSlurmctldDebug          = info\nSlurmctldHost[0]        = adevi\nSlurmctldHost[1]        = adevj\nSlurmctldLogFile        = /var/log/slurmctld.log\n...\n\nSlurmctld(primary) at adevi is UP\nSlurmctld(backup) at adevj is UP\n Shutdown all Slurm daemons on all nodes.\n\nadev0: scontrol shutdown\n\nUpgrades\nSlurm supports in-place upgrades between certain versions. Important details\nabout the steps necessary to perform an upgrade and the potential complications\nto prepare for are contained on this page:\nUpgrade Guide\nFreeBSD\nFreeBSD administrators can install the latest stable Slurm as a binary\npackage using:\n\npkg install slurm-wlm\n\nOr, it can be built and installed from source using:\n\ncd /usr/ports/sysutils/slurm-wlm && make install\n\nThe binary package installs a minimal Slurm configuration suitable for\ntypical compute nodes.  Installing from source allows the user to enable\noptions such as mysql and gui tools via a configuration menu.\nLast modified 16 August 2024\n"
            },
            {
                "title": "Scheduler support\n\n",
                "content": "Slurm can be configured with rather simple or quite sophisticated\nscheduling algorithms depending upon your needs and willingness to\nmanage the configuration (much of which requires a database).\nThe first configuration parameter of interest is PriorityType\nwith two options available: basic (first-in-first-out) and\nmultifactor.\nThe multifactor plugin will assign a priority to jobs based upon\na multitude of configuration parameters (age, size, fair-share allocation,\netc.) and its details are beyond the scope of this document.\nSee the Multifactor Job Priority Plugin\ndocument for details.The SchedType configuration parameter controls how queued\njobs are scheduled and several options are available.\n\nbuiltin will initiate jobs strictly in their priority order,\ntypically (first-in-first-out) \nbackfill will initiate a lower-priority job if doing so does\nnot delay the expected initiation time of higher priority jobs; essentially\nusing smaller jobs to fill holes in the resource allocation plan. Effective\nbackfill scheduling does require users to specify job time limits.\ngang time-slices jobs in the same partition/queue and can be\nused to preempt jobs from lower-priority queues in order to execute\njobs in higher priority queues.\n\nFor more information about scheduling options see\nGang Scheduling,\nPreemption,\nResource Reservation Guide,\nResource Limits and\nSharing Consumable Resources.\nResource selection\n\n\nThe resource selection mechanism used by Slurm is controlled by the\nSelectType configuration parameter.\nIf you want to execute multiple jobs per node, but track and manage allocation\nof the processors, memory and other resources, the cons_tres (consumable\ntrackable resources) plugin is recommended.\nFor more information, please see\nConsumable Resources in Slurm.\nLogging\nSlurm uses syslog to record events if the SlurmctldLogFile and\nSlurmdLogFile locations are not set.\nAccounting\nSlurm supports accounting records being written to a simple text file,\ndirectly to a database (MySQL or MariaDB), or to a daemon securely\nmanaging accounting data for multiple clusters. For more information\nsee Accounting. \nCompute node access\n\n\nSlurm does not by itself limit access to allocated compute nodes,\nbut it does provide mechanisms to accomplish this.\nThere is a Pluggable Authentication Module (PAM) for restricting access\nto compute nodes available for download.\nWhen installed, the Slurm PAM module will prevent users from logging\ninto any node that has not be assigned to that user.\nOn job termination, any processes initiated by the user outside of\nSlurm's control may be killed using an Epilog script configured\nin slurm.conf.\nConfiguration\nThe Slurm configuration file includes a wide variety of parameters.\nThis configuration file must be available on each node of the cluster and\nmust have consistent contents. A full\ndescription of the parameters is included in the slurm.conf man page. Rather than\nduplicate that information, a minimal sample configuration file is shown below.\nYour slurm.conf file should define at least the configuration parameters defined\nin this sample and likely additional ones. Any text\nfollowing a \"#\" is considered a comment. The keywords in the file are\nnot case sensitive, although the argument typically is (e.g., \"SlurmUser=slurm\"\nmight be specified as \"slurmuser=slurm\"). The control machine, like\nall other machine specifications, can include both the host name and the name\nused for communications. In this case, the host's name is \"mcri\" and\nthe name \"emcri\" is used for communications.\nIn this case \"emcri\" is the private management network interface\nfor the host \"mcri\". Port numbers to be used for\ncommunications are specified as well as various timer values.\nThe SlurmUser must be created as needed prior to starting Slurm\nand must exist on all nodes in your cluster.\nThe parent directories for Slurm's log files, process ID files,\nstate save directories, etc. are not created by Slurm.\nThey must be created and made writable by SlurmUser as needed prior to\nstarting Slurm daemons.\nThe StateSaveLocation is used to store information about the current\nstate of the cluster, including information about queued, running and recently\ncompleted jobs. The directory used should be on a low-latency local disk to\nprevent file system delays from affecting Slurm performance. If using a backup\nhost, the StateSaveLocation should reside on a file system shared by the two\nhosts. We do not recommend using NFS to make the directory accessible to both\nhosts, but do recommend a shared mount that is accessible to the two\ncontrollers and allows low-latency reads and writes to the disk. If a\ncontroller comes up without access to the state information, queued and\nrunning jobs will be cancelled.\nA description of the nodes and their grouping into partitions is required.\nA simple node range expression may optionally be used to specify\nranges of nodes to avoid building a configuration file with large\nnumbers of entries. The node range expression can contain one\npair of square brackets with a sequence of comma separated\nnumbers and/or ranges of numbers separated by a \"-\"\n(e.g. \"linux[0-64,128]\", or \"lx[15,18,32-33]\").\nUp to two numeric ranges can be included in the expression\n(e.g. \"rack[0-63]_blade[0-41]\").\nIf one or more numeric expressions are included, one of them\nmust be at the end of the name (e.g. \"unit[0-31]rack\" is invalid),\nbut arbitrary names can always be used in a comma separated list.\nNode names can have up to three name specifications:\nNodeName is the name used by all Slurm tools when referring to the node,\nNodeAddr is the name or IP address Slurm uses to communicate with the node, and\nNodeHostname is the name returned by the command /bin/hostname -s.\nOnly NodeName is required (the others default to the same name),\nalthough supporting all three parameters provides complete control over\nnaming and addressing the nodes.  See the slurm.conf man page for\ndetails on all configuration parameters.\nNodes can be in more than one partition and each partition can have different\nconstraints (permitted users, time limits, job size limits, etc.).\nEach partition can thus be considered a separate queue.\nPartition and node specifications use node range expressions to identify\nnodes in a concise fashion. This configuration file defines a 1154-node cluster\nfor Slurm, but it might be used for a much larger cluster by just changing a few\nnode range expressions. Specify the minimum processor count (CPUs), real memory\nspace (RealMemory, megabytes), and temporary disk space (TmpDisk, megabytes) that\na node should have to be considered available for use. Any node lacking these\nminimum configuration values will be considered DOWN and not scheduled.\nNote that a more extensive sample configuration file is provided in\netc/slurm.conf.example. We also have a web-based\nconfiguration tool which can\nbe used to build a simple configuration file, which can then be\nmanually edited for more complex configurations.\n\n#\n# Sample /etc/slurm.conf for mcr.llnl.gov\n#\nSlurmctldHost=mcri(12.34.56.78)\nSlurmctldHost=mcrj(12.34.56.79)\n#\nAuthType=auth/munge\nEpilog=/usr/local/slurm/etc/epilog\nJobCompLoc=/var/tmp/jette/slurm.job.log\nJobCompType=jobcomp/filetxt\nPluginDir=/usr/local/slurm/lib/slurm\nProlog=/usr/local/slurm/etc/prolog\nSchedulerType=sched/backfill\nSelectType=select/linear\nSlurmUser=slurm\nSlurmctldPort=7002\nSlurmctldTimeout=300\nSlurmdPort=7003\nSlurmdSpoolDir=/var/spool/slurmd.spool\nSlurmdTimeout=300\nStateSaveLocation=/var/spool/slurm.state\nTreeWidth=16\n#\n# Node Configurations\n#\nNodeName=DEFAULT CPUs=2 RealMemory=2000 TmpDisk=64000 State=UNKNOWN\nNodeName=mcr[0-1151] NodeAddr=emcr[0-1151]\n#\n# Partition Configurations\n#\nPartitionName=DEFAULT State=UP\nPartitionName=pdebug Nodes=mcr[0-191] MaxTime=30 MaxNodes=32 Default=YES\nPartitionName=pbatch Nodes=mcr[192-1151]\n\nSecurity\nBesides authentication of Slurm communications based upon the value\nof the AuthType, digital signatures are used in job step\ncredentials.\nThis signature is used by slurmctld to construct a job step\ncredential, which is sent to srun and then forwarded to\nslurmd to initiate job steps.\nThis design offers improved performance by removing much of the\njob step initiation overhead from the  slurmctld  daemon.\nThe digital signature mechanism is specified by the CredType\nconfiguration parameter and the default mechanism is MUNGE. \nPluggable Authentication Module (PAM) support\n\n\nA PAM module (Pluggable Authentication Module) is available for Slurm that\ncan prevent a user from accessing a node which he has not been allocated,\nif that mode of operation is desired.\nStarting the Daemons\n\n\nFor testing purposes you may want to start by just running slurmctld and slurmd\non one node. By default, they execute in the background. Use the -D\noption for each daemon to execute them in the foreground and logging will be done\nto your terminal. The -v option will log events\nin more detail with more v's increasing the level of detail (e.g. -vvvvvv).\nYou can use one window to execute \"slurmctld -D -vvvvvv\",\na second window to execute \"slurmd -D -vvvvv\".\nYou may see errors such as \"Connection refused\" or \"Node X not responding\"\nwhile one daemon is operative and the other is being started, but the\ndaemons can be started in any order and proper communications will be\nestablished once both daemons complete initialization.\nYou can use a third window to execute commands such as\n\"srun -N1 /bin/hostname\" to confirm functionality.\nAnother important option for the daemons is \"-c\"\nto clear previous state information. Without the \"-c\"\noption, the daemons will restore any previously saved state information: node\nstate, job state, etc. With the \"-c\" option all\npreviously running jobs will be purged and node state will be restored to the\nvalues specified in the configuration file. This means that a node configured\ndown manually using the scontrol command will\nbe returned to service unless noted as being down in the configuration file.\nIn practice, Slurm consistently restarts with preservation.\nAdministration Examples\n\n\nscontrol can be used to print all system information\nand modify most of it. Only a few examples are shown below. Please see the scontrol\nman page for full details. The commands and options are all case insensitive.\nPrint detailed state of all jobs in the system.\n\nadev0: scontrol\nscontrol: show job\nJobId=475 UserId=bob(6885) Name=sleep JobState=COMPLETED\n   Priority=4294901286 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:53:41 EndTime=03/19-12:53:59\n   NodeList=adev8 NodeListIndecies=-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n\nJobId=476 UserId=bob(6885) Name=sleep JobState=RUNNING\n   Priority=4294901285 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:54:01 EndTime=NONE\n   NodeList=adev8 NodeListIndecies=8,8,-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n Print the detailed state of job 477 and change its priority to\nzero. A priority of zero prevents a job from being initiated (it is held in \"pending\"\nstate).\n\nadev0: scontrol\nscontrol: show job 477\nJobId=477 UserId=bob(6885) Name=sleep JobState=PENDING\n   Priority=4294901286 Partition=batch BatchFlag=0\n   more data removed....\nscontrol: update JobId=477 Priority=0\n\nPrint the state of node adev13 and drain it. To drain a node, specify a new\nstate of DRAIN, DRAINED, or DRAINING. Slurm will automatically set it to the appropriate\nvalue of either DRAINING or DRAINED depending on whether the node is allocated\nor not. Return it to service later.\n\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=ALLOCATED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=DRAIN\nscontrol: show node adev13\nNodeName=adev13 State=DRAINING CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: quit\nLater\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=DRAINED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=IDLE\n Reconfigure all Slurm daemons on all nodes. This should\nbe done after changing the Slurm configuration file.\n\nadev0: scontrol reconfig\n Print the current Slurm configuration. This also reports if the\nprimary and secondary controllers (slurmctld daemons) are responding. To just\nsee the state of the controllers, use the command ping.\n\nadev0: scontrol show config\nConfiguration data as of 2019-03-29T12:20:45\n...\nSlurmctldAddr           = eadevi\nSlurmctldDebug          = info\nSlurmctldHost[0]        = adevi\nSlurmctldHost[1]        = adevj\nSlurmctldLogFile        = /var/log/slurmctld.log\n...\n\nSlurmctld(primary) at adevi is UP\nSlurmctld(backup) at adevj is UP\n Shutdown all Slurm daemons on all nodes.\n\nadev0: scontrol shutdown\n\nUpgrades\nSlurm supports in-place upgrades between certain versions. Important details\nabout the steps necessary to perform an upgrade and the potential complications\nto prepare for are contained on this page:\nUpgrade Guide\nFreeBSD\nFreeBSD administrators can install the latest stable Slurm as a binary\npackage using:\n\npkg install slurm-wlm\n\nOr, it can be built and installed from source using:\n\ncd /usr/ports/sysutils/slurm-wlm && make install\n\nThe binary package installs a minimal Slurm configuration suitable for\ntypical compute nodes.  Installing from source allows the user to enable\noptions such as mysql and gui tools via a configuration menu.\nLast modified 16 August 2024\n"
            },
            {
                "title": "Resource selection\n\n",
                "content": "The resource selection mechanism used by Slurm is controlled by the\nSelectType configuration parameter.\nIf you want to execute multiple jobs per node, but track and manage allocation\nof the processors, memory and other resources, the cons_tres (consumable\ntrackable resources) plugin is recommended.\nFor more information, please see\nConsumable Resources in Slurm.LoggingSlurm uses syslog to record events if the SlurmctldLogFile and\nSlurmdLogFile locations are not set.AccountingSlurm supports accounting records being written to a simple text file,\ndirectly to a database (MySQL or MariaDB), or to a daemon securely\nmanaging accounting data for multiple clusters. For more information\nsee Accounting. Compute node access\n\nSlurm does not by itself limit access to allocated compute nodes,\nbut it does provide mechanisms to accomplish this.\nThere is a Pluggable Authentication Module (PAM) for restricting access\nto compute nodes available for download.\nWhen installed, the Slurm PAM module will prevent users from logging\ninto any node that has not be assigned to that user.\nOn job termination, any processes initiated by the user outside of\nSlurm's control may be killed using an Epilog script configured\nin slurm.conf.ConfigurationThe Slurm configuration file includes a wide variety of parameters.\nThis configuration file must be available on each node of the cluster and\nmust have consistent contents. A full\ndescription of the parameters is included in the slurm.conf man page. Rather than\nduplicate that information, a minimal sample configuration file is shown below.\nYour slurm.conf file should define at least the configuration parameters defined\nin this sample and likely additional ones. Any text\nfollowing a \"#\" is considered a comment. The keywords in the file are\nnot case sensitive, although the argument typically is (e.g., \"SlurmUser=slurm\"\nmight be specified as \"slurmuser=slurm\"). The control machine, like\nall other machine specifications, can include both the host name and the name\nused for communications. In this case, the host's name is \"mcri\" and\nthe name \"emcri\" is used for communications.\nIn this case \"emcri\" is the private management network interface\nfor the host \"mcri\". Port numbers to be used for\ncommunications are specified as well as various timer values.The SlurmUser must be created as needed prior to starting Slurm\nand must exist on all nodes in your cluster.\nThe parent directories for Slurm's log files, process ID files,\nstate save directories, etc. are not created by Slurm.\nThey must be created and made writable by SlurmUser as needed prior to\nstarting Slurm daemons.The StateSaveLocation is used to store information about the current\nstate of the cluster, including information about queued, running and recently\ncompleted jobs. The directory used should be on a low-latency local disk to\nprevent file system delays from affecting Slurm performance. If using a backup\nhost, the StateSaveLocation should reside on a file system shared by the two\nhosts. We do not recommend using NFS to make the directory accessible to both\nhosts, but do recommend a shared mount that is accessible to the two\ncontrollers and allows low-latency reads and writes to the disk. If a\ncontroller comes up without access to the state information, queued and\nrunning jobs will be cancelled.A description of the nodes and their grouping into partitions is required.\nA simple node range expression may optionally be used to specify\nranges of nodes to avoid building a configuration file with large\nnumbers of entries. The node range expression can contain one\npair of square brackets with a sequence of comma separated\nnumbers and/or ranges of numbers separated by a \"-\"\n(e.g. \"linux[0-64,128]\", or \"lx[15,18,32-33]\").\nUp to two numeric ranges can be included in the expression\n(e.g. \"rack[0-63]_blade[0-41]\").\nIf one or more numeric expressions are included, one of them\nmust be at the end of the name (e.g. \"unit[0-31]rack\" is invalid),\nbut arbitrary names can always be used in a comma separated list.Node names can have up to three name specifications:\nNodeName is the name used by all Slurm tools when referring to the node,\nNodeAddr is the name or IP address Slurm uses to communicate with the node, and\nNodeHostname is the name returned by the command /bin/hostname -s.\nOnly NodeName is required (the others default to the same name),\nalthough supporting all three parameters provides complete control over\nnaming and addressing the nodes.  See the slurm.conf man page for\ndetails on all configuration parameters.Nodes can be in more than one partition and each partition can have different\nconstraints (permitted users, time limits, job size limits, etc.).\nEach partition can thus be considered a separate queue.\nPartition and node specifications use node range expressions to identify\nnodes in a concise fashion. This configuration file defines a 1154-node cluster\nfor Slurm, but it might be used for a much larger cluster by just changing a few\nnode range expressions. Specify the minimum processor count (CPUs), real memory\nspace (RealMemory, megabytes), and temporary disk space (TmpDisk, megabytes) that\na node should have to be considered available for use. Any node lacking these\nminimum configuration values will be considered DOWN and not scheduled.\nNote that a more extensive sample configuration file is provided in\netc/slurm.conf.example. We also have a web-based\nconfiguration tool which can\nbe used to build a simple configuration file, which can then be\nmanually edited for more complex configurations.\n#\n# Sample /etc/slurm.conf for mcr.llnl.gov\n#\nSlurmctldHost=mcri(12.34.56.78)\nSlurmctldHost=mcrj(12.34.56.79)\n#\nAuthType=auth/munge\nEpilog=/usr/local/slurm/etc/epilog\nJobCompLoc=/var/tmp/jette/slurm.job.log\nJobCompType=jobcomp/filetxt\nPluginDir=/usr/local/slurm/lib/slurm\nProlog=/usr/local/slurm/etc/prolog\nSchedulerType=sched/backfill\nSelectType=select/linear\nSlurmUser=slurm\nSlurmctldPort=7002\nSlurmctldTimeout=300\nSlurmdPort=7003\nSlurmdSpoolDir=/var/spool/slurmd.spool\nSlurmdTimeout=300\nStateSaveLocation=/var/spool/slurm.state\nTreeWidth=16\n#\n# Node Configurations\n#\nNodeName=DEFAULT CPUs=2 RealMemory=2000 TmpDisk=64000 State=UNKNOWN\nNodeName=mcr[0-1151] NodeAddr=emcr[0-1151]\n#\n# Partition Configurations\n#\nPartitionName=DEFAULT State=UP\nPartitionName=pdebug Nodes=mcr[0-191] MaxTime=30 MaxNodes=32 Default=YES\nPartitionName=pbatch Nodes=mcr[192-1151]\nSecurityBesides authentication of Slurm communications based upon the value\nof the AuthType, digital signatures are used in job step\ncredentials.\nThis signature is used by slurmctld to construct a job step\ncredential, which is sent to srun and then forwarded to\nslurmd to initiate job steps.\nThis design offers improved performance by removing much of the\njob step initiation overhead from the  slurmctld  daemon.\nThe digital signature mechanism is specified by the CredType\nconfiguration parameter and the default mechanism is MUNGE. Pluggable Authentication Module (PAM) support\n\nA PAM module (Pluggable Authentication Module) is available for Slurm that\ncan prevent a user from accessing a node which he has not been allocated,\nif that mode of operation is desired.Starting the Daemons\n\nFor testing purposes you may want to start by just running slurmctld and slurmd\non one node. By default, they execute in the background. Use the -D\noption for each daemon to execute them in the foreground and logging will be done\nto your terminal. The -v option will log events\nin more detail with more v's increasing the level of detail (e.g. -vvvvvv).\nYou can use one window to execute \"slurmctld -D -vvvvvv\",\na second window to execute \"slurmd -D -vvvvv\".\nYou may see errors such as \"Connection refused\" or \"Node X not responding\"\nwhile one daemon is operative and the other is being started, but the\ndaemons can be started in any order and proper communications will be\nestablished once both daemons complete initialization.\nYou can use a third window to execute commands such as\n\"srun -N1 /bin/hostname\" to confirm functionality.Another important option for the daemons is \"-c\"\nto clear previous state information. Without the \"-c\"\noption, the daemons will restore any previously saved state information: node\nstate, job state, etc. With the \"-c\" option all\npreviously running jobs will be purged and node state will be restored to the\nvalues specified in the configuration file. This means that a node configured\ndown manually using the scontrol command will\nbe returned to service unless noted as being down in the configuration file.\nIn practice, Slurm consistently restarts with preservation.Administration Examples\n\nscontrol can be used to print all system information\nand modify most of it. Only a few examples are shown below. Please see the scontrol\nman page for full details. The commands and options are all case insensitive.Print detailed state of all jobs in the system.\nadev0: scontrol\nscontrol: show job\nJobId=475 UserId=bob(6885) Name=sleep JobState=COMPLETED\n   Priority=4294901286 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:53:41 EndTime=03/19-12:53:59\n   NodeList=adev8 NodeListIndecies=-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n\nJobId=476 UserId=bob(6885) Name=sleep JobState=RUNNING\n   Priority=4294901285 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:54:01 EndTime=NONE\n   NodeList=adev8 NodeListIndecies=8,8,-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\nPrint the detailed state of job 477 and change its priority to\nzero. A priority of zero prevents a job from being initiated (it is held in \"pending\"\nstate).\nadev0: scontrol\nscontrol: show job 477\nJobId=477 UserId=bob(6885) Name=sleep JobState=PENDING\n   Priority=4294901286 Partition=batch BatchFlag=0\n   more data removed....\nscontrol: update JobId=477 Priority=0\nPrint the state of node adev13 and drain it. To drain a node, specify a new\nstate of DRAIN, DRAINED, or DRAINING. Slurm will automatically set it to the appropriate\nvalue of either DRAINING or DRAINED depending on whether the node is allocated\nor not. Return it to service later.\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=ALLOCATED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=DRAIN\nscontrol: show node adev13\nNodeName=adev13 State=DRAINING CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: quit\nLater\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=DRAINED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=IDLE\nReconfigure all Slurm daemons on all nodes. This should\nbe done after changing the Slurm configuration file.\nadev0: scontrol reconfig\nPrint the current Slurm configuration. This also reports if the\nprimary and secondary controllers (slurmctld daemons) are responding. To just\nsee the state of the controllers, use the command ping.\nadev0: scontrol show config\nConfiguration data as of 2019-03-29T12:20:45\n...\nSlurmctldAddr           = eadevi\nSlurmctldDebug          = info\nSlurmctldHost[0]        = adevi\nSlurmctldHost[1]        = adevj\nSlurmctldLogFile        = /var/log/slurmctld.log\n...\n\nSlurmctld(primary) at adevi is UP\nSlurmctld(backup) at adevj is UP\nShutdown all Slurm daemons on all nodes.\nadev0: scontrol shutdown\nUpgradesSlurm supports in-place upgrades between certain versions. Important details\nabout the steps necessary to perform an upgrade and the potential complications\nto prepare for are contained on this page:\nUpgrade GuideFreeBSDFreeBSD administrators can install the latest stable Slurm as a binary\npackage using:\npkg install slurm-wlm\nOr, it can be built and installed from source using:\ncd /usr/ports/sysutils/slurm-wlm && make install\nThe binary package installs a minimal Slurm configuration suitable for\ntypical compute nodes.  Installing from source allows the user to enable\noptions such as mysql and gui tools via a configuration menu.Last modified 16 August 2024"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/troubleshoot.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Slurm Troubleshooting Guide",
                "content": "This guide is meant as a tool to help system administrators\nor operators troubleshoot Slurm failures and restore services.\nThe Frequently Asked Questions document\nmay also prove useful.\nSlurm is not responding\nJobs are not getting scheduled\nJobs and nodes are stuck in COMPLETING state\nNodes are getting set to a DOWN state\nNetworking and configuration problems\nSlurm is not responding\n\n\nExecute \"scontrol ping\" to determine if the primary\nand backup controllers are responding.\n\nIf it responds for you, this could be a networking\nor configuration problem specific to some user or node in the\ncluster.\nIf not responding, directly login to the machine and try again\nto rule out network and configuration problems.\nIf still not responding, check if there is an active slurmctld\ndaemon by executing \"ps -el | grep slurmctld\".\nIf slurmctld is not running, restart it (typically as user root\nusing the command \"/etc/init.d/slurm start\").\nYou should check the log file (SlurmctldLog in the\nslurm.conf file) for an indication of why it failed.\nIf slurmctld is running but not responding (a very rare situation),\nthen kill and restart it (typically as user root using the commands\n\"/etc/init.d/slurm stop\" and then \"/etc/init.d/slurm start\").\nIf it hangs again, increase the verbosity of debug messages\n(increase SlurmctldDebug in the slurm.conf file)\nand restart.\nAgain check the log file for an indication of why it failed.\nIf it continues to fail without an indication as to the failure\nmode, restart without preserving state (typically as user root\nusing the commands \"/etc/init.d/slurm stop\"\nand then \"/etc/init.d/slurm startclean\").\nNote: All running jobs and other state information will be lost.\nJobs are not getting scheduled\n\nThis is dependent upon the scheduler used by Slurm.\nExecuting the command \"scontrol show config | grep SchedulerType\"\nto determine this.\nFor any scheduler, you can check priorities of jobs using the\ncommand \"scontrol show job\".\nIf the scheduler type is builtin, then jobs will be executed\nin the order of submission for a given partition.\nEven if resources are available to initiate jobs immediately,\nit will be deferred until no previously submitted job is pending.\nIf the scheduler type is backfill, then jobs will generally\nbe executed in the order of submission for a given partition with one\nexception: later submitted jobs will be initiated early if doing so\ndoes not delay the expected execution time of an earlier submitted job.\nIn order for backfill scheduling to be effective, users jobs should\nspecify reasonable time limits.\nIf jobs do not specify time limits, then all jobs will receive the\nsame time limit (that associated with the partition), and the ability\nto backfill schedule jobs will be limited.\nThe backfill scheduler does not alter job specifications of required\nor excluded nodes, so jobs which specify nodes will substantially\nreduce the effectiveness of backfill scheduling.\nSee the backfill documentation\nfor more details.\nJobs and nodes are stuck in COMPLETING state\n\nThis is typically due to non-killable processes associated with the job.\nSlurm will continue to attempt terminating the processes with SIGKILL, but\nsome jobs may be stuck performing I/O and non-killable.\nThis is typically due to a file system problem and may be addressed in\na couple of ways.\nFix the file system and/or reboot the node. -OR-\nSet the node to a DOWN state and then return it to service\n(\"scontrol update NodeName=<node> State=down Reason=hung_proc\"\nand \"scontrol update NodeName=<node> State=resume\").\nThis permits other jobs to use the node, but leaves the non-killable\nprocess in place.\nIf the process should ever complete the I/O, the pending SIGKILL\nshould terminate it immediately. -OR-\nUse the UnkillableStepProgram and UnkillableStepTimeout\nconfiguration parameters to automatically respond to processes which can not\nbe killed, by sending email or rebooting the node. For more information,\nsee the slurm.conf documentation.\n\nIf it doesn't look like your job is stuck because of filesystem problems\nit may take some debugging to find the cause.  If you can reproduce\nthe behavior you can set the SlurmdDebug level to 'debug' and restart\nslurmd on a node you'll use to reproduce the problem.  The slurmd.log\nfile should then have more information to help troubleshoot the issue.\n\nLooking at slurmctld.log may also provide clues. If nodes stop responding,\nyou may want to look into why since they may prevent job cleanup and\ncause jobs to remain in a COMPLETING state. When looking for connectivity\nproblems, the relevant log entries should look something like this:\n\nerror: Nodes node[00,03,25] not responding\nNode node00 now responding\n\nNodes are getting set to a DOWN state\n\n\nCheck the reason why the node is down using the command\n\"scontrol show node <name>\".\nThis will show the reason why the node was set down and the\ntime when it happened.\nIf there is insufficient disk space, memory space, etc. compared\nto the parameters specified in the slurm.conf file then\neither fix the node or change slurm.conf.\nIf the reason is \"Not responding\", then check communications\nbetween the control machine and the DOWN node using the command\n\"ping <address>\" being sure to specify the\nNodeAddr values configured in slurm.conf.\nIf ping fails, then fix the network or addresses in slurm.conf.\nNext, login to a node that Slurm considers to be in a DOWN\nstate and check if the slurmd daemon is running with the command\n\"ps -el | grep slurmd\".\nIf slurmd is not running, restart it (typically as user root\nusing the command \"/etc/init.d/slurm start\").\nYou should check the log file (SlurmdLog in the\nslurm.conf file) for an indication of why it failed.\nYou can get the status of the running slurmd daemon by\nexecuting the command \"scontrol show slurmd\" on\nthe node of interest.\nCheck the value of \"Last slurmctld msg time\" to determine\nif the slurmctld is able to communicate with the slurmd.\nIf slurmd is running but not responding (a very rare situation),\nthen kill and restart it (typically as user root using the commands\n\"/etc/init.d/slurm stop\" and then \"/etc/init.d/slurm start\").\nIf still not responding, try again to rule out\nnetwork and configuration problems.\nIf still not responding, increase the verbosity of debug messages\n(increase SlurmdDebug in the slurm.conf file)\nand restart.\nAgain check the log file for an indication of why it failed.\nIf still not responding without an indication as to the failure\nmode, restart without preserving state (typically as user root\nusing the commands \"/etc/init.d/slurm stop\"\nand then \"/etc/init.d/slurm startclean\").\nNote: All jobs and other state information on that node will be lost.\nNetworking and configuration problems\n\n\nCheck the controller and/or slurmd log files (SlurmctldLog\nand SlurmdLog in the slurm.conf file) for an indication\nof why it is failing.\nCheck for consistent slurm.conf and credential files on\nthe node(s) experiencing problems.\nIf this is user-specific problem, check that the user is\nconfigured on the controller computer(s) as well as the\ncompute nodes.\nThe user doesn't need to be able to login, but his user ID\nmust exist.\nCheck that compatible versions of Slurm exists on all of\nthe nodes (execute \"sinfo -V\" or \"rpm -qa | grep slurm\").\nThe Slurm version number contains three period-separated numbers\nthat represent both the major Slurm release and maintenance release level.\nThe first two parts combine together to represent the major release, and match\nthe year and month of that major release. The third number in the version\ndesignates a specific maintenance level:\nyear.month.maintenance-release (e.g. 17.11.5 is major Slurm release 17.11, and\nmaintenance version 5).\nThus version 17.11.x was initially released in November 2017.\nSlurm daemons will support RPCs and state files from the two previous major\nreleases (e.g. a version 17.11.x SlurmDBD will support slurmctld daemons and\ncommands with a version of 17.11.x, 17.02.x or 16.05.x).\nLast modified 28 May 2020"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/job_container_tmpfs.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "job_container/tmpfs",
                "content": "Overviewjob_container/tmpfs is an optional plugin that provides job-specific, private\ntemporary file system space.When enabled on the cluster, a filesytem namespace will be created for each\njob with a unique, private instance of /tmp and /dev/shm for the job to use.\nThese directories can be changed with the Dirs= option in\njob_container.conf. The contents of these directories will be removed at job\ntermination.Installation\n\nThis plugin is built and installed as part of the default build, no extra\ninstallation steps are required.SetupSlurm must be configured to load the job container plugin by adding\nJobContainerType=job_container/tmpfs and PrologFlags=contain in\nslurm.conf. Additional configuration must be done in the job_container.conf\nfile, which should be placed in the same directory as slurm.conf.Job containers can be configured for all nodes, or for a subset of nodes.\nAs an example, if all nodes will be configured the same way, you would put the\nfollowing in your job_container.conf:\nAutoBasePath=true\nBasePath=/var/nvme/storage\nA full description of the parameters available in the job_container.conf\nfile can be found here.Initial Testing\n\nAn easy way to verify that the container is working is to run a job and\nensure that the /tmp directory is empty (since it normally has some other\nfiles) and that \".\" is owned by the user that submitted the job.\ntim@slurm-ctld:~$ srun ls -al /tmp\ntotal 8\ndrwx------  2 tim    root 4096 Feb 10 17:14 .\ndrwxr-xr-x 21 root   root 4096 Nov 15 08:46 ..\nWhile a job is running, root should be able to confirm that\n/$BasePath/$JobID/_tmp exists and is empty. This directory is bind\nmounted into the job. /$BasePath/$JobID should be owned by root,\nand is not intended to be accessible to the user.SPANKThis plugin interfaces with the SPANK api, and automatically joins the job's\ncontainer in the following functions:\nspank_task_init_privileged()\nspank_task_init()\nIn addition to the job itself, the TaskProlog will also be executed inside\nthe container.Last modified 29 November 2023"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/priority_multifactor.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Multifactor Priority Plugin",
                "content": "Contents\n Introduction\n Multifactor Job Priority Plugin\n Job Priority Factors In General\n Age Factor\n Association Factor\n Job Size Factor\n Nice Factor\n Partition Factor\n Quality of Service (QOS) Factor\n Site Factor\n TRES Factors\n Fairshare Factor\n The sprio utility\n Configuration\n Configuration Example\nIntroductionBy default, Slurm has the priority/multifactor plugin set, which schedules\njobs based on several factorsIn most cases it is preferable to use the Multifactor Priority plugin,\nhowever basic First In, First Out scheduling is available by setting\nPriorityType=priority/basic in the slurm.conf file. FIFO scheduling\nshould be configured when Slurm is controlled by an external scheduler. (See\nConfiguration below)There are several considerations the scheduler makes when making\nscheduling decisions. Jobs are selected to be evaluated by the scheduler\nin the following order:\nJobs that can preempt\nJobs with an advanced reservation\nPartition PriorityTier\nJob priority\nJob submit time\nJob ID\nThis is important to keep in mind because the job with the highest priority\nmay not be the first to be evaluated by the scheduler. The job priority is\nconsidered when there are multiple jobs that can be evaluated at once, such\nas jobs requesting partitions with the same PriorityTier.Multifactor 'Factors'\n\n There are nine factors in the Multifactor Job Priority plugin that\ninfluence job priority:\n Age\n the length of time a job has been waiting in the queue, eligible to be scheduled\n Association\n a factor associated with each association\n Fair-share\n the difference between the portion of the computing resource that has been promised and the amount of resources that has been consumed\n Job size\n the number of nodes or CPUs a job is allocated\n Nice\n a factor that can be controlled by users to prioritize their own jobs.\n Partition\n a factor associated with each node partition\n QOS\n a factor associated with each Quality Of Service\n Site\n a factor dictated by an administrator or a site-developed job_submit or site_factor plugin\n TRES\n each TRES Type has its own factor for a job which represents the number of\nrequested/allocated TRES Type in a given partition\n Additionally, a weight can be assigned to each of the above\n  factors.  This provides the ability to enact a policy that blends a\n  combination of any of the above factors in any portion desired.  For\n  example, a site could configure fair-share to be the dominant factor\n  (say 70%), set the job size and the age factors to each contribute\n  15%, and set the partition and QOS influences to zero.Job Priority Factors In General\n\n The job's priority at any given time will be a weighted sum of all the factors that have been enabled in the slurm.conf file.  Job priority can be expressed as:\nJob_priority =\n\tsite_factor +\n\t(PriorityWeightAge) * (age_factor) +\n\t(PriorityWeightAssoc) * (assoc_factor) +\n\t(PriorityWeightFairshare) * (fair-share_factor) +\n\t(PriorityWeightJobSize) * (job_size_factor) +\n\t(PriorityWeightPartition) * (priority_job_factor) +\n\t(PriorityWeightQOS) * (QOS_factor) +\n\tSUM(TRES_weight_cpu * TRES_factor_cpu,\n\t    TRES_weight_<type> * TRES_factor_<type>,\n\t    ...)\n\t- nice_factor\n All of the factors in this formula are floating point numbers that\n  range from 0.0 to 1.0.  The weights are unsigned, 32 bit integers.\n  The job's priority is an integer that ranges between 0 and\n  4294967295.  The larger the number, the higher the job will be\n  positioned in the queue, and the sooner the job will be scheduled.\n  A job's priority, and hence its order in the queue, can vary over\n  time.  For example, the longer a job sits in the queue, the higher\n  its priority will grow when the age_weight is non-zero.The default behavior is for slurmctld to \"normalize\" the priority values\nin relation to the one with the highest value. This makes it so that the\nmost priority a job can get from any factor is equal to the\nPriorityWeight* value for that factor. Using Partitions as an example,\nif 'PartitionA' had a PriorityJobFactor of 20 and 'PartitionB' had a\nPriorityJobFactor of 10 and the PriorityWeightPartition was set\nto 5000, then the calculation for the priority that any job would gain for\nthe partition would look like this:\n\n# PartitionA\n5000 * (20 / 20) = 5000\n\n# PartitionB\n5000 * (10 / 20) = 2500\n\nYou can change the default behavior so that it doesn't normalize the\npriority values, but uses the raw PriorityJobFactor values instead,\nwith PriorityFlags=NO_NORMAL_PART. In that case the calculation of\nthe partition based priority would look like this:\n\n# PartitionA\n5000 * 20 = 100000\n\n# PartitionB\n5000 * 10 = 50000\n\nSee the other priority factors you can configure to not be normalized in the\nPriorityFlags section of the\ndocumentation. IMPORTANT: The weight values should be high enough to get a\n  good set of significant digits since all the factors are floating\n  point numbers from 0.0 to 1.0. For example, one job could have a\n  fair-share factor of .59534 and another job could have a fair-share\n  factor of .50002.  If the fair-share weight is only set to 10, both\n  jobs would have the same fair-share priority. Therefore, set the\n  weights high enough to avoid this scenario, starting around 1000 or\n  so for those factors you want to make predominant.Age FactorNote: Computing the age factor requires the installation\nand operation of the Slurm Accounting\nDatabase. The age factor represents the length of time a job has been sitting in the queue and eligible to run.  In general, the longer a job waits in the queue, the larger its age factor grows.  However, the age factor for a dependent job will not change while it waits for the job it depends on to complete.  Also, the age factor will not change when scheduling is withheld for a job whose node or time limits exceed the cluster's current limits. At some configurable length of time (PriorityMaxAge), the age factor will max out to 1.0.Association Factor Each association can be assigned an integer priority. The larger the\nnumber, the greater the job priority will be for jobs that request\nthis association. This priority value is normalized to the highest\npriority of all the association to become the association factor.Job Size FactorThe job size factor correlates to the number of nodes or CPUs the job has\nrequested.  This factor can be configured to favor larger jobs or smaller jobs\nbased on the state of the PriorityFavorSmall boolean in the slurm.conf\nfile.  When PriorityFavorSmall is NO, the larger the job, the greater\nits job size factor will be.  A job that requests all the nodes on the machine\nwill get a job size factor of 1.0.  When the PriorityFavorSmall Boolean\nis YES, the single node job will receive the 1.0 job size factor.The PriorityFlags value of SMALL_RELATIVE_TO_TIME alters this\nbehavior as follows. The job size in CPUs is divided by the time limit in\nminutes. The result is divided by the total number of CPUs in the system.\nThus a full-system job with a time limit of one will receive a job size factor\nof 1.0, while a tiny job with a large time limit will receive a job size factor\nclose to 0.0.Nice Factor Users can adjust the priority of their own jobs by setting the nice value\non their jobs. Like the system nice, positive values negatively impact a job's\npriority and negative values increase a job's priority. Only privileged users\ncan specify a negative value. The adjustment range is +/-2147483645.\nPartition Factor\n\n Each node partition can be assigned an integer priority.  The\nlarger the number, the greater the job priority will be for jobs that\nrequest to run in this partition.  This priority value is then\nnormalized to the highest priority of all the partitions to become the\npartition factor.Quality of Service (QOS) Factor\n\n Each QOS can be assigned an integer priority.  The larger the\nnumber, the greater the job priority will be for jobs that request\nthis QOS.  This priority value is then normalized to the highest\npriority of all the QOSs to become the QOS factor.Site Factor The site factor is a factor that can be set either using scontrol,\nthrough a job_submit or site_factor plugin. An example use case, might be a\njob_submit plugin\nthat sets a specific priority based on how many resources are requested.TRES Factors\nEach TRES Type has its own priority factor for a job which represents the amount\nof TRES Type requested/allocated in a given partition. For global TRES Types,\nsuch as Licenses and Burst Buffers, the factor represents the number of\nTRES Type requested/allocated in the whole system. The more a given TRES Type is\nrequested/allocated on a job, the greater the job priority will be for that job.\nFair-share Factor\n\nNote: Computing the fair-share factor requires the installation\nand operation of the Slurm Accounting\nDatabase to provide the assigned shares and the consumed,\ncomputing resources described below. The fair-share component to a job's priority influences the order in which a user's queued jobs are scheduled to run based on the portion of the computing resources they have been allocated and the resources their jobs have already consumed.  The fair-share factor does not involve a fixed allotment, whereby a user's access to a machine is cut off once that allotment is reached. Instead, the fair-share factor serves to prioritize queued jobs such that those jobs charging accounts that are under-serviced are scheduled first, while jobs charging accounts that are over-serviced are scheduled when the machine would otherwise go idle. Slurm's fair-share factor is a floating point number between 0.0 and 1.0 that reflects the shares of a computing resource that a user has been allocated and the amount of computing resources the user's jobs have consumed.  The higher the value, the higher is the placement in the queue of jobs waiting to be scheduled. By default, the computing resource is the computing cycles delivered by a\nmachine in the units of allocated_cpus*seconds. Other resources can be taken into\naccount by configuring a partition's TRESBillingWeights option. The\nTRESBillingWeights option allows you to account for consumed resources other\nthan just CPUs by assigning different billing weights to different Trackable\nResources (TRES) such as CPUs, nodes, memory, licenses and generic resources\n(GRES). For example, when billing only for CPUs, if a job requests 1CPU and 64GB\nof memory on a 16CPU, 64GB node the job will only be billed for 1CPU when it\nreally used the whole node.\n By default, when TRESBillingWeights is configured, a job is billed for each\nindividual TRES used. The billable TRES is calculated as the sum of all TRES\ntypes multiplied by their corresponding billing weight.\n For example, the following jobs on a partition configured with\nTRESBillingWeights=CPU=1.0,Mem=0.25G and 16CPU, 64GB nodes would be billed as:\n\n      CPUs       Mem GB\nJob1: (1 *1.0) + (60*0.25) = (1 + 15) = 16\nJob2: (16*1.0) + (1 *0.25) = (16+.25) = 16.25\nJob3: (16*1.0) + (60*0.25) = (16+ 15) = 31\n\n\nAnother method of calculating the billable TRES is by taking the MAX of the\nindividual TRESs on a node (e.g. cpus, mem, gres) plus the SUM of the global\nTRESs (e.g. licenses). For example the above job's billable TRES would\nbe calculated as:\n\n          CPUs      Mem GB\nJob1: MAX((1 *1.0), (60*0.25)) = 15\nJob2: MAX((15*1.0), (1 *0.25)) = 15\nJob3: MAX((16*1.0), (64*0.25)) = 16\n\nThis method is turned on by defining the MAX_TRES priority flags in the\nslurm.conf.\n\"Fair Tree\" Fairshare\nAs of the 19.05 release, the \"Fair Tree\" fairshare algorithm has been made the\ndefault. Please see the Fair Tree Fairshare\ndocumentation for further details.\n\"Classic\" Fairshare\n\n\nAs of the 19.05 release, the \"classic\" fairshare algorithm is no longer the\ndefault, and will only be used if PriorityFlags=NO_FAIR_TREE is\nexplicitly configured. Documentation describing that algorithm has been moved\nto a separate Classic Fairshare\ndocumentation page.\nThe sprio utility\n\n The sprio command provides a summary of the six factors\nthat comprise each job's scheduling priority.  While squeue has\nformat options (%p and %Q) that display a job's composite priority,\nsprio can be used to display a breakdown of the priority components\nfor each job.  In addition, the sprio -w option displays the\nweights (PriorityWeightAge, PriorityWeightFairshare, etc.) for each\nfactor as it is currently configured.ConfigurationThe following slurm.conf parameters are used to configure the Multifactor Job Priority Plugin. See slurm.conf(5) man page for more details.\n PriorityType\n Set this value to \"priority/multifactor\" to enable the Multifactor Job Priority Plugin.\n PriorityDecayHalfLife\n This determines the contribution of historical usage on the\n  composite usage value.  The larger the number, the longer past usage\n  affects fair-share.  If set to 0 no decay will be applied.  This is helpful if\n  you want to enforce hard time limits per association.  If set to 0\n  PriorityUsageResetPeriod must be set to some interval.\n  The unit is a time string (i.e. min, hr:min:00, days-hr:min:00, or\n  days-hr).  The default value is 7-0 (7 days).\n PriorityCalcPeriod\n The period of time in minutes in which the half-life decay will\n  be re-calculated.  The default value is 5 (minutes).\n PriorityUsageResetPeriod\n At this interval the usage of associations will be reset to 0.\n  This is used if you want to enforce hard limits of time usage per\n  association.  If PriorityDecayHalfLife is set to be 0 no decay will\n  happen and this is the only way to reset the usage accumulated by\n  running jobs.  By default this is turned off and it is advised to\n  use the PriorityDecayHalfLife option to avoid not having anything\n  running on your cluster, but if your schema is set up to only allow\n  certain amounts of time on your system this is the way to do it.\n  Applicable only if PriorityType=priority/multifactor. The unit is a\n  time string (i.e. NONE, NOW, DAILY, WEEKLY).  The default is NONE.\n\nNONE: Never clear historic usage. The default value.\nNOW: Clear the historic usage now. Executed at startup and reconfiguration time.\nDAILY: Cleared every day at midnight.\nWEEKLY: Cleared every week on Sunday at time 00:00.\nMONTHLY: Cleared on the first day of each month at time 00:00.\nQUARTERLY: Cleared on the first day of each quarter at time 00:00.\nYEARLY: Cleared on the first day of each year at time 00:00.\n\n PriorityFavorSmall\n A boolean that sets the polarity of the job size factor.  The\n  default setting is NO which results in larger node sizes having a\n  larger job size factor.  Setting this parameter to YES means that\n  the smaller the job, the greater the job size factor will be.\n PriorityMaxAge\n Specifies the queue wait time at which the age factor maxes out.\n  The unit is a time string (i.e. min, hr:min:00, days-hr:min:00, or\n  days-hr).  The default value is 7-0 (7 days).\n PriorityWeightAge\n An unsigned integer that scales the contribution of the age factor.\n PriorityWeightAssoc\n An unsigned integer that scales the contribution of the association factor.\n PriorityWeightFairshare\n An unsigned integer that scales the contribution of the fair-share factor.\n PriorityWeightJobSize\n An unsigned integer that scales the contribution of the job size factor.\n PriorityWeightPartition\n An unsigned integer that scales the contribution of the partition factor.\n PriorityWeightQOS\n An unsigned integer that scales the contribution of the quality of service factor.\n PriorityWeightTRES\n A list of TRES Types and weights that scales the contribution of each TRES\n  Type's factor.\n\n PriorityFlags\n Flags to modify priority behavior. Applicable only if\nPriorityType=priority/multifactor.\n\nACCRUE_ALWAYS: If set, priority age factor will be increased despite job\n\tdependencies or holds. Accrue limits are ignored.\nCALCULATE_RUNNING: If set, priorities will be recalculated not only for\n\tpending jobs, but also running and suspended jobs.\nDEPTH_OBLIVIOUS: If set, priority will be calculated based similar to the\n\tnormal multifactor calculation, but depth of the associations in the\n\ttree do not adversely effect their priority. This option automatically\n\tenables NO_FAIR_TREE.\nNO_FAIR_TREE: Disables the \"fair tree\" algorithm, and reverts to \"classic\"\n\tfair share priority scheduling.\nINCR_ONLY: If set, priority values will only increase in value. Job\n\tpriority will never decrease in value.\nMAX_TRES: If set, the weighted TRES value (e.g. TRESBillingWeights) is\n\tcalculated as the MAX of individual TRESs on a node (e.g. cpus, mem,\n\tgres) plus the sum of all global TRESs (e.g. licenses).\nNO_NORMAL_ALL: If set, all NO_NORMAL_* flags are set.\nNO_NORMAL_ASSOC: If set, the association factor is not normalized against\n\tthe highest association priority.\nNO_NORMAL_PART: If set, the partition factor is not normalized against the\n\thighest partition PriorityJobFactor.\nNO_NORMAL_QOS: If set, the QOS factor is not normalized against the\n\thighest qos priority.\nNO_NORMAL_TRES: If set, the TRES factor is not normalized against the job's\n\tpartition TRES counts.\nSMALL_RELATIVE_TO_TIME: If set, the job's size component will be based\n\tupon not the job size alone, but the job's size divided by its time\n\tlimit.\n\n Note:  As stated above, the six priority factors range from 0.0 to 1.0.  As such, the PriorityWeight terms may need to be set to a high enough value (say, 1000) to resolve very tiny differences in priority factors.  This is especially true with the fair-share factor, where two jobs may differ in priority by as little as .001. (or even less!)Configuration Example\n\n The following are sample slurm.conf file settings for the\n  Multifactor Job Priority Plugin. The first example is for running the plugin applying decay over\n  time to reduce usage.  Hard limits can be used in this\n  configuration, but will have less effect since usage will decay\n  over time instead of having no decay over time.\n# Activate the Multifactor Job Priority Plugin with decay\nPriorityType=priority/multifactor\n\n# 2 week half-life\nPriorityDecayHalfLife=14-0\n\n# The larger the job, the greater its job size priority.\nPriorityFavorSmall=NO\n\n# The job's age factor reaches 1.0 after waiting in the\n# queue for 2 weeks.\nPriorityMaxAge=14-0\n\n# This next group determines the weighting of each of the\n# components of the Multifactor Job Priority Plugin.\n# The default value for each of the following is 1.\nPriorityWeightAge=1000\nPriorityWeightFairshare=10000\nPriorityWeightJobSize=1000\nPriorityWeightPartition=1000\nPriorityWeightQOS=0 # don't use the qos factor\n This example is for running the plugin with no decay on usage,\n  thus making a reset of usage necessary.\n# Activate the Multifactor Job Priority Plugin with decay\nPriorityType=priority/multifactor\n\n# apply no decay\nPriorityDecayHalfLife=0\n\n# reset usage after 1 month\nPriorityUsageResetPeriod=MONTHLY\n\n# The larger the job, the greater its job size priority.\nPriorityFavorSmall=NO\n\n# The job's age factor reaches 1.0 after waiting in the\n# queue for 2 weeks.\nPriorityMaxAge=14-0\n\n# This next group determines the weighting of each of the\n# components of the Multifactor Job Priority Plugin.\n# The default value for each of the following is 1.\nPriorityWeightAge=1000\nPriorityWeightFairshare=10000\nPriorityWeightJobSize=1000\nPriorityWeightPartition=1000\nPriorityWeightQOS=0 # don't use the qos factor\nLast modified 03 August 2023"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/topology.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Topology Guide",
                "content": "Slurm can be configured to support topology-aware resource\nallocation to optimize job performance.\nSlurm supports several modes of operation, one to optimize performance on\nsystems with a three-dimensional torus interconnect and another for\na hierarchical interconnect.\nThe hierarchical mode of operation supports both fat-tree or dragonfly networks,\nusing slightly different algorithms.Slurm's native mode of resource selection is to consider the nodes\nas a one-dimensional array.\nJobs are allocated resources on a best-fit basis.\nFor larger jobs, this minimizes the number of sets of consecutive nodes\nallocated to the job.Three-dimension Topology\n\nSome larger computers rely upon a three-dimensional torus interconnect.\nThe Cray XT and XE systems also have three-dimensional\ntorus interconnects, but do not require that jobs execute in adjacent nodes.\nOn those systems, Slurm only needs to allocate resources to a job which\nare nearby on the network.\nSlurm accomplishes this using a\nHilbert curve\nto map the nodes from a three-dimensional space into a one-dimensional\nspace.\nSlurm's native best-fit algorithm is thus able to achieve a high degree\nof locality for jobs.\n\nHierarchical Networks\n\n\nSlurm can also be configured to allocate resources to jobs on a\nhierarchical network to minimize network contention.\nThe basic algorithm is to identify the lowest level switch in the\nhierarchy that can satisfy a job's request and then allocate resources\non its underlying leaf switches using a best-fit algorithm.\nUse of this logic requires a configuration setting of\nTopologyPlugin=topology/tree.\nNote that slurm uses a best-fit algorithm on the currently\navailable resources. This may result in an allocation with\nmore than the optimum number of switches. The user can request\na maximum number of leaf switches for the job as well as a\nmaximum time willing to wait for that number using the --switches\noption with the salloc, sbatch and srun commands. The parameters can\nalso be changed for pending jobs using the scontrol and squeue commands.\nAt some point in the future Slurm code may be provided to\ngather network topology information directly.\nNow the network topology information must be included\nin a topology.conf configuration file as shown in the\nexamples below.\nThe first example describes a three level switch in which\neach switch has two children.\nNote that the SwitchName values are arbitrary and only\nused for bookkeeping purposes, but a name must be specified on\neach line.\nThe leaf switch descriptions contain a SwitchName field\nplus a Nodes field to identify the nodes connected to the\nswitch.\nHigher-level switch descriptions contain a SwitchName field\nplus a Switches field to identify the child switches.\nSlurm's hostlist expression parser is used, so the node and switch\nnames need not be consecutive (e.g. \"Nodes=tux[0-3,12,18-20]\"\nand \"Switches=s[0-2,4-8,12]\" will parse fine).\n\nAn optional LinkSpeed option can be used to indicate the\nrelative performance of the link.\nThe units used are arbitrary and this information is currently not used.\nIt may be used in the future to optimize resource allocations.\nThe first example shows what a topology would look like for an\neight node cluster in which all switches have only two children as\nshown in the diagram (not a very realistic configuration, but\nuseful for an example).\n\n# topology.conf\n# Switch Configuration\nSwitchName=s0 Nodes=tux[0-1]\nSwitchName=s1 Nodes=tux[2-3]\nSwitchName=s2 Nodes=tux[4-5]\nSwitchName=s3 Nodes=tux[6-7]\nSwitchName=s4 Switches=s[0-1]\nSwitchName=s5 Switches=s[2-3]\nSwitchName=s6 Switches=s[4-5]\n\n\nThe next example is for a network with two levels and\neach switch has four connections.\n\n# topology.conf\n# Switch Configuration\nSwitchName=s0 Nodes=tux[0-3]   LinkSpeed=900\nSwitchName=s1 Nodes=tux[4-7]   LinkSpeed=900\nSwitchName=s2 Nodes=tux[8-11]  LinkSpeed=900\nSwitchName=s3 Nodes=tux[12-15] LinkSpeed=1800\nSwitchName=s4 Switches=s[0-3]  LinkSpeed=1800\nSwitchName=s5 Switches=s[0-3]  LinkSpeed=1800\nSwitchName=s6 Switches=s[0-3]  LinkSpeed=1800\nSwitchName=s7 Switches=s[0-3]  LinkSpeed=1800\n\n\nAs a practical matter, listing every switch connection\ndefinitely results in a slower scheduling algorithm for Slurm\nto optimize job placement.\nThe application performance may achieve little benefit from such optimization.\nListing the leaf switches with their nodes plus one top level switch\nshould result in good performance for both applications and Slurm.\nThe previous example might be configured as follows:\n\n# topology.conf\n# Switch Configuration\nSwitchName=s0 Nodes=tux[0-3]\nSwitchName=s1 Nodes=tux[4-7]\nSwitchName=s2 Nodes=tux[8-11]\nSwitchName=s3 Nodes=tux[12-15]\nSwitchName=s4 Switches=s[0-3]\n\nNote that compute nodes on switches that lack a common parent switch can\nbe used, but no job will span leaf switches without a common parent\n(unless the TopologyParam=TopoOptional option is used).\nFor example, it is legal to remove the line \"SwitchName=s4 Switches=s[0-3]\"\nfrom the above topology.conf file.\nIn that case, no job will span more than four compute nodes on any single leaf\nswitch.\nThis configuration can be useful if one wants to schedule multiple physical\nclusters as a single logical cluster under the control of a single slurmctld\ndaemon.\nIf you have nodes that are in separate networks and are associated with\nunique switches in your topology.conf file, it's possible that you\ncould get in a situation where a job isn't able to run.  If a job requests\nnodes that are in the different networks, either by requesting the nodes\ndirectly or by requesting a feature, the job will fail because the requested\nnodes can't communicate with each other.  We recommend placing nodes in\nseparate network segments in disjoint partitions.\nFor systems with a dragonfly network, configure Slurm with\nTopologyPlugin=topology/tree plus TopologyParam=dragonfly.\nIf a single job can not be entirely placed within a single network leaf\nswitch, the job will be spread across as many leaf switches as possible\nin order to optimize the job's network bandwidth.\nNOTE: When using the topology/tree plugin, Slurm identifies\nthe network switches which provide the best fit for pending jobs. If nodes\nhave a Weight defined, this will override the resource selection based\non network topology. If optimizing resource selection by node weight is more\nimportant than optimizing network topology then do NOT use the\ntopology/tree plugin.\nConfiguration Generators\n\n\nThe following independently maintained tools may be useful in generating the\ntopology.conf file for certain switch types:\n\n\nInfiniband switch - slurmibtopology\n\nhttps://github.com/OleHolmNielsen/Slurm_tools/tree/master/slurmibtopology\nOmni-Path (OPA) switch - opa2slurm\n\nhttps://gitlab.com/jtfrey/opa2slurm\nAWS Elastic Fabric Adapter (EFA) - ec2-topology\n\nhttps://github.com/aws-samples/ec2-topology-aware-for-slurm\n\nUser Options\nFor use with the topology/tree plugin, user can also specify the maximum\nnumber of leaf switches to be used for their job with the maximum time the\njob should wait for this optimized configuration. The syntax for this option\nis \"--switches=count[@time]\".\nThe system administrator can limit the maximum time that any job can\nwait for this optimized configuration using the SchedulerParameters\nconfiguration parameter with the max_switch_wait option.\nEnvironment Variables\n\n\nIf the topology/tree plugin is used, two environment variables will be set\nto describe that job's network topology. Note that these environment variables\nwill contain different data for the tasks launched on each node. Use of these\nenvironment variables is at the discretion of the user.\nSLURM_TOPOLOGY_ADDR:\nThe value will be set to the names network switches which may be involved in\nthe job's communications from the system's top level switch down to the leaf\nswitch and ending with node name. A period is used to separate each hardware\ncomponent name.\nSLURM_TOPOLOGY_ADDR_PATTERN:\nThis is set only if the system has the topology/tree plugin configured.\nThe value will be set component types listed in SLURM_TOPOLOGY_ADDR.\nEach component will be identified as either \"switch\" or \"node\".\nA period is used to separate each hardware component type.\nLast modified 01 April 2024\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Hierarchical Networks\n\n",
                "content": "Slurm can also be configured to allocate resources to jobs on a\nhierarchical network to minimize network contention.\nThe basic algorithm is to identify the lowest level switch in the\nhierarchy that can satisfy a job's request and then allocate resources\non its underlying leaf switches using a best-fit algorithm.\nUse of this logic requires a configuration setting of\nTopologyPlugin=topology/tree.Note that slurm uses a best-fit algorithm on the currently\navailable resources. This may result in an allocation with\nmore than the optimum number of switches. The user can request\na maximum number of leaf switches for the job as well as a\nmaximum time willing to wait for that number using the --switches\noption with the salloc, sbatch and srun commands. The parameters can\nalso be changed for pending jobs using the scontrol and squeue commands.At some point in the future Slurm code may be provided to\ngather network topology information directly.\nNow the network topology information must be included\nin a topology.conf configuration file as shown in the\nexamples below.\nThe first example describes a three level switch in which\neach switch has two children.\nNote that the SwitchName values are arbitrary and only\nused for bookkeeping purposes, but a name must be specified on\neach line.\nThe leaf switch descriptions contain a SwitchName field\nplus a Nodes field to identify the nodes connected to the\nswitch.\nHigher-level switch descriptions contain a SwitchName field\nplus a Switches field to identify the child switches.\nSlurm's hostlist expression parser is used, so the node and switch\nnames need not be consecutive (e.g. \"Nodes=tux[0-3,12,18-20]\"\nand \"Switches=s[0-2,4-8,12]\" will parse fine).\nAn optional LinkSpeed option can be used to indicate the\nrelative performance of the link.\nThe units used are arbitrary and this information is currently not used.\nIt may be used in the future to optimize resource allocations.The first example shows what a topology would look like for an\neight node cluster in which all switches have only two children as\nshown in the diagram (not a very realistic configuration, but\nuseful for an example).\n# topology.conf\n# Switch Configuration\nSwitchName=s0 Nodes=tux[0-1]\nSwitchName=s1 Nodes=tux[2-3]\nSwitchName=s2 Nodes=tux[4-5]\nSwitchName=s3 Nodes=tux[6-7]\nSwitchName=s4 Switches=s[0-1]\nSwitchName=s5 Switches=s[2-3]\nSwitchName=s6 Switches=s[4-5]\nThe next example is for a network with two levels and\neach switch has four connections.\n# topology.conf\n# Switch Configuration\nSwitchName=s0 Nodes=tux[0-3]   LinkSpeed=900\nSwitchName=s1 Nodes=tux[4-7]   LinkSpeed=900\nSwitchName=s2 Nodes=tux[8-11]  LinkSpeed=900\nSwitchName=s3 Nodes=tux[12-15] LinkSpeed=1800\nSwitchName=s4 Switches=s[0-3]  LinkSpeed=1800\nSwitchName=s5 Switches=s[0-3]  LinkSpeed=1800\nSwitchName=s6 Switches=s[0-3]  LinkSpeed=1800\nSwitchName=s7 Switches=s[0-3]  LinkSpeed=1800\nAs a practical matter, listing every switch connection\ndefinitely results in a slower scheduling algorithm for Slurm\nto optimize job placement.\nThe application performance may achieve little benefit from such optimization.\nListing the leaf switches with their nodes plus one top level switch\nshould result in good performance for both applications and Slurm.\nThe previous example might be configured as follows:\n\n# topology.conf\n# Switch Configuration\nSwitchName=s0 Nodes=tux[0-3]\nSwitchName=s1 Nodes=tux[4-7]\nSwitchName=s2 Nodes=tux[8-11]\nSwitchName=s3 Nodes=tux[12-15]\nSwitchName=s4 Switches=s[0-3]\n\nNote that compute nodes on switches that lack a common parent switch can\nbe used, but no job will span leaf switches without a common parent\n(unless the TopologyParam=TopoOptional option is used).\nFor example, it is legal to remove the line \"SwitchName=s4 Switches=s[0-3]\"\nfrom the above topology.conf file.\nIn that case, no job will span more than four compute nodes on any single leaf\nswitch.\nThis configuration can be useful if one wants to schedule multiple physical\nclusters as a single logical cluster under the control of a single slurmctld\ndaemon.\nIf you have nodes that are in separate networks and are associated with\nunique switches in your topology.conf file, it's possible that you\ncould get in a situation where a job isn't able to run.  If a job requests\nnodes that are in the different networks, either by requesting the nodes\ndirectly or by requesting a feature, the job will fail because the requested\nnodes can't communicate with each other.  We recommend placing nodes in\nseparate network segments in disjoint partitions.\nFor systems with a dragonfly network, configure Slurm with\nTopologyPlugin=topology/tree plus TopologyParam=dragonfly.\nIf a single job can not be entirely placed within a single network leaf\nswitch, the job will be spread across as many leaf switches as possible\nin order to optimize the job's network bandwidth.\nNOTE: When using the topology/tree plugin, Slurm identifies\nthe network switches which provide the best fit for pending jobs. If nodes\nhave a Weight defined, this will override the resource selection based\non network topology. If optimizing resource selection by node weight is more\nimportant than optimizing network topology then do NOT use the\ntopology/tree plugin.\nConfiguration Generators\n\n\nThe following independently maintained tools may be useful in generating the\ntopology.conf file for certain switch types:\n\n\nInfiniband switch - slurmibtopology\n\nhttps://github.com/OleHolmNielsen/Slurm_tools/tree/master/slurmibtopology\nOmni-Path (OPA) switch - opa2slurm\n\nhttps://gitlab.com/jtfrey/opa2slurm\nAWS Elastic Fabric Adapter (EFA) - ec2-topology\n\nhttps://github.com/aws-samples/ec2-topology-aware-for-slurm\n\nUser Options\nFor use with the topology/tree plugin, user can also specify the maximum\nnumber of leaf switches to be used for their job with the maximum time the\njob should wait for this optimized configuration. The syntax for this option\nis \"--switches=count[@time]\".\nThe system administrator can limit the maximum time that any job can\nwait for this optimized configuration using the SchedulerParameters\nconfiguration parameter with the max_switch_wait option.\nEnvironment Variables\n\n\nIf the topology/tree plugin is used, two environment variables will be set\nto describe that job's network topology. Note that these environment variables\nwill contain different data for the tasks launched on each node. Use of these\nenvironment variables is at the discretion of the user.\nSLURM_TOPOLOGY_ADDR:\nThe value will be set to the names network switches which may be involved in\nthe job's communications from the system's top level switch down to the leaf\nswitch and ending with node name. A period is used to separate each hardware\ncomponent name.\nSLURM_TOPOLOGY_ADDR_PATTERN:\nThis is set only if the system has the topology/tree plugin configured.\nThe value will be set component types listed in SLURM_TOPOLOGY_ADDR.\nEach component will be identified as either \"switch\" or \"node\".\nA period is used to separate each hardware component type.\nLast modified 01 April 2024\n"
            },
            {
                "title": "User Options",
                "content": "For use with the topology/tree plugin, user can also specify the maximum\nnumber of leaf switches to be used for their job with the maximum time the\njob should wait for this optimized configuration. The syntax for this option\nis \"--switches=count[@time]\".\nThe system administrator can limit the maximum time that any job can\nwait for this optimized configuration using the SchedulerParameters\nconfiguration parameter with the max_switch_wait option.Environment Variables\n\nIf the topology/tree plugin is used, two environment variables will be set\nto describe that job's network topology. Note that these environment variables\nwill contain different data for the tasks launched on each node. Use of these\nenvironment variables is at the discretion of the user.SLURM_TOPOLOGY_ADDR:\nThe value will be set to the names network switches which may be involved in\nthe job's communications from the system's top level switch down to the leaf\nswitch and ending with node name. A period is used to separate each hardware\ncomponent name.SLURM_TOPOLOGY_ADDR_PATTERN:\nThis is set only if the system has the topology/tree plugin configured.\nThe value will be set component types listed in SLURM_TOPOLOGY_ADDR.\nEach component will be identified as either \"switch\" or \"node\".\nA period is used to separate each hardware component type.Last modified 01 April 2024"
            },
            {
                "title": "Configuration Generators\n",
                "content": "topology.conf\nInfiniband switch - slurmibtopology\n\nhttps://github.com/OleHolmNielsen/Slurm_tools/tree/master/slurmibtopology\nOmni-Path (OPA) switch - opa2slurm\n\nhttps://gitlab.com/jtfrey/opa2slurm\nAWS Elastic Fabric Adapter (EFA) - ec2-topology\n\nhttps://github.com/aws-samples/ec2-topology-aware-for-slurm\nUser OptionsFor use with the topology/tree plugin, user can also specify the maximum\nnumber of leaf switches to be used for their job with the maximum time the\njob should wait for this optimized configuration. The syntax for this option\nis \"--switches=count[@time]\".\nThe system administrator can limit the maximum time that any job can\nwait for this optimized configuration using the SchedulerParameters\nconfiguration parameter with the max_switch_wait option.Environment Variables\n\nIf the topology/tree plugin is used, two environment variables will be set\nto describe that job's network topology. Note that these environment variables\nwill contain different data for the tasks launched on each node. Use of these\nenvironment variables is at the discretion of the user.SLURM_TOPOLOGY_ADDR:\nThe value will be set to the names network switches which may be involved in\nthe job's communications from the system's top level switch down to the leaf\nswitch and ending with node name. A period is used to separate each hardware\ncomponent name.SLURM_TOPOLOGY_ADDR_PATTERN:\nThis is set only if the system has the topology/tree plugin configured.\nThe value will be set component types listed in SLURM_TOPOLOGY_ADDR.\nEach component will be identified as either \"switch\" or \"node\".\nA period is used to separate each hardware component type.Last modified 01 April 2024"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/burst_buffer.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Slurm Burst Buffer Guide",
                "content": "\nOverview\nConfiguration (for system administrators)\n\nCommon Configuration\nDatawarp\nLua\n\n\nLua Implementation (for system\nadministrators)\n\nHow does burst_buffer.lua run?\nWarnings\n\n\nBurst Buffer Resources\n\nDatawarp\nLua\n\n\nJob Submission Commands\n\nDatawarp\nLua\n\n\nPersistent Burst Buffer Creation and Deletion Directives\nHeterogeneous Job Support\nCommand-line Job Options\n\nDatawarp\nLua\n\n\nSymbol Replacement\nStatus Commands\nAdvanced Reservations\nJob Dependencies\nBurst Buffer States and Job States\nOverviewThis guide explains how to use Slurm burst buffer plugins. Where appropriate,\nit explains how these plugins work in order to give guidance about how to best\nuse these plugins.The Slurm burst buffer plugins call a script at different points during the\nlifetime of a job:\nAt job submission\nWhile the job is pending after an estimated start time is\nestablished. This is called \"stage-in.\"\nOnce the job has been scheduled but has not started running yet.\nThis is called \"pre-run.\"\nOnce the job has completed or been cancelled, but Slurm has not\nreleased resources for the job yet. This is called \"stage-out.\"\nOnce the job has completed, and Slurm has released resources for\nthe job. This is called \"teardown.\"\nThis script runs on the slurmctld node. These are the supported plugins:\ndatawarp\nlua\nDatawarp\n\nThis plugin provides hooks to Cray's Datawarp APIs. Datawarp implements burst\nbuffers, which are a shared high-speed storage resource. Slurm provides support\nfor allocating these resources, staging files in, scheduling compute nodes for\njobs using these resources, and staging files out. Burst buffers can also be\nused as temporary storage during a job's lifetime, without file staging.\nAnother typical use case is for persistent storage, not associated with any\nspecific job.Lua\n\nThis plugin provides hooks to an API that is defined by a Lua script. This\nplugin was developed to provide system administrators with a way to do any task\n(not only file staging) at different points in a job's life cycle. These tasks\nmight include file staging, node maintenance, or any other task that is desired\nto run during one or more of the five job states listed above.The burst buffer APIs will only be called for a job that specifically\nrequests using them. The Job Submission Commands section\nexplains how a job can request using the burst buffer APIs.Configuration (for system administrators)\n\nCommon Configuration\n\n\nTo enable a burst buffer plugin, set BurstBufferType in\nslurm.conf. If it is not set, then no burst buffer plugin will be loaded.\nOnly one burst buffer plugin may be specified.\nIn slurm.conf, you may set DebugFlags=BurstBuffer for detailed\nlogging from the burst buffer plugin. This will result in very verbose logging\nand is not intended for prolonged use in a production system, but this may be\nuseful for debugging.\nTRES limits for burst buffers can be\nconfigured by association or QOS in the same way that TRES limits can be\nconfigured for nodes, CPUs, or any GRES. To make Slurm track burst buffer\nresources, add bb/datawarp (for the datawarp plugin) or\nbb/lua (for the lua plugin) to AccountingStorageTres\nin slurm.conf.\nThe size of a job's burst buffer requirements can be used as a factor in\nsetting the job priority as described in the\nmultifactor priority document.\nThe Burst Buffer Resources section explains how\nthese resources are defined.\nBurst-buffer-specific configurations can be set in burst_buffer.conf.\nConfiguration settings include things like which users may use burst buffers,\ntimeouts, paths to burst buffer scripts, etc. See the\nburst_buffer.conf manual\nfor more information.\nThe JSON-C library must be installed in order to build Slurm's\nburst_buffer/datawarp and burst_buffer/lua plugins,\nwhich must parse JSON format data. See Slurm's\nJSON installation information for\ndetails.\nDatawarp\n\nslurm.conf:\nBurstBufferType=burst_buffer/datawarp\nThe datawarp plugin calls two scripts:\ndw_wlm_cli - the Slurm burst_buffer/datawarp plugin calls this\nscript to perform burst buffer functions. It should have been provided by Cray.\nThe location of this script is defined by GetSysState in burst_buffer.conf. A\ntemplate of this script is provided with Slurm:\nsrc/plugins/burst_buffer/datawarp/dw_wlm_cli\ndwstat - the Slurm burst_buffer/datawarp plugin calls this script to\nget status information. It should have been provided by Cray. The location of\nthis script is defined by GetSysStatus in burst_buffer.conf. A template of this\nscript is provided with Slurm:\nsrc/plugins/burst_buffer/datawarp/dwstat\nLuaslurm.conf:\nBurstBufferType=burst_buffer/lua\nThe lua plugin calls a single script which must be named burst_buffer.lua.\nThis script needs to exist in the same directory as slurm.conf. The following\nfunctions are required to exist, although they may do nothing but return\nsuccess:\nslurm_bb_job_process\nslurm_bb_pools\nslurm_bb_job_teardown\nslurm_bb_setup\nslurm_bb_data_in\nslurm_bb_test_data_in\nslurm_bb_real_size\nslurm_bb_paths\nslurm_bb_pre_run\nslurm_bb_post_run\nslurm_bb_data_out\nslurm_bb_test_data_out\nslurm_bb_get_status\nA template of burst_buffer.lua is provided with Slurm:\netc/burst_buffer.lua.exampleThis template documents many more details about the functions such as\nrequired parameters, when each function is called, return values for each\nfunction, and some simple examples.Lua Implementation\n\nThis purpose of this section is to provide additional information about the\nLua plugin to help system administrators who desire to implement the Lua API.\nThe most important points in this section are:\nSome functions in burst_buffer.lua must run quickly and cannot be killed;\nthe remaining functions are allowed to run for as long as needed and can be\nkilled.\nA maximum of 512 copies of burst_buffer.lua are allowed to run concurrently\nin order to avoid exceeding system limits.\nHow does burst_buffer.lua run?\n\nLua scripts may either be run by themselves in a separate process via the\nfork() and exec() system calls, or they may be called\nvia Lua's C API from within an existing process. One of the goals of the lua\nplugin was to avoid calling fork() from within slurmctld because\nit can severely harm performance of the slurmctld. The datawarp plugin calls\nfork() and exec() from slurmctld for every burst\nbuffer API call, and this has been shown to severely harm slurmctld\nperformance. Therefore, slurmctld calls burst_buffer.lua using Lua's C API\ninstead of using fork().Some functions in burst_buffer.lua are allowed to run for a long time, but\nthey may need to be killed if the job is cancelled, if slurmctld is restarted,\nor if they run for longer than the configured timeout in burst_buffer.conf.\nHowever, a call to a Lua script via Lua's C API cannot be killed from within\nthe same process; only killing the entire process that called the Lua\nscript can kill the Lua script.To address this situation, burst_buffer.lua is called in two different\nways:\nThe slurm_bb_job_process, slurm_bb_pools and\nslurm_bb_paths functions are called from slurmctld.\nBecause of the explanation above,\na script running one of these functions cannot be killed. Since these functions\nare called while slurmctld holds some mutexes, it will be extremely harmful to\nslurmctld performance and responsiveness if they are slow. Because it is faster\nto call these functions directly than to call fork() to create a\nnew process, this was deemed an acceptable tradeoff. As a result, these\nfunctions cannot be killed.\nThe remaining functions in burst_buffer.lua are able to run longer without\nadverse effects. These need to be able to be killed. These functions are called\nfrom a lightweight Slurm daemon called slurmscriptd. Whenever one of these\nfunctions needs to run, slurmctld tells slurmscriptd to run that function;\nslurmscriptd then calls fork() to create a new process, then calls\nthe appropriate function. This avoids calling fork() from\nslurmctld while still providing a way to kill running copies of burst_buffer.lua\nwhen needed. As a result, these functions can be killed, and they will be\nkilled if they run for longer than the appropriate timeout value as configured\nin burst_buffer.conf.\nThe way in which each function is called is also documented in the\nburst_buffer.lua.example file.Warnings\n\nDo not install a signal handler in burst_buffer.lua because\nit is called directly from slurmctld. If slurmctld receives a signal, it\ncould attempt to run the signal handler from burst_buffer.lua, even after a call\nto burst_buffer.lua is completed, which results in a crash.Burst Buffer Resources\n\nThe burst buffer API may define burst buffer resource \"pools\" from which a\njob may request a certain amount of pool space. If a pool does not have\nsufficient space to fulfill a job's request, that job will remain pending until\nthe pool does have enough space. Once the pool has enough space, Slurm may begin\nstage-in for the job. When stage-in begins, Slurm subtracts the job's requested\nspace from the pool's available space. When teardown completes, Slurm adds the\njob's requested space back into the pool's available space. The\nJob Submission Commands section explains how a job may\nrequest space from a pool. Pool space is a scalar quantity.Datawarp\n\n\nPools are defined by dw_wlm_cli, and represent bytes. This\nscript prints a JSON-formatted string defining the pools to stdout.\nIf a job does not request a pool, then the pool defined by\nDefaultPool in burst_buffer.conf will be used. If a job does\nnot request a pool and DefaultPool\nis not defined, then the job will be rejected.\nLua\n\n\nPools are optional in this plugin, and can represent anything.\nDefaultPool in burst_buffer.conf is not used in this\nplugin.\nPools are defined by burst_buffer.lua in the function\nslurm_bb_pools. If pools are not desired, then this function should\njust return slurm.SUCCESS. If pools are desired, then this function\nshould return two values: (1) slurm.SUCCESS, and (2) a\nJSON-formatted string defining the pools. An example is provided in\nburst_buffer.lua.example. The current valid fields in the JSON string are:\n\nid - a string defining the name of the pool\nquantity - a number defining the amount of space in the\n\tpool\ngranularity - a number defining the lowest resolution of\n\tspace that may be allocated from this pool. If a job does not request a\n\tnumber that is a multiple of granularity, then the job's request will\n\tbe rounded up to the nearest multiple of granularity. For example,\n\tif granularity equals 1000, then the smallest amount of space that may\n\tbe allocated from this pool for a single job is 1000. If a job requests\n\tless than 1000 units from this pool, then the job's request will be\n\trounded up to 1000.\n\nJob Submission Commands\n\nThe normal mode of operation is for batch jobs to specify burst buffer\nrequirements within the batch script. Commented batch script lines containing a\nspecific directive (depending on which plugin is being used) will inform Slurm\nthat it should run the burst buffer stages for that job. These lines will also\ndescribe the burst buffer requirements for the job.The salloc and srun commands can specify burst buffer requirements with the\n--bb and --bbf options. This is described in the\nCommand-line Job Options section.All burst buffer directives should be specified in comments at the top of\nthe batch script. They may be placed before, after, or interspersed with any\n#SBATCH directives. All burst buffer stages happen at specific\npoints in the job's life cycle, as described in the\nOverview section; they do not happen during the job's\nexecution. For example, all of the persistent burst buffer (used only by the\ndatawarp plugin) creations and deletions happen before the job's compute\nportion happens. In a similar fashion, you can't run stage-in at various points\nin the script execution; burst buffer stage-in is performed before the job\nbegins and stage-out is performed after the job completes.For both plugins, a job may request a certain amount of space (size or\ncapacity) from a burst buffer resource pool.\nA pool specification is simply a string that matches the name of the\npool. For example: pool=pool1\nA capacity specification is a number indicating the amount of space\nrequired from the pool. A capacity specification can include a suffix of\n\"N\" (nodes), \"K|KiB\", \"M|MiB\", \"G|GiB\", \"T|TiB\", \"P|PiB\" (for powers of 1024)\nand \"KB\", \"MB\", \"GB\", \"TB\", \"PB\" (for powers of 1000). NOTE: Usually\nSlurm interprets KB, MB, GB, TB, PB, units as powers of 1024, but for Burst\nBuffers size specifications Slurm supports both IEC/SI formats. This is because\nthe CRAY API supports both formats.\nAt job submission, Slurm performs basic directive validation and also runs a\nfunction in the burst buffer script. This function can perform validation of\nthe directives used in the job script. If Slurm determines options are invalid,\nor if the burst buffer script returns an error, the job will be rejected and an\nerror message will be returned directly to the user.Note that unrecognized options may be ignored in order to support backward\ncompatibility (i.e. a job submission would not fail in the case of an option\nrecognized by some versions of Slurm, but not recognized by other versions). If\nthe job is accepted, but later fails (e.g. some problem staging files), the job\nwill be held and its \"Reason\" field will be set to an error message provided by\nthe underlying infrastructure.Users may also request to be notified by email upon completion of burst\nbuffer stage out using the --mail-type=stage_out or\n--mail-type=all option. The subject line of the email will be of\nthis form:\nSLURM Job_id=12 Name=my_app Staged Out, StageOut time 00:05:07\nThe following plugin subsections give additional information that is\nspecific to each plugin and provide example job scripts. Command-line examples\nare given in the\nCommand-line Job Options section.Datawarp\n\nThe directive of #DW (for \"DataWarp\") is used for burst buffer\ndirectives when using the burst_buffer/datawarp plugin. Please\nreference Cray documentation for details about the DataWarp options. For\nDataWarp systems, the directive of #BB can be used to create or\ndelete persistent burst buffer storage.\n\nNOTE: The #BB directive is used since the\ncommand is interpreted by Slurm and not by the Cray Datawarp software. This is\ndiscussed more in the Persistent Burst Buffer\nsection.For job-specific burst buffers, it is required to specify a burst buffer\ncapacity. If the job does not specify capacity then the job will\nbe rejected. A job may also specify the pool from which it wants resources; if\nthe job does not specify a pool, then the pool specified by DefaultPool in\nburst_buffer.conf will be used (if configured).The following job script requests burst buffer resources from the default\npool and requests files to be staged in and staged out:\n#!/bin/bash\n#DW jobdw type=scratch capacity=1GB access_mode=striped,private pfs=/scratch\n#DW stage_in type=file source=/tmp/a destination=/ss/file1\n#DW stage_out type=file destination=/tmp/b source=/ss/file1\nsrun application.sh\nLua\n\nThe default directive for this plugin is #BB_LUA. The directive\nused by this plugin may be changed by setting the Directive option in\nburst_buffer.conf. Since the directive must always begin with a #\nsign (which starts a comment in a shell script) this option should specify only\nthe string following the # sign. For example, if burst_buffer.conf\ncontains the following:Directive=BB_EXAMPLEthen the burst buffer directive will be #BB_EXAMPLE.If the Directive option is not specified in burst_buffer.conf, then\nthe default directive for this plugin (#BB_LUA) will be used.Since this plugin was designed to be generic and flexible, this plugin only\nrequires the directive to be given. If the directive is given, Slurm will run\nall burst buffer stages for the job.Example of the minimum information required for all burst buffer stages to\nrun for the job:\n#!/bin/bash\n#BB_LUA\nsrun application.sh\nBecause burst buffer pools are optional for this plugin (see the Burst Buffer Resources section), a job is not required to\nspecify a pool or capacity. If pools are provided by the burst buffer API,\nthen a job may request a pool and capacity:\n#!/bin/bash\n#BB_LUA pool=pool1 capacity=1K\nsrun application.sh\nA job may choose whether or not to specify a pool. If a job does not specify\na pool, then the job is still allowed to run and the burst buffer stages will\nstill run for this job (as long as the burst buffer directive was given). If\nthe job specifies a pool but that pool is not found, then the job is\nrejected.The system administrator may validate burst buffer options in the\nslurm_bb_job_process function in burst_buffer.lua. This might\ninclude requiring a job to specify a pool or validating any additional options\nthat the system administrator decides to implement.Persistent Burst Buffer Creation and Deletion Directives\n\nThis section only applies to the datawarp plugin, since persistent burst\nbuffers are not used in any other burst buffer plugin.These options are used to create and delete persistent burst buffers:\n#BB create_persistent name=<name> capacity=<number>\n[access=<access>] [pool=<pool> [type=<type>]\n#BB destroy_persistent name=<name> [hurry]\nOptions for creating and deleting persistent burst buffers:\nname - The persistent burst buffer name may not start with a numeric\nvalue (numeric names are reserved for job-specific burst buffers).\ncapacity - Described in the\nJob Submission Commands section.\npool - Described in the\nJob Submission Commands section.\naccess - The access parameter identifies the buffer access mode.\nSupported access modes for the datawarp plugin include:\n\nstriped\nprivate\nldbalance\n\ntype - The type parameter identifies the buffer type. Supported type\nmodes for the datawarp plugin include:\n\ncache\nscratch\n\nMultiple persistent burst buffers may be created or deleted within a single\njob.Example - Creating two persistent burst buffers:\n#!/bin/bash\n#BB create_persistent name=alpha capacity=32GB access=striped type=scratch\n#BB create_persistent name=beta capacity=16GB access=striped type=scratch\nsrun application.sh\nExample - Destroying two persistent burst buffers:\n#!/bin/bash\n#BB destroy_persistent name=alpha\n#BB destroy_persistent name=beta\nsrun application.sh\nPersistent burst buffers can be created and deleted by a job requiring no\ncompute resources. Submit a job with the desired burst buffer directives and\nspecify a node count of zero (e.g. sbatch -N0 setup_buffers.bash).\nAttempts to submit a zero size job without burst buffer directives or with\njob-specific burst buffer directives will generate an error. Note that zero\nsize jobs are not supported for job arrays or heterogeneous job\nallocations.NOTE: The ability to create and destroy persistent burst buffers may\nbe limited by the Flags option in the burst_buffer.conf file.\nSee the burst_buffer.conf man page for\nmore information.\nBy default only privileged users\n(i.e. Slurm operators and administrators)\ncan create or destroy persistent burst buffers.Heterogeneous Job Support\n\nHeterogeneous jobs may request burst buffers. Burst buffer hooks will run\nonce for each component that has burst buffer directives. For example, if a\nheterogeneous job has three components and two of them have burst buffer\ndirectives, the burst buffer hooks will run once for each of the two components\nwith burst buffer directives, but not for the third component without burst\nbuffer directives. Further information and examples can be found in the\nheterogeneous jobs page.\nCommand-line Job Options\n\nIn addition to putting burst buffer directives in the batch script, the\ncommand-line options --bb and --bbf may also include\nburst buffer directives. These command-line options are available for salloc,\nsbatch, and srun. Note that the --bb option cannot create or\ndestroy persistent burst buffers.The --bbf option takes as an argument a filename and that file\nshould contain a collection of burst buffer operations identical to those used\nfor batch jobs.Alternatively, the --bb option may be used to specify burst\nbuffer directives as the option argument. The behavior of this option depends\non which burst buffer plugin is used. When the --bb option is\nused, Slurm parses this option and creates a temporary burst buffer script file\nthat is used internally by the burst buffer plugins.Datawarp\n\nWhen using the --bb option, the format of the directives can\neither be identical to those used in a batch script OR a very limited set of\noptions can be used, which are translated to the equivalent script for later\nprocessing. The following options are allowed:\naccess=&ltaccess>\ncapacity=&ltnumber>\nswap=&ltnumber>\ntype=&lttype>\npool=&ltname>\nMultiple options should be space separated. If a swap option is specified,\nthe job must also specify the required node count.Example:\n# Sample execute line:\nsrun --bb=\"capacity=1G access=striped type=scratch\" a.out\n\n# Equivalent script as generated by Slurm's burst_buffer/datawarp plugin\n#DW jobdw capacity=1GiB access_mode=striped type=scratch\nLua\n\nThis plugin does not do any special parsing or translating of burst buffer\ndirectives given by the --bb option. When using the\n--bb option, the format is identical to the batch script: Slurm\nonly enforces that the burst buffer directive must be specified. See additional\ninformation in the Lua subsection of Job Submission\nCommands.Example:\n# Sample execute line:\nsrun --bb=\"#BB_LUA pool=pool1 capacity=1K\"\n\n# Equivalent script as generated by Slurm's burst_buffer/lua plugin\n#BB_LUA pool=pool1 capacity=1K\nSymbol Replacement\n\nSlurm supports a number of symbols that can be used to automatically\nfill in certain job details, e.g. to make stage-in or stage-out directory\npaths vary with each job submission.Supported symbols include:\n\n\n%%%\n%AArray Master Job Id\n%aArray Task Id\n%dWorkdir\n%jJob Id\n%uUser Name\n%xJob Name\n\\\\Stop further processing of the line\n\nStatus CommandsBurst buffer information that Slurm tracks is available by using the\nscontrol show burst command or by using the sview command's\nBurst Buffer tab. Examples follow.Datawarp plugin example:\n$ scontrol show burst\nName=datawarp DefaultPool=wlm_pool Granularity=200GiB TotalSpace=5800GiB FreeSpace=4600GiB UsedSpace=1600GiB\n  Flags=EmulateCray\n  StageInTimeout=86400 StageOutTimeout=86400 ValidateTimeout=5 OtherTimeout=300\n  GetSysState=/home/marshall/slurm/master/install/c1/sbin/dw_wlm_cli\n  GetSysStatus=/home/marshall/slurm/master/install/c1/sbin/dwstat\n  Allocated Buffers:\n    JobID=169509 CreateTime=2021-08-11T10:19:06 Pool=wlm_pool Size=1200GiB State=allocated UserID=marshall(1017)\n    JobID=169508 CreateTime=2021-08-11T10:18:46 Pool=wlm_pool Size=400GiB State=staged-in UserID=marshall(1017)\n  Per User Buffer Use:\n    UserID=marshall(1017) Used=1600GiB\nLua plugin example:\n$ scontrol show burst\nName=lua DefaultPool=(null) Granularity=1 TotalSpace=0 FreeSpace=0 UsedSpace=0\n  PoolName[0]=pool1 Granularity=1KiB TotalSpace=10000KiB FreeSpace=9750KiB UsedSpace=250KiB\n  PoolName[1]=pool2 Granularity=2 TotalSpace=10 FreeSpace=10 UsedSpace=0\n  PoolName[2]=pool3 Granularity=1 TotalSpace=4 FreeSpace=4 UsedSpace=0\n  PoolName[3]=pool4 Granularity=1 TotalSpace=5GB FreeSpace=4GB UsedSpace=1GB\n  Flags=DisablePersistent\n  StageInTimeout=86400 StageOutTimeout=86400 ValidateTimeout=5 OtherTimeout=300\n  GetSysState=(null)\n  GetSysStatus=(null)\n  Allocated Buffers:\n    JobID=169504 CreateTime=2021-08-11T10:13:38 Pool=pool1 Size=250KiB State=allocated UserID=marshall(1017)\n    JobID=169502 CreateTime=2021-08-11T10:12:06 Pool=pool4 Size=1GB State=allocated UserID=marshall(1017)\n  Per User Buffer Use:\n    UserID=marshall(1017) Used=1000256KB\nAccess to a burst buffer status API is available from scontrol using the\nscontrol show bbstat ... or scontrol show dwstat ...\ncommands. Options following bbstat or dwstat on the\nscontrol execute line are passed directly to the bbstat or dwstat commands, as\nshown below. In the datawarp plugin, this command calls Cray's dwstat script.\nSee Cray Datawarp documentation for details about dwstat options and output. In\nthe lua plugin, this command calls the slurm_bb_get_status\nfunction in burst_buffer.lua.Datawarp plugin example:\n/opt/cray/dws/default/bin/dwstat\n$ scontrol show dwstat\n    pool units quantity    free gran'\nwlm_pool bytes  7.28TiB 7.28TiB 1GiB'\n\n$ scontrol show dwstat sessions\n sess state      token creator owner             created expiration nodes\n  832 CA---  783000000  tester 12345 2015-09-08T16:20:36      never    20\n  833 CA---  784100000  tester 12345 2015-09-08T16:21:36      never     1\n  903 D---- 1875700000  tester 12345 2015-09-08T17:26:05      never     0\n\n$ scontrol show dwstat configurations\n conf state inst    type access_type activs\n  715 CA---  753 scratch      stripe      1\n  716 CA---  754 scratch      stripe      1\n  759 D--T-  807 scratch      stripe      0\n  760 CA---  808 scratch      stripe      1\nA Lua plugin example can be found in the slurm_bb_get_status\nfunction in the etc/burst_buffer.lua.example file provided\nwith Slurm.Advanced Reservations\n\nBurst buffer resources can be placed in an advanced reservation using the\nBurstBuffer option.\nThe argument consists of four elements:\n[plugin:][pool:]#[units]\n\nplugin is the burst buffer plugin name, currently either \"datawarp\"\nor \"lua\".\npool specifies a burst buffer resource pool.\nIf \"type\" is not specified, the number is a measure of storage space.\n# (meaning number) should be replaced with a positive integer.\nunits has the same format as the suffix of capacity in the\nJob Submission Commands section.\nJobs using this reservation are not restricted to these burst buffer\nresources, but may use these reserved resources plus any which are generally\navailable. Some examples follow.\n\n$ scontrol create reservation starttime=now duration=60 \\\n  users=alan flags=any_nodes \\\n  burstbuffer=datawarp:100G\n\n$ scontrol create reservation StartTime=noon duration=60 \\\n  users=brenda NodeCnt=8 \\\n  BurstBuffer=datawarp:20G\n\n$ scontrol create reservation StartTime=16:00 duration=60 \\\n  users=joseph flags=any_nodes \\\n  BurstBuffer=datawarp:pool_test:4G\n\nJob Dependencies\n\n\nIf two jobs use burst buffers and one is dependent on the other (e.g.\nsbatch --dependency=afterok:123 ...) then the second job will not\nbegin until the first job completes and its burst buffer stage-out completes.\nIf the second job does not use a burst buffer, but is dependent upon the first\njob's completion, then it will not wait for the stage-out operation of the first\njob to complete.\nThe second job can be made to wait for the first job's stage-out operation to\ncomplete using the \"afterburstbuffer\" dependency option (e.g.\nsbatch --dependency=afterburstbuffer:123 ...).\nBurst Buffer States and Job States\n\n\nThese are the different possible burst buffer states:\n\npending\nallocating\nallocated\ndeleting\ndeleted\nstaging-in\nstaged-in\npre-run\nalloc-revoke\nrunning\nsuspended\npost-run\nstaging-out\nteardown\nteardown-fail\ncomplete\n\nThese states appear in the \"BurstBufferState\" field in the output of\nscontrol show job. This field only appears for jobs that requested\na burst buffer. The states allocating, allocated,\ndeleting and deleted are used\nfor persistent burst buffers only (not for job-specific burst buffers). The\nstate alloc-revoke happens if a failure in Slurm's select plugin\noccurs in between Slurm allocating resources for a job and actually starting\nthe job. This should never happen.\nWhen a job requests a burst buffer, this is what the job and burst buffer\nstate transitions look like:\n\nJob is submitted. Job state and burst buffer state are both\npending.\nBurst buffer stage-in starts. Job state: pending with reason:\nBurstBufferStageIn. Burst buffer state: staging-in.\n\nWhen stage-in completes, the job is eligible to be scheduled (barring any\nother limits). Job state: pending. Burst buffer state:\nstaged-in.\nWhen the job is scheduled and allocated resources, the burst buffer pre-run\nstage begins. Job state: running+configuring. Burst buffer state:\npre-run.\nWhen pre-run finishes, the configuring flag is cleared from\nthe job and the job can actually start running. Job state and burst buffer\nstate are both running.\nWhen the job completes (even if it fails), burst buffer stage-out starts.\nJob state: stage-out. Burst buffer state:\nstaging-out.\nWhen stage-out completes, teardown starts. Job state: complete.\nBurst buffer state: teardown.\n\nThere are some situations which will change the state transitions. Examples\ninclude:\n\nBurst buffer operation failures:\n\nIf teardown fails, then the burst buffer state changes to\n\tteardown-fail.  Teardown will be retried. For the burst_buffer/lua\n\tplugin, teardown will run a maximum of 3 times before giving up and\n\tdestroying the burst buffer.\nIf either stage-in or stage-out fail and Flags=teardownFailure is\n\tconfigured in burst_buffer.conf, then teardown runs. Otherwise, the job\n\tis held and the burst buffer remains in the same state so it may be\n\tinspected and manually destroyed with scancel --hurry.\nIf pre-run fails, then the job is held and teardown runs.\n\nWhen a job is cancelled, the current burst buffer script for that job\n(if running) is killed. If scancel --hurry was used, or if the job\nnever ran, stage-out is skipped and it goes straight to teardown. Otherwise,\nstage-out begins.\nIf slurmctld is stopped, Slurm kills all running burst buffer scripts for\nall jobs and burst buffer state is saved for each job. When slurmctld restarts,\nfor each job it reads the burst buffer state and does one of the following:\n\nPending - Do nothing, since no burst buffer scripts were\n\tkilled.\nStaging-in, staged-in - run teardown, wait for a short time,\n\tthen restart stage-in.\nPre-run - Restart pre-run.\nRunning - Do nothing, since no burst buffer scripts were\n\tkilled.\nPost-run, staging-out - Restart post-run.\nTeardown, teardown-fail - Restart teardown.\n\n\nNOTE: There are many other things not listed here that affect the job\nstate. This document focuses on burst buffers and does not attempt to address\nall possible job state transitions.\nLast modified 21 August 2023\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Job Dependencies\n\n",
                "content": "If two jobs use burst buffers and one is dependent on the other (e.g.\nsbatch --dependency=afterok:123 ...) then the second job will not\nbegin until the first job completes and its burst buffer stage-out completes.\nIf the second job does not use a burst buffer, but is dependent upon the first\njob's completion, then it will not wait for the stage-out operation of the first\njob to complete.\nThe second job can be made to wait for the first job's stage-out operation to\ncomplete using the \"afterburstbuffer\" dependency option (e.g.\nsbatch --dependency=afterburstbuffer:123 ...).Burst Buffer States and Job States\n\nThese are the different possible burst buffer states:\npending\nallocating\nallocated\ndeleting\ndeleted\nstaging-in\nstaged-in\npre-run\nalloc-revoke\nrunning\nsuspended\npost-run\nstaging-out\nteardown\nteardown-fail\ncomplete\nThese states appear in the \"BurstBufferState\" field in the output of\nscontrol show job. This field only appears for jobs that requested\na burst buffer. The states allocating, allocated,\ndeleting and deleted are used\nfor persistent burst buffers only (not for job-specific burst buffers). The\nstate alloc-revoke happens if a failure in Slurm's select plugin\noccurs in between Slurm allocating resources for a job and actually starting\nthe job. This should never happen.When a job requests a burst buffer, this is what the job and burst buffer\nstate transitions look like:\nJob is submitted. Job state and burst buffer state are both\npending.\nBurst buffer stage-in starts. Job state: pending with reason:\nBurstBufferStageIn. Burst buffer state: staging-in.\n\nWhen stage-in completes, the job is eligible to be scheduled (barring any\nother limits). Job state: pending. Burst buffer state:\nstaged-in.\nWhen the job is scheduled and allocated resources, the burst buffer pre-run\nstage begins. Job state: running+configuring. Burst buffer state:\npre-run.\nWhen pre-run finishes, the configuring flag is cleared from\nthe job and the job can actually start running. Job state and burst buffer\nstate are both running.\nWhen the job completes (even if it fails), burst buffer stage-out starts.\nJob state: stage-out. Burst buffer state:\nstaging-out.\nWhen stage-out completes, teardown starts. Job state: complete.\nBurst buffer state: teardown.\nThere are some situations which will change the state transitions. Examples\ninclude:\nBurst buffer operation failures:\n\nIf teardown fails, then the burst buffer state changes to\n\tteardown-fail.  Teardown will be retried. For the burst_buffer/lua\n\tplugin, teardown will run a maximum of 3 times before giving up and\n\tdestroying the burst buffer.\nIf either stage-in or stage-out fail and Flags=teardownFailure is\n\tconfigured in burst_buffer.conf, then teardown runs. Otherwise, the job\n\tis held and the burst buffer remains in the same state so it may be\n\tinspected and manually destroyed with scancel --hurry.\nIf pre-run fails, then the job is held and teardown runs.\n\nWhen a job is cancelled, the current burst buffer script for that job\n(if running) is killed. If scancel --hurry was used, or if the job\nnever ran, stage-out is skipped and it goes straight to teardown. Otherwise,\nstage-out begins.\nIf slurmctld is stopped, Slurm kills all running burst buffer scripts for\nall jobs and burst buffer state is saved for each job. When slurmctld restarts,\nfor each job it reads the burst buffer state and does one of the following:\n\nPending - Do nothing, since no burst buffer scripts were\n\tkilled.\nStaging-in, staged-in - run teardown, wait for a short time,\n\tthen restart stage-in.\nPre-run - Restart pre-run.\nRunning - Do nothing, since no burst buffer scripts were\n\tkilled.\nPost-run, staging-out - Restart post-run.\nTeardown, teardown-fail - Restart teardown.\n\nNOTE: There are many other things not listed here that affect the job\nstate. This document focuses on burst buffers and does not attempt to address\nall possible job state transitions.Last modified 21 August 2023"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/licenses.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Licenses Guide",
                "content": "Licenses Overview\n\nSlurm can help with software license management by assigning available\nlicenses to jobs at scheduling time. If the licenses are not available, jobs\nare kept pending until licenses become available.  Licenses in Slurm are\nessentially shared resources, meaning configured resources that are not tied to\na specific host but are associated with the entire cluster.Licenses in Slurm can be configured in two ways:\nLocal Licenses:\nLocal licenses are local to the cluster using the\nslurm.conf in which they are configured.\n\nRemote Licenses:\nRemote licenses are served by the database and are configured using the\nsacctmgr command. Remote licenses are dynamic in nature as upon running\nthe sacctmgr command, the slurmdbd updates all clusters the\nlicenses are assigned to.\n\nLocal Licenses\n\nLocal licenses are defined in the slurm.conf using the Licenses\noption.slurm.conf:\nLicenses=fluent:30,ansys:100\nConfigured licenses can be viewed using the scontrol command.\n$ scontrol show lic\nLicenseName=ansys\n    Total=100 Used=0 Free=100 Remote=no\nLicenseName=fluent\n    Total=30 Used=0 Free=30 Remote=no\nRequesting licenses is done by using the -L, or --licenses, submission\noption.\n$ sbatch -L ansys:2 script.sh\nSubmitted batch job 5212\n\n$ scontrol show lic\nLicenseName=ansys\n    Total=100 Used=2 Free=98 Remote=no\nLicenseName=fluent\n    Total=30 Used=0 Free=30 Remote=no\nLicenses may also be requested using the --tres-per-task option for\njob submission. If this approach is used, the license must also be defined in\nthe AccountingStorageTRES option of the slurm.conf.slurm.conf:\nLicenses=fluent:30\nAccountingStorageTRES=license/fluent\nRequesting licenses with the --tres-per-task submission option.\n$ sbatch --tres-per-task=license/fluent:4 script.sh\nSubmitted batch job 6482\n\n$ scontrol show lic\nLicenseName=fluent\n    Total=30 Used=4 Free=26 Reserved=0 Remote=no\nRemote Licenses\n\nUse CaseA site has two license servers, one serves 100 Nastran licenses provided by\nFlexNet and the other serves 50 Matlab licenses from Reprise License\nManagement. The site has two clusters named \"fluid\" and \"pdf\" dedicated to run\nsimulation jobs using both products. The managers want to split the number of\nNastran licenses equally between clusters, but assign 70% of the Matlab\nlicenses to cluster \"pdf\" and the remaining 30% to cluster \"fluid\".Configuring Slurm for the use case\n\nHere we assume that both clusters have been configured correctly in the\nslurmdbd using the sacctmgr command.\n$ sacctmgr show clusters format=cluster,controlhost\n   Cluster     ControlHost\n---------- ---------------\n     fluid     143.11.1.3\n       pdf     144.12.3.2\nThe licenses are added using the sacctmgr command, specifying the\ntotal count of licenses and the percentage that should be allocated\nto each cluster. This can be done either in one step or through a\nmulti-step process.One step:\n$ sacctmgr add resource name=nastran cluster=fluid,pdf \\\n  count=100 allowed=50 server=flex_host servertype=flexlm type=license\n Adding Resource(s)\n  nastran@flex_host\n   Cluster - fluid\t50\n   Cluster - pdf\t50\n Settings\n  Name           = nastran\n  Server         = flex_host\n  Description    = nastran\n  ServerType     = flexlm\n  Count          = 100\n  Type           = License\nMulti-step:\n$ sacctmgr add resource name=matlab count=50 server=rlm_host \\\n  servertype=rlm type=license\n Adding Resource(s)\n  matlab@rlm_host\n Settings\n  Name           = matlab\n  Server         = rlm_host\n  Description    = matlab\n  ServerType     = rlm\n  Count          = 50\n  Type           = License\n\n$ sacctmgr add resource name=matlab server=rlm_host \\\n  cluster=pdf allowed=70\n Adding Resource(s)\n  matlab@rlm_host\n   Cluster - pdf\t70\n Settings\n  Name           = matlab\n  Server         = rlm_host\n  Count          = 50\n  LastConsumed   = 0\n  Flags          = (null)\n  Type           = License\n\n$ sacctmgr add resource name=matlab server=rlm_host \\\n  cluster=fluid allowed=30\n Adding Resource(s)\n  matlab@rlm_host\n   Cluster - fluid\t30\n Settings\n  Name           = matlab\n  Server         = rlm_host\n  Count          = 50\n  LastConsumed   = 0\n  Flags          = (null)\n  Type           = License\nThe sacctmgr command will now display the grand total\nof licenses.\n$ sacctmgr show resource\n      Name     Server     Type  Count LastConsumed Allocated ServerType                Flags\n---------- ---------- -------- ------ ------------ --------- ---------- --------------------\n   nastran  flex_host  License    100            0       100     flexlm\n    matlab   rlm_host  License     50            0       100        rlm\n$ sacctmgr show resource withclusters\n      Name     Server     Type  Count LastConsumed Allocated ServerType    Cluster  Allowed                Flags\n---------- ---------- -------- ------ ------------ --------- ---------- ---------- -------- --------------------\n   nastran  flex_host  License    100            0       100     flexlm      fluid       50\n   nastran  flex_host  License    100            0       100     flexlm        pdf       50 \n    matlab   rlm_host  License     50            0       100        rlm      fluid       30\n    matlab   rlm_host  License     50            0       100        rlm        pdf       70\nThe configured licenses are now visible on both clusters using the\nscontrol command.\n# On cluster \"pdf\":\n$ scontrol show lic\nLicenseName=matlab@rlm_host\n    Total=35 Used=0 Free=35 Reserved=0 Remote=yes\n    LastConsumed=0 LastDeficit=0 LastUpdate=2023-02-28T17:01:44\nLicenseName=nastran@flex_host\n    Total=50 Used=0 Free=50 Reserved=0 Remote=yes\n    LastConsumed=0 LastDeficit=0 LastUpdate=2023-02-28T17:01:44\n\n# On cluster \"fluid\":\n$ scontrol show lic\nLicenseName=matlab@rlm_host\n    Total=15 Used=0 Free=15 Reserved=0 Remote=yes\n    LastConsumed=0 LastDeficit=0 LastUpdate=2023-02-28T17:01:44\nLicenseName=nastran@flex_host\n    Total=50 Used=0 Free=50 Reserved=0 Remote=yes\n    LastConsumed=0 LastDeficit=0 LastUpdate=2023-02-28T17:01:44\nWhen submitting jobs to remote licenses, the name and server must be\nused.\n$ sbatch -L nastran@flex_host script.sh\nSubmitted batch job 5172\nLicense percentages and counts can be modified as shown below:\n$ sacctmgr modify resource name=matlab server=rlm_host set \\\n  count=200\n Modified server resource ...\n  matlab@rlm_host\n  Cluster - fluid\t- matlab@rlm_host\n  Cluster - pdf\t- matlab@rlm_host\n\n$ sacctmgr modify resource name=matlab server=rlm_host \\\n  cluster=pdf set allowed=60\n Modified server resource ...\n  Cluster - pdf\t- matlab@rlm_host\n\n$ sacctmgr show resource withclusters\n      Name     Server     Type  Count LastConsumed Allocated ServerType    Cluster  Allowed                Flags\n---------- ---------- -------- ------ ------------ --------- ---------- ---------- -------- --------------------\n   nastran  flex_host  License    100            0       100     flexlm      fluid       50\n   nastran  flex_host  License    100            0       100     flexlm        pdf       50\n    matlab   rlm_host  License    200            0        90        rlm      fluid       30\n    matlab   rlm_host  License    200            0        90        rlm        pdf       60\nLicenses can be deleted either on the cluster or all together as shown:\n$ sacctmgr delete resource where name=matlab server=rlm_host cluster=fluid\n Deleting resource(s)...\n Deleting resource(s)...\n  Cluster - fluid\t- matlab@rlm_host\n\n$ sacctmgr delete resource where name=nastran server=flex_host\n Deleting resource(s)...\n  nastran@flex_host\n  Cluster - fluid\t- nastran@flex_host\n  Cluster - pdf\t- nastran@flex_host\n\n$ sacctmgr show resource withclusters\n      Name     Server     Type  Count LastConsumed Allocated ServerType    Cluster  Allowed                Flags\n---------- ---------- -------- ------ ------------ --------- ---------- ---------- -------- --------------------\n    matlab   rlm_host  License    200            0        60        rlm        pdf       60\nStarting with Slurm 23.02, a new Absolute flag is available that\nindicates the license allowed values for each cluster are to be treated as\nabsolute license counts rather than percentages.Some brief examples of license management using this flag.\n$ sacctmgr -i add resource name=deluxe cluster=fluid,pdf count=150 allowed=70 \\\n  server=flex_host servertype=flexlm flags=absolute\n Adding Resource(s)\n  deluxe@flex_host\n   Cluster - fluid\t70\n   Cluster - pdf\t70\n Settings\n  Name           = deluxe\n  Server         = flex_host\n  Description    = deluxe\n  ServerType     = flexlm\n  Count          = 150\n  Flags          = Absolute\n  Type           = Unknown\n\n$ sacctmgr show resource withclusters\n      Name     Server     Type  Count LastConsumed Allocated ServerType    Cluster  Allowed                Flags \n---------- ---------- -------- ------ ------------ --------- ---------- ---------- -------- -------------------- \n    deluxe  flex_host  License    150            0       140     flexlm      fluid       70             Absolute \n    deluxe  flex_host  License    150            0       140     flexlm        pdf       70             Absolute\n\n$ sacctmgr -i update resource deluxe set allowed=25 where cluster=fluid\n Modified server resource ...\n  Cluster - fluid\t- deluxe@flex_host\n\n$ sacctmgr show resource withclusters\n      Name     Server     Type  Count LastConsumed Allocated ServerType    Cluster  Allowed                Flags \n---------- ---------- -------- ------ ------------ --------- ---------- ---------- -------- -------------------- \n    deluxe  flex_host  License    150            0        95     flexlm      fluid       25             Absolute \n    deluxe  flex_host  License    150            0        95     flexlm        pdf       70             Absolute \nThis can also be established as the default for all newly created licenses\nby adding AllResourcesAbsolute=yes to slurmdbd.conf (and restarting\nSlurmDBD to make the change take effect).Dynamic licenses\n\nStarting with Slurm 23.02, the LastConsumed field for remote licenses\nis designed to be periodically updated with the active use count from a license\nserver. An example script for FlexLM's lmstat command is provided below \u2014\nsimilar scripts can be easily constructed for other license management\nstacks.\n#!/bin/bash\n\nset -euxo pipefail\n\nLMSTAT=/opt/foobar/bin/lmstat\nLICENSE=foobar\n\nconsumed=$(${LMSTAT} | grep \"Users of ${LICENSE}\"|sed \"s/.*Total of \\([0-9]\\+\\) licenses in use)/\\1/\")\n\nsacctmgr -i update resource ${LICENSE} set lastconsumed=${consumed}\nWhen the LastConsumed value is changed through sacctmgr an update is\nautomatically pushed to the Slurm controllers. They will use this value\nto calculate a LastDeficit value \u2014 this value indicates how many\nlicenses that have \"gone missing\" from the cluster's perspective and will\nneed to be set aside temporarily.E.g., on this cluster 100 \"foobar\" licenses are available, and we are\nallocating access to 80 of them on the \"blackhole\" cluster:\n$ sacctmgr add resource foobar count=100 flags=absolute cluster=blackhole allowed=80\n Adding Resource(s)\n  foobar@slurmdb\n   Cluster - blackhole\t80\n Settings\n  Name           = foobar\n  Server         = slurmdb\n  Description    = foobar\n  Count          = 100\n  Flags          = Absolute\n  Type           = Unknown\nWould you like to commit changes? (You have 30 seconds to decide)\n(N/y): y\n$ scontrol show license\nLicenseName=foobar@slurmdb\n    Total=80 Used=0 Free=80 Reserved=0 Remote=yes\n    LastConsumed=0 LastDeficit=0 LastUpdate=2023-02-28T16:36:55\nNow, our cron job comes in and updates the LastConsumed value to 30, while\nthe cluster has yet to allocate any licenses to jobs:\n$ sacctmgr -i update resource foobar set lastconsumed=30\n Modified server resource ...\n  foobar@slurmdb\n  Cluster - blackhole\t- foobar@slurmdb\n$ scontrol show license\nLicenseName=foobar@slurmdb\n    Total=80 Used=0 Free=70 Reserved=0 Remote=yes\n    LastConsumed=30 LastDeficit=10 LastUpdate=2023-02-28T16:39:27\nNote that the cluster has now calculated a deficit of 10 licenses, and\nhas noticed that it should only schedule up to 70 licenses at the moment.\nThe cluster knows that up to 20 licenses are reserved for other clusters or\nexternal use at the moment. However, since LastConsumed was set to 30 this\nimplies an additional 10 licenses have \"gone rogue\" and their usage cannot\nbe accounted for. Thus the cluster must not assign those to any pending jobs,\nas it's likely that the job would fail to acquire the desired licenses.If a further update (likely driven through cron) now reduces the\nLastConsumed count to 10, the deficit is now considered to have disappeared,\nand the cluster will make all 80 assigned licenses available again:\n$ sacctmgr -i update resource foobar set lastconsumed=20\n Modified server resource ...\n  foobar@slurmdb\n  Cluster - blackhole\t- foobar@slurmdb\n$ scontrol show license\nLicenseName=foobar@slurmdb\n    Total=80 Used=0 Free=80 Reserved=0 Remote=yes\n    LastConsumed=20 LastDeficit=0 LastUpdate=2023-02-28T16:44:26\nLast modified 25 April 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/documentation.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": " Documentation",
                "content": "NOTE: This documentation is for Slurm version 24.05.\nDocumentation for older versions of Slurm are distributed with the source, or\nmay be found in the archive.\nAlso see Tutorials and\nPublications and Presentations.Slurm Users\n\nQuick Start User Guide\nCommand/option Summary (two pages)\nMan Pages\nRosetta Stone of Workload Managers\nJob Array Support\nHeterogeneous Job Support\nCPU Management User and Administrator Guide\nMPI and UPC Users Guide\nSupport for Multi-core/Multi-threaded Architectures\nMulti-Cluster Operation\nProfiling Using HDF5 User Guide\nJob Reason Codes\nJob State Codes\nJob Exit Codes\nResource Binding\nSpecific Systems\n\nIntel Knights Landing (KNL) User and Administrator Guide\n\n\nSlurm Administrators\n\nQuick Start Administrator Guide\nUpgrade Guide\nAccounting\nAdvanced Resource Reservation Guide\nAuthentication Plugins\nBurst Buffer Guide\nCgroups Guide\n\"Configless\" Slurm Operation\nConfiguration Tool (Full version)\nConfiguration Tool (Simplified version)\nContainers\nCPU Management User and Administrator Guide\nDynamic Nodes\nElasticsearch Guide\nJob Completion Kafka plugin Guide\njob_container/tmpfs - Job Specific Temporary File Management\nJSON Web Tokens Authentication\nFederated Scheduling Guide\nJob Containment (SSH Session Control) with pam_slurm_adopt\nKubernetes Guide\nLarge Cluster Administration Guide\nLicense Management\nMulti-Category Security (MCS) Guide\nName Service Caching Through NSS Slurm\nNetwork Configuration Guide\nOpenAPI Plugin Release Notes\nPower Saving Guide (power down idle nodes)\nProlog and Epilog Guide\nSlurm REST API\n\nQuick Start Guide\nAPI Reference\nAPI Implementation Details\nClient Guide\n\n\nSlurm SELinux Context Management\nTroubleshooting Guide\nUser Permissions\nWCKey Management\nWorkload Prioritization\n\nMultifactor Job Priority\nClassic Fairshare Algorithm\nDepth-Oblivious Fair-share Factor\nFair Tree Fairshare Algorithm\n\n\nSlurm Scheduling\n\nScheduling Configuration Guide\nConsumable Resources Guide\nCore Specialization\nGang Scheduling\nGeneric Resource (GRES) Scheduling\nHigh Throughput Computing Guide\nPreemption\nQuality of Service (QOS)\nResource Limits\nResource Reservation Guide\nSharing Consumable Resources\nTopology\nTrackable Resources (TRES)\n\n\nCloud\n\nCloud Scheduling Guide\nSlurm on Google Cloud Platform\nDeploying Slurm with ParallelCluster on Your AWS Cluster\nSlurm on Microsoft Azure and CycleCloud\n\n\nSlurm Developers\n\nContributor Agreement\nProgrammer Guide\nApplication Programmer Interface (API) Guide\nAdding Files or Plugins to Slurm\nDesign Information\n\nGeneric Resource (GRES) Design Guide\nJob Launch Design Guide\nSelect Plugin Design Guide\n\n\nPlugin Programmer Guide\nPlugin Interface Details\n\nCommand Line Filter Plugin Programmer Guide\nJob Submission Plugin Programmer Guide\nPrEp Plugin Programmer Guide\nSite Factor (Priority) Plugin Programmer Guide\n\n\nLast modified 22 August 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/accounting.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Accounting and Resource Limits",
                "content": "Contents\nOverview\nInfrastructure\n\nStorage Backup Host\n\nSlurm JobComp Configuration\nSlurm Accounting Configuration Before Build\nSlurm Accounting Configuration After Build\nSlurmDBD Configuration\n\nSlurmDBD Archive and Purge\n\nMySQL Configuration\nArchive Server\nTools\nDatabase Configuration\nCluster Options\nAccount Options\nUser Options\nLimit Enforcement\nModifying Entities\nRemoving Entities\nOverviewSlurm can be configured to collect accounting information for every\njob and job step executed.\nAccounting records can be written to a simple text file or a database.\nInformation is available about both currently executing jobs and\njobs which have already terminated.\nThe sacct command can report resource usage for running or terminated\njobs including individual tasks, which can be useful to detect load imbalance\nbetween the tasks.\nThe sstat command can be used to status only currently running jobs.\nIt also can give you valuable information about imbalance between tasks.\nThe sreport can be used to generate reports based upon all jobs\nexecuted in a particular time interval.There are three distinct plugin types associated with resource accounting.\nThe Slurm configuration parameters (in slurm.conf) associated with\nthese plugins include:\nAccountingStorageType controls how detailed job and job\nstep information is recorded. You can store this information in a\ntext file or into SlurmDBD.\nJobAcctGatherType is operating system dependent and\ncontrols what mechanism is used to collect accounting information.\nSupported values are jobacct_gather/linux,\njobacct_gather/cgroup and jobacct_gather/none\n(no information collected).\nJobCompType controls how job completion information is\nrecorded. This can be used to record basic job information such\nas job name, user name, allocated nodes, start time, completion\ntime, exit status, etc. If the preservation of only basic job\ninformation is required, this plugin should satisfy your needs\nwith minimal overhead. You can store this information in a text file,\nor MySQL or MariaDB database.\nThe use of sacct to view information about jobs\nis dependent upon AccountingStorageType\nbeing configured to collect and store that information.\nThe use of sreport is dependent upon some database being\nused to store that information.The use of sacct or sstat to view information about resource usage\n  within jobs is dependent upon both JobAcctGatherType and AccountingStorageType\n  being configured to collect and store that information.Storing the accounting information into text files is\nvery simple. Just configure the appropriate plugin (e.g.\nJobCompType=jobcomp/filetxt) and then specify the\npathname of the file (e.g. JobCompLoc=/var/log/slurm/job_completions).\nUse the logrotate or similar tool to prevent the\nlog files from getting too large.\nSend a SIGUSR2 signal to the slurmctld daemon\nafter moving the files, but before compressing them so\nthat new log files will be created.Storing the data directly into a database from Slurm may seem\nattractive, but it requires the availability of user name and\npassword data not only for the Slurm control daemon (slurmctld),\nbut also for user commands which need to access the data (sacct, sreport, and\nsacctmgr).\nMaking potentially sensitive information available to all users makes\ndatabase security more difficult to provide. Sending the data through\nan intermediate daemon can provide better security and performance\n(through caching data). SlurmDBD (Slurm Database Daemon) provides such services.\nSlurmDBD is written in C, multi-threaded, secure and fast.\nThe configuration required to use SlurmDBD will be described below.\nStoring information directly into a database would be similar.Note that SlurmDBD relies upon existing Slurm plugins\nfor authentication and Slurm SQL for database use, but the other Slurm\ncommands and daemons are not required on the host where\nSlurmDBD is installed. Install the slurm and slurm-slurmdbd\nRPMs on the server where SlurmDBD is to run.Note if you switch from using the MySQL plugin to use the SlurmDBD plugin\n  you must make sure the cluster has been added to the database.  The MySQL\n  plugin doesn't require this, but also will not hurt things if you have it\n  there when using the MySQL plugin. You can verify with\nsacctmgr list cluster\nIf the cluster isn't there, add it (where my cluster's name was\nsnowflake):\nsacctmgr add cluster snowflake\nFailure to do so will result in the slurmctld failing to talk to the slurmdbd\nafter the switch.  If you plan to upgrade to a new version of Slurm don't switch\nplugins at the same time or you may get unexpected results.  Do one then the\nother.If SlurmDBD is configured for use but not responding then slurmctld\nwill utilize an internal cache until SlurmDBD is returned to service.\nThe cached data is written by slurmctld to local storage upon shutdown\nand recovered at startup.\nIf SlurmDBD is not available when slurmctld starts, a cache of \nvalid bank accounts, user limits, etc. based upon their state when the \ndaemons were last communicating will be used. \nNote that SlurmDBD must be responding when slurmctld is first started\nsince no cache of this critical data will be available.\nJob and step accounting records generated by slurmctld will be \nwritten to a cache as needed and transferred to SlurmDBD when returned to \nservice. Note that if SlurmDBD is down long enough for the number of queued\nrecords to exceed the maximum queue size then messages will begin to be\ndropped.Infrastructure\n\nWith the SlurmDBD, we are able to collect data from multiple\nclusters in a single location.\nThis does impose some constraints on the user naming and IDs.\nAccounting is maintained by user name (not user ID), but a\ngiven user name should refer to the same person across all\nof the computers.\nAuthentication relies upon user ID numbers, so those must\nbe uniform across all computers communicating with each\nSlurmDBD, at least for users requiring authentication.\nIn particular, the configured SlurmUser must have the\nsame name and ID across all clusters.\nIf you plan to have administrators of user accounts, limits,\netc. they must also have consistent names and IDs across all\nclusters.\nIf you plan to restrict access to accounting records (e.g.\nonly permit a user to view records of his jobs), then all\nusers should have consistent names and IDs.NOTE: By default only lowercase usernames are supported, but you\ncan configure Parameters=PreserveCaseUser in your slurmdbd.conf to\nallow usernames with uppercase characters.The best way to ensure security of the data is by authenticating\ncommunications to the SlurmDBD and we recommend\nMUNGE for that purpose.\nIf you have one cluster managed by Slurm and execute the SlurmDBD\non that one cluster, the normal MUNGE configuration will suffice.\nOtherwise MUNGE should then be installed on all nodes of all\nSlurm managed clusters, plus the machine where SlurmDBD executes.\nYou then have a choice of either having a single MUNGE key for\nall of these computers or maintaining a unique key for each of the\nclusters plus a second key for communications between the clusters\nfor better security.\nMUNGE enhancements are planned to support two keys within a single\nconfiguration file, but presently two different daemons must be\nstarted with different configurations to support two different keys\n(create two key files and start the daemons with the\n--key-file option to locate the proper key plus the\n--socket option to specify distinct local domain sockets for each).\nThe pathname of local domain socket will be needed in the Slurm\nand SlurmDBD configuration files (slurm.conf and slurmdbd.conf\nrespectively, more details are provided below).Whether you use any authentication module or not you will need to have\na way for the SlurmDBD to get UIDs for users and/or admins.  If using\nMUNGE, it is ideal for your users to have the same id on all your\nclusters.  If this is the case you should have a combination of every cluster's\n/etc/passwd file on the database server to allow the DBD to resolve\nnames for authentication.  If using MUNGE and a user's name is not in\nthe passwd file the action will fail.  If not using MUNGE, you should\nadd anyone you want to be an administrator or operator to the passwd file.\nIf they plan on running sacctmgr or any of the accounting tools they\nshould have the same UID, or they will not authenticate correctly.  An\nLDAP server could also serve as a way to gather this information.Storage Backup Host\n\nA backup instance of slurmdbd can be configured by specifying\n\nAccountingStorageBackupHost in slurm.conf, as well as\nDbdBackupHost in\nslurmdbd.conf. The backup host should be on a different machine than the one\nhosting the primary instance of slurmdbd. Both instances of slurmdbd should\nhave access to the same database, share the same munge key(s), and have the\nsame users with the same UID/GIDs. The\nnetwork page has a visual representation\nof how this might look.Slurm JobComp Configuration\n\nPresently job completion is not supported with the SlurmDBD, but can be\nwritten directly to a database, script or flat file. If you are\nrunning with the accounting storage plugin, use of the job completion plugin\nis probably redundant. If you would like to configure this, some of the more\nimportant parameters include:\nJobCompHost:\nOnly needed if using a database. The name or address of the host where\nthe database server executes.\nJobCompLoc:\nOnly needed if using a flat file. Location of file to write the job\ncompletion data to.\nJobCompPass:\nOnly needed if using a database. Password for the user connecting to\nthe database. Since the password can not be securely maintained,\nstoring the information directly in a database is not recommended.\nJobCompPort:\nOnly needed if using a database. The network port that the database\naccepts communication on.\nJobCompType:\nType of jobcomp plugin set to \"jobcomp/mysql\" or \"jobcomp/filetxt\".\nJobCompUser:\nOnly needed if using a database. User name to connect to\nthe database with.\nJobCompParams:\nPass arbitrary text string to job completion plugin.\n\nSlurm Accounting Configuration Before Build\n\nYou can configure SlurmDBD to communicate with a database by using\nAccountingStorageType=accounting_storage/slurmdbd. This allows\nthe creation of user entities called \"associations\", which consist of the\ncluster, a user, account and optionally a partition.MySQL or MariaDB is the preferred database.\nNOTE: If you have an existing Slurm accounting database and\nplan to upgrade your database server to MariaDB 10.2.1 or later from an older\nversion of MariaDB or any version of MySQL, ensure you are running slurmdbd\n22.05.7 or later. These versions will gracefully handle changes to MariaDB\ndefault values that can cause problems for slurmdbd.\nTo enable this database support\none only needs to have the development package for the database they\nwish to use on the system. Slurm uses the InnoDB storage\nengine in MySQL to make rollback possible.  This must be available on your\nMySQL installation or rollback will not work.\n\nThe slurm configure\nscript uses mysql_config to find out the information it needs\nabout installed libraries and headers. You can specify where your\nmysql_config script is with the\n--with-mysql_conf=/path/to/mysql_config option when configuring your\nslurm build.\nOn a successful configure, output is something like this: \n\nchecking for mysql_config... /usr/bin/mysql_config\nMySQL test program built properly.\n\nNOTE: Before running the slurmdbd for the first time, review the\ncurrent setting for MySQL's innodb_buffer_pool_size.\nConsider setting this\nvalue large enough to handle the size of the database. Having this value\ntoo small can be problematic when converting large tables over to the new\ndatabase schema or when purging old records. We recommend assigning a\nsignificant portion of the system memory to this, keeping in mind the other\nresource requirements on the machine running MySQL/MariaDB, somewhere between\n5 and 50 percent of the available memory.\ninnodb_log_file_size Should also be increased from default to reduce\nunnecessary small writes to disk.  For MySQL 8.0.30+,\ninnodb_redo_log_capacity should be used instead.\nWhen using AccountingStoreFlags=job_env,job_script or older SQL servers,\nit is also important to check the value of max_allowed_packet.  When the\npacket size is too small and a large job script is used, the SQL server may\nreject the sql query as being too large.  This value should be at least 16MB,\nand must be larger than the value of max_script_size.\nSetting innodb_lock_wait_timeout to 900 is reccomended to allow some\npotentially extended queries to complete successfully.\n\nSee the following example:\n\n\nmysql> SHOW VARIABLES LIKE 'innodb_buffer_pool_size';\n+-------------------------+------------+\n| Variable_name           | Value      |\n+-------------------------+------------+\n| innodb_buffer_pool_size | 4294967296 |\n+-------------------------+------------+\n1 row in set (0.001 sec)\n\n\n$cat my.cnf\n...\n[mysqld]\ninnodb_buffer_pool_size=4096M\ninnodb_log_file_size=64M\ninnodb_lock_wait_timeout=900\nmax_allowed_packet=16M\n...\n\nAlso, in MySQL versions prior to 5.7 the default row format was set to\nCOMPACT which could cause some issues during an upgrade when creating\ntables. In more recent versions it was changed to DYNAMIC. The row\nformat of a table determines how its rows are physically stored in pages and\ndirectly affects the performance of queries and DML operations. In very specific\nsituations using a format other than DYNAMIC can lead to rows not fitting into\npages and MySQL can throw an error during the creation of the table because of\nthat. Therefore it is recommended to read carefully about the row format before\ncreating your database tables if you are not using DYNAMIC by default, and\nconsider setting that if your database version supports it. If the\nfollowing InnoDB error shows up during an upgrade, the table can then be\naltered (may take some time) to set the row format to DYNAMIC in order to\nallow the conversion to proceed:\n\n[Warning] InnoDB: Cannot add field ... in table ... because after adding it, the row size is Y which is greater than maximum allowed size (X) for a record on index leaf page.\n\nYou can see what the default row format is by showing the\ninnodb_default_row_format variable:\n\nmysql> SHOW VARIABLES LIKE 'innodb_default_row_format';\n+---------------------------+---------+\n| Variable_name             | Value   |\n+---------------------------+---------+\n| innodb_default_row_format | dynamic |\n+---------------------------+---------+\n1 row in set (0.001 sec)\n\nYou can also see how the tables are created by running the following command,\nwhere db_name is the name of your Slurm database (StorageLoc) set in\nyour slurmdbd.conf:\n\nmysql> SHOW TABLE STATUS IN db_name;\n\n\nSlurm Accounting Configuration After Build\n\n\nFor simplicity's sake, we are going to proceed under the assumption that you\nare running with the SlurmDBD. You can communicate with a storage plugin\ndirectly, but that offers minimal security. \nSeveral Slurm configuration parameters must be set to support\narchiving information in SlurmDBD. SlurmDBD has a separate configuration\nfile which is documented in a separate section.\nNote that you can write accounting information to SlurmDBD\nwhile job completion records are written to a text file or\nnot maintained at all.\nIf you don't set the configuration parameters that begin\nwith \"AccountingStorage\" then accounting information will not be\nreferenced or recorded.\n\nAccountingStorageEnforce:\nThis option contains a comma separated list of options you may want to\n enforce.  The valid options are any comma separated combination of\n\nassociations - This will prevent users from running jobs if\ntheir association is not in the database. This option will\nprevent users from accessing invalid accounts.\n\nlimits - This will enforce limits set on associations and qos'.\n  By setting this option, the\n  'associations' option is automatically set.  If a qos is used the\n  limits will be enforced, but 'qos' described below is still needed\n  if you want to enforce access to the qos.\n\nnojobs - This will make it so no job information is stored in\n  accounting.  By setting this 'nosteps' is also set.\n\nnosteps - This will make it so no step information is stored in\n  accounting. Both nojobs and nosteps could be helpful in an\n  environment where you want to use limits but don't really care about\n  utilization.\n\nqos - This will require all jobs to specify (either overtly or by\n  default) a valid qos (Quality of Service).  QOS values are defined for\n  each association in the database.  By setting this option, the\n  'associations' option is automatically set.  If you want QOS limits to be\n  enforced you need to use the 'limits' option.\n\nsafe - This will ensure a job will only be launched when using an\n  association or qos that has a TRES-minutes limit set if the job will be\n  able to run to completion. Without this option set, jobs will be\n  launched as long as their usage hasn't reached the TRES-minutes limit\n  which can lead to jobs being launched but then killed when the limit is\n  reached.\n  With the 'safe' option set, a job won't be killed due to limits,\n  even if the limits are changed after a job was started and the\n  association or qos violates the updated limits.\n  By setting this option, both the 'associations' option and the\n  'limits' option are set automatically.\n\nwckeys - This will prevent users from running jobs under a wckey\n  that they don't have access to.  By using this option, the\n  'associations' option is automatically set.  The 'TrackWCKey' option is also\n  set to true.\n\n\nNOTE: The association is a combination of cluster, account,\nuser names and optional partition name.\n\nWithout AccountingStorageEnforce being set (the default behavior)\njobs will be executed based upon policies configured in Slurm on each\ncluster.\n\nAccountingStorageExternalHost:\nA comma separated list of external slurmdbds (<host/ip>[:port][,...]) to\nregister with. If no port is given, the AccountingStoragePort will be\nused. This allows clusters registered with the external slurmdbd to communicate\nwith each other using the --cluster/-M client command options.\n\nThe cluster will add itself to the external slurmdbd if it doesn't exist.\nIf a non-external cluster already exists on the external slurmdbd, the\nslurmctld will ignore registering to the external slurmdbd.\n\nAccountingStorageHost: The name or address of the host where\nSlurmDBD executes\nAccountingStoragePass: The password used to gain access to the\ndatabase to store the accounting data. Only used for database type storage\nplugins, ignored otherwise. In the case of Slurm DBD (Database Daemon) with\nMUNGE authentication this can be configured to use a MUNGE daemon specifically\nconfigured to provide authentication between clusters while the default MUNGE\ndaemon provides authentication within a cluster. In that case,\nAccountingStoragePass should specify the named port to be used for\ncommunications with the alternate MUNGE daemon\n(e.g. \"/var/run/munge/global.socket.2\"). The default value is NULL.\nAccountingStoragePort:\nThe network port that SlurmDBD accepts communication on.\nAccountingStorageType:\nSet to \"accounting_storage/slurmdbd\".\nClusterName:\nSet to a unique name for each Slurm-managed cluster so that\naccounting records from each can be identified.\nTrackWCKey:\nBoolean.  If you want to track wckeys (Workload Characterization Key)\n  of users.  A Wckey is an orthogonal way to do accounting against possibly\n  unrelated accounts. When a job is run, use the --wckey option to specify a\n  value and accounting records will be collected by this wckey.\n\n\nSlurmDBD Configuration\n\n\nSlurmDBD requires its own configuration file called \"slurmdbd.conf\".\nThis file should be only on the computer where SlurmDBD executes and\nshould only be readable by the user which executes SlurmDBD (e.g. \"slurm\").\nThis file should be protected from unauthorized access since it\ncontains a database login name and password.\nSee slurmdbd.conf(5) for a more complete\ndescription of the configuration parameters.\nSome of the more important parameters include:\n\nAuthInfo:\nIf using SlurmDBD with a second MUNGE daemon, store the pathname of\nthe named socket used by MUNGE to provide enterprise-wide.\nOtherwise the default MUNGE daemon will be used.\nAuthType:\nDefine the authentication method for communications between Slurm\ncomponents. A value of \"auth/munge\" is recommended.\nDbdHost:\nThe name of the machine where the Slurm Database Daemon is executed.\nThis should be a node name without the full domain name (e.g. \"lx0001\").\nThis defaults to localhost but should be supplied to avoid a\nwarning message.\nDbdPort:\nThe port number that the Slurm Database Daemon (slurmdbd) listens\nto for work. The default value is SLURMDBD_PORT as established at system\nbuild time. If none is explicitly specified, it will be set to 6819.\nThis value must be equal to the AccountingStoragePort parameter in the\nslurm.conf file.\nLogFile:\nFully qualified pathname of a file into which the Slurm Database Daemon's\nlogs are written.\nThe default value is none (performs logging via syslog).\nPluginDir:\nIdentifies the places in which to look for Slurm plugins.\nThis is a colon-separated list of directories, like the PATH\nenvironment variable.\nThe default value is the prefix given at configure time + \"/lib/slurm\".\nSlurmUser:\nThe name of the user that the slurmdbd daemon executes as.\nThis user must exist on the machine executing the Slurm Database Daemon\nand have the same UID as the hosts on which slurmctld execute.\nFor security purposes, a user other than \"root\" is recommended.\nThe default value is \"root\". This name should also be the same SlurmUser\non all clusters reporting to the SlurmDBD.\nNOTE: If this user is different from the one set for slurmctld\nand is not root, it must be added to accounting with AdminLevel=Admin and\nslurmctld must be restarted.\n\nStorageHost:\nDefine the name of the host the database is running where we are going\nto store the data.\nIdeally this should be the host on which SlurmDBD executes, but could\nbe a different machine.\nStorageLoc:\nSpecifies the name of the database where accounting\nrecords are written. For databases the default database is\nslurm_acct_db. Note the name can not have a '/' in it or the\ndefault will be used.\nStoragePass:\nDefine the password used to gain access to the database to store\nthe job accounting data.\nStoragePort:\nDefine the port on which the database is listening.\nStorageType:\nDefine the accounting storage mechanism.\nThe only acceptable value at present is \"accounting_storage/mysql\".\nThe value \"accounting_storage/mysql\" indicates that accounting records\nshould be written to a MySQL or MariaDB database specified by the\nStorageLoc parameter.\nThis value must be specified.\nStorageUser:\nDefine the name of the user we are going to connect to the database\nwith to store the job accounting data.\n\nSlurmDBD Archive and Purge\n\n\nAs time goes on, the slurm database can grow large enough that it is hard to\nmanage. To maintain the database at a reasonable size, slurmdbd supports\narchiving and purging data based on its age. Purged data will be deleted from\nthe database, but you can choose to archive the data as it is being purged.\nArchived data will be placed in flat files that can later be loaded into a\nslurmdbd by sacctmgr.\nArchive and Purge options come in the form of Archive${*} and\nPurge${*}After. See slurmdbd.conf(5)\nfor more details on the available configuration parameters.\nThe units for the purge options are important. For example:\nPurgeJobsAfter=12months will purge jobs more than 12 months old at the\nbeginning of each month, while PurgeJobsAfter=365days will purge jobs\nolder than 365 days old at the beginning of each day. This distinction can be\nuseful for very active clusters, reducing the amount of data that needs to be\npurged at one time.\nMySQL Configuration\n\n\nNOTE: If you have an existing Slurm accounting database and\nplan to upgrade your database server to MariaDB 10.2.1 (or newer) from a\npre-10.2.1 version or from any version of MySQL, please contact SchedMD\nfor assistance.\nWhile Slurm will create the database tables automatically you will need to\nmake sure the StorageUser is given permissions in the MySQL or MariaDB database\nto do so.\nAs the mysql user grant privileges to that user using a\ncommand such as:\nGRANT ALL ON StorageLoc.* TO 'StorageUser'@'StorageHost';\n(The ticks are needed)\n(You need to be root to do this. Also in the info for password\nusage there is a line that starts with '->'. This a continuation\nprompt since the previous mysql statement did not end with a ';'. It\nassumes that you wish to input more info.)\nIf you want Slurm to create the database itself, and any future databases,\nyou can change your grant line to be *.* instead of StorageLoc.*\nLive example:\n\nmysql@snowflake:~$ mysql\nWelcome to the MySQL monitor.Commands end with ; or \\g.\nYour MySQL connection id is 538\nServer version: 5.0.51a-3ubuntu5.1 (Ubuntu)\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the buffer.\n\nmysql> create user 'slurm'@'localhost' identified by 'password';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> grant all on slurm_acct_db.* TO 'slurm'@'localhost';\nQuery OK, 0 rows affected (0.00 sec)\n\nYou may also need to do the same with the system name in order\nfor mysql to work correctly:\n\nmysql> grant all on slurm_acct_db.* TO 'slurm'@'system0';\nQuery OK, 0 rows affected (0.00 sec)\nwhere 'system0' is the localhost or database storage host.\n\nor with a password...\n\nmysql> grant all on slurm_acct_db.* TO 'slurm'@'localhost'\n    -> identified by 'some_pass' with grant option;\nQuery OK, 0 rows affected (0.00 sec)\n\nThe same is true in the case, you made to do the same with the \nsystem name:\n\nmysql> grant all on slurm_acct_db.* TO 'slurm'@'system0'\n    -> identified by 'some_pass' with grant option;\nwhere 'system0' is the localhost or database storage host.\n\nVerify you have InnoDB support\n\nmysql> SHOW ENGINES;\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\n| Engine             | Support | Comment                                                        | Transactions | XA   | Savepoints |\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\n| ...                |         |                                                                |              |      |            |\n| InnoDB             | DEFAULT | Supports transactions, row-level locking, and foreign keys     | YES          | YES  | YES        |\n| ...                |         |                                                                |              |      |            |\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\n\nThen create the database:\n\nmysql> create database slurm_acct_db;\n\nThis will grant user 'slurm' access to do what it needs to do on the local\nhost or the storage host system.  This must be done before the SlurmDBD will\nwork properly. After you grant permission to the user 'slurm' in mysql then\nyou can start SlurmDBD and the other Slurm daemons. You start SlurmDBD by\ntyping its pathname '/usr/sbin/slurmdbd' or '/etc/init.d/slurmdbd start'. You\ncan verify that SlurmDBD is running by typing 'ps aux | grep\nslurmdbd'.\n\nIf the SlurmDBD is not running you can\nuse the -v option when you start SlurmDBD to get more detailed\ninformation.  Starting the SlurmDBD in daemon mode with the '-D' option can\nalso help in debugging so you don't have to go to the log to find the\nproblem.\nArchive Server\nIf ongoing access to Archived/Purged data is required at your site, it is\npossible to create an archive instance of slurmdbd.  Data previously archived\nand purged from the production database can be loaded into the archive server,\nkeeping the production database at a manageable size while making sure old\nrecords are still accessible.\nThe archive instance of slurmdbd should not be able to communicate with the\nproduction server. Ideally they would have separate instances of MySQL/MariaDB\nthat they use to store their data. The Slurm controller (slurmctld) should\nnever communicate with the archive slurmdbd.\nWhen configuring an archive server, there are certain database entries\nthat need to match the production server in order for the archived information\nto show up correctly. In order to make sure the unique identifiers match,\nmysqldump should be used to export the Association, QOS and TRES information.\nThe command to export these tables should look like this, with the appropriate\nvalues substituted for <slurm_user>, <db_name>, and\n<cluster>:\n\nmysqldump -u <slurm_user> -p <db_name> <cluster>_assoc_table qos_table tres_table > slurm.sql\n\nWhile mysqldump should be used to transfer the information from these tables,\nit should not be used to transfer information that will be generated with the\nArchive/Purge process. If mysqldump is used to try to get the desired\ninformation, there will likely be a slight difference and when trying to load\narchive files later there will either be a gap in records or duplicate\nrecords that prevent the archive file from loading correctly.\nTools\nSlurm includes a few tools to let you work with accounting data;\nsacct, sacctmgr, and sreport.\nThese tools all get or set data through the SlurmDBD daemon.\n\nsacct is used to retrieve details, stored in the database, about\nrunning and completed jobs.\nsacctmgr is used to manage entities in the database. These include\nclusters, accounts, user associations, QOSs, etc.\nsreport is used to generate various reports on usage collected over a\ngiven time period.\n\nSee the man pages for each command for more information.\nWhile sreport provides the ability to quickly generate reports with some\nof the most commonly requested information, sites frequently want additional\ncontrol over how the information is displayed. There are some third-party tools\nthat can assist in generating dashboards with graphs of relevant information\nabout your cluster. These are not maintained or supported by SchedMD, but these\nutilities have been useful for some sites:\n\n\nGrafana: Allows creation of dashboards with various graphs using data\ncollected by Prometheus or InfluxDB.\nInfluxDB:\nIncludes an exporter tool that collects performance metrics from Slurm.\nPrometheus\n: Includes an exporter tool that collects performance metrics from\nSlurm.\n\nDatabase Configuration\n\n\nAccounting records are maintained based upon what we refer\nto as an Association,\nwhich consists of four elements: cluster, account, user names and\nan optional partition name. Use the sacctmgr\ncommand to create and manage these records.\nNOTE: There is an order to set up accounting associations.\nYou must define clusters before you add accounts and you must add accounts\nbefore you can add users.\nFor example, to add a cluster named \"snowflake\" to the database\nexecute this line\n(NOTE: as of 20.02, slurmctld will add the cluster to the database upon\nstart if it doesn't exist. Associations still need to be created after\naddition):\n\n\nsacctmgr add cluster snowflake\n\nAdd accounts \"none\" and \"test\" to cluster \"snowflake\" with an execute\nline of this sort:\n\nsacctmgr add account none,test Cluster=snowflake \\\n  Description=\"none\" Organization=\"none\"\n\nIf you have more clusters you want to add these accounts, to you\ncan either not specify a cluster, which will add the accounts to all\nclusters in the system, or comma separate the cluster names you want\nto add to in the cluster option.\nNote that multiple accounts can be added at the same time\nby comma separating the names.\nA description of the account and the organization to which\nit belongs must be specified.\nThese terms can be used later to generate accounting reports.\nAccounts may be arranged in a hierarchical fashion. For example, accounts\nchemistry and physics may be children of the account science.\nThe hierarchy may have an arbitrary depth.\nJust specify the parent='' option in the add account line to construct\nthe hierarchy.\nFor the example above execute\n\nsacctmgr add account science \\\n Description=\"science accounts\" Organization=science\nsacctmgr add account chemistry,physics parent=science \\\n Description=\"physical sciences\" Organization=science\n\nAdd users to accounts using similar syntax.\nFor example, to permit user da to execute jobs on all clusters\nwith a default account of test execute:\n\nsacctmgr add user brian Account=physics\nsacctmgr add user da DefaultAccount=test\n\nIf AccountingStorageEnforce=associations is configured in\nthe slurm.conf of the cluster snowflake then user da would be\nallowed to run in account test and any other accounts added\nin the future.\nAny attempt to use other accounts will result in the job being\naborted.\nAccount test will be the default if he doesn't specify one in\nthe job submission command.\nAssociations can also be created that are tied to specific partitions.\nWhen using the \"add user\" command of sacctmgr you can include the\nPartition=<PartitionName> option to create an association that\nis unique to other associations with the same Account and User.\nCluster Options\n\n\nWhen either adding or modifying a cluster, these are the options\navailable with sacctmgr:\n\nName= Cluster name\n\nAccount Options\n\n\nWhen either adding or modifying an account, the following sacctmgr\noptions are available:\n\nCluster= Only add this account to these clusters.\nThe account is added to all defined clusters by default.\nDescription= Description of the account. (Default is\n  account name)\nName= Name of account. Note the name must be unique and can not\nrepresent different bank accounts at different points in the account\nhierarchy\nOrganization=Organization of the account. (Default is\n  parent account unless parent account is root then organization is\n  set to the account name.)\nParent= Make this account a child of this other account\n  (already added).\n\nUser Options\n\n\nWhen either adding or modifying a user, the following sacctmgr\noptions are available:\n\n\nAccount= Account(s) to add user to\nAdminLevel= This field is used to allow a user to add accounting\nprivileges to this user. Valid options are\n\nNone\nOperator: can add, modify, and remove any database object (user,\naccount, etc), and add other operators\nOn a SlurmDBD served slurmctld these users can\n\nView information that is blocked to regular uses by a PrivateData\n  flag\nCreate/Alter/Delete Reservations\n\n\nAdmin: These users have the same level of privileges as an\noperator in the database.  They can also alter anything on a served\nslurmctld as if they were the slurm user or root.\n\nCluster= Only add to accounts on these clusters (default is all clusters)\nDefaultAccount= Default account for the user, used when no account\nis specified when a job is submitted. (Required on creation)\nDefaultWCKey= Default wckey for the user, used when no wckey\nis specified when a job is submitted. (Only used when tracking wckeys.)\nName= User name\nNewName= Use to rename a user in the accounting database\nPartition= Name of Slurm partition this association applies to\n\nLimit Enforcement\n\n\nVarious limits and limit enforcement are described in\n  the Resource Limits web page.\nTo enable any limit enforcement you must at least have\nAccountingStorageEnforce=limits in your slurm.conf.\nOtherwise, even if you have limits set, they will not be enforced.\nOther options for AccountingStorageEnforce and the explanation for\neach are found on the Resource\nLimits document.\nModifying Entities\n\n\nWhen modifying entities, you can specify many different options in\nSQL-like fashion, using key words like where and set.\nA typical execute line has the following form:\n\nsacctmgr modify <entity> set <options> where <options>\n\nFor example:\n\nsacctmgr modify user set default=none where default=test\n\nwill change all users with a default account of \"test\" to account \"none\".\nOnce an entity has been added, modified or removed, the change is\nsent to the appropriate Slurm daemons and will be available for use\ninstantly.\nRemoving Entities\n\n\nRemoving entities using an execute line similar to the modify example above,\nbut without the set options.\nFor example, remove all users with a default account \"test\" using the following\nexecute line:\n\nsacctmgr remove user where default=test\n\nwill remove all user records where the default account is \"test\".\n\nsacctmgr remove user brian where account=physics\n\nwill remove user \"brian\" from account \"physics\". If user \"brian\" has\naccess to other accounts, those user records will remain.\nNote: In most cases, removed entities are preserved in the slurm database,\nbut flagged as deleted.\nIf an entity has existed for less than 1 day, the entity will be removed\ncompletely. This is meant to clean up after typographical errors.\nRemoving user associations or accounts, however, will cause slurmctld to lose\ntrack of usage data for that user/account.\nLast modified 17 July 2024\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "\nSlurm Accounting Configuration After Build\n\n",
                "content": "For simplicity's sake, we are going to proceed under the assumption that you\nare running with the SlurmDBD. You can communicate with a storage plugin\ndirectly, but that offers minimal security. Several Slurm configuration parameters must be set to support\narchiving information in SlurmDBD. SlurmDBD has a separate configuration\nfile which is documented in a separate section.\nNote that you can write accounting information to SlurmDBD\nwhile job completion records are written to a text file or\nnot maintained at all.\nIf you don't set the configuration parameters that begin\nwith \"AccountingStorage\" then accounting information will not be\nreferenced or recorded.\nAccountingStorageEnforce:\nThis option contains a comma separated list of options you may want to\n enforce.  The valid options are any comma separated combination of\n\nassociations - This will prevent users from running jobs if\ntheir association is not in the database. This option will\nprevent users from accessing invalid accounts.\n\nlimits - This will enforce limits set on associations and qos'.\n  By setting this option, the\n  'associations' option is automatically set.  If a qos is used the\n  limits will be enforced, but 'qos' described below is still needed\n  if you want to enforce access to the qos.\n\nnojobs - This will make it so no job information is stored in\n  accounting.  By setting this 'nosteps' is also set.\n\nnosteps - This will make it so no step information is stored in\n  accounting. Both nojobs and nosteps could be helpful in an\n  environment where you want to use limits but don't really care about\n  utilization.\n\nqos - This will require all jobs to specify (either overtly or by\n  default) a valid qos (Quality of Service).  QOS values are defined for\n  each association in the database.  By setting this option, the\n  'associations' option is automatically set.  If you want QOS limits to be\n  enforced you need to use the 'limits' option.\n\nsafe - This will ensure a job will only be launched when using an\n  association or qos that has a TRES-minutes limit set if the job will be\n  able to run to completion. Without this option set, jobs will be\n  launched as long as their usage hasn't reached the TRES-minutes limit\n  which can lead to jobs being launched but then killed when the limit is\n  reached.\n  With the 'safe' option set, a job won't be killed due to limits,\n  even if the limits are changed after a job was started and the\n  association or qos violates the updated limits.\n  By setting this option, both the 'associations' option and the\n  'limits' option are set automatically.\n\nwckeys - This will prevent users from running jobs under a wckey\n  that they don't have access to.  By using this option, the\n  'associations' option is automatically set.  The 'TrackWCKey' option is also\n  set to true.\n\n\nNOTE: The association is a combination of cluster, account,\nuser names and optional partition name.\n\nWithout AccountingStorageEnforce being set (the default behavior)\njobs will be executed based upon policies configured in Slurm on each\ncluster.\n\nAccountingStorageExternalHost:\nA comma separated list of external slurmdbds (<host/ip>[:port][,...]) to\nregister with. If no port is given, the AccountingStoragePort will be\nused. This allows clusters registered with the external slurmdbd to communicate\nwith each other using the --cluster/-M client command options.\n\nThe cluster will add itself to the external slurmdbd if it doesn't exist.\nIf a non-external cluster already exists on the external slurmdbd, the\nslurmctld will ignore registering to the external slurmdbd.\n\nAccountingStorageHost: The name or address of the host where\nSlurmDBD executes\nAccountingStoragePass: The password used to gain access to the\ndatabase to store the accounting data. Only used for database type storage\nplugins, ignored otherwise. In the case of Slurm DBD (Database Daemon) with\nMUNGE authentication this can be configured to use a MUNGE daemon specifically\nconfigured to provide authentication between clusters while the default MUNGE\ndaemon provides authentication within a cluster. In that case,\nAccountingStoragePass should specify the named port to be used for\ncommunications with the alternate MUNGE daemon\n(e.g. \"/var/run/munge/global.socket.2\"). The default value is NULL.\nAccountingStoragePort:\nThe network port that SlurmDBD accepts communication on.\nAccountingStorageType:\nSet to \"accounting_storage/slurmdbd\".\nClusterName:\nSet to a unique name for each Slurm-managed cluster so that\naccounting records from each can be identified.\nTrackWCKey:\nBoolean.  If you want to track wckeys (Workload Characterization Key)\n  of users.  A Wckey is an orthogonal way to do accounting against possibly\n  unrelated accounts. When a job is run, use the --wckey option to specify a\n  value and accounting records will be collected by this wckey.\n\nSlurmDBD Configuration\n\nSlurmDBD requires its own configuration file called \"slurmdbd.conf\".\nThis file should be only on the computer where SlurmDBD executes and\nshould only be readable by the user which executes SlurmDBD (e.g. \"slurm\").\nThis file should be protected from unauthorized access since it\ncontains a database login name and password.\nSee slurmdbd.conf(5) for a more complete\ndescription of the configuration parameters.\nSome of the more important parameters include:\nAuthInfo:\nIf using SlurmDBD with a second MUNGE daemon, store the pathname of\nthe named socket used by MUNGE to provide enterprise-wide.\nOtherwise the default MUNGE daemon will be used.\nAuthType:\nDefine the authentication method for communications between Slurm\ncomponents. A value of \"auth/munge\" is recommended.\nDbdHost:\nThe name of the machine where the Slurm Database Daemon is executed.\nThis should be a node name without the full domain name (e.g. \"lx0001\").\nThis defaults to localhost but should be supplied to avoid a\nwarning message.\nDbdPort:\nThe port number that the Slurm Database Daemon (slurmdbd) listens\nto for work. The default value is SLURMDBD_PORT as established at system\nbuild time. If none is explicitly specified, it will be set to 6819.\nThis value must be equal to the AccountingStoragePort parameter in the\nslurm.conf file.\nLogFile:\nFully qualified pathname of a file into which the Slurm Database Daemon's\nlogs are written.\nThe default value is none (performs logging via syslog).\nPluginDir:\nIdentifies the places in which to look for Slurm plugins.\nThis is a colon-separated list of directories, like the PATH\nenvironment variable.\nThe default value is the prefix given at configure time + \"/lib/slurm\".\nSlurmUser:\nThe name of the user that the slurmdbd daemon executes as.\nThis user must exist on the machine executing the Slurm Database Daemon\nand have the same UID as the hosts on which slurmctld execute.\nFor security purposes, a user other than \"root\" is recommended.\nThe default value is \"root\". This name should also be the same SlurmUser\non all clusters reporting to the SlurmDBD.\nNOTE: If this user is different from the one set for slurmctld\nand is not root, it must be added to accounting with AdminLevel=Admin and\nslurmctld must be restarted.\n\nStorageHost:\nDefine the name of the host the database is running where we are going\nto store the data.\nIdeally this should be the host on which SlurmDBD executes, but could\nbe a different machine.\nStorageLoc:\nSpecifies the name of the database where accounting\nrecords are written. For databases the default database is\nslurm_acct_db. Note the name can not have a '/' in it or the\ndefault will be used.\nStoragePass:\nDefine the password used to gain access to the database to store\nthe job accounting data.\nStoragePort:\nDefine the port on which the database is listening.\nStorageType:\nDefine the accounting storage mechanism.\nThe only acceptable value at present is \"accounting_storage/mysql\".\nThe value \"accounting_storage/mysql\" indicates that accounting records\nshould be written to a MySQL or MariaDB database specified by the\nStorageLoc parameter.\nThis value must be specified.\nStorageUser:\nDefine the name of the user we are going to connect to the database\nwith to store the job accounting data.\nSlurmDBD Archive and Purge\n\nAs time goes on, the slurm database can grow large enough that it is hard to\nmanage. To maintain the database at a reasonable size, slurmdbd supports\narchiving and purging data based on its age. Purged data will be deleted from\nthe database, but you can choose to archive the data as it is being purged.\nArchived data will be placed in flat files that can later be loaded into a\nslurmdbd by sacctmgr.Archive and Purge options come in the form of Archive${*} and\nPurge${*}After. See slurmdbd.conf(5)\nfor more details on the available configuration parameters.The units for the purge options are important. For example:\nPurgeJobsAfter=12months will purge jobs more than 12 months old at the\nbeginning of each month, while PurgeJobsAfter=365days will purge jobs\nolder than 365 days old at the beginning of each day. This distinction can be\nuseful for very active clusters, reducing the amount of data that needs to be\npurged at one time.MySQL Configuration\n\nNOTE: If you have an existing Slurm accounting database and\nplan to upgrade your database server to MariaDB 10.2.1 (or newer) from a\npre-10.2.1 version or from any version of MySQL, please contact SchedMD\nfor assistance.While Slurm will create the database tables automatically you will need to\nmake sure the StorageUser is given permissions in the MySQL or MariaDB database\nto do so.\nAs the mysql user grant privileges to that user using a\ncommand such as:GRANT ALL ON StorageLoc.* TO 'StorageUser'@'StorageHost';\n(The ticks are needed)(You need to be root to do this. Also in the info for password\nusage there is a line that starts with '->'. This a continuation\nprompt since the previous mysql statement did not end with a ';'. It\nassumes that you wish to input more info.)If you want Slurm to create the database itself, and any future databases,\nyou can change your grant line to be *.* instead of StorageLoc.*Live example:\nmysql@snowflake:~$ mysql\nWelcome to the MySQL monitor.Commands end with ; or \\g.\nYour MySQL connection id is 538\nServer version: 5.0.51a-3ubuntu5.1 (Ubuntu)\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the buffer.\n\nmysql> create user 'slurm'@'localhost' identified by 'password';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> grant all on slurm_acct_db.* TO 'slurm'@'localhost';\nQuery OK, 0 rows affected (0.00 sec)\nYou may also need to do the same with the system name in order\nfor mysql to work correctly:\nmysql> grant all on slurm_acct_db.* TO 'slurm'@'system0';\nQuery OK, 0 rows affected (0.00 sec)\nwhere 'system0' is the localhost or database storage host.\nor with a password...\nmysql> grant all on slurm_acct_db.* TO 'slurm'@'localhost'\n    -> identified by 'some_pass' with grant option;\nQuery OK, 0 rows affected (0.00 sec)\nThe same is true in the case, you made to do the same with the \nsystem name:\nmysql> grant all on slurm_acct_db.* TO 'slurm'@'system0'\n    -> identified by 'some_pass' with grant option;\nwhere 'system0' is the localhost or database storage host.\nVerify you have InnoDB support\nmysql> SHOW ENGINES;\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\n| Engine             | Support | Comment                                                        | Transactions | XA   | Savepoints |\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\n| ...                |         |                                                                |              |      |            |\n| InnoDB             | DEFAULT | Supports transactions, row-level locking, and foreign keys     | YES          | YES  | YES        |\n| ...                |         |                                                                |              |      |            |\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\nThen create the database:\nmysql> create database slurm_acct_db;\nThis will grant user 'slurm' access to do what it needs to do on the local\nhost or the storage host system.  This must be done before the SlurmDBD will\nwork properly. After you grant permission to the user 'slurm' in mysql then\nyou can start SlurmDBD and the other Slurm daemons. You start SlurmDBD by\ntyping its pathname '/usr/sbin/slurmdbd' or '/etc/init.d/slurmdbd start'. You\ncan verify that SlurmDBD is running by typing 'ps aux | grep\nslurmdbd'.\n\nIf the SlurmDBD is not running you can\nuse the -v option when you start SlurmDBD to get more detailed\ninformation.  Starting the SlurmDBD in daemon mode with the '-D' option can\nalso help in debugging so you don't have to go to the log to find the\nproblem.\nArchive Server\nIf ongoing access to Archived/Purged data is required at your site, it is\npossible to create an archive instance of slurmdbd.  Data previously archived\nand purged from the production database can be loaded into the archive server,\nkeeping the production database at a manageable size while making sure old\nrecords are still accessible.\nThe archive instance of slurmdbd should not be able to communicate with the\nproduction server. Ideally they would have separate instances of MySQL/MariaDB\nthat they use to store their data. The Slurm controller (slurmctld) should\nnever communicate with the archive slurmdbd.\nWhen configuring an archive server, there are certain database entries\nthat need to match the production server in order for the archived information\nto show up correctly. In order to make sure the unique identifiers match,\nmysqldump should be used to export the Association, QOS and TRES information.\nThe command to export these tables should look like this, with the appropriate\nvalues substituted for <slurm_user>, <db_name>, and\n<cluster>:\n\nmysqldump -u <slurm_user> -p <db_name> <cluster>_assoc_table qos_table tres_table > slurm.sql\n\nWhile mysqldump should be used to transfer the information from these tables,\nit should not be used to transfer information that will be generated with the\nArchive/Purge process. If mysqldump is used to try to get the desired\ninformation, there will likely be a slight difference and when trying to load\narchive files later there will either be a gap in records or duplicate\nrecords that prevent the archive file from loading correctly.\nTools\nSlurm includes a few tools to let you work with accounting data;\nsacct, sacctmgr, and sreport.\nThese tools all get or set data through the SlurmDBD daemon.\n\nsacct is used to retrieve details, stored in the database, about\nrunning and completed jobs.\nsacctmgr is used to manage entities in the database. These include\nclusters, accounts, user associations, QOSs, etc.\nsreport is used to generate various reports on usage collected over a\ngiven time period.\n\nSee the man pages for each command for more information.\nWhile sreport provides the ability to quickly generate reports with some\nof the most commonly requested information, sites frequently want additional\ncontrol over how the information is displayed. There are some third-party tools\nthat can assist in generating dashboards with graphs of relevant information\nabout your cluster. These are not maintained or supported by SchedMD, but these\nutilities have been useful for some sites:\n\n\nGrafana: Allows creation of dashboards with various graphs using data\ncollected by Prometheus or InfluxDB.\nInfluxDB:\nIncludes an exporter tool that collects performance metrics from Slurm.\nPrometheus\n: Includes an exporter tool that collects performance metrics from\nSlurm.\n\nDatabase Configuration\n\n\nAccounting records are maintained based upon what we refer\nto as an Association,\nwhich consists of four elements: cluster, account, user names and\nan optional partition name. Use the sacctmgr\ncommand to create and manage these records.\nNOTE: There is an order to set up accounting associations.\nYou must define clusters before you add accounts and you must add accounts\nbefore you can add users.\nFor example, to add a cluster named \"snowflake\" to the database\nexecute this line\n(NOTE: as of 20.02, slurmctld will add the cluster to the database upon\nstart if it doesn't exist. Associations still need to be created after\naddition):\n\n\nsacctmgr add cluster snowflake\n\nAdd accounts \"none\" and \"test\" to cluster \"snowflake\" with an execute\nline of this sort:\n\nsacctmgr add account none,test Cluster=snowflake \\\n  Description=\"none\" Organization=\"none\"\n\nIf you have more clusters you want to add these accounts, to you\ncan either not specify a cluster, which will add the accounts to all\nclusters in the system, or comma separate the cluster names you want\nto add to in the cluster option.\nNote that multiple accounts can be added at the same time\nby comma separating the names.\nA description of the account and the organization to which\nit belongs must be specified.\nThese terms can be used later to generate accounting reports.\nAccounts may be arranged in a hierarchical fashion. For example, accounts\nchemistry and physics may be children of the account science.\nThe hierarchy may have an arbitrary depth.\nJust specify the parent='' option in the add account line to construct\nthe hierarchy.\nFor the example above execute\n\nsacctmgr add account science \\\n Description=\"science accounts\" Organization=science\nsacctmgr add account chemistry,physics parent=science \\\n Description=\"physical sciences\" Organization=science\n\nAdd users to accounts using similar syntax.\nFor example, to permit user da to execute jobs on all clusters\nwith a default account of test execute:\n\nsacctmgr add user brian Account=physics\nsacctmgr add user da DefaultAccount=test\n\nIf AccountingStorageEnforce=associations is configured in\nthe slurm.conf of the cluster snowflake then user da would be\nallowed to run in account test and any other accounts added\nin the future.\nAny attempt to use other accounts will result in the job being\naborted.\nAccount test will be the default if he doesn't specify one in\nthe job submission command.\nAssociations can also be created that are tied to specific partitions.\nWhen using the \"add user\" command of sacctmgr you can include the\nPartition=<PartitionName> option to create an association that\nis unique to other associations with the same Account and User.\nCluster Options\n\n\nWhen either adding or modifying a cluster, these are the options\navailable with sacctmgr:\n\nName= Cluster name\n\nAccount Options\n\n\nWhen either adding or modifying an account, the following sacctmgr\noptions are available:\n\nCluster= Only add this account to these clusters.\nThe account is added to all defined clusters by default.\nDescription= Description of the account. (Default is\n  account name)\nName= Name of account. Note the name must be unique and can not\nrepresent different bank accounts at different points in the account\nhierarchy\nOrganization=Organization of the account. (Default is\n  parent account unless parent account is root then organization is\n  set to the account name.)\nParent= Make this account a child of this other account\n  (already added).\n\nUser Options\n\n\nWhen either adding or modifying a user, the following sacctmgr\noptions are available:\n\n\nAccount= Account(s) to add user to\nAdminLevel= This field is used to allow a user to add accounting\nprivileges to this user. Valid options are\n\nNone\nOperator: can add, modify, and remove any database object (user,\naccount, etc), and add other operators\nOn a SlurmDBD served slurmctld these users can\n\nView information that is blocked to regular uses by a PrivateData\n  flag\nCreate/Alter/Delete Reservations\n\n\nAdmin: These users have the same level of privileges as an\noperator in the database.  They can also alter anything on a served\nslurmctld as if they were the slurm user or root.\n\nCluster= Only add to accounts on these clusters (default is all clusters)\nDefaultAccount= Default account for the user, used when no account\nis specified when a job is submitted. (Required on creation)\nDefaultWCKey= Default wckey for the user, used when no wckey\nis specified when a job is submitted. (Only used when tracking wckeys.)\nName= User name\nNewName= Use to rename a user in the accounting database\nPartition= Name of Slurm partition this association applies to\n\nLimit Enforcement\n\n\nVarious limits and limit enforcement are described in\n  the Resource Limits web page.\nTo enable any limit enforcement you must at least have\nAccountingStorageEnforce=limits in your slurm.conf.\nOtherwise, even if you have limits set, they will not be enforced.\nOther options for AccountingStorageEnforce and the explanation for\neach are found on the Resource\nLimits document.\nModifying Entities\n\n\nWhen modifying entities, you can specify many different options in\nSQL-like fashion, using key words like where and set.\nA typical execute line has the following form:\n\nsacctmgr modify <entity> set <options> where <options>\n\nFor example:\n\nsacctmgr modify user set default=none where default=test\n\nwill change all users with a default account of \"test\" to account \"none\".\nOnce an entity has been added, modified or removed, the change is\nsent to the appropriate Slurm daemons and will be available for use\ninstantly.\nRemoving Entities\n\n\nRemoving entities using an execute line similar to the modify example above,\nbut without the set options.\nFor example, remove all users with a default account \"test\" using the following\nexecute line:\n\nsacctmgr remove user where default=test\n\nwill remove all user records where the default account is \"test\".\n\nsacctmgr remove user brian where account=physics\n\nwill remove user \"brian\" from account \"physics\". If user \"brian\" has\naccess to other accounts, those user records will remain.\nNote: In most cases, removed entities are preserved in the slurm database,\nbut flagged as deleted.\nIf an entity has existed for less than 1 day, the entity will be removed\ncompletely. This is meant to clean up after typographical errors.\nRemoving user associations or accounts, however, will cause slurmctld to lose\ntrack of usage data for that user/account.\nLast modified 17 July 2024\n"
            },
            {
                "title": "Archive Server",
                "content": "If ongoing access to Archived/Purged data is required at your site, it is\npossible to create an archive instance of slurmdbd.  Data previously archived\nand purged from the production database can be loaded into the archive server,\nkeeping the production database at a manageable size while making sure old\nrecords are still accessible.The archive instance of slurmdbd should not be able to communicate with the\nproduction server. Ideally they would have separate instances of MySQL/MariaDB\nthat they use to store their data. The Slurm controller (slurmctld) should\nnever communicate with the archive slurmdbd.When configuring an archive server, there are certain database entries\nthat need to match the production server in order for the archived information\nto show up correctly. In order to make sure the unique identifiers match,\nmysqldump should be used to export the Association, QOS and TRES information.\nThe command to export these tables should look like this, with the appropriate\nvalues substituted for <slurm_user>, <db_name>, and\n<cluster>:\nmysqldump -u <slurm_user> -p <db_name> <cluster>_assoc_table qos_table tres_table > slurm.sql\nWhile mysqldump should be used to transfer the information from these tables,\nit should not be used to transfer information that will be generated with the\nArchive/Purge process. If mysqldump is used to try to get the desired\ninformation, there will likely be a slight difference and when trying to load\narchive files later there will either be a gap in records or duplicate\nrecords that prevent the archive file from loading correctly.ToolsSlurm includes a few tools to let you work with accounting data;\nsacct, sacctmgr, and sreport.\nThese tools all get or set data through the SlurmDBD daemon.\nsacct is used to retrieve details, stored in the database, about\nrunning and completed jobs.\nsacctmgr is used to manage entities in the database. These include\nclusters, accounts, user associations, QOSs, etc.\nsreport is used to generate various reports on usage collected over a\ngiven time period.\nSee the man pages for each command for more information.While sreport provides the ability to quickly generate reports with some\nof the most commonly requested information, sites frequently want additional\ncontrol over how the information is displayed. There are some third-party tools\nthat can assist in generating dashboards with graphs of relevant information\nabout your cluster. These are not maintained or supported by SchedMD, but these\nutilities have been useful for some sites:\n\nGrafana: Allows creation of dashboards with various graphs using data\ncollected by Prometheus or InfluxDB.\nInfluxDB:\nIncludes an exporter tool that collects performance metrics from Slurm.\nPrometheus\n: Includes an exporter tool that collects performance metrics from\nSlurm.\nDatabase Configuration\n\nAccounting records are maintained based upon what we refer\nto as an Association,\nwhich consists of four elements: cluster, account, user names and\nan optional partition name. Use the sacctmgr\ncommand to create and manage these records.NOTE: There is an order to set up accounting associations.\nYou must define clusters before you add accounts and you must add accounts\nbefore you can add users.For example, to add a cluster named \"snowflake\" to the database\nexecute this line\n(NOTE: as of 20.02, slurmctld will add the cluster to the database upon\nstart if it doesn't exist. Associations still need to be created after\naddition):\n\nsacctmgr add cluster snowflake\nAdd accounts \"none\" and \"test\" to cluster \"snowflake\" with an execute\nline of this sort:\nsacctmgr add account none,test Cluster=snowflake \\\n  Description=\"none\" Organization=\"none\"\nIf you have more clusters you want to add these accounts, to you\ncan either not specify a cluster, which will add the accounts to all\nclusters in the system, or comma separate the cluster names you want\nto add to in the cluster option.\nNote that multiple accounts can be added at the same time\nby comma separating the names.\nA description of the account and the organization to which\nit belongs must be specified.\nThese terms can be used later to generate accounting reports.\nAccounts may be arranged in a hierarchical fashion. For example, accounts\nchemistry and physics may be children of the account science.\nThe hierarchy may have an arbitrary depth.\nJust specify the parent='' option in the add account line to construct\nthe hierarchy.\nFor the example above execute\nsacctmgr add account science \\\n Description=\"science accounts\" Organization=science\nsacctmgr add account chemistry,physics parent=science \\\n Description=\"physical sciences\" Organization=science\nAdd users to accounts using similar syntax.\nFor example, to permit user da to execute jobs on all clusters\nwith a default account of test execute:\nsacctmgr add user brian Account=physics\nsacctmgr add user da DefaultAccount=test\nIf AccountingStorageEnforce=associations is configured in\nthe slurm.conf of the cluster snowflake then user da would be\nallowed to run in account test and any other accounts added\nin the future.\nAny attempt to use other accounts will result in the job being\naborted.\nAccount test will be the default if he doesn't specify one in\nthe job submission command.Associations can also be created that are tied to specific partitions.\nWhen using the \"add user\" command of sacctmgr you can include the\nPartition=<PartitionName> option to create an association that\nis unique to other associations with the same Account and User.Cluster Options\n\nWhen either adding or modifying a cluster, these are the options\navailable with sacctmgr:\n\nName= Cluster name\n\nAccount Options\n\n\nWhen either adding or modifying an account, the following sacctmgr\noptions are available:\n\nCluster= Only add this account to these clusters.\nThe account is added to all defined clusters by default.\nDescription= Description of the account. (Default is\n  account name)\nName= Name of account. Note the name must be unique and can not\nrepresent different bank accounts at different points in the account\nhierarchy\nOrganization=Organization of the account. (Default is\n  parent account unless parent account is root then organization is\n  set to the account name.)\nParent= Make this account a child of this other account\n  (already added).\n\nUser Options\n\n\nWhen either adding or modifying a user, the following sacctmgr\noptions are available:\n\n\nAccount= Account(s) to add user to\nAdminLevel= This field is used to allow a user to add accounting\nprivileges to this user. Valid options are\n\nNone\nOperator: can add, modify, and remove any database object (user,\naccount, etc), and add other operators\nOn a SlurmDBD served slurmctld these users can\n\nView information that is blocked to regular uses by a PrivateData\n  flag\nCreate/Alter/Delete Reservations\n\n\nAdmin: These users have the same level of privileges as an\noperator in the database.  They can also alter anything on a served\nslurmctld as if they were the slurm user or root.\n\nCluster= Only add to accounts on these clusters (default is all clusters)\nDefaultAccount= Default account for the user, used when no account\nis specified when a job is submitted. (Required on creation)\nDefaultWCKey= Default wckey for the user, used when no wckey\nis specified when a job is submitted. (Only used when tracking wckeys.)\nName= User name\nNewName= Use to rename a user in the accounting database\nPartition= Name of Slurm partition this association applies to\n\nLimit Enforcement\n\n\nVarious limits and limit enforcement are described in\n  the Resource Limits web page.\nTo enable any limit enforcement you must at least have\nAccountingStorageEnforce=limits in your slurm.conf.\nOtherwise, even if you have limits set, they will not be enforced.\nOther options for AccountingStorageEnforce and the explanation for\neach are found on the Resource\nLimits document.\nModifying Entities\n\n\nWhen modifying entities, you can specify many different options in\nSQL-like fashion, using key words like where and set.\nA typical execute line has the following form:\n\nsacctmgr modify <entity> set <options> where <options>\n\nFor example:\n\nsacctmgr modify user set default=none where default=test\n\nwill change all users with a default account of \"test\" to account \"none\".\nOnce an entity has been added, modified or removed, the change is\nsent to the appropriate Slurm daemons and will be available for use\ninstantly.\nRemoving Entities\n\n\nRemoving entities using an execute line similar to the modify example above,\nbut without the set options.\nFor example, remove all users with a default account \"test\" using the following\nexecute line:\n\nsacctmgr remove user where default=test\n\nwill remove all user records where the default account is \"test\".\n\nsacctmgr remove user brian where account=physics\n\nwill remove user \"brian\" from account \"physics\". If user \"brian\" has\naccess to other accounts, those user records will remain.\nNote: In most cases, removed entities are preserved in the slurm database,\nbut flagged as deleted.\nIf an entity has existed for less than 1 day, the entity will be removed\ncompletely. This is meant to clean up after typographical errors.\nRemoving user associations or accounts, however, will cause slurmctld to lose\ntrack of usage data for that user/account.\nLast modified 17 July 2024\n"
            },
            {
                "title": "Account Options\n\n",
                "content": "When either adding or modifying an account, the following sacctmgr\noptions are available:\n\nCluster= Only add this account to these clusters.\nThe account is added to all defined clusters by default.\nDescription= Description of the account. (Default is\n  account name)\nName= Name of account. Note the name must be unique and can not\nrepresent different bank accounts at different points in the account\nhierarchy\nOrganization=Organization of the account. (Default is\n  parent account unless parent account is root then organization is\n  set to the account name.)\nParent= Make this account a child of this other account\n  (already added).\n\nUser Options\n\n\nWhen either adding or modifying a user, the following sacctmgr\noptions are available:\n\n\nAccount= Account(s) to add user to\nAdminLevel= This field is used to allow a user to add accounting\nprivileges to this user. Valid options are\n\nNone\nOperator: can add, modify, and remove any database object (user,\naccount, etc), and add other operators\nOn a SlurmDBD served slurmctld these users can\n\nView information that is blocked to regular uses by a PrivateData\n  flag\nCreate/Alter/Delete Reservations\n\n\nAdmin: These users have the same level of privileges as an\noperator in the database.  They can also alter anything on a served\nslurmctld as if they were the slurm user or root.\n\nCluster= Only add to accounts on these clusters (default is all clusters)\nDefaultAccount= Default account for the user, used when no account\nis specified when a job is submitted. (Required on creation)\nDefaultWCKey= Default wckey for the user, used when no wckey\nis specified when a job is submitted. (Only used when tracking wckeys.)\nName= User name\nNewName= Use to rename a user in the accounting database\nPartition= Name of Slurm partition this association applies to\n\nLimit Enforcement\n\n\nVarious limits and limit enforcement are described in\n  the Resource Limits web page.\nTo enable any limit enforcement you must at least have\nAccountingStorageEnforce=limits in your slurm.conf.\nOtherwise, even if you have limits set, they will not be enforced.\nOther options for AccountingStorageEnforce and the explanation for\neach are found on the Resource\nLimits document.\nModifying Entities\n\n\nWhen modifying entities, you can specify many different options in\nSQL-like fashion, using key words like where and set.\nA typical execute line has the following form:\n\nsacctmgr modify <entity> set <options> where <options>\n\nFor example:\n\nsacctmgr modify user set default=none where default=test\n\nwill change all users with a default account of \"test\" to account \"none\".\nOnce an entity has been added, modified or removed, the change is\nsent to the appropriate Slurm daemons and will be available for use\ninstantly.\nRemoving Entities\n\n\nRemoving entities using an execute line similar to the modify example above,\nbut without the set options.\nFor example, remove all users with a default account \"test\" using the following\nexecute line:\n\nsacctmgr remove user where default=test\n\nwill remove all user records where the default account is \"test\".\n\nsacctmgr remove user brian where account=physics\n\nwill remove user \"brian\" from account \"physics\". If user \"brian\" has\naccess to other accounts, those user records will remain.\nNote: In most cases, removed entities are preserved in the slurm database,\nbut flagged as deleted.\nIf an entity has existed for less than 1 day, the entity will be removed\ncompletely. This is meant to clean up after typographical errors.\nRemoving user associations or accounts, however, will cause slurmctld to lose\ntrack of usage data for that user/account.\nLast modified 17 July 2024\n"
            },
            {
                "title": "User Options\n\n",
                "content": "When either adding or modifying a user, the following sacctmgr\noptions are available:\n\n\nAccount= Account(s) to add user to\nAdminLevel= This field is used to allow a user to add accounting\nprivileges to this user. Valid options are\n\nNone\nOperator: can add, modify, and remove any database object (user,\naccount, etc), and add other operators\nOn a SlurmDBD served slurmctld these users can\n\nView information that is blocked to regular uses by a PrivateData\n  flag\nCreate/Alter/Delete Reservations\n\n\nAdmin: These users have the same level of privileges as an\noperator in the database.  They can also alter anything on a served\nslurmctld as if they were the slurm user or root.\n\nCluster= Only add to accounts on these clusters (default is all clusters)\nDefaultAccount= Default account for the user, used when no account\nis specified when a job is submitted. (Required on creation)\nDefaultWCKey= Default wckey for the user, used when no wckey\nis specified when a job is submitted. (Only used when tracking wckeys.)\nName= User name\nNewName= Use to rename a user in the accounting database\nPartition= Name of Slurm partition this association applies to\n\nLimit Enforcement\n\n\nVarious limits and limit enforcement are described in\n  the Resource Limits web page.\nTo enable any limit enforcement you must at least have\nAccountingStorageEnforce=limits in your slurm.conf.\nOtherwise, even if you have limits set, they will not be enforced.\nOther options for AccountingStorageEnforce and the explanation for\neach are found on the Resource\nLimits document.\nModifying Entities\n\n\nWhen modifying entities, you can specify many different options in\nSQL-like fashion, using key words like where and set.\nA typical execute line has the following form:\n\nsacctmgr modify <entity> set <options> where <options>\n\nFor example:\n\nsacctmgr modify user set default=none where default=test\n\nwill change all users with a default account of \"test\" to account \"none\".\nOnce an entity has been added, modified or removed, the change is\nsent to the appropriate Slurm daemons and will be available for use\ninstantly.\nRemoving Entities\n\n\nRemoving entities using an execute line similar to the modify example above,\nbut without the set options.\nFor example, remove all users with a default account \"test\" using the following\nexecute line:\n\nsacctmgr remove user where default=test\n\nwill remove all user records where the default account is \"test\".\n\nsacctmgr remove user brian where account=physics\n\nwill remove user \"brian\" from account \"physics\". If user \"brian\" has\naccess to other accounts, those user records will remain.\nNote: In most cases, removed entities are preserved in the slurm database,\nbut flagged as deleted.\nIf an entity has existed for less than 1 day, the entity will be removed\ncompletely. This is meant to clean up after typographical errors.\nRemoving user associations or accounts, however, will cause slurmctld to lose\ntrack of usage data for that user/account.\nLast modified 17 July 2024\n"
            },
            {
                "title": "Limit Enforcement\n\n",
                "content": "Various limits and limit enforcement are described in\n  the Resource Limits web page.To enable any limit enforcement you must at least have\nAccountingStorageEnforce=limits in your slurm.conf.\nOtherwise, even if you have limits set, they will not be enforced.\nOther options for AccountingStorageEnforce and the explanation for\neach are found on the Resource\nLimits document.Modifying Entities\n\nWhen modifying entities, you can specify many different options in\nSQL-like fashion, using key words like where and set.\nA typical execute line has the following form:\n\nsacctmgr modify <entity> set <options> where <options>\n\nFor example:\n\nsacctmgr modify user set default=none where default=test\n\nwill change all users with a default account of \"test\" to account \"none\".\nOnce an entity has been added, modified or removed, the change is\nsent to the appropriate Slurm daemons and will be available for use\ninstantly.\nRemoving Entities\n\n\nRemoving entities using an execute line similar to the modify example above,\nbut without the set options.\nFor example, remove all users with a default account \"test\" using the following\nexecute line:\n\nsacctmgr remove user where default=test\n\nwill remove all user records where the default account is \"test\".\n\nsacctmgr remove user brian where account=physics\n\nwill remove user \"brian\" from account \"physics\". If user \"brian\" has\naccess to other accounts, those user records will remain.\nNote: In most cases, removed entities are preserved in the slurm database,\nbut flagged as deleted.\nIf an entity has existed for less than 1 day, the entity will be removed\ncompletely. This is meant to clean up after typographical errors.\nRemoving user associations or accounts, however, will cause slurmctld to lose\ntrack of usage data for that user/account.\nLast modified 17 July 2024\n"
            },
            {
                "title": "Removing Entities\n\n",
                "content": "Removing entities using an execute line similar to the modify example above,\nbut without the set options.\nFor example, remove all users with a default account \"test\" using the following\nexecute line:\nsacctmgr remove user where default=test\nwill remove all user records where the default account is \"test\".\nsacctmgr remove user brian where account=physics\nwill remove user \"brian\" from account \"physics\". If user \"brian\" has\naccess to other accounts, those user records will remain.Note: In most cases, removed entities are preserved in the slurm database,\nbut flagged as deleted.\nIf an entity has existed for less than 1 day, the entity will be removed\ncompletely. This is meant to clean up after typographical errors.\nRemoving user associations or accounts, however, will cause slurmctld to lose\ntrack of usage data for that user/account.Last modified 17 July 2024"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/containers.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Containers Guide",
                "content": "Contents\nOverview\nKnown limitations\nPrerequisites\nRequired software\nExample configurations for various OCI Runtimes\nTesting OCI runtime outside of Slurm\nRequesting container jobs or steps\nIntegration with Rootless Docker\nIntegration with Podman\nOCI Container bundle\nExample OpenMPI v5 + PMIx v4 container\nContainer support via Plugin\n\nShifter\nENROOT and Pyxis\nSarus\n\nOverviewContainers are being adopted in HPC workloads.\nContainers rely on existing kernel features to allow greater user control over\nwhat applications see and can interact with at any given time. For HPC\nWorkloads, these are usually restricted to the\nmount namespace.\nSlurm natively supports the requesting of unprivileged OCI Containers for jobs\nand steps.Setting up containers requires several steps:\n\nSet up the kernel and a\n    container runtime.\nDeploy a suitable oci.conf file accessible to\n    the compute nodes (examples below).\nRestart or reconfigure slurmd on the compute nodes.\nGenerate OCI bundles for containers that are needed\n    and place them on the compute nodes.\nVerify that you can run containers directly through\n    the chosen OCI runtime.\nVerify that you can request a container through\n    Slurm.\n\nKnown limitations\n\nThe following is a list of known limitations of the Slurm OCI container\nimplementation.\nAll containers must run under unprivileged (i.e. rootless) invocation.\nAll commands are called by Slurm as the user with no special\npermissions.\nCustom container networks are not supported. All containers should work\nwith the \"host\"\nnetwork.\nSlurm will not transfer the OCI container bundle to the execution\nnodes. The bundle must already exist on the requested path on the\nexecution node.\nContainers are limited by the OCI runtime used. If the runtime does not\nsupport a certain feature, then that feature will not work for any job\nusing a container.\noci.conf must be configured on the execution node for the job, otherwise the\nrequested container will be ignored by Slurm (but can be used by the\njob or any given plugin).\nPrerequisitesThe host kernel must be configured to allow user land containers:$ sudo sysctl -w kernel.unprivileged_userns_clone=1Docker also provides a tool to verify the kernel configuration:\n$ dockerd-rootless-setuptool.sh check --force\n[INFO] Requirements are satisfied\nRequired software:\n\n\nFully functional\n\nOCI runtime. It needs to be able to run outside of Slurm first.\nFully functional OCI bundle generation tools. Slurm requires OCI\nContainer compliant bundles for jobs.\nExample configurations for various OCI Runtimes\n\n\nThe OCI Runtime\nSpecification provides requirements for all compliant runtimes but\ndoes not expressly provide requirements on how runtimes will use\narguments. In order to support as many runtimes as possible, Slurm provides\npattern replacement for commands issued for each OCI runtime operation.\nThis will allow a site to edit how the OCI runtimes are called as needed to\nensure compatibility.\n\nFor runc and crun, there are two sets of examples provided.\nThe OCI runtime specification only provides the start and create\noperations sequence, but these runtimes provides a much more efficient run\noperation. Sites are strongly encouraged to use the run operation\n(if provided) as the start and create operations require that\nSlurm poll the OCI runtime to know when the containers have completed execution.\nWhile Slurm attempts to be as efficient as possible with polling, it will\nresult in a thread using CPU time inside of the job and slower response of\nSlurm to catch when container execution is complete.\n\nThe examples provided have been tested to work but are only suggestions. Sites\nare expected to ensure that the resultant root directory used will be secure\nfrom cross user viewing and modifications. The examples provided point to\n\"/run/user/%U\" where %U will be replaced with the numeric user id. Systemd\nmanages \"/run/user/\" (independently of Slurm) and will likely need additional\nconfiguration to ensure the directories exist on compute nodes when the users\nwill not log in to the nodes directly. This configuration is generally achieved\nby calling\n\nloginctl to enable lingering sessions. Be aware that the directory in this\nexample will be cleaned up by systemd once the user session ends on the node.\noci.conf example for runc using create/start:\n\n\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeQuery=\"runc --rootless=true --root=/run/user/%U/ state %n.%u.%j.%s.%t\"\nRunTimeCreate=\"runc --rootless=true --root=/run/user/%U/ create %n.%u.%j.%s.%t -b %b\"\nRunTimeStart=\"runc --rootless=true --root=/run/user/%U/ start %n.%u.%j.%s.%t\"\nRunTimeKill=\"runc --rootless=true --root=/run/user/%U/ kill -a %n.%u.%j.%s.%t\"\nRunTimeDelete=\"runc --rootless=true --root=/run/user/%U/ delete --force %n.%u.%j.%s.%t\"\n\noci.conf example for runc using run (recommended over using\ncreate/start):\n\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeQuery=\"runc --rootless=true --root=/run/user/%U/ state %n.%u.%j.%s.%t\"\nRunTimeKill=\"runc --rootless=true --root=/run/user/%U/ kill -a %n.%u.%j.%s.%t\"\nRunTimeDelete=\"runc --rootless=true --root=/run/user/%U/ delete --force %n.%u.%j.%s.%t\"\nRunTimeRun=\"runc --rootless=true --root=/run/user/%U/ run %n.%u.%j.%s.%t -b %b\"\n\noci.conf example for crun using create/start:\n\n\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeQuery=\"crun --rootless=true --root=/run/user/%U/ state %n.%u.%j.%s.%t\"\nRunTimeKill=\"crun --rootless=true --root=/run/user/%U/ kill -a %n.%u.%j.%s.%t\"\nRunTimeDelete=\"crun --rootless=true --root=/run/user/%U/ delete --force %n.%u.%j.%s.%t\"\nRunTimeCreate=\"crun --rootless=true --root=/run/user/%U/ create --bundle %b %n.%u.%j.%s.%t\"\nRunTimeStart=\"crun --rootless=true --root=/run/user/%U/ start %n.%u.%j.%s.%t\"\n\noci.conf example for crun using run (recommended over using\ncreate/start):\n\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeQuery=\"crun --rootless=true --root=/run/user/%U/ state %n.%u.%j.%s.%t\"\nRunTimeKill=\"crun --rootless=true --root=/run/user/%U/ kill -a %n.%u.%j.%s.%t\"\nRunTimeDelete=\"crun --rootless=true --root=/run/user/%U/ delete --force %n.%u.%j.%s.%t\"\nRunTimeRun=\"crun --rootless=true --root=/run/user/%U/ run --bundle %b %n.%u.%j.%s.%t\"\n\n\noci.conf example for nvidia-container-runtime using create/start:\n\n\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeQuery=\"nvidia-container-runtime --rootless=true --root=/run/user/%U/ state %n.%u.%j.%s.%t\"\nRunTimeCreate=\"nvidia-container-runtime --rootless=true --root=/run/user/%U/ create %n.%u.%j.%s.%t -b %b\"\nRunTimeStart=\"nvidia-container-runtime --rootless=true --root=/run/user/%U/ start %n.%u.%j.%s.%t\"\nRunTimeKill=\"nvidia-container-runtime --rootless=true --root=/run/user/%U/ kill -a %n.%u.%j.%s.%t\"\nRunTimeDelete=\"nvidia-container-runtime --rootless=true --root=/run/user/%U/ delete --force %n.%u.%j.%s.%t\"\n\n\noci.conf example for nvidia-container-runtime using run (recommended over using\ncreate/start):\n\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeQuery=\"nvidia-container-runtime --rootless=true --root=/run/user/%U/ state %n.%u.%j.%s.%t\"\nRunTimeKill=\"nvidia-container-runtime --rootless=true --root=/run/user/%U/ kill -a %n.%u.%j.%s.%t\"\nRunTimeDelete=\"nvidia-container-runtime --rootless=true --root=/run/user/%U/ delete --force %n.%u.%j.%s.%t\"\nRunTimeRun=\"nvidia-container-runtime --rootless=true --root=/run/user/%U/ run %n.%u.%j.%s.%t -b %b\"\n\noci.conf example for\n\nSingularity v4.1.3 using native runtime:\n\n\nIgnoreFileConfigJson=true\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeRun=\"singularity exec --userns %r %@\"\nRunTimeKill=\"kill -s SIGTERM %p\"\nRunTimeDelete=\"kill -s SIGKILL %p\"\n\noci.conf example for\n\nSingularity v4.0.2 in OCI mode:\n\nSingularity v4.x requires setuid mode for OCI support.\n\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeQuery=\"sudo singularity oci state %n.%u.%j.%s.%t\"\nRunTimeRun=\"sudo singularity oci run --bundle %b %n.%u.%j.%s.%t\"\nRunTimeKill=\"sudo singularity oci kill %n.%u.%j.%s.%t\"\nRunTimeDelete=\"sudo singularity oci delete %n.%u.%j.%s.%t\"\n\nWARNING: Singularity (v4.0.2) requires sudo or setuid binaries\nfor OCI support, which is a security risk since the user is able to modify\nthese calls. This example is only provided for testing purposes.WARNING:\n\nUpstream singularity development of the OCI interface appears to have\nceased and sites should use the user\nnamespace support instead.oci.conf example for hpcng Singularity v3.8.0:\n\n\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nOCIRunTimeQuery=\"sudo singularity oci state %n.%u.%j.%s.%t\"\nOCIRunTimeCreate=\"sudo singularity oci create --bundle %b %n.%u.%j.%s.%t\"\nOCIRunTimeStart=\"sudo singularity oci start %n.%u.%j.%s.%t\"\nOCIRunTimeKill=\"sudo singularity oci kill %n.%u.%j.%s.%t\"\nOCIRunTimeDelete=\"sudo singularity oci delete %n.%u.%j.%s.%t\n\nWARNING: Singularity (v3.8.0) requires sudo or setuid binaries\nfor OCI support, which is a security risk since the user is able to modify\nthese calls. This example is only provided for testing purposes.WARNING:\n\nUpstream singularity development of the OCI interface appears to have\nceased and sites should use the user\nnamespace support instead.oci.conf example for\nCharliecloud (v0.30)\n\n\nIgnoreFileConfigJson=true\nCreateEnvFile=newline\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeRun=\"env -i PATH=/usr/local/bin:/usr/local/sbin:/usr/bin:/usr/sbin:/bin/:/sbin/ USER=$(whoami) HOME=/home/$(whoami)/ ch-run -w --bind /etc/group:/etc/group --bind /etc/passwd:/etc/passwd --bind /etc/slurm:/etc/slurm --bind %m:/var/run/slurm/ --bind /var/run/munge/:/var/run/munge/ --set-env=%e --no-passwd %r -- %@\"\nRunTimeKill=\"kill -s SIGTERM %p\"\nRunTimeDelete=\"kill -s SIGKILL %p\"\n\noci.conf example for\nEnroot (3.3.0)\n\n\nIgnoreFileConfigJson=true\nCreateEnvFile=newline\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeRun=\"/usr/local/bin/enroot-start-wrapper %b %m %e -- %@\"\nRunTimeKill=\"kill -s SIGINT %p\"\nRunTimeDelete=\"kill -s SIGTERM %p\"\n\n/usr/local/bin/enroot-start-wrapper:\n\n#!/bin/bash\nBUNDLE=\"$1\"\nSPOOLDIR=\"$2\"\nENVFILE=\"$3\"\nshift 4\nIMAGE=\n\nexport USER=$(whoami)\nexport HOME=\"$BUNDLE/\"\nexport TERM\nexport ENROOT_SQUASH_OPTIONS='-comp gzip -noD'\nexport ENROOT_ALLOW_SUPERUSER=n\nexport ENROOT_MOUNT_HOME=y\nexport ENROOT_REMAP_ROOT=y\nexport ENROOT_ROOTFS_WRITABLE=y\nexport ENROOT_LOGIN_SHELL=n\nexport ENROOT_TRANSFER_RETRIES=2\nexport ENROOT_CACHE_PATH=\"$SPOOLDIR/\"\nexport ENROOT_DATA_PATH=\"$SPOOLDIR/\"\nexport ENROOT_TEMP_PATH=\"$SPOOLDIR/\"\nexport ENROOT_ENVIRON=\"$ENVFILE\"\n\nif [ ! -f \"$BUNDLE\" ]\nthen\n        IMAGE=\"$SPOOLDIR/container.sqsh\"\n        enroot import -o \"$IMAGE\" -- \"$BUNDLE\" && \\\n        enroot create \"$IMAGE\"\n        CONTAINER=\"container\"\nelse\n        CONTAINER=\"$BUNDLE\"\nfi\n\nenroot start -- \"$CONTAINER\" \"$@\"\nrc=$?\n\n[ $IMAGE ] && unlink $IMAGE\n\nexit $rc\n\nTesting OCI runtime outside of Slurm\n\nSlurm calls the OCI runtime directly in the job step. If it fails,\nthen the job will also fail.\nGo to the directory containing the OCI Container bundle:\ncd $ABS_PATH_TO_BUNDLE\nExecute OCI Container runtime (You can find a few examples on how to build\na bundle below):\n$OCIRunTime $ARGS create test --bundle $PATH_TO_BUNDLE\n$OCIRunTime $ARGS start test\n$OCIRunTime $ARGS kill test\n$OCIRunTime $ARGS delete test\nIf these commands succeed, then the OCI runtime is correctly\nconfigured and can be tested in Slurm.\n\nRequesting container jobs or steps\n\n\nsalloc, srun and sbatch (in Slurm 21.08+) have the\n'--container' argument, which can be used to request container runtime\nexecution. The requested job container will not be inherited by the steps\ncalled, excluding the batch and interactive steps.\n\nBatch step inside of container:\nsbatch --container $ABS_PATH_TO_BUNDLE --wrap 'bash -c \"cat /etc/*rel*\"'\n\nBatch job with step 0 inside of container:\n\nsbatch --wrap 'srun bash -c \"--container $ABS_PATH_TO_BUNDLE cat /etc/*rel*\"'\n\nInteractive step inside of container:\nsalloc --container $ABS_PATH_TO_BUNDLE bash -c \"cat /etc/*rel*\"\nInteractive job step 0 inside of container:\nsalloc srun --container $ABS_PATH_TO_BUNDLE bash -c \"cat /etc/*rel*\"\n\nJob with step 0 inside of container:\nsrun --container $ABS_PATH_TO_BUNDLE bash -c \"cat /etc/*rel*\"\nJob with step 1 inside of container:\nsrun srun --container $ABS_PATH_TO_BUNDLE bash -c \"cat /etc/*rel*\"\n\nIntegration with Rootless Docker (Docker Engine v20.10+ & Slurm-23.02+)\n\nSlurm's scrun can be directly integrated with Rootless Docker to\nrun containers as jobs. No special user permissions are required and should\nnot be granted to use this functionality.Prerequisites\nslurm.conf must be configured to use Munge\nauthentication.AuthType=auth/munge\nscrun.lua\nmust be configured for site storage configuration.\n\n\tConfigure kernel to allow pings\n\n\tConfigure rootless dockerd to allow listening on privileged ports\n\t\n\n\tscrun.lua must be present on any node where scrun may be run. The\n\texample should be sufficent for most environments but paths should be\n\tmodified to match available local storage.\noci.conf must be present on any node where any\n\tcontainer job may be run. Example configurations for\n\t\n\tknown OCI runtimes are provided above. Examples may require\n\tpaths to be correct to installation locations.\nLimitations\nJWT authentication is not supported.\nDocker container building is not currently functional pending merge of\n Docker pull request.\nDocker does not expose configuration options to disable security\noptions needed to run jobs. This requires that all calls to docker provide the\nfollowing command line arguments.  This can be done via shell variable, an\nalias, wrapper function, or wrapper script:\n--security-opt label:disable --security-opt seccomp=unconfined --security-opt apparmor=unconfined --net=none\nDocker's builtin security functionality is not required (or wanted) for\ncontainers being run by Slurm.  Docker is only acting as a container image\nlifecycle manager. The containers will be executed remotely via Slurm following\nthe existing security configuration in Slurm outside of unprivileged user\ncontrol.\nAll containers must use the\n\"none\" networking driver\n. Attempting to use bridge, overlay, host, ipvlan, or macvlan can result in\nscrun being isolated from the network and not being able to communicate with\nthe Slurm controller. The container is run by Slurm on the compute nodes which\nmakes having Docker setup a network isolation layer ineffective for the\ncontainer.\ndocker exec command is not supported.\ndocker compose command is not supported.\ndocker pause command is not supported.\ndocker unpause command is not supported.\ndocker swarm command is not supported.\nSetup procedure\n Install and\nconfigure Rootless Docker Rootless Docker must be fully operational and\nable to run containers before continuing.\n\nSetup environment for all docker calls:\nexport DOCKER_HOST=unix://$XDG_RUNTIME_DIR/docker.sock\nAll commands following this will expect this environment variable to be set.\nStop rootless docker: systemctl --user stop docker\nConfigure Docker to call scrun instead of the default OCI runtime.\n\n\nTo configure for all users: /etc/docker/daemon.json\nTo configure per user: ~/.config/docker/daemon.json\n\nSet the following fields to configure Docker:\n{\n    \"experimental\": true,\n    \"iptables\": false,\n    \"bridge\": \"none\",\n    \"no-new-privileges\": true,\n    \"rootless\": true,\n    \"selinux-enabled\": false,\n    \"default-runtime\": \"slurm\",\n    \"runtimes\": {\n        \"slurm\": {\n            \"path\": \"/usr/local/bin/scrun\"\n        }\n    },\n    \"data-root\": \"/run/user/${USER_ID}/docker/\",\n    \"exec-root\": \"/run/user/${USER_ID}/docker-exec/\"\n}\nCorrect path to scrun as if installation prefix was configured. Replace\n${USER_ID} with numeric user id or target a different directory with global\nwrite permissions and sticky bit. Rootless docker requires a different root\ndirectory than the system's default to avoid permission errors.\nIt is strongly suggested that sites consider using inter-node shared\nfilesystems to store Docker's containers. While it is possible to have a\nscrun.lua script to push and pull images for each deployment, there can be a\nmassive performance penalty.  Using a shared filesystem will avoid moving these\nfiles around.Possible configuration additions to daemon.json to use a\nshared filesystem with  vfs storage\ndriver:\n{\n  \"storage-driver\": \"vfs\",\n  \"data-root\": \"/path/to/shared/filesystem/user_name/data/\",\n  \"exec-root\": \"/path/to/shared/filesystem/user_name/exec/\",\n}\nAny node expected to be able to run containers from Docker must have ability to\natleast read the filesystem used. Full write privileges are suggested and will\nbe required if changes to the container filesystem are desired.\nConfigure dockerd to not setup network namespace, which will break scrun's\n\tability to talk to the Slurm controller.\n\n\nTo configure for all users:\n/etc/systemd/user/docker.service.d/override.conf\nTo configure per user:\n~/.config/systemd/user/docker.service.d/override.conf\n\n\n[Service]\nEnvironment=\"DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER=none\"\nEnvironment=\"DOCKERD_ROOTLESS_ROOTLESSKIT_NET=host\"\n\n\nReload docker's service unit in systemd:\nsystemctl --user daemon-reload\nStart rootless docker: systemctl --user start docker\nVerify Docker is using scrun:\nexport DOCKER_SECURITY=\"--security-opt label=disable --security-opt seccomp=unconfined  --security-opt apparmor=unconfined --net=none\"\ndocker run $DOCKER_SECURITY hello-world\ndocker run $DOCKER_SECURITY alpine /bin/printenv SLURM_JOB_ID\ndocker run $DOCKER_SECURITY alpine /bin/hostname\ndocker run $DOCKER_SECURITY -e SCRUN_JOB_NUM_NODES=10 alpine /bin/hostname\n\nIntegration with Podman (Slurm-23.02+)\n\n\nSlurm's scrun can be directly integrated with\nPodman\nto run containers as jobs. No special user permissions are required and\nshould not be granted to use this functionality.\nPrerequisites\nSlurm must be fully configured and running on host running podman.\nslurm.conf must be configured to use Munge\nauthentication.AuthType=auth/munge\nscrun.lua must be configured for site storage\nconfiguration.\n\n\tscrun.lua must be present on any node where scrun may be run. The\n\texample should be sufficent for most environments but paths should be\n\tmodified to match available local storage.\noci.conf\n\tmust be present on any node where any container job may be run.\n\tExample configurations for\n\t\n\tknown OCI runtimes are provided above. Examples may require\n\tpaths to be correct to installation locations.\nLimitations\nJWT authentication is not supported.\nAll containers must use\n\nhost networking\npodman exec command is not supported.\npodman kube command is not supported.\npodman pod command is not supported.\nSetup procedure\nInstall Podman\n\nConfigure rootless Podman\nVerify rootless podman is configured\n\t$ podman info --format '{{.Host.Security.Rootless}}'\ntrue\nVerify rootless Podman is fully functional before adding Slurm support:\n\nThe value printed by the following commands should be the same:\n\t$ id\n$ podman run --userns keep-id alpine id\n$ sudo id\n$ podman run --userns nomap alpine id\n\n\nConfigure Podman to call scrun instead of the  default OCI runtime.\nSee \nupstream documentation for details on configuration locations and loading\norder for containers.conf.\n\nTo configure for all users:\n/etc/containers/containers.conf\nTo configure per user:\n$XDG_CONFIG_HOME/containers/containers.conf\nor\n~/.config/containers/containers.conf\n(if $XDG_CONFIG_HOME is not defined).\n\nSet the following configuration parameters to configure Podman's containers.conf:\n[containers]\napparmor_profile = \"unconfined\"\ncgroupns = \"host\"\ncgroups = \"enabled\"\ndefault_sysctls = []\nlabel = false\nnetns = \"host\"\nno_hosts = true\npidns = \"host\"\nutsns = \"host\"\nuserns = \"host\"\nlog_driver = \"journald\"\n\n[engine]\ncgroup_manager = \"systemd\"\nruntime = \"slurm\"\nremote = false\n\n[engine.runtimes]\nslurm = [\n\t\"/usr/local/bin/scrun\",\n\t\"/usr/bin/scrun\"\n]\nCorrect path to scrun as if installation prefix was configured.\nThe \"cgroup_manager\" field will need to be swapped to \"cgroupfs\" on systems\nnot running systemd.\nIt is strongly suggested that sites consider using inter-node shared\nfilesystems to store Podman's containers. While it is possible to have a\nscrun.lua script to push and pull images for each deployment, there can be a\nmassive performance penalty. Using a shared filesystem will avoid moving these\nfiles around.\n\nTo configure for all users: /etc/containers/storage.conf\nTo configure per user: $XDG_CONFIG_HOME/containers/storage.conf\n\nPossible configuration additions to storage.conf to use a shared filesystem with\n\nvfs storage driver:\n[storage]\ndriver = \"vfs\"\nrunroot = \"$HOME/containers\"\ngraphroot = \"$HOME/containers\"\n\n[storage.options]\npull_options = {use_hard_links = \"true\", enable_partial_images = \"true\"}\n\n\n[storage.options.vfs]\nignore_chown_errors = \"true\"\nAny node expected to be able to run containers from Podman must have ability to\natleast read the filesystem used. Full write privileges are suggested and will\nbe required if changes to the container filesystem are desired.\n Verify Podman is using scrun:\npodman run hello-world\npodman run alpine printenv SLURM_JOB_ID\npodman run alpine hostname\npodman run alpine -e SCRUN_JOB_NUM_NODES=10 hostname\nsalloc podman run --env-host=true alpine hostname\nsalloc sh -c 'podman run -e SLURM_JOB_ID=$SLURM_JOB_ID alpine hostname'\n\nOptional: Create alias for Docker:\n\talias docker=podman or\n\talias docker='podman --config=/some/path \"$@\"'\n\nTroubleshooting\nPodman runs out of locks:\n$ podman run alpine uptime\nError: allocating lock for new container: allocation failed; exceeded num_locks (2048)\n\n\nTry renumbering:podman system renumber\nTry reseting all storage:podman system reset\n\n\nOCI Container bundle\n\nThere are multiple ways to generate an OCI Container bundle. The\ninstructions below are the method we found the easiest. The OCI standard\nprovides the requirements for any given bundle:\n\nFilesystem Bundle\nHere are instructions on how to generate a container using a few\nalternative container solutions:\nCreate an image and prepare it for use with runc:\n    \n\n\tUse an existing tool to create a filesystem image in /image/rootfs:\n\t\n\n\t\tdebootstrap:\n\t\tsudo debootstrap stable /image/rootfs http://deb.debian.org/debian/\n\n\n\t\tyum:\n\t\tsudo yum --config /etc/yum.conf --installroot=/image/rootfs/ --nogpgcheck --releasever=${CENTOS_RELEASE} -y\n\n\n\t\tdocker:\n\t\t\nmkdir -p ~/oci_images/alpine/rootfs\ncd ~/oci_images/\ndocker pull alpine\ndocker create --name alpine alpine\ndocker export alpine | tar -C ~/oci_images/alpine/rootfs -xf -\ndocker rm alpine\n\n\n\n\tConfigure a bundle for runtime to execute:\n\t\nUse runc\n\t    to generate a config.json:\n\t    \ncd ~/oci_images/alpine\nrunc --rootless=true spec --rootless\n\nTest running image:\n\nsrun --container ~/oci_images/alpine/ uptime\n\nUse umoci\n    and skopeo to generate a full image:\n    \nmkdir -p ~/oci_images/\ncd ~/oci_images/\nskopeo copy docker://alpine:latest oci:alpine:latest\numoci unpack --rootless --image alpine ~/oci_images/alpine\nsrun --container ~/oci_images/alpine uptime\n\n    Use \n    singularity to generate a full image:\n    \nmkdir -p ~/oci_images/alpine/\ncd ~/oci_images/alpine/\nsingularity pull alpine\nsudo singularity oci mount ~/oci_images/alpine/alpine_latest.sif ~/oci_images/alpine\nmv config.json singularity_config.json\nrunc spec --rootless\nsrun --container ~/oci_images/alpine/ uptime\nExample OpenMPI v5 + PMIx v4 container\n\nDockerfile\nFROM almalinux:latest\nRUN dnf -y update && dnf -y upgrade && dnf install -y yum-utils && dnf config-manager --set-enabled powertools\nRUN dnf -y install make automake gcc gcc-c++ kernel-devel bzip2 python3 wget libevent-devel hwloc-devel munge-devel\n\nWORKDIR /usr/local/src/\nRUN wget 'https://github.com/openpmix/openpmix/releases/download/v4.2.2/pmix-4.2.2.tar.bz2' -O - | tar -xvjf -\nWORKDIR /usr/local/src/pmix-4.2.2/\nRUN ./configure && make -j && make install\n\nWORKDIR /usr/local/src/\nRUN wget --inet4-only 'https://download.open-mpi.org/release/open-mpi/v5.0/openmpi-5.0.0rc9.tar.gz' -O - | tar -xvzf -\nWORKDIR /usr/local/src/openmpi-5.0.0rc9\nRUN ./configure --disable-pty-support --enable-ipv6 --without-slurm --with-pmix --enable-debug && make -j && make install\n\nWORKDIR /usr/local/src/openmpi-5.0.0rc9/examples\nRUN make && cp -v hello_c ring_c connectivity_c spc_example /usr/local/bin\nContainer support via Plugin\nSlurm allows container developers to create SPANK\nPlugins that can be called at various points of job execution to support\ncontainers. Any site using one of these plugins to start containers should\nnot have an \"oci.conf\" configuration file. The \"oci.conf\" file activates the\nbuiltin container functionality which may conflict with the SPANK based plugin\nfunctionality.The following projects are third party container solutions that have been\ndesigned to work with Slurm, but they have not been tested or validated by\nSchedMD.ShifterShifter is a container\nproject out of NERSC\nto provide HPC containers with full scheduler integration.\n\n\nShifter provides full\n\t\t\n\t\t\tinstructions to integrate with Slurm.\n\t\nPresentations about Shifter and Slurm:\n\t\t\n \n\t\t\t\tNever Port Your Code Again - Docker functionality with Shifter using SLURM\n\t\t\t \n \n\t\t\t\tShifter: Containers in HPC Environments\n\t\t\t \n\n\n\nENROOT and PyxisEnroot is a user namespace\ncontainer system sponsored by NVIDIA\nthat supports:\n\nSlurm integration via\n\t\tpyxis\n\nNative support for Nvidia GPUs\nFaster Docker image imports\n\nSarusSarus is a privileged\ncontainer system sponsored by ETH Zurich\nCSCS that supports:\n\n\n\n\t\t\tSlurm image synchronization via OCI hook\n\nNative OCI Image support\nNVIDIA GPU Support\nSimilar design to Shifter\n\nOverview slides of Sarus are\n\n\there.\nLast modified 08 October 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/pdfs/summary.pdf",
        "sections": []
    },
    {
        "url": "https://slurm.schedmd.com/job_exit_code.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Job Exit Codes",
                "content": "A job's exit code (aka exit status, return code and completion\ncode) is captured by Slurm and saved as part of the job record.  For\nsbatch jobs, the exit code that is captured is the output of the batch\nscript.  For salloc jobs, the exit code will be the return value of\nthe exit call that terminates the salloc session.  For srun, the exit\ncode will be the return value of the command that srun executes.Any non-zero exit code will be assumed to be a job failure and will\nresult in a Job State of FAILED with a Reason of\n\"NonZeroExitCode\".The exit code is an 8 bit unsigned number ranging between 0 and\n255.  While it is possible for a job to return a negative exit code,\nSlurm will display it as an unsigned value in the 0 - 255 range.Job Step Exit Codes\n\nWhen a job contains multiple job steps, the exit code of each\nexecutable invoked by srun is saved individually to the job step\nrecord.Signaled JobsWhen a job or step is sent a signal that causes its termination,\nSlurm also captures the signal number and saves it to the job or step\nrecord.Displaying Exit Codes and Signals\n\nSlurm displays a job's exit code in the output of the scontrol\nshow job and the sview utility.  Slurm displays job step\nexit codes in the output of the scontrol show step and the\nsview utility.\n\nWhen a signal was responsible for a job or step's termination, the\nsignal number will be displayed after the exit code, delineated by a\ncolon(:).\nDatabase Job/Step Records\nThe Slurm control daemon sends job and step records to the Slurm\ndatabase when the Slurm accounting_storage plugin is installed.  Job\nand step records sent to the Slurm db can be viewed using the\nsacct command.  The default sacct output contains an\nExitCode field whose format mirrors the output of scontrol and\nsview described above.\nDerived Exit Code and Comment String\n\n\nAfter reading the above description of a job's exit code, one can\nimagine a scenario where a central task of a batch job fails but the\nscript returns an exit code of zero, indicating success.  In many\ncases, a user may not be able to ascertain the success or failure of a\njob until after they have examined the job's output files.\nThe job includes a \"derived exit code\" field.\nIt is initially set to the value of the highest\nexit code returned by all of the job's steps (srun invocations).  The\njob's derived exit code is determined by the Slurm control daemon\nand sent to the database when the accounting_storage plugin is\nenabled.\nIn addition to the derived exit code, the job record in the Slurm\ndatabase contains a comment string.  This is initialized to the job's\ncomment string (when AccountingStoreFlags parameter in the\nslurm.conf contains 'job_comment') and can only be changed by the user.\nA new option has been added to the sacctmgr command to\nprovide the user the means to modify these two fields of the job\nrecord.  No other modification to the job record is allowed.  For\nthose who prefer a simpler command specifically designed to view and\nmodify the derived exit code and comment string, the\nsjobexitmod wrapper has been created (see below).\nThe user now has the means to annotate a job's exit code after it\ncompletes and provide a description of what failed.  This includes the\nability to annotate a successful completion to jobs that appear to\nhave failed but actually succeeded.\nThe sjobexitmod command\n\n\nThe sjobexitmod command is available to display and update the\ntwo derived exit fields of the Slurm db's job record.\nsjobexitmod can first be used to display the existing exit code\n/ string for a job:\n\n> sjobexitmod -l 123\nJobID Account NNodes NodeList     State ExitCode DerivedExitCode Comment\n----- ------- ------ -------- --------- -------- --------------- -------\n123        lc      1     tux0 COMPLETED      0:0             0:0\n\n\nIf a change is desired, sjobexitmod can modify the derived fields:\n\n\n> sjobexitmod -e 49 -r \"out of memory\" 123\n\n Modification of job 123 was successful.\n\n> sjobexitmod -l 123\nJobID Account NNodes NodeList     State ExitCode DerivedExitCode Comment\n----- ------- ------ -------- --------- -------- --------------- -------\n123        lc      1     tux0 COMPLETED      0:0            49:0 out of memory\n\nThe existing sacct command also supports the two new derived\nexit fields:\n\n> sacct -X -j 123 -o JobID,NNodes,State,ExitCode,DerivedExitcode,Comment\nJobID   NNodes      State ExitCode DerivedExitCode        Comment\n------ ------- ---------- -------- --------------- --------------\n123          1  COMPLETED      0:0            49:0  out of memory\n\nLast modified 15 April 2015\n"
            },
            {
                "title": "Derived Exit Code and Comment String\n\n",
                "content": "After reading the above description of a job's exit code, one can\nimagine a scenario where a central task of a batch job fails but the\nscript returns an exit code of zero, indicating success.  In many\ncases, a user may not be able to ascertain the success or failure of a\njob until after they have examined the job's output files.The job includes a \"derived exit code\" field.\nIt is initially set to the value of the highest\nexit code returned by all of the job's steps (srun invocations).  The\njob's derived exit code is determined by the Slurm control daemon\nand sent to the database when the accounting_storage plugin is\nenabled.In addition to the derived exit code, the job record in the Slurm\ndatabase contains a comment string.  This is initialized to the job's\ncomment string (when AccountingStoreFlags parameter in the\nslurm.conf contains 'job_comment') and can only be changed by the user.A new option has been added to the sacctmgr command to\nprovide the user the means to modify these two fields of the job\nrecord.  No other modification to the job record is allowed.  For\nthose who prefer a simpler command specifically designed to view and\nmodify the derived exit code and comment string, the\nsjobexitmod wrapper has been created (see below).The user now has the means to annotate a job's exit code after it\ncompletes and provide a description of what failed.  This includes the\nability to annotate a successful completion to jobs that appear to\nhave failed but actually succeeded.The sjobexitmod command\n\nThe sjobexitmod command is available to display and update the\ntwo derived exit fields of the Slurm db's job record.\nsjobexitmod can first be used to display the existing exit code\n/ string for a job:\n> sjobexitmod -l 123\nJobID Account NNodes NodeList     State ExitCode DerivedExitCode Comment\n----- ------- ------ -------- --------- -------- --------------- -------\n123        lc      1     tux0 COMPLETED      0:0             0:0\nsjobexitmod\n> sjobexitmod -e 49 -r \"out of memory\" 123\n\n Modification of job 123 was successful.\n\n> sjobexitmod -l 123\nJobID Account NNodes NodeList     State ExitCode DerivedExitCode Comment\n----- ------- ------ -------- --------- -------- --------------- -------\n123        lc      1     tux0 COMPLETED      0:0            49:0 out of memory\nThe existing sacct command also supports the two new derived\nexit fields:\n> sacct -X -j 123 -o JobID,NNodes,State,ExitCode,DerivedExitcode,Comment\nJobID   NNodes      State ExitCode DerivedExitCode        Comment\n------ ------- ---------- -------- --------------- --------------\n123          1  COMPLETED      0:0            49:0  out of memory\nLast modified 15 April 2015"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Database Job/Step Records",
                "content": "The Slurm control daemon sends job and step records to the Slurm\ndatabase when the Slurm accounting_storage plugin is installed.  Job\nand step records sent to the Slurm db can be viewed using the\nsacct command.  The default sacct output contains an\nExitCode field whose format mirrors the output of scontrol and\nsview described above.Derived Exit Code and Comment String\n\nAfter reading the above description of a job's exit code, one can\nimagine a scenario where a central task of a batch job fails but the\nscript returns an exit code of zero, indicating success.  In many\ncases, a user may not be able to ascertain the success or failure of a\njob until after they have examined the job's output files.The job includes a \"derived exit code\" field.\nIt is initially set to the value of the highest\nexit code returned by all of the job's steps (srun invocations).  The\njob's derived exit code is determined by the Slurm control daemon\nand sent to the database when the accounting_storage plugin is\nenabled.In addition to the derived exit code, the job record in the Slurm\ndatabase contains a comment string.  This is initialized to the job's\ncomment string (when AccountingStoreFlags parameter in the\nslurm.conf contains 'job_comment') and can only be changed by the user.A new option has been added to the sacctmgr command to\nprovide the user the means to modify these two fields of the job\nrecord.  No other modification to the job record is allowed.  For\nthose who prefer a simpler command specifically designed to view and\nmodify the derived exit code and comment string, the\nsjobexitmod wrapper has been created (see below).The user now has the means to annotate a job's exit code after it\ncompletes and provide a description of what failed.  This includes the\nability to annotate a successful completion to jobs that appear to\nhave failed but actually succeeded.The sjobexitmod command\n\nThe sjobexitmod command is available to display and update the\ntwo derived exit fields of the Slurm db's job record.\nsjobexitmod can first be used to display the existing exit code\n/ string for a job:\n> sjobexitmod -l 123\nJobID Account NNodes NodeList     State ExitCode DerivedExitCode Comment\n----- ------- ------ -------- --------- -------- --------------- -------\n123        lc      1     tux0 COMPLETED      0:0             0:0\nsjobexitmod\n> sjobexitmod -e 49 -r \"out of memory\" 123\n\n Modification of job 123 was successful.\n\n> sjobexitmod -l 123\nJobID Account NNodes NodeList     State ExitCode DerivedExitCode Comment\n----- ------- ------ -------- --------- -------- --------------- -------\n123        lc      1     tux0 COMPLETED      0:0            49:0 out of memory\nThe existing sacct command also supports the two new derived\nexit fields:\n> sacct -X -j 123 -o JobID,NNodes,State,ExitCode,DerivedExitcode,Comment\nJobID   NNodes      State ExitCode DerivedExitCode        Comment\n------ ------- ---------- -------- --------------- --------------\n123          1  COMPLETED      0:0            49:0  out of memory\nLast modified 15 April 2015"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/network.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Network Configuration Guide",
                "content": "Contents\nOverview\nCommunication for slurmctld\nCommunication for slurmdbd\nCommunication for slurmd\nCommunication for client commands\nCommunication for multiple controllers\nCommunication with multiple clusters\nCommunication in a federation\nCommunication with IPv6\nOverviewThere are a lot of components in a Slurm cluster that need to be able\nto communicate with each other. Some sites have security requirements that\nprevent them from opening all communications between the machines and will\nneed to be able to selectively open just the ports that are necessary.\nThis document will go over what is needed for different components to be\nable to talk to each other.Below is a diagram of a fairly typical cluster, with slurmctld\nand slurmdbd on separate machines. In smaller clusters, MySQL can run\non the same machine as the slurmdbd, but in most cases it is preferable\nto have it run on a dedicated machine. slurmd runs on the\ncompute nodes and the client commands can be installed and run from machines\nof your choosing.\n\n  Typical configuration\nCommunication for slurmctld\n\nThe default port used by slurmctld to listen for incoming requests\nis 6817. This port can be changed with the\nSlurmctldPort slurm.conf\nparameter. Slurmctld listens for incoming requests on that port and responds\nback on the same connection opened by the requestor.The machine running slurmctld needs to be able to establish\noutbound connections as well. It needs to communicate with slurmdbd\non port 6819 by default (see the slurmdbd\nsection for information on how to change this). It also needs to communicate\nwith slurmd on the compute nodes on port 6818 by default (see the\nslurmd section for information on how to change\nthis).By default, the slurmctld will listen for IPv4 traffic.  IPv6\ncommunication can be enabled by adding EnableIPv6 to the\n\nCommunicationParameters in your slurm.conf. With IPv6 enabled, you can\ndisable IPv4 by adding DisableIPv4 to the\n\nCommunicationParameters. These settings must match in both slurmdbd.conf\nand slurm.conf (see the slurmdbd section).Communication for slurmdbd\n\nThe default port used by slurmdbd to listen for incoming requests\nis 6819. This port can be changed with the\nDbdPort slurmdbd.conf parameter.\nSlurmdbd listens for incoming requests on that port and responds back\non the same connection opened by the requestor.The machine running slurmdbd needs to be able to reach the\nMySQL or MariaDB server on port 3306 by default (the port is\nconfigurable on the database side).\nThis port can be changed with the\nStoragePort slurmdbd.conf\nparameter. It also needs to be able to initiate\na connection to slurmctld on port 6819 by default (see the\nslurmctld section for information on how to\nchange this).By default, the slurmdbd will listen for IPv4 traffic.  IPv6\ncommunication can be enabled by adding EnableIPv6 to the\n\nCommunicationParameters in your slurmdbd.conf. With IPv6 enabled, you can\ndisable IPv4 by adding DisableIPv4 to the\n\nCommunicationParameters. These settings must match in both slurmdbd.conf\nand slurm.conf (see the slurmctld section).Communication for slurmd\n\nThe default port used by slurmd to listen for incoming requests\nfrom slurmctld is 6818. This port can be changed with the\nSlurmdPort slurm.conf\nparameter.The machines running srun also use a range of ports to be able\nto communicate with slurmstepd. By default these ports are chosen\nat random from the ephemeral port range, but you can use the\nSrunPortRange to specify\na range of ports from which they can be chosen. This is necessary\nfor login nodes that are behind a firewall.The machines running slurmd need to be able to establish\nconnections with slurmctld on port 6817 by default (see\nthe slurmctld section for information on how to\nchange this).By default, the slurmd communicates over IPv4.  Please see the\nslurmctld section for details on how to change this\nas the slurm.conf parameter affects slurmd daemons as well.Communication for client commands\n\nThe majority of the client commands will communicate with slurmctld\non port 6817 by default (see the slurmctld\nsection for information on how to change this) to get the information they\nneed. This includes the following commands:\nsalloc\nsacctmgr\nsbatch\nsbcast\nscancel\nscontrol\nsdiag\nsinfo\nsprio\nsqueue\nsshare\nsstat\nstrigger\nsview\nThere are also commands that communicate directly with slurmdbd on\nport 6819 by default (see the slurmdbd section\nfor information on how to change this). The following commands get information\nfrom slurmdbd:\nsacct\nsacctmgr\nsreport\nWhen a user starts a job using srun there has to be a communication\npath from the machine where srun is called to the node(s) the job is\nallocated. Communication follows the sequence outlined below:\n1a. srun sends job allocation request to slurmctld\n    1b. slurmctld grants allocation and returns details\n    2a. srun sends step create request to slurmctld\n    2b. slurmctld responds with step credential\n    3.  srun opens sockets for I/O\n    4.  srun forwards credential with task info to slurmd\n    5.  slurmd forwards request as needed (per fanout)\n    6.  slurmd forks/execs slurmstepd\n    7.  slurmstepd connects I/O and launches tasks\n    8.  On task termination, slurmstepd notifies srun\n    9.  srun notifies slurmctld of job termination\n    10. slurmctld verifies termination of all processes via slurmd and\n            releases resources for next job\n\n\n  srun communication\nCommunication with multiple controllers\n\nYou can configure a secondary slurmctld and/or slurmdbd to\nserve as a fallback if the primary should go down. The ports involved don't\nchange, but there are additional communication paths that need to be taken\ninto consideration. The client commands need to be able to reach both\nmachines running slurmctld as well as both machines running\nslurmdbd. Both instances of slurmctld need to be able to\nreach both instances of slurmdbd and each slurmdbd needs\nto be able to reach the MySQL server.\n\n  Fallback slurmctld and slurmdbd\nCommunication with multiple clusters\n\nIn environments where multiple slurmctld instances share the same\nslurmdbd you can configure each cluster to stand on their own and allow\nusers to specify a cluster to submit their jobs to. Ports\nused by the different daemons don't change, but all instances of\nslurmctld need to be able to communicate with the same instance of\nslurmdbd. You can read more about multi cluster configurations in the\nMulti-Cluster Operation\ndocumentation.\n\n  Multi-Cluster configuration\nCommunication in a federation\n\nSlurm also provides the ability to schedule jobs in a peer-to-peer fashion\nbetween multiple clusters, allowing jobs to run on the cluster that has\navailable resources first. The difference in communication needs between this\nand a multi-cluster configuration is that the two instances of slurmctld\nneed to be able to communicate with each other. There are more details about\nusing a\nFederation in the\ndocumentation.\n\n  Federation configuration\nCommunication with IPv6\n\nThe slurmctld, slurmdbd, and slurmd daemons will,\nby default, communicate using IPv4, but they can be configured to use IPv6.\nThis is handled by setting CommunicationParameters=EnableIPv6\nin your slurm.conf and slurmdbd.conf, then restarting all of the daemons.\nThe slurmd may operate over IPv4 OR IPv6 in this mode. IPv4 can be\ndisabled by setting CommunicationParameters=EnableIPv6,DisableIPv4.\nIn is mode, everything must have a valid IPv6 address or the connection will\nfail.The slurmctld expects a node to map to a single IP address (which\nwill be the first address returned when looking up the IP of the node with\ngetaddrinfo()). If you enable IPv6 on an existing cluster and the\nnodes have IPv6 addresses, you must restart the slurmd daemons for\ncommunication over IPv6 to be established.The presence of precedence ::ffff:0:0/96  100 in /etc/gai.conf\nwill cause IPv4 addresses to be returned BEFORE an IPv6 address. This might\ncause a situation where you have enabled IPv6 for Slurm, but are still seeing nodes\ncommunicate with IPv4. If there is confusion as to which address is being used\nyou can call scontrol setdebugflags +NET to enable network related\ndebug logging in your slurmctld.log.If IPv4 and IPv6 are enabled, the loopback interface may still resolve to\n127.0.0.1. This is not necessarily an indication of a problem.Last modified 25 November 2020"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/federation.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Slurm Federated Scheduling Guide",
                "content": "\nOverview\nConfiguration\nFederated Job IDs\nJob Submission\nJob Scheduling\nJob Requeue\nInteractive Jobs\nCanceling Jobs\nJob Modification\nJob Arrays\nStatus Commands\nGlossary\nLimitations\nOverviewSlurm includes support for creating a federation of clusters\nand scheduling jobs in a peer-to-peer fashion between them. Jobs submitted to a\nfederation receive a unique job ID that is unique among all clusters in the\nfederation. A job is submitted to the local cluster (the cluster defined in the\nslurm.conf) and is then replicated across the clusters in the federation. Each\ncluster then independently attempts to the schedule the job based off of its own\nscheduling policies. The clusters coordinate with the \"origin\" cluster (cluster\nthe job was submitted to) to schedule the job.\n\n\nNOTE: This is not intended as a high-throughput environment. If\nscheduling more than 50,000 jobs a day, consider configuring fewer clusters that\nthe sibling jobs can be submitted to or directing load\nto the local cluster only (e.g. --cluster-constraint= or -M submission options\ncould be used to do this).\n\nConfiguration\n\n\n\nA federation is created using the sacctmgr command to create a federation in the\ndatabase and by adding clusters to a federation.\n\n\nTo create a federation use:\n\nsacctmgr add federation <federation_name> [clusters=<list_of_clusters>]\n\n\nClusters can be added or removed from a federation using:\n\nNOTE: A cluster can only be a member of one federation at a time.\n\nsacctmgr modify federation <federation_name> set clusters[+-]=<list_of_clusters>\nsacctmgr modify cluster <cluster_name> set federation=<federation_name>\nsacctmgr modify federation <federation_name> set clusters=\nsacctmgr modify cluster <cluster_name> set federation=\n\nNOTE: If a cluster is removed from a federation without first being\ndrained, running jobs on the removed cluster, or that originated from the\nremoved cluster, will continue to run as non-federated jobs. If a job is pending\non the origin cluster, the job will remain pending on the origin cluster as a\nnon-federated job and the remaining sibling jobs will be removed. If the origin\ncluster is being removed and the job is pending and is only viable on one\ncluster then it will remain pending on the viable cluster as a non-federated\njob. If the origin cluster is being removed and the job is pending and viable on\nmultiple clusters other than the origin cluster, then the remaining pending jobs\nwill remain pending as a federated job and the remaining sibling clusters will\nschedule amongst themselves to start the job.\n\n\n\nFederations can be deleted using:\n\nsacctmgr delete federation <federation_name>\n\n\n\nGeneric features can be assigned to clusters and can be requested at submission\nusing the --cluster-constraint=[!]<feature_list> option:\n\nsacctmgr modify cluster <cluster_name> set features[+-]=<feature_list>\n\n\nA cluster's federated state can be set using:\n\nsacctmgr modify cluster <cluster_name> set fedstate=<state>\n\nwhere possible states are:\n\nACTIVE: Cluster will actively accept and schedule federated\n\t\tjobs\nINACTIVE: Cluster will not schedule or accept any jobs\nDRAIN: Cluster will not accept any new jobs and will let\n\t\texisting federated jobs complete\nDRAIN+REMOVE: Cluster will not accept any new jobs and will\n\t\tremove itself from the federation once all federated jobs have\n\t\tcompleted. When removed from the federation, the cluster will\n\t\taccept jobs as a non-federated cluster\n\n\nFederation configuration can be viewed used using:\n\nsacctmgr show federation [tree]\nsacctmgr show cluster withfed\n\n\nAfter clusters are added to a federation and the controllers are started their\nstatus can be viewed from the controller using:\n\nscontrol show federation\n\n\n\nBy default the status commands will show a local view. A default federated view\ncan be set by configuring the following parameter in the slurm.conf:\n\nFederationParameters=fed_display\n\nFederated Job IDs\nWhen a job is submitted to a federation it gets a federated job id. Job ids in\nthe federation are unique across all clusters in the federation. A federated\njob ID is made by utilizing an unsigned 32 bit integer to assign the cluster's\nID and the cluster's local ID.\n\n\nBits 0-25:  Local Job ID\nBits 26-31: Cluster Origin ID\n\n\nFederated job IDs allow the controllers to know which cluster the job was\nsubmitted to by looking at the cluster origin id of the job.\n\n\nJob Submission\n\n\n\nWhen a federated cluster receives a job submission, it will submit copies of the\njob (sibling jobs) to each eligible cluster. Each cluster will then\nindependently attempt to schedule the job.\n\n\nJobs can be directed to specific clusters in the federation using the\n-M,--clusters=<cluster_list> and the new\n--cluster-constraint=[!]<constraint_list> options.\n\n\nUsing the -M,--clusters=<cluster_list> the submission command\n(sbatch, salloc, srun) will pick one cluster from the list of clusters to submit\nthe job to and will also pass along the list of clusters with the job. The\nclusters in the list will be the only viable clusters that siblings jobs can be\nsubmitted to. For example the submission:\n\ncluster1$ sbatch -Mcluster2,cluster3 script.sh\n\nwill submit the job to either cluster2 or cluster3 and will only submit sibling\njobs to cluster2 and cluster3 even if there are more clusters in the federation.\n\n\nUsing the --cluster-constraint=[!]<constraint_list> option will\nsubmit sibling jobs to only the clusters that have the requested cluster\nfeature(s) -- or don't have the feature(s) if using !. Cluster features\nare added using the sacctmgr modify cluster <cluster_name> set\nfeatures[+-]=<feature_list> option.\n\n\nNOTE: When using the ! option, add quotes around the option to\nprevent the shell from interpreting the ! (e.g\n--cluster-constraint='!highmem').\n\n\nWhen using both the --cluster-constraint= and\n--clusters= options together, the origin cluster will only submit\nsibling jobs to clusters that meet both requirements.\n\n\nHeld or dependent jobs are kept on the origin cluster until they are released\nor are no longer dependent, at which time they are submitted to other viable\nclusters in the federation. If a job becomes held or dependent\nafter being submitted, the job is removed from every cluster but the origin.\n\nJob Scheduling\n\n\n\nEach cluster in the federation independently attempts to schedule each job with\nthe exception of coordinating with the origin cluster (cluster where the\njob was submitted to) to allocate resources to a federated job. When a cluster\ndetermines it can attempt to allocate resources for a job it communicates with\nthe origin cluster to verify that no other cluster is attempting to allocate\nresources at the same time. If no other cluster is attempting to allocate\nresources the cluster will attempt to allocate resources for the job. If it\nsucceeds then it will notify the origin cluster that it started the job and the\norigin cluster will notify the clusters with sibling jobs to remove the sibling\njobs and put them in a revoked state. If the cluster was unable to\nallocate resources to the job then it lets the origin cluster know so that other\nclusters can attempt to schedule the job. If it was the main scheduler\nattempting to allocate resources then the main scheduler will stop looking at\nfurther jobs in the job's partition. If it was the backfill scheduler attempting\nto allocate resources then the resources will be reserved for the job.\n\n\nIf an origin cluster is down, then the remote siblings will coordinate with a\njob's viable siblings to schedule the job. When the origin cluster comes back\nup, it will sync with the other siblings.\n\nJob Requeue\n\n\n\nWhen a federated job is requeued the origin cluster is notified and the origin\ncluster will then submit new sibling jobs to viable clusters and the federated\njob is eligible to start on a different cluster than the one it ran on.\n\n\nslurm.conf options RequeueExit and RequeueExitHold are controlled\nby the origin cluster.\n\nInteractive Jobs\n\n\n\nInteractive jobs -- jobs submitted with srun and salloc -- can be submitted to\nthe local cluster and get an allocation from a different cluster. When an salloc\njob allocation is granted by a cluster other than the local cluster, a new\nenvironment variable, SLURM_WORKING_CLUSTER, will be set with the remote sibling\ncluster's IP address, port and RPC version so that any sruns will know which\ncluster to communicate with.\n\n\nNOTE: It is required that all compute nodes must be accessible to all\nsubmission hosts for this to work.\n\nNOTE: The current implementation of the MPI interfaces in Slurm require\nthe SlurmdSpooldir to be the same on the host where the srun is being run as it\nis on the compute nodes in the allocation. If they aren't, a workaround is to\nget an allocation that puts the user on the actual compute node. Then the sruns\non the compute nodes will be using the slurm.conf that corresponds to the\ncorrect cluster. Setting LaunchParameters=use_interactive_step\nslurm.conf will put the user on an actual compute node when using salloc.\n\nCanceling Jobs\n\n\n\nCancel requests in the federation will cancel the running sibling job or all\npending sibling jobs. Specific pending sibling jobs can be removed by using\nscancel's --sibling=<cluster_name> option to remove the\nsibling job from the job's active sibling list.\n\nJob Modification\n\n\nJob modifications are routed to the origin cluster where the origin cluster will\npush out the changes to each sibling job.\n\n\nJob Arrays\nCurrently, job arrays only run on the origin cluster.\n\n\nStatus Commands\n\n\n\nBy default, status commands, such as: squeue, sinfo, sprio, sacct, sreport, will\nshow a view local to the local cluster. A unified view of the jobs in the\nfederation can be viewed using the --federation option to each status\ncommand. The --federation command causes the status command to first\ncheck if the local cluster is part of a federation. If it is then the command\nwill query each cluster in parallel for job info and will combine the\ninformation into one unified view.\n\n\nA new FederationParameters=fed_display slurm.conf parameter has been\nadded so that all status commands will present a federated view by default --\nequivalent to setting the --federation option for each status command.\nThe federated view can be overridden using the --local option. Using the\n--clusters,-M option will also override the federated view and give a\nlocal view for the given cluster(s).\n\n\nUsing the existing --clusters,-M option, the status commands will output\nthe information in the same format that exists today where each cluster's\ninformation is listed separately.\n\nsqueue\n\nsqueue also has a new --sibling option that will show each sibling job\nrather than merge them into one.\n\n\nSeveral new long format options have been added to display the job's federated\ninformation:\n\n\ncluster: Name of the cluster that is running the job or\n\t\tjob step.\n\t\n\nsiblingsactive: Cluster names of where federated sibling\n\t\tjobs exist.\n\t\n\nsiblingsactiveraw: Cluster IDs of where federated\n\t\tsibling jobs exist.\n\t\n\nsiblingsviable: Cluster names of where federated sibling\n\t\tjobs are viable to run.\n\t\n\nsiblingsviableraw: Cluster names of where federated\n\t\tsibling jobs are viable to run.\n\t\n\n\n\nsqueue output can be sorted using the -S cluster option.\n\nsinfo\n\nsinfo will show the partitions from each cluster in one view. In a federated\nview, the cluster name is displayed with each partition. The cluster name can be\nspecified in the format options using the short format %V or the long\nformat cluster options. The output can be sorted by cluster names using\nthe -S %[+-]V option.\n\nsprio\n\nIn a federated view, sprio displays the job information from the local cluster\nor from the first cluster to report the job. Since each sibling job could have a\ndifferent priority on each cluster it may be helpful to use the --sibling\noption to show all records of a job to get a better picture of a job's priority.\nThe name of the cluster reporting the job record can be displayed using the\n%c format option. The cluster name is shown by default when using\n--sibling option.\n\nsacct\n\nBy default, sacct will not display \"revoked\" jobs and will show the job from the\ncluster that ran the job. However, \"revoked\" jobs can be viewed using the\n--duplicate/-D option.\n\nsreport\n\nsreport will combine the reports from each cluster and display them as one.\n\nscontrol\n\nThe following scontrol options will display a federated view:\n\nshow [--federation|--sibling] jobs\nshow [--federation] steps\ncompleting\n\n\n\nThe following scontrol options are handled in a federation. If the command is\nrun from a cluster other than the federated cluster it will be routed to the\norigin cluster.\n\nhold\nuhold\nrelease\nrequeue\nrequeuehold\nsuspend\nupdate job\n\n\n\nAll other scontrol options should be directed to the specific cluster either by\nissuing the command on the cluster or using the --cluster/-M  option.\n\nGlossary\n\n\nFederated Job: A job that is submitted to the federated\n\t\tcluster. It has a unique job ID across all clusters (Origin\n\t\tCluster ID + Local Job ID).\n\t\n\nSibling Job: A copy of the federated job that is\n\t\tsubmitted to other federated clusters.\n\t\n\nLocal Cluster: The cluster found in the slurm.conf that\n\t\tthe commands will talk to by default.\n\t\n\nOrigin Cluster: The cluster that the federated job was\n\t\toriginally submitted to. The origin cluster submits sibling jobs\n\t\tto other clusters in the federation. The origin cluster\n\t\tdetermines whether a sibling job can run or not. Communications\n\t\tfor the federated job are routed through the origin cluster.\n\t\n\nSibling Cluster: The cluster that is associated with a\n\t\tsibling job.\n\t\n\nOrigin Job: The federated job that resides on the cluster\n\t\tthat it was originally submitted to.\n\t\n\nRevoked (RV) State: The state that the origin job is in\n\t\twhile the origin job is not actively being scheduled on the\n\t\torigin cluster (e.g. not a viable sibling or one of the sibling\n\t\tjobs is running on a remote cluster). Or the state that a remote\n\t\tsibling job is put in when another sibling is allocated nodes.\n\t\n\nViable Sibling: a cluster that is eligible to run a\n\t\tsibling job based off of the requested clusters, cluster\n\t\tfeatures and state of the cluster (e.g. active, draining, etc.).\n\t\n\nActive Sibling: a sibling job that actively has a\n\t\tsibling job and is able to schedule the job.\n\t\n\nLimitations\n\n\n\nA federated job that fails due to resources (partition, node counts,\n\t\tetc.) on the local cluster will be rejected and won't be\n\t\tsubmitted to other sibling clusters even if it could run on\n\t\tthem.\nJob arrays only run on the cluster that they were submitted to.\nJob modification must succeed on the origin cluster for the changes\n\t\tto be pushed to the sibling jobs on remote clusters. \nModifications to anything other than jobs are disabled in sview.\nsview grid is disabled in a federated view.\n\nLast modified 9 June 2021\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Configuration\n\n",
                "content": "\nA federation is created using the sacctmgr command to create a federation in the\ndatabase and by adding clusters to a federation.\n\nsacctmgr add federation <federation_name> [clusters=<list_of_clusters>]\nNOTE\nsacctmgr modify federation <federation_name> set clusters[+-]=<list_of_clusters>\nsacctmgr modify cluster <cluster_name> set federation=<federation_name>\nsacctmgr modify federation <federation_name> set clusters=\nsacctmgr modify cluster <cluster_name> set federation=\nNOTE\nsacctmgr delete federation <federation_name>\n--cluster-constraint=[!]<feature_list>\nsacctmgr modify cluster <cluster_name> set features[+-]=<feature_list>\n\nsacctmgr modify cluster <cluster_name> set fedstate=<state>\n\nACTIVE: Cluster will actively accept and schedule federated\n\t\tjobs\nINACTIVE: Cluster will not schedule or accept any jobs\nDRAIN: Cluster will not accept any new jobs and will let\n\t\texisting federated jobs complete\nDRAIN+REMOVE: Cluster will not accept any new jobs and will\n\t\tremove itself from the federation once all federated jobs have\n\t\tcompleted. When removed from the federation, the cluster will\n\t\taccept jobs as a non-federated cluster\n\nsacctmgr show federation [tree]\nsacctmgr show cluster withfed\n\nscontrol show federation\n\nFederationParameters=fed_display\nFederated Job IDs\nBits 0-25:  Local Job ID\nBits 26-31: Cluster Origin ID\nJob Submission\n\n\nWhen a federated cluster receives a job submission, it will submit copies of the\njob (sibling jobs) to each eligible cluster. Each cluster will then\nindependently attempt to schedule the job.\n\nJobs can be directed to specific clusters in the federation using the\n-M,--clusters=<cluster_list> and the new\n--cluster-constraint=[!]<constraint_list> options.\n\nUsing the -M,--clusters=<cluster_list> the submission command\n(sbatch, salloc, srun) will pick one cluster from the list of clusters to submit\nthe job to and will also pass along the list of clusters with the job. The\nclusters in the list will be the only viable clusters that siblings jobs can be\nsubmitted to. For example the submission:\n\ncluster1$ sbatch -Mcluster2,cluster3 script.sh\n\nwill submit the job to either cluster2 or cluster3 and will only submit sibling\njobs to cluster2 and cluster3 even if there are more clusters in the federation.\n\nUsing the --cluster-constraint=[!]<constraint_list> option will\nsubmit sibling jobs to only the clusters that have the requested cluster\nfeature(s) -- or don't have the feature(s) if using !. Cluster features\nare added using the sacctmgr modify cluster <cluster_name> set\nfeatures[+-]=<feature_list> option.\n\nNOTE: When using the ! option, add quotes around the option to\nprevent the shell from interpreting the ! (e.g\n--cluster-constraint='!highmem').\n\nWhen using both the --cluster-constraint= and\n--clusters= options together, the origin cluster will only submit\nsibling jobs to clusters that meet both requirements.\n\nHeld or dependent jobs are kept on the origin cluster until they are released\nor are no longer dependent, at which time they are submitted to other viable\nclusters in the federation. If a job becomes held or dependent\nafter being submitted, the job is removed from every cluster but the origin.\nJob Scheduling\n\n\nEach cluster in the federation independently attempts to schedule each job with\nthe exception of coordinating with the origin cluster (cluster where the\njob was submitted to) to allocate resources to a federated job. When a cluster\ndetermines it can attempt to allocate resources for a job it communicates with\nthe origin cluster to verify that no other cluster is attempting to allocate\nresources at the same time. If no other cluster is attempting to allocate\nresources the cluster will attempt to allocate resources for the job. If it\nsucceeds then it will notify the origin cluster that it started the job and the\norigin cluster will notify the clusters with sibling jobs to remove the sibling\njobs and put them in a revoked state. If the cluster was unable to\nallocate resources to the job then it lets the origin cluster know so that other\nclusters can attempt to schedule the job. If it was the main scheduler\nattempting to allocate resources then the main scheduler will stop looking at\nfurther jobs in the job's partition. If it was the backfill scheduler attempting\nto allocate resources then the resources will be reserved for the job.\n\nIf an origin cluster is down, then the remote siblings will coordinate with a\njob's viable siblings to schedule the job. When the origin cluster comes back\nup, it will sync with the other siblings.\nJob Requeue\n\n\nWhen a federated job is requeued the origin cluster is notified and the origin\ncluster will then submit new sibling jobs to viable clusters and the federated\njob is eligible to start on a different cluster than the one it ran on.\n\nslurm.conf options RequeueExit and RequeueExitHold are controlled\nby the origin cluster.\nInteractive Jobs\n\n\nInteractive jobs -- jobs submitted with srun and salloc -- can be submitted to\nthe local cluster and get an allocation from a different cluster. When an salloc\njob allocation is granted by a cluster other than the local cluster, a new\nenvironment variable, SLURM_WORKING_CLUSTER, will be set with the remote sibling\ncluster's IP address, port and RPC version so that any sruns will know which\ncluster to communicate with.\n\nNOTE: It is required that all compute nodes must be accessible to all\nsubmission hosts for this to work.\n\nNOTE: The current implementation of the MPI interfaces in Slurm require\nthe SlurmdSpooldir to be the same on the host where the srun is being run as it\nis on the compute nodes in the allocation. If they aren't, a workaround is to\nget an allocation that puts the user on the actual compute node. Then the sruns\non the compute nodes will be using the slurm.conf that corresponds to the\ncorrect cluster. Setting LaunchParameters=use_interactive_step\nslurm.conf will put the user on an actual compute node when using salloc.\nCanceling Jobs\n\n\nCancel requests in the federation will cancel the running sibling job or all\npending sibling jobs. Specific pending sibling jobs can be removed by using\nscancel's --sibling=<cluster_name> option to remove the\nsibling job from the job's active sibling list.\nJob Modification\n\nJob ArraysStatus Commands\n\n\nBy default, status commands, such as: squeue, sinfo, sprio, sacct, sreport, will\nshow a view local to the local cluster. A unified view of the jobs in the\nfederation can be viewed using the --federation option to each status\ncommand. The --federation command causes the status command to first\ncheck if the local cluster is part of a federation. If it is then the command\nwill query each cluster in parallel for job info and will combine the\ninformation into one unified view.\n\nA new FederationParameters=fed_display slurm.conf parameter has been\nadded so that all status commands will present a federated view by default --\nequivalent to setting the --federation option for each status command.\nThe federated view can be overridden using the --local option. Using the\n--clusters,-M option will also override the federated view and give a\nlocal view for the given cluster(s).\n\nUsing the existing --clusters,-M option, the status commands will output\nthe information in the same format that exists today where each cluster's\ninformation is listed separately.\nsqueue\nsqueue also has a new --sibling option that will show each sibling job\nrather than merge them into one.\n\nSeveral new long format options have been added to display the job's federated\ninformation:\n\n\ncluster: Name of the cluster that is running the job or\n\t\tjob step.\n\t\n\nsiblingsactive: Cluster names of where federated sibling\n\t\tjobs exist.\n\t\n\nsiblingsactiveraw: Cluster IDs of where federated\n\t\tsibling jobs exist.\n\t\n\nsiblingsviable: Cluster names of where federated sibling\n\t\tjobs are viable to run.\n\t\n\nsiblingsviableraw: Cluster names of where federated\n\t\tsibling jobs are viable to run.\n\t\n\n\nsqueue output can be sorted using the -S cluster option.\nsinfo\nsinfo will show the partitions from each cluster in one view. In a federated\nview, the cluster name is displayed with each partition. The cluster name can be\nspecified in the format options using the short format %V or the long\nformat cluster options. The output can be sorted by cluster names using\nthe -S %[+-]V option.\nsprio\nIn a federated view, sprio displays the job information from the local cluster\nor from the first cluster to report the job. Since each sibling job could have a\ndifferent priority on each cluster it may be helpful to use the --sibling\noption to show all records of a job to get a better picture of a job's priority.\nThe name of the cluster reporting the job record can be displayed using the\n%c format option. The cluster name is shown by default when using\n--sibling option.\nsacct\nBy default, sacct will not display \"revoked\" jobs and will show the job from the\ncluster that ran the job. However, \"revoked\" jobs can be viewed using the\n--duplicate/-D option.\nsreport\nsreport will combine the reports from each cluster and display them as one.\nscontrol\nThe following scontrol options will display a federated view:\n\nshow [--federation|--sibling] jobs\nshow [--federation] steps\ncompleting\n\n\nThe following scontrol options are handled in a federation. If the command is\nrun from a cluster other than the federated cluster it will be routed to the\norigin cluster.\n\nhold\nuhold\nrelease\nrequeue\nrequeuehold\nsuspend\nupdate job\n\n\nAll other scontrol options should be directed to the specific cluster either by\nissuing the command on the cluster or using the --cluster/-M  option.\nGlossary\n\nFederated Job: A job that is submitted to the federated\n\t\tcluster. It has a unique job ID across all clusters (Origin\n\t\tCluster ID + Local Job ID).\n\t\n\nSibling Job: A copy of the federated job that is\n\t\tsubmitted to other federated clusters.\n\t\n\nLocal Cluster: The cluster found in the slurm.conf that\n\t\tthe commands will talk to by default.\n\t\n\nOrigin Cluster: The cluster that the federated job was\n\t\toriginally submitted to. The origin cluster submits sibling jobs\n\t\tto other clusters in the federation. The origin cluster\n\t\tdetermines whether a sibling job can run or not. Communications\n\t\tfor the federated job are routed through the origin cluster.\n\t\n\nSibling Cluster: The cluster that is associated with a\n\t\tsibling job.\n\t\n\nOrigin Job: The federated job that resides on the cluster\n\t\tthat it was originally submitted to.\n\t\n\nRevoked (RV) State: The state that the origin job is in\n\t\twhile the origin job is not actively being scheduled on the\n\t\torigin cluster (e.g. not a viable sibling or one of the sibling\n\t\tjobs is running on a remote cluster). Or the state that a remote\n\t\tsibling job is put in when another sibling is allocated nodes.\n\t\n\nViable Sibling: a cluster that is eligible to run a\n\t\tsibling job based off of the requested clusters, cluster\n\t\tfeatures and state of the cluster (e.g. active, draining, etc.).\n\t\n\nActive Sibling: a sibling job that actively has a\n\t\tsibling job and is able to schedule the job.\n\t\nLimitations\n\n\nA federated job that fails due to resources (partition, node counts,\n\t\tetc.) on the local cluster will be rejected and won't be\n\t\tsubmitted to other sibling clusters even if it could run on\n\t\tthem.\nJob arrays only run on the cluster that they were submitted to.\nJob modification must succeed on the origin cluster for the changes\n\t\tto be pushed to the sibling jobs on remote clusters. \nModifications to anything other than jobs are disabled in sview.\nsview grid is disabled in a federated view.\nLast modified 9 June 2021"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/jobcomp_kafka.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Job Completion Kafka plugin guide",
                "content": "When configured, the jobcomp/kafka plugin attempts to publish a subset\nof the fields for each completed job record to a Kafka server.RequirementsThe plugin serializes the subset of fields to JSON before each produce\nattempt. The serialization is done using the Slurm serialization plugins, so the\nlibjson-c development files are an indirect prerequisite for this plugin.\nThe plugin offloads part of the client producer work to librdkafka and\nconsumes its API, thus the library development files are another prerequisite.\n\nlibrdkafka\n\t\t development files\nlibjson-c development\n\t\t files\nConfigurationThe plugin is configured with the following\nslurm.conf options:\n\nJobCompType\nShould be set to jobcomp/kafka.\nJobCompType=jobcomp/kafka\n\n\nJobCompLoc This string\nrepresents an absolute path to a file containing 'key=value' pairs configuring\n\n\tlibrdkafka properties. For the plugin to work properly, this file\nneeds to exist and at least the bootstrap.servers property needs to be\nbe configured.\nJobCompLoc=/arbitrary/path/to/rdkafka.conf\nNOTE: There is no default value for JobCompLoc when this plugin is\nconfigured, and thus it needs to be explicitly set.\nNOTE: The librdkafka parameters configured in the file\nreferenced by this option take effect upon slurmctld restart.\nNOTE: The plugin doesn't validate these parameters, but just logs\nan error and fails if any parameter passed to the library API function\nrd_kafka_conf_set() fails.\nAn example configuration file could look like this:\n\nbootstrap.servers=kafkahost1:9092\ndebug=broker,topic,msg\nlinger.ms=400\nlog_level=7\n\n\n\nJobCompParams Comma\nseparated list of extra configurable parameters. Please refer to the slurm.conf\nman page for specific details. Example:\nJobCompParams=flush_timeout=200,poll_interval=3,requeue_on_msg_timeout,\ntopic=mycluster\nNOTE: Changes to this option do not require a slurmctld restart.\nReconfiguration or SIGHUP is sufficient for them to take effect.\n\n\nDebugFlags Optional\nJobComp debug flag for extra plugin specific logging.\nDebugFlags=JobComp\n\nPlugin FunctionalityFor each finished job, the plugin jobcomp_p_log_record() operation is\nexecuted. A subset of the job record fields are serialized into a JSON string\nvia the Slurm serialization plugins. Then the serialized string is attempted\nto be produced using the librdkafka rd_kafka_producev() API call.Producing a message to librdkafka can be done even if the Kafka server is\ndown. But an error returned from this call makes it so the message is discarded.\nProduced messages accumulate in the librdkafka out queue for up to \"linger.ms\"\nmilliseconds (a configurable librdkafka parameter) before building\na message set from the accumulated messages.Then the librdkafka library transmits a produce request. While no \"ACK\" is\nreceived, messages are conceptually considered to be \"in-flight\" according to\nthe library documentation. The library then receives a produce response, which\ncan be handled in one of two ways:\nretriable error\nThe library will automatically attempt a retry if\nno library limit parameter is hit.\npermanent error or success\nThe message will be removed\nfrom the library out queue and is staged to the library delivery report queue.\nThe following diagram illustrates the functionality being described:The jobcomp/kafka plugin has a background poll handler thread that\nperiodically calls the librdkafka API rd_kafka_poll() function. How frequent\nthe thread makes the call is configurable via JobCompParams=poll_interval.\nThe call makes it so that messages in the library delivery report queue are\npulled and handled back to the plugin delivery report callback, which takes\ndifferent actions depending on the error message the library set. By default,\nsuccessful messages are just logged if DebugFlags=JobComp is enabled, and\nmessages with permanent errors are discarded, unless the error is message timed\nout and JobCompParams is configured with \"requeue_on_msg_timeout\", which would\ninstruct the callback to attempt to produce the message again.On plugin termination, the fini() operation is executed. The rd_kafka_purge()\nlibrary API function is called which purges librdkafka out queue messages. The\nrd_kafka_flush() API call is also called, which waits until all outstanding\nproduce requests (and potentially other types of requests) are completed.\nHow much to wait is also configurable via JobCompParams=flush_timeout\nparameter. Purged messages are always saved to the plugin state file in the\nSlurm StateSaveLocation, and messages purged while \"in-flight\" are discarded.\nNOTE: You must be using librdkafka v1.0.0 or later in order to take\nadvantage of the purge functionality described above. With previous versions\nthe outgoing queue can not be purged to the state file on shutdown, which\nmeans that any messages that weren't delivered before the termination of the\nkafka plugin will be lost.On plugin initialization, after parsing the configuration, saved messages in\nthe state are loaded and attempted to be produced again. So undelivered messages\nshould be resilient to slurmctld restarts.The Kafka broker \"host:port\" list should be explicitly configured in the\nfile referenced by JobCompLoc option explained above. The default topic is\nthe configured Slurm ClusterName, but it can also be configured via\nJobCompParams=topic parameter.The jobcomp/kafka plugin mostly logs informational messages to the\nJobComp DebugFlag, except for error messages. The librdkafka by default logs\nto the application stderr, but the plugin configures the library to forcefully\nlog to syslog instead. The library logging level and debug contexts are also\nconfigurable via the file referenced by JobCompLoc, as well as the rest of the\nlibrary configuration parameters.Last modified 22 February 2023"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/select_design.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Select Plugin Design Guide",
                "content": "OverviewThe select plugin is responsible for selecting compute resources to be\nallocated to a job, plus allocating and deallocating those resources.\nThe select plugin is aware of the systems topology, based upon data structures\nestablished by the topology plugin. It can also over-subscribe resources to\nsupport gang scheduling (time slicing of parallel jobs), if so configured.\nOther architectures would rely upon the select/linear\nor select/cons_tres plugins. The select/linear plugin allocates\nwhole nodes to jobs and is the simplest implementation.\nThe select/cons_tres plugin (cons_tres is an abbreviation for\ntrackable resources) can allocate individual sockets, cores, threads\nor CPUs within a node. It also includes the ability to manage other generic\nresources, such as GPUs.\nThe select/cons_tres plugin is slightly slower than\nselect/linear, but contains far more complex logic.Mode of OperationThe select/linear and select/cons_tres plugins have\nsimilar modes of operation. The obvious difference is that data structures\nin select/linear are node-centric, while those in\nselect/cons_tres contain information at a finer resolution (sockets, cores,\nthreads, or CPUs depending upon the SelectTypeParameters configuration\nparameter). The description below is generic and applies to the above two\nplugin implementations. Note that each of these plugins is able to manage\nmemory allocations. If you need to track other resources, such as GPUs,\nyou should use the select/cons_tres plugin.Per node data structures include memory (configured and allocated),\nGRES (configured and allocated, in a List data structure), plus a flag\nindicating if the node has been allocated using an exclusive option (preventing\nother jobs from being allocated resources on that same node). The other key\ndata structure is used to enforce the per-partition OverSubscribe\nconfiguration parameter and tracks how many jobs have been allocated each\ncompute resource (e.g. CPU) in each\npartition. This data structure is different between the plugins based upon\nthe resolution of the resource allocation (e.g. nodes or CPUs).Most of the logic in the select plugin is dedicated to identifying resources\nto be allocated to a new job. Input to that function includes: a pointer to the\nnew job, a bitmap identifying nodes which could be used, node counts (minimum,\nmaximum, and desired), a count of how many jobs of that partition the job can\nshare resources with, and a list of jobs which can be preempted to initiate the\nnew job. The first phase is to determine of all usable nodes, which nodes\nwould best satisfy the resource requirement. This consists of a best-fit\nalgorithm that groups nodes based upon network topology (if the topology/tree\nplugin is configured) or based upon consecutive nodes (by default). Once the\nbest nodes are identified, resources are accumulated for the new job until its\nresource requirements are satisfied.If the job can not be started with currently available resources, the plugin\nwill attempt to identify jobs which can be preempted in order to initiate the\nnew job. A copy of the current system state will be created including details\nabout all resources and active jobs. Preemptable jobs will then be removed\nfrom this simulated system state until the new job can be initiated. When\nsufficient resources are available for the new job, the jobs actually needing\nto be preempted for its initiation will be preempted (this may be a subset of\nthe jobs whose preemption is simulated).Other functions exist to support suspending jobs, resuming jobs, terminating\njobs, shrinking job allocations, un/packing job state information,\nun/packing node state information, etc. The operation of those functions is\nrelatively straightforward and not detailed here.Last modified 29 January 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/job_launch.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Job Launch Design Guide",
                "content": "OverviewThis guide describes at a high level the processes which occur in order\nto initiate a job including the daemons and plugins involved in the process.\nIt describes the process of job allocation, step allocation, task launch and\njob termination. The functionality of tens of thousands of lines of code\nhas been distilled here to a couple of pages of text, so much detail is\nmissing.Job Allocation\n\nThe first step of the process is to create a job allocation, which is\na claim on compute resources. A job allocation can be created using the\nsalloc, sbatch or srun command. The salloc and\nsbatch commands create resource allocations while the srun\ncommand will create a resource allocation (if not already running within one)\nplus launch tasks. Each of these commands will fill in a data structure\nidentifying the specifications of the job allocation requirement (e.g. node\ncount, task count, etc.) based upon command line options and environment\nvariables and send the RPC to the slurmctld daemon. The UID and GID of\nthe user launching the job will be included in a credential which will be used\nlater to restrict access to the job, so further steps run in the allocation\nwill need to be launched using the same UID and GID as the one used to create\nthe allocation. If the new job\nrequest is the highest priority, the slurmctld daemon will attempt\nto select resources for it immediately, otherwise it will validate that the job\nrequest can be satisfied at some time and queue the request. In either case\nthe request will receive a response almost immediately containing one of the\nfollowing:\nA job ID and the resource allocation specification (nodes, cpus, etc.)\nA job ID and notification of the job being in a queued state OR\nAn error code\nThe process of selecting resources for a job request involves multiple steps,\nsome of which involve plugins. The process is as follows:\nCall job_submit plugins to modify the request as appropriate\nValidate that the options are valid for this user (e.g. valid partition\nname, valid limits, etc.)\nDetermine if this job is the highest priority runnable job, if so then\nreally try to allocate resources for it now, otherwise only validate that it\ncould run if no other jobs existed\nDetermine which nodes could be used for the job. If the feature\nspecification uses an exclusive OR option, then multiple iterations of the\nselection process below will be required with disjoint sets of nodes\nCall the select plugin to select the best resources for the request\nThe select plugin will consider network topology and the topology within\na node (e.g. sockets, cores, and threads) to select the best resources for the\njob\nIf the job can not be initiated using available resources and preemption\nsupport is configured, the select plugin will also determine if the job\ncan be initiated after preempting lower priority jobs. If so then initiate\npreemption as needed to start the job\nStep Allocation\n\nThe srun command is always used for job step creation. It fills in\na job step request RPC using information from the command line and environment\nvariables then sends that request to the slurmctld daemon. It is\nimportant to note that many of the srun options are intended for job\nallocation and are not supported by the job step request RPC (for example the\nsocket, core and thread information is not supported). If a job step uses\nall of the resources allocated to the job then the lack of support for some\noptions is not important. If one wants to execute multiple job steps using\nvarious subsets of resources allocated to the job, this shortcoming could\nprove problematic. It is also worth noting that the logic used to select\nresources for a job step is relatively simple and entirely contained within\nthe slurmctld daemon code (the select plugin is not used for job\nsteps). If the request can not be immediately satisfied due to a request for\nexclusive access to resources, the appropriate error message will be sent and\nthe srun command will retry the request on a periodic basis.\n(NOTE: It would be desirable to queue the job step requests to support\njob step dependencies and better performance in the initiation of job steps,\nbut that is not currently supported.)\nIf the request can be satisfied, the response contains a digitally signed\ncredential (by the cred plugin) identifying the resources to be used.Task Launch\n\nThe srun command builds a task launch request data structure\nincluding the credential, executable name, file names, etc. and sends it to\nthe slurmd daemon on node zero of the job step allocation. The\nslurmd daemon validates the signature and forwards the request to the\nslurmd daemons on other nodes to launch tasks for that job step. The\ndegree of fanout in this message forwarding is configurable using the\nTreeWidth parameter. Each slurmd daemon tests that the job has\nnot been cancelled since the credential was issued (due to a possible race \ncondition) and spawns a slurmstepd program to manage the job step.\nNote that the slurmctld daemon is not directly involved in task\nlaunch in order to minimize the overhead on this critical resource.Each slurmstepd program executes a single job step.\nBesides the functions listed below, the slurmstepd program also\nexecutes several SPANK plugin functions at various times.\nPerforms MPI setup (using the appropriate plugin)\nCalls the switch plugin to perform any needed network configuration\nCreates a container for the job step using a proctrack plugin\nChange user ID to that of the user\nConfigures I/O for the tasks (either using files or a socket connection back\nto the srun command\nSets up environment variables for the tasks including many task-specific\nenvironment variables\nFork/exec the tasks\nJob Step Termination\n\nThere are several ways in which a job step or job can terminate, each with\nslight variation in the logic executed. The simplest case is if the tasks run\nto completion. The srun will note the termination of output from the\ntasks and notify the slurmctld daemon that the job step has completed.\nslurmctld will simply log the job step termination. The job step can\nalso be explicitly cancelled by a user, reach the end of its time limit, etc.\nand those follow a sequence of steps very similar to that for job termination,\nwhich is described below.Job Termination\n\nJob termination can either be user initiated (e.g. scancel command) or system\ninitiated (e.g. time limit reached). The termination ultimately requires\nthe slurmctld daemon to notify the slurmd daemons on allocated\nnodes that the job is to be ended. The slurmd daemon does the following:\n\nSend a SIGCONT and SIGTERM signal to any user tasks\nWait KillWait seconds if there are any user tasks\nSend a SIGKILL signal to any user tasks\nWait for all tasks to complete\nExecute any Epilog program\nSend an epilog_complete RPC to the slurmctld daemon\n\nJob Accounting Records\n\n\nWhen Slurm is configured to use SlurmDBD to store job records (i.e.\nAccountingStorageType=accounting_storage=slurmdbd), there are multiple\nrecords that get stored for each job. There is a record for the job as a\nwhole as well as entries for the following types of job steps:\n\nextern step \u2014 A step created for each job as long as you have\nPrologFlags=contain in your slurm.conf. Each node in the job will\nhave a slurmstepd process created for the extern step.\npam_slurm_adopt uses this step to contain\nexternal connections.\nbatch step \u2014 A step created for jobs that were submitted with\nsbatch. The batch host, or the primary node for the job, will run an instance\nof slurmstepd for the batch step, which is used to run the script provided\nto sbatch.\ninteractive step \u2014 A step created for jobs that were\nsubmitted with salloc when LaunchParameters=use_interactive_step is\nconfigured in your slurm.conf. The node on which you have the interactive\nshell will run an instance of slurmstepd to run the shell or the command\nprovided to salloc.\nnormal step \u2014 A job can have multiple normal steps, which will\nappear in sacct as <job_id>.<step_id>. These steps\nare created when srun is called from inside the job and the slurmstepd created\nwill run the command passed to srun. Each step will have one instance of\nslurmstepd created per node in the step and each instance of slurmstepd can\nrun multiple tasks in the same step.\n\nLast modified 1 August 2022\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Job Accounting Records\n\n",
                "content": "When Slurm is configured to use SlurmDBD to store job records (i.e.\nAccountingStorageType=accounting_storage=slurmdbd), there are multiple\nrecords that get stored for each job. There is a record for the job as a\nwhole as well as entries for the following types of job steps:\n\nextern step \u2014 A step created for each job as long as you have\nPrologFlags=contain in your slurm.conf. Each node in the job will\nhave a slurmstepd process created for the extern step.\npam_slurm_adopt uses this step to contain\nexternal connections.\nbatch step \u2014 A step created for jobs that were submitted with\nsbatch. The batch host, or the primary node for the job, will run an instance\nof slurmstepd for the batch step, which is used to run the script provided\nto sbatch.\ninteractive step \u2014 A step created for jobs that were\nsubmitted with salloc when LaunchParameters=use_interactive_step is\nconfigured in your slurm.conf. The node on which you have the interactive\nshell will run an instance of slurmstepd to run the shell or the command\nprovided to salloc.\nnormal step \u2014 A job can have multiple normal steps, which will\nappear in sacct as <job_id>.<step_id>. These steps\nare created when srun is called from inside the job and the slurmstepd created\nwill run the command passed to srun. Each step will have one instance of\nslurmstepd created per node in the step and each instance of slurmstepd can\nrun multiple tasks in the same step.\n\nLast modified 1 August 2022\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/gang_scheduling.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Gang Scheduling",
                "content": "\nSlurm supports timesliced gang scheduling in which two or more jobs are\nallocated to the same resources in the same partition and these jobs are\nalternately suspended to let one job at a time have dedicated access to the\nresources for a configured period of time.\n\nSlurm also supports preemptive job scheduling that allows a job in a\nhigher PriorityTier partition, or in a preempting QOS, to preempt other\njobs. Preemption is related to Gang scheduling\nbecause SUSPEND is one of the PreemptionModes, and it uses the Gang\nscheduler to resume suspended jobs.\n\nA workload manager that supports timeslicing can improve responsiveness\nand utilization by allowing more jobs to begin running sooner.\nShorter-running jobs no longer have to wait in a queue behind longer-running\njobs.\nInstead they can be run \"in parallel\" with the longer-running jobs, which will\nallow them to start and finish quicker.\nThroughput is also improved because overcommitting the resources provides\nopportunities for \"local backfilling\" to occur (see example below).\n\nThe gang scheduling logic works on each partition independently.\nIf a new job has been allocated to resources in a partition that have already\nbeen allocated to an existing job, then the plugin will suspend the new job\nuntil the configured SchedulerTimeslice interval has elapsed.\nThen it will suspend the running job and let the new job make use of the\nresources for a SchedulerTimeslice interval.\nThis will continue until one of the jobs terminates.\n\nNOTE: Heterogeneous jobs are excluded from gang scheduling operations.\nConfiguration\nThere are several important configuration parameters relating to\ngang scheduling:\n\n\nSelectType: The Slurm gang scheduler supports nodes allocated by the\nselect/linear plugin, socket/core/CPU resources allocated by the\nselect/cons_tres plugin.\n\n\nSelectTypeParameters: Since resources will be getting overallocated\nwith jobs (suspended jobs remain in memory), the resource selection plugin\nshould be configured to track the amount of memory used by each job to ensure\nthat memory page swapping does not occur.\nWhen select/linear is chosen, we recommend setting\nSelectTypeParameters=CR_Memory.\nWhen select/cons_tres is chosen, we recommend\nincluding Memory as a resource\n(e.g. SelectTypeParameters=CR_Core_Memory).\n\n\nDefMemPerCPU: Since job requests may not explicitly specify\na memory requirement, we also recommend configuring\nDefMemPerCPU (default memory per allocated CPU) or\nDefMemPerNode (default memory per allocated node).\nIt may also be desirable to configure\nMaxMemPerCPU (maximum memory per allocated CPU) or\nMaxMemPerNode (maximum memory per allocated node) in slurm.conf.\nUsers can use the --mem or --mem-per-cpu option\nat job submission time to specify their memory requirements.\nNote that in order to gang schedule jobs, all jobs must be able to fit into\nmemory at the same time.\n\n\nJobAcctGatherType and JobAcctGatherFrequency:\nIf you wish to enforce memory limits, either that task/cgroup must be\nconfigured to limit each job's memory use or accounting must be enabled\nusing the JobAcctGatherType and JobAcctGatherFrequency\nparameters. If accounting is enabled and a job exceeds its configured\nmemory limits, it will be canceled in order to prevent it from\nadversely affecting other jobs sharing the same resources.\n\n\nPreemptMode: set the GANG option.\nSee the slurm.conf manpage for other options that may be specified to\nenable job preemption in addition to GANG.\nIn order to use gang scheduling, the GANG option must be specified at\nthe cluster level.\n\nNOTE: Gang scheduling is performed independently for each partition, so\nif you only want time-slicing by OverSubscribe, without any preemption,\nthen configuring partitions with overlapping nodes is not recommended.\nOn the other hand, if you want to use PreemptType=preempt/partition_prio\nto allow jobs from higher PriorityTier partitions to Suspend jobs from\nlower PriorityTier partitions, then you will need overlapping partitions,\nand PreemptMode=SUSPEND,GANG to use the Gang scheduler to resume the\nsuspended job(s).\nIn any case, time-slicing won't happen between jobs on different partitions.\n\n\nSchedulerTimeSlice: The default timeslice interval is 30 seconds.\nTo change this duration, set SchedulerTimeSlice to the desired interval\n(in seconds) in slurm.conf. For example, to set the timeslice interval\nto one minute, set SchedulerTimeSlice=60. Short values can increase\nthe overhead of gang scheduling.\n\n\nOverSubscribe: Configure the partition's OverSubscribe setting to\nFORCE for all partitions in which timeslicing is to take place.\nThe FORCE option supports an additional parameter that controls\nhow many jobs can share a compute resource (FORCE[:max_share]). By default the\nmax_share value is 4. To allow up to 6 jobs from this partition to be\nallocated to a common resource, set OverSubscribe=FORCE:6. To only let 2 jobs\ntimeslice on the same resources, set OverSubscribe=FORCE:2.\n\n\nIn order to enable gang scheduling after making the configuration changes\ndescribed above, restart Slurm if it is already running. Any change to the\nplugin settings in Slurm requires a full restart of the daemons. If you\njust change the partition OverSubscribe setting, this can be updated with\nscontrol reconfig.\nTimeslicer Design and Operation\n\n\nWhen enabled, the gang scheduler keeps track of the resources\nallocated to all jobs. For each partition an \"active bitmap\" is maintained that\ntracks all concurrently running jobs in the Slurm cluster. Each time a new\njob is allocated to resources in a partition, the gang scheduler\ncompares these newly allocated resources with the resources already maintained\nin the \"active bitmap\".\nIf these two sets of resources are disjoint then the new job is added to the \"active bitmap\". If these two sets of resources overlap then\nthe new job is suspended. All jobs are tracked in a per-partition job queue\nwithin the gang scheduler logic.\n\nA separate timeslicer thread is spawned by the gang scheduler\non startup. This thread sleeps for the configured SchedulerTimeSlice\ninterval. When it wakes up, it checks each partition for suspended jobs. If\nsuspended jobs are found then the timeslicer thread moves all running\njobs to the end of the job queue. It then reconstructs the \"active bitmap\" for\nthis partition beginning with the suspended job that has waited the longest to\nrun (this will be the first suspended job in the run queue). Each following job\nis then compared with the new \"active bitmap\", and if the job can be run\nconcurrently with the other \"active\" jobs then the job is added. Once this is\ncomplete then the timeslicer thread suspends any currently running jobs\nthat are no longer part of the \"active bitmap\", and resumes jobs that are new\nto the \"active bitmap\".\n\nThis timeslicer thread algorithm for rotating jobs is designed to prevent jobs from starving (remaining in the suspended state indefinitely) and\nto be as fair as possible in the distribution of runtime while still keeping\nall of the resources as busy as possible.\n\nThe gang scheduler suspends jobs via the same internal functions that\nsupport scontrol suspend and scontrol resume.\nA good way to observe the operation of the timeslicer is by running\nsqueue -i<time> in a terminal window where time is set\nequal to SchedulerTimeSlice.\nA Simple Example\n\n\nThe following example is configured with select/linear and OverSubscribe=FORCE.\nThis example takes place on a small cluster of 5 nodes:\n\n[user@n16 load]$ sinfo\nPARTITION AVAIL  TIMELIMIT NODES  STATE NODELIST\nactive*      up   infinite     5   idle n[12-16]\n\nHere are the Scheduler settings (excerpt of output):\n\n[user@n16 load]$ scontrol show config\n...\nPreemptMode             = GANG\n...\nSchedulerTimeSlice      = 30\nSchedulerType           = sched/builtin\n...\n\nThe myload script launches a simple load-generating app that runs\nfor the given number of seconds. Submit myload to run on all nodes:\n\n[user@n16 load]$ sbatch -N5 ./myload 300\nsbatch: Submitted batch job 3\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n    3    active  myload  user     0:05     5 n[12-16]\n\nSubmit it again and watch the gang scheduler suspend it:\n\n[user@n16 load]$ sbatch -N5 ./myload 300\nsbatch: Submitted batch job 4\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n    3    active  myload  user  R  0:13     5 n[12-16]\n    4    active  myload  user  S  0:00     5 n[12-16]\n\nAfter 30 seconds the gang scheduler swaps jobs, and now job 4 is the\nactive one:\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n    4    active  myload  user  R  0:08     5 n[12-16]\n    3    active  myload  user  S  0:41     5 n[12-16]\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n    4    active  myload  user  R  0:21     5 n[12-16]\n    3    active  myload  user  S  0:41     5 n[12-16]\n\nAfter another 30 seconds the gang scheduler sets job 3 running again:\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n    3    active  myload  user  R  0:50     5 n[12-16]\n    4    active  myload  user  S  0:30     5 n[12-16]\n\nA possible side effect of timeslicing: Note that jobs that are\nimmediately suspended may cause their srun commands to produce the\nfollowing output:\n\n[user@n16 load]$ cat slurm-4.out\nsrun: Job step creation temporarily disabled, retrying\nsrun: Job step creation still disabled, retrying\nsrun: Job step creation still disabled, retrying\nsrun: Job step creation still disabled, retrying\nsrun: Job step created\n\nThis occurs because srun is attempting to launch a jobstep in an\nallocation that has been suspended. The srun process will continue in a\nretry loop to launch the jobstep until the allocation has been resumed and the\njobstep can be launched.\n\nWhen the gang scheduler is enabled, this type of output in the user\njobs should be considered benign.\nMore examples\nThe following example shows how the timeslicer algorithm keeps the resources\nbusy. Job 10 runs continually, while jobs 9 and 11 are timesliced:\n\n[user@n16 load]$ sbatch -N3 ./myload 300\nsbatch: Submitted batch job 9\n\n[user@n16 load]$ sbatch -N2 ./myload 300\nsbatch: Submitted batch job 10\n\n[user@n16 load]$ sbatch -N3 ./myload 300\nsbatch: Submitted batch job 11\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n    9    active  myload  user  R  0:11     3 n[12-14]\n   10    active  myload  user  R  0:08     2 n[15-16]\n   11    active  myload  user  S  0:00     3 n[12-14]\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n   10    active  myload  user  R  0:50     2 n[15-16]\n   11    active  myload  user  R  0:12     3 n[12-14]\n    9    active  myload  user  S  0:41     3 n[12-14]\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n   10    active  myload  user  R  1:04     2 n[15-16]\n   11    active  myload  user  R  0:26     3 n[12-14]\n    9    active  myload  user  S  0:41     3 n[12-14]\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n    9    active  myload  user  R  0:46     3 n[12-14]\n   10    active  myload  user  R  1:13     2 n[15-16]\n   11    active  myload  user  S  0:30     3 n[12-14]\n\nThe next example displays \"local backfilling\":\n\n[user@n16 load]$ sbatch -N3 ./myload 300\nsbatch: Submitted batch job 12\n\n[user@n16 load]$ sbatch -N5 ./myload 300\nsbatch: Submitted batch job 13\n\n[user@n16 load]$ sbatch -N2 ./myload 300\nsbatch: Submitted batch job 14\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n   12    active  myload  user  R  0:14     3 n[12-14]\n   14    active  myload  user  R  0:06     2 n[15-16]\n   13    active  myload  user  S  0:00     5 n[12-16]\n\nWithout timeslicing and without the backfill scheduler enabled, job 14 has to\nwait for job 13 to finish.\n\nThis is called \"local\" backfilling because the backfilling only occurs with\njobs close enough in the queue to get allocated by the scheduler as part of\noversubscribing the resources. Recall that the number of jobs that can\novercommit a resource is controlled by the OverSubscribe=FORCE:max_share value,\nso this value effectively controls the scope of \"local backfilling\".\n\nNormal backfill algorithms check all jobs in the wait queue.\nConsumable Resource Examples\n\n\nThe following two examples illustrate the primary difference between\nCR_CPU and CR_Core when consumable resource selection is enabled\n(select/cons_tres).\n\nWhen CR_CPU (or CR_CPU_Memory) is configured then the selector\ntreats the CPUs as simple, interchangeable computing resources\nunless task affinity is enabled. However when task affinity is enabled\nwith CR_CPU or CR_Core (or CR_Core_Memory) is enabled, the\nselector treats the CPUs as individual resources that are specifically\nallocated to jobs.\nThis subtle difference is highlighted when timeslicing is enabled.\n\nIn both examples 6 jobs are submitted. Each job requests 2 CPUs per node, and\nall of the nodes contain two quad-core processors. The timeslicer will\ninitially let the first 4 jobs run and suspend the last 2 jobs.\nThe manner in which these jobs are timesliced depends upon the configured\nSelectTypeParameters.\n\nIn the first example CR_Core_Memory is configured. Note that jobs 46\nand 47 don't ever get suspended. This is because they are not sharing\ntheir cores with any other job.\nJobs 48 and 49 were allocated to the same cores as jobs 44 and 45.\nThe timeslicer recognizes this and timeslices only those jobs:\n\n[user@n16 load]$ sinfo\nPARTITION AVAIL  TIMELIMIT NODES  STATE NODELIST\nactive*      up   infinite     5   idle n[12-16]\n\n[user@n16 load]$ scontrol show config | grep Select\nSelectType              = select/cons_tres\nSelectTypeParameters    = CR_CORE_MEMORY\n\n[user@n16 load]$ sinfo -o \"%20N %5D %5c %5z\"\nNODELIST             NODES CPUS  S:C:T\nn[12-16]             5     8     2:4:1\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 44\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 45\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 46\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 47\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 48\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 49\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n   44    active  myload  user  R  0:09     5 n[12-16]\n   45    active  myload  user  R  0:08     5 n[12-16]\n   46    active  myload  user  R  0:08     5 n[12-16]\n   47    active  myload  user  R  0:07     5 n[12-16]\n   48    active  myload  user  S  0:00     5 n[12-16]\n   49    active  myload  user  S  0:00     5 n[12-16]\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n   46    active  myload  user  R  0:49     5 n[12-16]\n   47    active  myload  user  R  0:48     5 n[12-16]\n   48    active  myload  user  R  0:06     5 n[12-16]\n   49    active  myload  user  R  0:06     5 n[12-16]\n   44    active  myload  user  S  0:44     5 n[12-16]\n   45    active  myload  user  S  0:43     5 n[12-16]\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n   44    active  myload  user  R  1:23     5 n[12-16]\n   45    active  myload  user  R  1:22     5 n[12-16]\n   46    active  myload  user  R  2:22     5 n[12-16]\n   47    active  myload  user  R  2:21     5 n[12-16]\n   48    active  myload  user  S  1:00     5 n[12-16]\n   49    active  myload  user  S  1:00     5 n[12-16]\n\nNote the runtime of all 6 jobs in the output of the last squeue command.\nJobs 46 and 47 have been running continuously, while jobs 44 and 45 are\nsplitting their runtime with jobs 48 and 49.\n\nThe next example has CR_CPU_Memory configured and the same 6 jobs are\nsubmitted. Here the selector and the timeslicer treat the CPUs as countable\nresources which results in all 6 jobs sharing time on the CPUs:\n\n[user@n16 load]$ sinfo\nPARTITION AVAIL  TIMELIMIT NODES  STATE NODELIST\nactive*      up   infinite     5   idle n[12-16]\n\n[user@n16 load]$ scontrol show config | grep Select\nSelectType              = select/cons_tres\nSelectTypeParameters    = CR_CPU_MEMORY\n\n[user@n16 load]$ sinfo -o \"%20N %5D %5c %5z\"\nNODELIST             NODES CPUS  S:C:T\nn[12-16]             5     8     2:4:1\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 51\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 52\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 53\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 54\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 55\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 56\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n   51    active  myload  user  R  0:11     5 n[12-16]\n   52    active  myload  user  R  0:11     5 n[12-16]\n   53    active  myload  user  R  0:10     5 n[12-16]\n   54    active  myload  user  R  0:09     5 n[12-16]\n   55    active  myload  user  S  0:00     5 n[12-16]\n   56    active  myload  user  S  0:00     5 n[12-16]\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n   51    active  myload  user  R  1:09     5 n[12-16]\n   52    active  myload  user  R  1:09     5 n[12-16]\n   55    active  myload  user  R  0:23     5 n[12-16]\n   56    active  myload  user  R  0:23     5 n[12-16]\n   53    active  myload  user  S  0:45     5 n[12-16]\n   54    active  myload  user  S  0:44     5 n[12-16]\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n   53    active  myload  user  R  0:55     5 n[12-16]\n   54    active  myload  user  R  0:54     5 n[12-16]\n   55    active  myload  user  R  0:40     5 n[12-16]\n   56    active  myload  user  R  0:40     5 n[12-16]\n   51    active  myload  user  S  1:16     5 n[12-16]\n   52    active  myload  user  S  1:16     5 n[12-16]\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n   51    active  myload  user  R  3:18     5 n[12-16]\n   52    active  myload  user  R  3:18     5 n[12-16]\n   53    active  myload  user  R  3:17     5 n[12-16]\n   54    active  myload  user  R  3:16     5 n[12-16]\n   55    active  myload  user  S  3:00     5 n[12-16]\n   56    active  myload  user  S  3:00     5 n[12-16]\n\nNote that the runtime of all 6 jobs is roughly equal. Jobs 51-54 ran first so\nthey're slightly ahead, but so far all jobs have run for at least 3 minutes.\n\nAt the core level this means that Slurm relies on the Linux kernel to move\njobs around on the cores to maximize performance.\nThis is different than when CR_Core_Memory was configured and the jobs\nwould effectively remain \"pinned\" to their specific cores for the duration of\nthe job.\nNote that CR_Core_Memory supports CPU binding, while\nCR_CPU_Memory does not.\nNote that manually suspending a job (i.e. \"scontrol suspend ...\") releases\nits CPUs for allocation to other jobs.\nResuming a previously suspended job may result in multiple jobs being\nallocated the same CPUs, which could trigger gang scheduling of jobs.\nUse of the scancel command to send SIGSTOP and SIGCONT signals would stop a\njob without releasing its CPUs for allocation to other jobs and would be a\npreferable mechanism in many cases.Last modified 29 January 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/quickstart.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Quick Start User Guide",
                "content": "OverviewSlurm is an open source,\nfault-tolerant, and highly scalable cluster management and job scheduling system\nfor large and small Linux clusters. Slurm requires no kernel modifications for\nits operation and is relatively self-contained. As a cluster workload manager,\nSlurm has three key functions. First, it allocates exclusive and/or non-exclusive\naccess to resources (compute nodes) to users for some duration of time so they\ncan perform work. Second, it provides a framework for starting, executing, and\nmonitoring work (normally a parallel job) on the set of allocated nodes. Finally,\nit arbitrates contention for resources by managing a queue of pending work.ArchitectureAs depicted in Figure 1, Slurm consists of a slurmd daemon running on\neach compute node and a central slurmctld daemon running on a management node\n(with optional fail-over twin).\nThe slurmd daemons provide fault-tolerant hierarchical communications.\nThe user commands include: sacct, sacctmgr, salloc,\nsattach, sbatch, sbcast, scancel, scontrol,\nscrontab, sdiag, sh5util, sinfo, sprio,\nsqueue, sreport, srun, sshare, sstat,\nstrigger and sview.\nAll of the commands can run anywhere in the cluster.\n\n  Figure 1. Slurm components\nThe entities managed by these Slurm daemons, shown in Figure 2, include\nnodes, the compute resource in Slurm,\npartitions, which group nodes into logical (possibly overlapping) sets,\njobs, or allocations of resources assigned to a user for\na specified amount of time, and\njob steps, which are sets of (possibly parallel) tasks within a job.\nThe partitions can be considered job queues, each of which has an assortment of\nconstraints such as job size limit, job time limit, users permitted to use it, etc.\nPriority-ordered jobs are allocated nodes within a partition until the resources\n(nodes, processors, memory, etc.) within that partition are exhausted. Once\na job is assigned a set of nodes, the user is able to initiate parallel work in\nthe form of job steps in any configuration within the allocation. For instance,\na single job step may be started that utilizes all nodes allocated to the job,\nor several job steps may independently use a portion of the allocation.\n\n  Figure 2. Slurm entities\nCommandsMan pages exist for all Slurm daemons, commands, and API functions. The command\noption --help also provides a brief summary of\noptions. Note that the command options are all case sensitive.sacct is used to report job or job\nstep accounting information about active or completed jobs.salloc is used to allocate resources\nfor a job in real time. Typically this is used to allocate resources and spawn a shell.\nThe shell is then used to execute srun commands to launch parallel tasks.sattach is used to attach standard\ninput, output, and error plus signal capabilities to a currently running\njob or job step. One can attach to and detach from jobs multiple times.sbatch is used to submit a job script\nfor later execution. The script will typically contain one or more srun commands\nto launch parallel tasks.sbcast is used to transfer a file\nfrom local disk to local disk on the nodes allocated to a job. This can be\nused to effectively use diskless compute nodes or provide improved performance\nrelative to a shared file system.scancel is used to cancel a pending\nor running job or job step. It can also be used to send an arbitrary signal to\nall processes associated with a running job or job step.scontrol is the administrative tool\nused to view and/or modify Slurm state. Note that many scontrol\ncommands can only be executed as user root.sinfo reports the state of partitions\nand nodes managed by Slurm. It has a wide variety of filtering, sorting, and formatting\noptions.sprio is used to display a detailed\nview of the components affecting a job's priority.squeue reports the state of jobs or\njob steps. It has a wide variety of filtering, sorting, and formatting options.\nBy default, it reports the running jobs in priority order and then the pending\njobs in priority order.srun is used to submit a job for\nexecution or initiate job steps in real time.\nsrun\nhas a wide variety of options to specify resource requirements, including: minimum\nand maximum node count, processor count, specific nodes to use or not use, and\nspecific node characteristics (so much memory, disk space, certain required\nfeatures, etc.).\nA job can contain multiple job steps executing sequentially or in parallel on\nindependent or shared resources within the job's node allocation.sshare displays detailed information\nabout fairshare usage on the cluster. Note that this is only viable when using\nthe priority/multifactor plugin.sstat is used to get information\nabout the resources utilized by a running job or job step.strigger is used to set, get or\nview event triggers. Event triggers include things such as nodes going down\nor jobs approaching their time limit.sview is a graphical user interface to\nget and update state information for jobs, partitions, and nodes managed by Slurm.ExamplesFirst we determine what partitions exist on the system, what nodes\nthey include, and general system state. This information is provided\nby the sinfo command.\nIn the example below we find there are two partitions: debug\nand batch.\nThe * following the name debug indicates this is the\ndefault partition for submitted jobs.\nWe see that both partitions are in an UP state.\nSome configurations may include partitions for larger jobs\nthat are DOWN except on weekends or at night. The information\nabout each partition may be split over more than one line so that\nnodes in different states can be identified.\nIn this case, the two nodes adev[1-2] are down.\nThe * following the state down indicate the nodes are\nnot responding. Note the use of a concise expression for node\nname specification with a common prefix adev and numeric\nranges or specific numbers identified. This format allows for\nvery large clusters to be easily managed.\nThe sinfo command\nhas many options to easily let you view the information of interest\nto you in whatever format you prefer.\nSee the man page for more information.\nadev0: sinfo\nPARTITION AVAIL  TIMELIMIT NODES  STATE NODELIST\ndebug*       up      30:00     2  down* adev[1-2]\ndebug*       up      30:00     3   idle adev[3-5]\nbatch        up      30:00     3  down* adev[6,13,15]\nbatch        up      30:00     3  alloc adev[7-8,14]\nbatch        up      30:00     4   idle adev[9-12]\nNext we determine what jobs exist on the system using the\nsqueue command. The\nST field is job state.\nTwo jobs are in a running state (R is an abbreviation\nfor Running) while one job is in a pending state\n(PD is an abbreviation for Pending).\nThe TIME field shows how long the jobs have run\nfor using the format days-hours:minutes:seconds.\nThe NODELIST(REASON) field indicates where the\njob is running or the reason it is still pending. Typical\nreasons for pending jobs are Resources (waiting\nfor resources to become available) and Priority\n(queued behind a higher priority job).\nThe squeue command\nhas many options to easily let you view the information of interest\nto you in whatever format you prefer.\nSee the man page for more information.\nadev0: squeue\nJOBID PARTITION  NAME  USER ST  TIME NODES NODELIST(REASON)\n65646     batch  chem  mike  R 24:19     2 adev[7-8]\n65647     batch   bio  joan  R  0:09     1 adev14\n65648     batch  math  phil PD  0:00     6 (Resources)\nThe scontrol command\ncan be used to report more detailed information about\nnodes, partitions, jobs, job steps, and configuration.\nIt can also be used by system administrators to make\nconfiguration changes. A couple of examples are shown\nbelow. See the man page for more information.\nadev0: scontrol show partition\nPartitionName=debug TotalNodes=5 TotalCPUs=40 RootOnly=NO\n   Default=YES OverSubscribe=FORCE:4 PriorityTier=1 State=UP\n   MaxTime=00:30:00 Hidden=NO\n   MinNodes=1 MaxNodes=26 DisableRootJobs=NO AllowGroups=ALL\n   Nodes=adev[1-5] NodeIndices=0-4\n\nPartitionName=batch TotalNodes=10 TotalCPUs=80 RootOnly=NO\n   Default=NO OverSubscribe=FORCE:4 PriorityTier=1 State=UP\n   MaxTime=16:00:00 Hidden=NO\n   MinNodes=1 MaxNodes=26 DisableRootJobs=NO AllowGroups=ALL\n   Nodes=adev[6-15] NodeIndices=5-14\n\n\nadev0: scontrol show node adev1\nNodeName=adev1 State=DOWN* CPUs=8 AllocCPUs=0\n   RealMemory=4000 TmpDisk=0\n   Sockets=2 Cores=4 Threads=1 Weight=1 Features=intel\n   Reason=Not responding [slurm@06/02-14:01:24]\n\n65648     batch  math  phil PD  0:00     6 (Resources)\nadev0: scontrol show job\nJobId=65672 UserId=phil(5136) GroupId=phil(5136)\n   Name=math\n   Priority=4294901603 Partition=batch BatchFlag=1\n   AllocNode:Sid=adev0:16726 TimeLimit=00:10:00 ExitCode=0:0\n   StartTime=06/02-15:27:11 EndTime=06/02-15:37:11\n   JobState=PENDING NodeList=(null) NodeListIndices=\n   NumCPUs=24 ReqNodes=1 ReqS:C:T=1-65535:1-65535:1-65535\n   OverSubscribe=1 Contiguous=0 CPUs/task=0 Licenses=(null)\n   MinCPUs=1 MinSockets=1 MinCores=1 MinThreads=1\n   MinMemory=0 MinTmpDisk=0 Features=(null)\n   Dependency=(null) Account=(null) Requeue=1\n   Reason=None Network=(null)\n   ReqNodeList=(null) ReqNodeListIndices=\n   ExcNodeList=(null) ExcNodeListIndices=\n   SubmitTime=06/02-15:27:11 SuspendTime=None PreSusTime=0\n   Command=/home/phil/math\n   WorkDir=/home/phil\nIt is possible to create a resource allocation and launch\nthe tasks for a job step in a single command line using the\nsrun command. Depending\nupon the MPI implementation used, MPI jobs may also be\nlaunched in this manner.\nSee the MPI section for more MPI-specific information.\nIn this example we execute /bin/hostname\non three nodes (-N3) and include task numbers on the output (-l).\nThe default partition will be used.\nOne task per node will be used by default.\nNote that the srun command has\nmany options available to control what resource are allocated\nand how tasks are distributed across those resources.\nadev0: srun -N3 -l /bin/hostname\n0: adev3\n1: adev4\n2: adev5\nThis variation on the previous example executes\n/bin/hostname in four tasks (-n4).\nOne processor per task will be used by default (note that we don't specify\na node count).\nadev0: srun -n4 -l /bin/hostname\n0: adev3\n1: adev3\n2: adev3\n3: adev3\nOne common mode of operation is to submit a script for later execution.\nIn this example the script name is my.script and we explicitly use\nthe nodes adev9 and adev10 (-w \"adev[9-10]\", note the use of a\nnode range expression).\nWe also explicitly state that the subsequent job steps will spawn four tasks\neach, which will ensure that our allocation contains at least four processors\n(one processor per task to be launched).\nThe output will appear in the file my.stdout (\"-o my.stdout\").\nThis script contains a timelimit for the job embedded within itself.\nOther options can be supplied as desired by using a prefix of \"#SBATCH\" followed\nby the option at the beginning of the script (before any commands to be executed\nin the script).\nOptions supplied on the command line would override any options specified within\nthe script.\nNote that my.script contains the command /bin/hostname\nthat executed on the first node in the allocation (where the script runs) plus\ntwo job steps initiated using the srun command\nand executed sequentially.\nadev0: cat my.script\n#!/bin/sh\n#SBATCH --time=1\n/bin/hostname\nsrun -l /bin/hostname\nsrun -l /bin/pwd\n\nadev0: sbatch -n4 -w \"adev[9-10]\" -o my.stdout my.script\nsbatch: Submitted batch job 469\n\nadev0: cat my.stdout\nadev9\n0: adev9\n1: adev9\n2: adev10\n3: adev10\n0: /home/jette\n1: /home/jette\n2: /home/jette\n3: /home/jette\nThe final mode of operation is to create a resource allocation\nand spawn job steps within that allocation.\nThe salloc command is used\nto create a resource allocation and typically start a shell within\nthat allocation.\nOne or more job steps would typically be executed within that allocation\nusing the srun command to launch the tasks\n(depending upon the type of MPI being used, the launch mechanism may\ndiffer, see MPI details below).\nFinally the shell created by salloc would\nbe terminated using the exit command.\nSlurm does not automatically migrate executable or data files\nto the nodes allocated to a job.\nEither the files must exists on local disk or in some global file system\n(e.g. NFS or Lustre).\nWe provide the tool sbcast to transfer\nfiles to local storage on allocated nodes using Slurm's hierarchical\ncommunications.\nIn this example we use sbcast to transfer\nthe executable program a.out to /tmp/joe.a.out on local storage\nof the allocated nodes.\nAfter executing the program, we delete it from local storage\ntux0: salloc -N1024 bash\n$ sbcast a.out /tmp/joe.a.out\nGranted job allocation 471\n$ srun /tmp/joe.a.out\nResult is 3.14159\n$ srun rm /tmp/joe.a.out\n$ exit\nsalloc: Relinquishing job allocation 471\nIn this example, we submit a batch job, get its status, and cancel it. \nadev0: sbatch test\nsrun: jobid 473 submitted\n\nadev0: squeue\nJOBID PARTITION NAME USER ST TIME  NODES NODELIST(REASON)\n  473 batch     test jill R  00:00 1     adev9\n\nadev0: scancel 473\n\nadev0: squeue\nJOBID PARTITION NAME USER ST TIME  NODES NODELIST(REASON)\nBest Practices, Large Job Counts\n\nConsider putting related work into a single Slurm job with multiple job\nsteps both for performance reasons and ease of management.\nEach Slurm job can contain a multitude of job steps and the overhead in\nSlurm for managing job steps is much lower than that of individual jobs.Job arrays are an efficient mechanism of\nmanaging a collection of batch jobs with identical resource requirements.\nMost Slurm commands can manage job arrays either as individual elements (tasks)\nor as a single entity (e.g. delete an entire job array in a single command).MPIMPI use depends upon the type of MPI being used.\nThere are three fundamentally different modes of operation used\nby these various MPI implementations.\n\nSlurm directly launches the tasks and performs initialization of\ncommunications through the PMI2 or PMIx APIs. (Supported by most\nmodern MPI implementations.)\nSlurm creates a resource allocation for the job and then\nmpirun launches tasks using Slurm's infrastructure (older versions of\nOpenMPI).\nSlurm creates a resource allocation for the job and then\nmpirun launches tasks using some mechanism other than Slurm,\nsuch as SSH or RSH.\nThese tasks are initiated outside of Slurm's monitoring\nor control. Slurm's epilog should be configured to purge\nthese tasks when the job's allocation is relinquished. The\nuse of pam_slurm_adopt is also strongly recommended.\n\nLinks to instructions for using several varieties of MPI\nwith Slurm are provided below.\n\nIntel MPI\nMPICH2\nMVAPICH2\nOpen MPI\n\nLast modified 29 June 2021\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/priority_multifactor3.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Depth-Oblivious Fair-share Factor",
                "content": "Contents\nIntroduction\nDepth-Oblivious Fair-Share Formula\nThe Effective Usage Ratio Under an Account Hierarchy\nConfiguration\nIntroductionThe depth-oblivious fair-share factor is a variant of the default\nfair-share factor which increases usable priority ranges and improves\nfairness between accounts in deep and/or irregular hierarchies. The\nreader is assumed to be familiar with the priority/multifactor plugin\nand only the specifics of the depth-oblivious factor are documented\nhereDepth-Oblivious Fair-Share Formula\n\n The main formula for calculating the fair-share factor of an account is:\n\tF = 2^(-R)\nwhere:\n F\n is the fair-share factor\n R\n is the effective usage ratio of the account\n This formula resembles the original fair-share formula, and\nproduces the same result for an account at the first level of the tree\n(under root). Indeed, for first-level accounts, the effective usage\nratio R is equal to the usage ratio r defined as: \n\tr = U/S\nwhere:\n S\n is the normalized shares\n U\n is the normalized usage factoring in half-life decay\n which is the same as the original formula. The Effective Usage Ratio Under an Account Hierarchy\n\n The generalized formula for R is a bit more complex. It involves a\nlocal usage ratio rl:\n\trl = r / (Uall_siblings/Sall_siblings)\n which is the ratio between the usage ratio of an account, and the\ntotal usage ratio of all the siblings at his level including\nitself. For example, assuming that all the children of an account have\nused in total two times their combined shares (which equal the shares\nof the parent account), but that one of the child has used only two\nthirds of his shares, the local usage ratio of that child will be of\none third.  The general formula for R is then defined by: \n\tR = Rparent * rl^k\nwhere:\n k\n varies between 0 and 1 and determines how much the effective usage\nratio of an account is determined by the usage ratio of its ancestors.\n To understand the formula for k, it is useful to first make a few\nobservations about the formula for R. On the one hand, if k equals 1,\nthe above formula gives R = Rparent * rl. For a\nsecond-level account, by plugging in the formula for rl,\nthis leads to R = r *\nUparent/Uall_siblings. Assuming jobs are\nsubmitted at leaf accounts, Uparent =\nUall_siblings which gives R = r. This means that if k\nequals 1, the fair-share factor of an account is only based on its own\nusage ratio. On the other hand, if k equals 0, R = Rparent\nwhich means the fair-share factor of an account is only based on the\nusage ratio of its ancestors.  The formula for k is: \n\n\tk = (1/(1+(5*ln(Rparent))^2)) if ln(Rparent)*ln(rl) <= 0\n\tk = 1 if ln(Rparent)*ln(rl) >= 0\n\n This formula is chosen to ensure that, if the usage of the\nancestors of an account is on target, the fair-share factor of the\naccount mainly depends on its own usage. Therefore k tends towards 1\nwhen Rparent tends towards 1. On the contrary, the more the\nancestors of an account have underused/overused their shares, the more\nthe fair-share factor of the account should get a bonus/malus by\nmoving towards the fair-share factor of its parent. Therefore, k tends\ntowards 0 when Rparent diverges from 1. However, if the\naccount usage imbalance is greater than its ancestors' in the same\ndirection, (for example, the ancestors have consumed two times their\nshares, and the child has consumed 3 times its shares), moving the\nfair-share factor back towards the one of the parent is not\nhelpful. As a result, k is kept to 1 in that case.\n\n\n  Figure 1. Plot of k as a function of Rparent\n\n\nConfiguration\nThe following slurm.conf parameters are used\nto enable the depth-oblivious flavor of the fair-share factor.  See\nslurm.conf(5) man page for more details.\n\nPriorityFlags\nSet to \"DEPTH_OBLIVIOUS\".\nPriorityType\nSet this value to \"priority/multifactor\".\n\n\nLast modified 26 June 2023\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Configuration",
                "content": "The following slurm.conf parameters are used\nto enable the depth-oblivious flavor of the fair-share factor.  See\nslurm.conf(5) man page for more details.\nPriorityFlags\nSet to \"DEPTH_OBLIVIOUS\".\nPriorityType\nSet this value to \"priority/multifactor\".\nLast modified 26 June 2023"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/qos.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Quality of Service (QOS)",
                "content": "One can specify a Quality of Service (QOS) for each job submitted\nto Slurm. The QOSs are defined in the Slurm database using the sacctmgr\ncommand. Jobs request a QOS using the \"--qos=\" option to the\nsbatch, salloc, and srun commands.Contents\nEffects on Jobs\n\n Scheduling Priority\n Preemption\n Resource Limits\n\n Partition QOS\n Relative QOS\n Other QOS Options\n Configuration\n Examples\n\nEffects on Jobs\n\nThe QOS associated with a job will affect the job in three key ways:\nscheduling priority, preemption, and resource limits.Job Scheduling Priority\n\nJob scheduling priority is made up of a number of factors as\ndescribed in the priority/multifactor plugin.  One\nof the factors is the QOS priority.  Each QOS is defined in the Slurm\ndatabase and includes an associated priority.  Jobs that request and\nare permitted a QOS will incorporate the priority associated with that\nQOS in the job's multi-factor priority\ncalculation.To enable the QOS priority component of the multi-factor priority\ncalculation, the \"PriorityWeightQOS\" configuration parameter must be\ndefined in the slurm.conf file and assigned an integer value greater\nthan zero. A job's QOS only affects is scheduling priority when the\nmulti-factor plugin is loaded.Job Preemption\n\nSlurm offers two ways for a queued job to preempt a running job,\nfree-up the running job's resources and allocate them to the queued\njob.  See the  Preemption description for\ndetails.The preemption method is determined by the \"PreemptType\"\nconfiguration parameter defined in slurm.conf.  When the \"PreemptType\"\nis set to \"preempt/qos\", a queued job's QOS will be used to determine\nwhether it can preempt a running job. It is important to note that the QOS\nused to determine if a job is eligible for preemption is the QOS associated\nwith the job and not a Partition QOS. The QOS can be assigned (using sacctmgr) a list of other\nQOSs that it can preempt.  When there is a queued job with a QOS that\nis allowed to preempt a running job of another QOS, the Slurm\nscheduler will preempt the running job. The QOS option PreemptExemptTime specifies the minimum run time before the\njob is considered for preemption. The QOS option takes precedence over the\nglobal option of the same name. A Partition QOS with PreemptExemptTime\ntakes precedence over a job QOS with PreemptExemptTime, unless the job QOS\nhas the OverPartQOS flag enabled.Resource LimitsEach QOS is assigned a set of limits which will be applied to the\njob.  The limits mirror the limits imposed by the\nuser/account/cluster/partition association defined in the Slurm\ndatabase and described in the  Resource\nLimits page.  When limits for a QOS have been defined, they\nwill take precedence over the association's limits.Partition QOS\n\nA QOS can be attached to a partition. This means the partition will have all\nthe same limits as the QOS. This does not associate jobs with the QOS, nor does\nit give the job any priority or preemption characteristics of the assigned QOS.\nJobs may separately request the same QOS or a different QOS to gain those\ncharacteristics. However, the Partition QOS limits will override the job's QOS.\nIf the opposite is desired you may configure the job's QOS with\nFlags=OverPartQOS which will reverse the order of precedence.This functionality may be used to implement a true \"floating\"\npartition, in which a partition may access a limited amount of resources with no\nrestrictions on which nodes it uses to get the resources. This is accomplished\nby assigning all nodes to the partition, then configuring a Partition QOS with\nGrpTRES set to the desired resource limits.NOTE: Most QOS attributes are set using the sacctmgr command.\nHowever, setting a QOS as a partition QOS is accomplished in slurm.conf\nthrough the QOS= option in the\nconfiguration of the associated partition. The QOS should be created using\nsacctmgr before it is assigned as a partition QOS.Relative QOS\n\nStarting in Slurm 23.11, a QOS may be configured to contain relative resource\nlimits instead of absolute limits by setting Flags=Relative.\nWhen this flag is set, all resource limits are treated as percentages of the\ntotal resources available. Values higher than 100 are interpreted as 100%.\nMemory limits should be set with no units. Although the default units (MB) will\nbe displayed, the limits will be enforced as a percentage (1MB = 1%).NOTE: When Flags=Relative is added to a QOS, slurmctld\nmust be restarted or reconfigured for the flag to take effect.Generally, the limits on a relative QOS will be calculated relative to the\nresources in the whole cluster. For example, cpu=50 would be\ninterpreted as 50% of all CPUs in the cluster.However, when a relative QOS is also assigned as a partition QOS, some unique\nconditions will apply:\nLimits will be calculated relative to the partition's resources;\nfor example, cpu=50 would be interpreted as 50% of all CPUs in the\nassociated partition.\nOnly one partition may have this QOS as its partition QOS.\nJobs will not be allowed to use it as a normal QOS.\nNOTE: To avoid unexpected job submission errors, it is recommended not\nto add a relative partition QOS to any association-based entities.\n\nOther QOS Options\n\n\nFlags Used by the slurmctld to override or enforce certain\ncharacteristics. To clear a previously set value use the modify command with a\nnew value of -1.\nValid options are:\n\n\nDenyOnLimit If set, jobs using this QOS will be rejected at\nsubmission time if they do not conform to the QOS 'Max' limits as\nstand-alone jobs.\nJobs that go over these limits when other jobs are considered, but conform\nto the limits when considered individually will not be rejected. Instead they\nwill pend until resources are available (as by default without DenyOnLimit).\nGroup limits (e.g. GrpTRES) will also be treated like 'Max' limits\n(e.g. MaxTRESPerNode) and jobs will be denied if they would violate the\nlimit as stand-alone jobs.\nThis currently only applies to QOS and Association limits.\nEnforceUsageThreshold If set, and the QOS also has a UsageThreshold,\nany jobs submitted with this QOS that fall below the UsageThreshold\nwill be held until their Fairshare Usage goes above the Threshold.\nNoDecay If set, this QOS will not have its GrpTRESMins,\nGrpWall and UsageRaw decayed by the slurm.conf PriorityDecayHalfLife\nor PriorityUsageResetPeriod settings.  This allows\na QOS to provide aggregate limits that, once consumed, will not be\nreplenished automatically.  Such a QOS will act as a time-limited quota\nof resources for an association that has access to it.  Account/user\nusage will still be decayed for associations using the QOS.  The QOS\nGrpTRESMins and GrpWall limits can be increased or\nthe QOS RawUsage value reset to 0 (zero) to again allow jobs submitted\nwith this QOS to run (if pending with QOSGrp{TRES}MinutesLimit or\nQOSGrpWallLimit reasons, where {TRES} is some type of trackable resource).\nNoReserve If this flag is set and backfill scheduling is used,\njobs using this QOS will not reserve resources in the backfill\nschedule's  map of resources allocated through time. This flag is\nintended for use with a QOS that may be preempted by jobs associated\nwith all other QOS (e.g use with a \"standby\" QOS). If this flag is\nused with a QOS which can not be preempted by all other QOS, it could\nresult in starvation of larger jobs.\nOverPartQOS If set, jobs using this QOS will be able to\noverride any limits used by the requested partition's QOS limits.\nPartitionMaxNodes If set, jobs using this QOS will be able to\noverride the requested partition's MaxNodes limit.\nPartitionMinNodes If set, jobs using this QOS will be able to\noverride the requested partition's MinNodes limit.\nPartitionTimeLimit If set, jobs using this QOS will be able to\noverride the requested partition's TimeLimit.\nRelative If set, the QOS limits will be treated as percentages of\nthe cluster or partition instead of absolute limits (numbers should be less than\n100). The controller should be restarted or reconfigured after adding the\nRelative flag to the QOS.\nIf this is used as a partition QOS:\n\nLimits will be calculated relative to the partition's resources.\nOnly one partition may have this QOS as its partition QOS.\nJobs will not be allowed to use it as a normal QOS.\n\nRequiresReservation If set, jobs using this QOS must designate a\nreservation when submitting a job.  This option can be useful in\nrestricting usage of a QOS that may have greater preemptive capability\nor additional resources to be allowed only within a reservation.\nUsageFactorSafe If set, and AccountingStorageEnforce includes\nSafe, jobs will only be able to run if the job can run to completion\nwith the UsageFactor applied.\n\n\nGraceTime Preemption grace time to be extended to a job\n  which has been selected for preemption.\nUsageFactor\nA float that is factored into a job's TRES usage (e.g. RawUsage, TRESMins,\nTRESRunMins). For example, if the usagefactor was 2, for every TRESBillingUnit\nsecond a job ran it would count for 2. If the usagefactor was .5, every second\nwould only count for half of the time. A setting of 0 would add no timed usage\nfrom the job.\n\n\nThe usage factor only applies to the job's QOS and not the partition QOS.\n\n\nIf the UsageFactorSafe flag is set and\nAccountingStorageEnforce includes Safe, jobs will only be\nable to run if the job can run to completion with the UsageFactor\napplied.\n\n\nIf the UsageFactorSafe flag is not set and\nAccountingStorageEnforce includes Safe, a job will be able to be\nscheduled without the UsageFactor applied and will be able to run\nwithout being killed due to limits.\n\n\nIf the UsageFactorSafe flag is not set and\nAccountingStorageEnforce does not include Safe, a job will be\nable to be scheduled without the UsageFactor applied and could be killed\ndue to limits.\n\n\nSee AccountingStorageEnforce in slurm.conf man page.\n\n\nDefault is 1. To clear a previously set value use the modify command with a new\nvalue of -1.\n\nUsageThreshold\nA float representing the lowest fairshare of an association allowable\nto run a job.  If an association falls below this threshold and has\npending jobs or submits new jobs those jobs will be held until the\nusage goes back above the threshold.  Use sshare to see current\nshares on the system.\nConfiguration To summarize the above, the QOSs and their associated limits are\ndefined in the Slurm database using the sacctmgr utility.  The\nQOS will only influence job scheduling priority when the multi-factor\npriority plugin is loaded and a non-zero \"PriorityWeightQOS\" has been\ndefined in the slurm.conf file.  The QOS will only determine job\npreemption when the \"PreemptType\" is defined as \"preempt/qos\" in the\nslurm.conf file.  Limits defined for a QOS (and described above) will\noverride the limits of the user/account/cluster/partition\nassociation.QOS examplesQOS manipulation examples. All QOS operations are done using\nthe sacctmgr command. The default output of 'sacctmgr show qos' is\nvery long given the large number of limits and options available\nso it is best to use the format option which filters the display.By default when a cluster is added to the database a default\nqos named normal is created.\n$ sacctmgr show qos format=name,priority\n      Name   Priority\n---------- ----------\n    normal          0\nAdd a new QOS\n$ sacctmgr add qos zebra\n Adding QOS(s)\n  zebra\n Settings\n  Description    = QOS Name\n\n$ sacctmgr show qos format=name,priority\n      Name   Priority\n---------- ----------\n    normal          0\n     zebra          0\nSet QOS priority\n$ sacctmgr modify qos zebra set priority=10\n Modified qos...\n  zebra\n\n$ sacctmgr show qos format=name,priority\n      Name   Priority\n---------- ----------\n    normal          0\n     zebra         10\nSet some other limits\n$ sacctmgr modify qos zebra set GrpTRES=cpu=24\n Modified qos...\n  zebra\n\n$ sacctmgr show qos format=name,priority,GrpTRES\n      Name   Priority       GrpTRES\n---------- ---------- -------------\n    normal          0\n     zebra         10        cpu=24\nAdd a QOS to a user account\n$ sacctmgr modify user crock set qos=zebra\n\n$ sacctmgr show assoc format=cluster,user,qos\n   Cluster       User                  QOS\n---------- ---------- --------------------\ncanis_major                          normal\ncanis_major      root                normal\ncanis_major                          normal\ncanis_major     crock                zebra\nUsers can belong to multiple QOSs\n$ sacctmgr modify user crock set qos+=alligator\n$ sacctmgr show assoc format=cluster,user,qos\n   Cluster       User                  QOS\n---------- ---------- --------------------\ncanis_major                          normal\ncanis_major      root                normal\ncanis_major                          normal\ncanis_major     crock       alligator,zebra\n\nFinally, delete a QOS\n$ sacctmgr delete qos alligator\n Deleting QOS(s)...\n  alligator\nLast modified 22 April 2023"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/preempt.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Preemption",
                "content": "\nSlurm supports job preemption, the act of \"stopping\" one or more \"low-priority\"\njobs to let a \"high-priority\" job run.\nJob preemption is implemented as a variation of Slurm's\nGang Scheduling logic.\nWhen a job that can preempt others is allocated resources that are\nalready allocated to one or more jobs that could be preempted by the first job,\nthe preemptable job(s) are preempted.\nBased on the configuration the preempted job(s) can be cancelled, or can be\nrequeued and started using other resources, or suspended and resumed once the\npreemptor job completes, or can even share resources with the preemptor using\nGang Scheduling.\n\nThe PriorityTier of the Partition of the job or its Quality Of Service (QOS)\ncan be used to identify which jobs can preempt or be preempted by other jobs.\nSlurm offers the ability to configure the preemption mechanism used on a per\npartition or per QOS basis.\nFor example, jobs in a low priority queue may get requeued,\nwhile jobs in a medium priority queue may get suspended.\nConfiguration\nThere are several important configuration parameters relating to preemption:\n\n\nSelectType: Slurm job preemption logic supports nodes allocated by the\nselect/linear plugin, socket/core/CPU resources allocated by the\nselect/cons_tres plugin.\n\n\nSelectTypeParameter: Since resources may be getting over-allocated\nwith jobs (suspended jobs remain in memory), the resource selection\nplugin should be configured to track the amount of memory used by each job to\nensure that memory page swapping does not occur.\nWhen select/linear is chosen, we recommend setting\nSelectTypeParameter=CR_Memory.\nWhen select/cons_tres is chosen, we recommend\nincluding Memory as a resource (e.g. SelectTypeParameter=CR_Core_Memory).\n\nNOTE: Unless PreemptMode=SUSPEND,GANG these memory management\nparameters are not critical.\n\n\nDefMemPerCPU: Since job requests may not explicitly specify\na memory requirement, we also recommend configuring\nDefMemPerCPU (default memory per allocated CPU) or\nDefMemPerNode (default memory per allocated node).\nIt may also be desirable to configure\nMaxMemPerCPU (maximum memory per allocated CPU) or\nMaxMemPerNode (maximum memory per allocated node) in slurm.conf.\nUsers can use the --mem or --mem-per-cpu option\nat job submission time to specify their memory requirements.\nNOTE: Unless PreemptMode=SUSPEND,GANG these memory management\nparameters are not critical.\n\n\nGraceTime: Specifies a time period for a job to execute after\nit is selected to be preempted. This option can be specified by partition or\nQOS using the slurm.conf file or database respectively.\nThe GraceTime is specified in\nseconds and the default value is zero, which results in no preemption delay.\nOnce a job has been selected for preemption, its end time is set to the\ncurrent time plus GraceTime. The job is immediately sent SIGCONT and\nSIGTERM signals in order to provide notification of its imminent termination.\nThis is followed by the SIGCONT, SIGTERM and SIGKILL signal sequence upon\nreaching its new end time.\nNOTE: This parameter is not used when PreemptMode=SUSPEND\nis configured or when suspending jobs with scontrol suspend. For\nsetting the preemption grace time in these cases, see\nsuspend_grace_time.\n\n\nJobAcctGatherType and JobAcctGatherFrequency: The \"maximum data segment\nsize\" and \"maximum virtual memory size\" system limits will be configured for\neach job to ensure that the job does not exceed its requested amount of memory.\nIf you wish to enable additional enforcement of memory limits, configure job\naccounting with the JobAcctGatherType and JobAcctGatherFrequency\nparameters. When accounting is enabled and a job exceeds its configured memory\nlimits, it will be canceled in order to prevent it from adversely affecting\nother jobs sharing the same resources.\nNOTE: Unless PreemptMode=SUSPEND,GANG these memory management\nparameters are not critical.\n\n\nPreemptMode: Mechanism used to preempt jobs or enable gang scheduling.\nWhen the PreemptType parameter is set to enable preemption, the\nPreemptMode in the main section of slurm.conf selects the default\nmechanism used to preempt the preemptable jobs for the cluster.\n\nPreemptMode may be specified on a per partition basis to override this\ndefault value if PreemptType=preempt/partition_prio. Alternatively, it\ncan be specified on a per QOS basis if PreemptType=preempt/qos.\nIn either case, a valid default PreemptMode value must be specified for\nthe cluster as a whole when preemption is enabled.\n\nThe GANG option is used to enable gang scheduling independent of whether\npreemption is enabled (i.e. independent of PreemptType).\nIt can be specified in addition to other PreemptMode settings, with the\ntwo options comma separated (e.g. PreemptMode=SUSPEND,GANG).\n\n\nOFF: Is the default value and disables job preemption and gang\nscheduling. It is only compatible with PreemptType=preempt/none.\n\n\nCANCEL: The preempted job will be cancelled.\n\n\nGANG: Enables gang scheduling (time slicing) of jobs in the same\npartition, and allows the resuming of suspended jobs. In order to use gang\nscheduling, the GANG option must be specified at the cluster level.\n\nNOTE: Gang scheduling is performed independently for each partition, so\nif you only want time-slicing by OverSubscribe, without any preemption,\nthen configuring partitions with overlapping nodes is not recommended.\nOn the other hand, if you want to use PreemptType=preempt/partition_prio\nto allow jobs from higher PriorityTier partitions to Suspend jobs from lower\nPriorityTier partitions, then you will need overlapping partitions, and\nPreemptMode=SUSPEND,GANG to use Gang scheduler to resume the suspended\njob(s).\nIn either case, time-slicing won't happen between jobs on different partitions.\n\n\nREQUEUE: Preempts jobs by requeuing them (if possible) or canceling\nthem. For jobs to be requeued they must have the \"--requeue\" sbatch option set\nor the cluster wide JobRequeue parameter in slurm.conf must be set to 1.\n\n\nSUSPEND: The preempted jobs will be suspended, and later the Gang\nscheduler will resume them. Therefore, the SUSPEND preemption mode always\nneeds the GANG option to be specified at the cluster level.\nAlso, because the suspended jobs will still use memory on the allocated nodes,\nSlurm needs to be able to track memory resources to be able to suspend jobs.\n\nNOTE: Because gang scheduling is performed independently for each\npartition, if using PreemptType=preempt/partition_prio then jobs in\nhigher PriorityTier partitions will suspend jobs in lower PriorityTier\npartitions to run on the released resources. Only when the preemptor job ends\nthen the suspended jobs will be resumed by the Gang scheduler.\nIf PreemptType=preempt/qos is configured and if the preempted job(s) and\nthe preemptor job from are on the same partition, then they will share\nresources with the Gang scheduler (time-slicing). If not (i.e. if the\npreemptees and preemptor are on different partitions) then the preempted jobs\nwill remain suspended until the preemptor ends.\n\n\n\n\nPreemptType: Specifies the plugin used to identify which jobs can be\npreempted in order to start a pending job.\n\npreempt/none: Job preemption is disabled (default).\npreempt/partition_prio: Job preemption is based upon partition\nPriorityTier. Jobs in higher PriorityTier partitions may preempt jobs\nfrom lower PriorityTier partitions. This is not compatible with\nPreemptMode=OFF.\npreempt/qos: Job preemption rules are specified by Quality Of\nService (QOS) specifications in the Slurm database. In the case of\nPreemptMode=SUSPEND, a preempting job needs to be submitted to a\npartition with a higher PriorityTier or to the same partition.\nThis option is not compatible with PreemptMode=OFF.\nA configuration of PreemptMode=SUSPEND is only supported by the\nSelectType=select/cons_tres plugin.\nSee the sacctmgr man page to configure the options\nof preempt/qos.\n\n\n\nPreemptExemptTime: Specifies minimum run time of jobs before they are\nconsidered for preemption. This is only honored when the PreemptMode\nis set to REQUEUE or CANCEL. It is specified as a time string:\nA time of -1 disables the option, equivalent to 0. Acceptable time formats\ninclude \"minutes\", \"minutes:seconds\", \"hours:minutes:seconds\", \"days\\-hours\",\n\"days\\-hours:minutes\", and \"days\\-hours:minutes:seconds\".\nPreemptEligibleTime is shown in the output of \"scontrol show job <job id>\"\n\n\nPriorityTier: Configure the partition's PriorityTier setting\nrelative to other partitions to control the preemptive behavior when\nPreemptType=preempt/partition_prio.\nIf two jobs from two\ndifferent partitions are allocated to the same resources, the job in the\npartition with the greater PriorityTier value will preempt the job in the\npartition with the lesser PriorityTier value. If the PriorityTier\nvalues of the two partitions are equal then no preemption will occur. The\ndefault PriorityTier value is 1.\n\nNOTE: In addition to being used for partition based preemption,\nPriorityTier also has an effect on scheduling. The scheduler will\nevaluate jobs in the partition(s) with the highest PriorityTier\nbefore evaluating jobs in other partitions, regardless of which jobs have\nthe highest Priority. The scheduler will consider the job priority when\nevaluating jobs within the partition(s) with the same PriorityTier.\n\n\nOverSubscribe: Configure the partition's OverSubscribe setting to\nFORCE for all partitions in which job preemption using a suspend/resume\nmechanism is used.\nThe FORCE option supports an additional parameter that controls\nhow many jobs can oversubscribe a compute resource (FORCE[:max_share]). By\ndefault the max_share value is 4. In order to preempt jobs (and not gang\nschedule them), always set max_share to 1. To allow up to 2 jobs from this\npartition to be allocated to a common resource (and gang scheduled), set\nOverSubscribe=FORCE:2.\nNOTE: PreemptType=preempt/qos will permit one additional job\nto be run on the partition if started due to job preemption. For example, a\nconfiguration of OverSubscribe=FORCE:1 will only permit one job per\nresource normally, but a second job can be started if done so through\npreemption based upon QOS.\n\n\nExclusiveUser: In partitions with ExclusiveUser=YES, jobs will be\nprevented from preempting or being preempted by any job from any other user.\nThe one exception is that these ExclusiveUser jobs will be able to preempt\n(but not be preempted by) fully \"--exclusive\" jobs from other users.\nThis is for the same reason that \"--exclusive=user\" blocks preemption, but this\npartition-level setting can only be overridden by making a job fully exclusive.\n\n\nMCSParameters: If MCS labels are set on jobs,\npreemption will be restricted to other jobs with the same MCS label. If this\nparameter is configured to use enforced,select, MCS labels will\nbe set by default on jobs, causing this restriction to be universal.\n\n\nTo enable preemption after making the configuration changes described above,\nrestart Slurm if it is already running. Any change to the plugin settings in\nSlurm requires a full restart of the daemons. If you just change the partition\nPriorityTier or OverSubscribe setting, this can be updated with\nscontrol reconfig.\n\nIf a job request restricts Slurm's ability to run jobs from multiple users or\naccounts on a node by using the \"--exclusive=user\" or \"--exclusive=mcs\" job\noptions, the job will be prevented from preempting or being preempted by any job\nthat does not match the user or MCS. The one exception is that these\nexclusive=user jobs will be able to preempt (but not be preempted by)\nfully \"--exclusive\" jobs from other users. If preemption is used, it is\ngenerally advisable to disable the \"--exclusive=user\" and \"--exclusive=mcs\"\njob options by using a job_submit plugin (set the value of \"job_desc.shared\"\nto \"NO_VAL16\").\n\nFor heterogeneous job to be considered for preemption all components\nmust be eligible for preemption. When a heterogeneous job is to be preempted\nthe first identified component of the job with the highest order PreemptMode\n(SUSPEND (highest), REQUEUE, CANCEL (lowest)) will be\nused to set the PreemptMode for all components. The GraceTime and user\nwarning signal for each component of the heterogeneous job remain unique.\n\nBecause licenses are not freed when jobs are suspended, jobs using licenses\nrequested by higher priority jobs will only be prempted when PreemptMode is\neither REQUEUE or CANCEL and\nPreemptParameters=reclaim_licenses is set.\nPreemption Design and Operation\n\n\nThe SelectType plugin will identify resources where a pending job can\nbegin execution. When PreemptMode is configured to CANCEL,\nSUSPEND or REQUEUE, the select plugin will also preempt running\njobs as needed to initiate the pending job. When\nPreemptMode=SUSPEND,GANG the select plugin will initiate the pending\njob and rely upon the gang scheduling logic to perform job suspend and resume,\nas described below.\n\nThe select plugin is passed an ordered list of preemptable jobs to consider for\neach pending job which is a candidate to start.\nThis list is sorted by either:\n\nQOS priority,\nPartition priority and job size (to favor preempting smaller jobs), or\nJob start time (with PreemptParameters=youngest_first).\n\nThe select plugin will determine if the pending job can start without preempting\nany jobs and if so, starts the job using available resources.\nOtherwise, the select plugin will simulate the preemption of each job in the\npriority ordered list and test if the job can be started after each preemption.\nOnce the job can be started, the higher priority jobs in the preemption queue\nwill not be considered, but the jobs to be preempted in the original list may\nbe sub-optimal.\nFor example, to start an 8 node job, the ordered preemption candidates may be\n2 node, 4 node and 8 node.\nPreempting all three jobs would allow the pending job to start, but by reordering\nthe preemption candidates it is possible to start the pending job after\npreempting only one job.\nTo address this issue, the preemption candidates are re-ordered with the final\njob requiring preemption placed first in the list and all of the other jobs\nto be preempted ordered by the number of nodes in their allocation which overlap\nthe resources selected for the pending job.\nIn the example above, the 8 node job would be moved to the first position in\nthe list.\nThe process of simulating the preemption of each job in the priority ordered\nlist will then be repeated for the final decision of which jobs to preempt.\nThis two stage process may preempt jobs which are not strictly in preemption\npriority order, but fewer jobs will be preempted than otherwise required.\nSee the PreemptParameters configuration parameter options of reorder_count\nand strict_order for preemption tuning parameters.\n\nWhen enabled, the gang scheduling logic (which is also supports job\npreemption) keeps track of the resources allocated to all jobs.\nFor each partition an \"active bitmap\" is maintained that tracks all\nconcurrently running jobs in the Slurm cluster.\nEach partition also maintains a job list for that partition, and a list of\n\"shadow\" jobs.\nThe \"shadow\" jobs are high priority job allocations that \"cast shadows\" on the\nactive bitmaps of the low priority jobs.\nJobs caught in these \"shadows\" will be preempted.\n\nEach time a new job is allocated to resources in a partition and begins\nrunning, the gang scheduler adds a \"shadow\" of this job to all lower priority\npartitions.\nThe active bitmap of these lower priority partitions are then rebuilt, with the shadow jobs added first.\nAny existing jobs that were replaced by one or more \"shadow\" jobs are\nsuspended (preempted). Conversely, when a high priority running job completes,\nits \"shadow\" goes away and the active bitmaps of the lower priority\npartitions are rebuilt to see if any suspended jobs can be resumed.\n\nThe gang scheduler plugin is designed to be reactive to the resource\nallocation decisions made by the \"select\" plugins.\nThe \"select\" plugins have been enhanced to recognize when job preemption has\nbeen configured, and to factor in the priority of each partition when selecting resources for a job.\nWhen choosing resources for each job, the selector avoids resources that are\nin use by other jobs (unless sharing has been configured, in which case it\ndoes some load-balancing).\nHowever, when job preemption is enabled, the select plugins may choose\nresources that are already in use by jobs from partitions with a lower\npriority setting, even when sharing is disabled in those partitions.\n\nThis leaves the gang scheduler in charge of controlling which jobs should run\non the over-allocated resources.\nIf PreemptMode=SUSPEND, jobs are suspended using the same internal\nfunctions that support scontrol suspend and scontrol resume.\nA good way to observe the operation of the gang scheduler is by running\nsqueue -i<time> in a terminal window.\nLimitations of Preemption During Backfill Scheduling\n\n\nFor performance reasons, the backfill scheduler reserves whole nodes for jobs,\nnot partial nodes. If during backfill scheduling a job preempts one or more\nother jobs, the whole nodes for those preempted jobs are reserved for the\npreemptor job, even if the preemptor job requested fewer resources than that.\nThese reserved nodes aren't available to other jobs during that backfill\ncycle, even if the other jobs could fit on the nodes. Therefore, jobs may\npreempt more resources during a single backfill iteration than they requested.\nA Simple Example\n\n\nThe following example is configured with select/linear and\nPreemptMode=SUSPEND,GANG.\nThis example takes place on a cluster of 5 nodes:\n\n[user@n16 ~]$ sinfo\nPARTITION AVAIL  TIMELIMIT NODES  STATE NODELIST\nactive*      up   infinite     5   idle n[12-16]\nhipri        up   infinite     5   idle n[12-16]\n\nHere are the Partition settings:\n\n[user@n16 ~]$ grep PartitionName /shared/slurm/slurm.conf\nPartitionName=DEFAULT OverSubscribe=FORCE:1 Nodes=n[12-16]\nPartitionName=active PriorityTier=1 Default=YES\nPartitionName=hipri  PriorityTier=2\n\nThe runit.pl script launches a simple load-generating app that runs\nfor the given number of seconds. Submit 5 single-node runit.pl jobs to\nrun on all nodes:\n\n[user@n16 ~]$ sbatch -N1 ./runit.pl 300\nsbatch: Submitted batch job 485\n[user@n16 ~]$ sbatch -N1 ./runit.pl 300\nsbatch: Submitted batch job 486\n[user@n16 ~]$ sbatch -N1 ./runit.pl 300\nsbatch: Submitted batch job 487\n[user@n16 ~]$ sbatch -N1 ./runit.pl 300\nsbatch: Submitted batch job 488\n[user@n16 ~]$ sbatch -N1 ./runit.pl 300\nsbatch: Submitted batch job 489\n[user@n16 ~]$ squeue -Si\nJOBID PARTITION     NAME   USER  ST   TIME  NODES NODELIST\n  485    active runit.pl   user   R   0:06      1 n12\n  486    active runit.pl   user   R   0:06      1 n13\n  487    active runit.pl   user   R   0:05      1 n14\n  488    active runit.pl   user   R   0:05      1 n15\n  489    active runit.pl   user   R   0:04      1 n16\n\nNow submit a short-running 3-node job to the hipri partition:\n\n[user@n16 ~]$ sbatch -N3 -p hipri ./runit.pl 30\nsbatch: Submitted batch job 490\n[user@n16 ~]$ squeue -Si\nJOBID PARTITION     NAME   USER  ST   TIME  NODES NODELIST\n  485    active runit.pl   user   S   0:27      1 n12\n  486    active runit.pl   user   S   0:27      1 n13\n  487    active runit.pl   user   S   0:26      1 n14\n  488    active runit.pl   user   R   0:29      1 n15\n  489    active runit.pl   user   R   0:28      1 n16\n  490     hipri runit.pl   user   R   0:03      3 n[12-14]\n\nJob 490 in the hipri partition preempted jobs 485, 486, and 487 from\nthe active partition. Jobs 488 and 489 in the active partition\nremained running.\n\nThis state persisted until job 490 completed, at which point the preempted jobs\nwere resumed:\n\n[user@n16 ~]$ squeue\nJOBID PARTITION     NAME   USER  ST   TIME  NODES NODELIST\n  485    active runit.pl   user   R   0:30      1 n12\n  486    active runit.pl   user   R   0:30      1 n13\n  487    active runit.pl   user   R   0:29      1 n14\n  488    active runit.pl   user   R   0:59      1 n15\n  489    active runit.pl   user   R   0:58      1 n16\nAnother Example\n\n\nIn this example we have three different partitions using three different\njob preemption mechanisms.\n\n# Excerpt from slurm.conf\nPartitionName=low Nodes=linux Default=YES OverSubscribe=NO      PriorityTier=10 PreemptMode=requeue\nPartitionName=med Nodes=linux Default=NO  OverSubscribe=FORCE:1 PriorityTier=20 PreemptMode=suspend\nPartitionName=hi  Nodes=linux Default=NO  OverSubscribe=FORCE:1 PriorityTier=30 PreemptMode=off\n\n$ sbatch tmp\nSubmitted batch job 94\n$ sbatch -p med tmp\nSubmitted batch job 95\n$ sbatch -p hi tmp\nSubmitted batch job 96\n$ squeue\n  JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)\n     96        hi      tmp      moe   R       0:04      1 linux\n     94       low      tmp      moe  PD       0:00      1 (Resources)\n     95       med      tmp      moe   S       0:02      1 linux\n(after job 96 completes)\n$ squeue\n  JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)\n     94       low      tmp      moe  PD       0:00      1 (Resources)\n     95       med      tmp      moe   R       0:24      1 linux\nAnother Example\n\n\nIn this example we have one partition on which we want to execute only one\njob per resource (e.g. core) at a time except when a job submitted to the\npartition from a high priority Quality Of Service (QOS) is submitted. In that\ncase, we want that second high priority job to be started and be gang scheduled\nwith the other jobs on overlapping resources.\n\n# Excerpt from slurm.conf\nPreemptMode=Suspend,Gang\nPreemptType=preempt/qos\nPartitionName=normal Nodes=linux Default=NO  OverSubscribe=FORCE:1\nFuture Ideas\n\n\nMore intelligence in the select plugins: This implementation of\npreemption relies on intelligent job placement by the select plugins.\n\nTake the following example:\n\n[user@n8 ~]$ sinfo\nPARTITION AVAIL  TIMELIMIT NODES  STATE NODELIST\nactive*      up   infinite     5   idle n[1-5]\nhipri        up   infinite     5   idle n[1-5]\n[user@n8 ~]$ sbatch -N1 -n2 ./sleepme 60\nsbatch: Submitted batch job 17\n[user@n8 ~]$ sbatch -N1 -n2 ./sleepme 60\nsbatch: Submitted batch job 18\n[user@n8 ~]$ sbatch -N1 -n2 ./sleepme 60\nsbatch: Submitted batch job 19\n[user@n8 ~]$ squeue\n  JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)\n     17    active  sleepme  cholmes   R       0:03      1 n1\n     18    active  sleepme  cholmes   R       0:03      1 n2\n     19    active  sleepme  cholmes   R       0:02      1 n3\n[user@n8 ~]$ sbatch -N3 -n6 -p hipri ./sleepme 20\nsbatch: Submitted batch job 20\n[user@n8 ~]$ squeue -Si\n  JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)\n     17    active  sleepme  cholmes   S       0:16      1 n1\n     18    active  sleepme  cholmes   S       0:16      1 n2\n     19    active  sleepme  cholmes   S       0:15      1 n3\n     20     hipri  sleepme  cholmes   R       0:03      3 n[1-3]\n[user@n8 ~]$ sinfo\nPARTITION AVAIL  TIMELIMIT NODES  STATE NODELIST\nactive*      up   infinite     3  alloc n[1-3]\nactive*      up   infinite     2   idle n[4-5]\nhipri        up   infinite     3  alloc n[1-3]\nhipri        up   infinite     2   idle n[4-5]\n\nIt would be more ideal if the \"hipri\" job were placed on nodes n[3-5], which\nwould allow jobs 17 and 18 to continue running. However, a new \"intelligent\"\nalgorithm would have to include factors such as job size and required nodes in\norder to support ideal placements such as this, which can quickly complicate\nthe design. Any and all help is welcome here!\nLast modified 21 May 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/faq.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Frequently Asked Questions",
                "content": "For Management\nIs Slurm really free?\nWhy should I use Slurm or other free software?\nWhy should I pay for free software?\nWhat does \"Slurm\" stand for?\nFor Researchers\nHow should I cite work involving Slurm?\nFor UsersDesigning Jobs\nHow can I run multiple jobs from within a single\n  script?\nHow can I run a job within an existing job\n  allocation?\nSlurm documentation refers to CPUs, cores and threads.\n  What exactly is considered a CPU?\nHow do I run specific tasks on certain nodes\n  in my allocation?\nHow can I get the task ID in the output or error file\n  name for a batch job?\nHow does Slurm establish the environment for my\n  job?\nCan the make command utilize the resources\n  allocated to a Slurm job?\nHow can I run an Ansys program with Slurm?\nSubmitting Jobs\nWhy are my srun options ignored?\nWhy does the srun --overcommit option not permit\n  multiple jobs to run on nodes?\nWhy is the srun --u/--unbuffered option adding\n  a carriage return to my output?\nWhat is the difference between the sbatch\n  and srun commands?\nCan tasks be launched with a remote (pseudo)\n  terminal?\nHow can I get shell prompts in interactive mode?\nCan Slurm export an X11 display on an allocated compute node?\nScheduling\nWhy is my job not running?\nWhy is the Slurm backfill scheduler not starting my\n  job?\nKilled Jobs\nWhy is my job killed prematurely?\nWhy is my batch job that launches no job steps being\n  killed?\nWhat does \"srun: Force Terminated job\"\n  indicate?\nWhat does this mean: \"srun: First task exited\n  30s ago\" followed by \"srun Job Failed\"?\nManaging Jobs\nHow can I temporarily prevent a job from running\n  (e.g. place it into a hold state)?\nCan I change my job's size after it has started\n  running?\nWhy does squeue (and \"scontrol show\n  jobid\") sometimes not display a job's estimated start time?\nCan squeue output be color coded?\nWhy is my job/node in a COMPLETING state?\nHow can a job in a complete or failed state be requeued?\nWhy is sview not coloring/highlighting nodes\n  properly?\nWhy is my MPICH2 or MVAPICH2 job not running with\n  Slurm? Why does the DAKOTA program not run with Slurm?\nResource Limits\nWhy are my resource limits not propagated?\nWhy are jobs not getting the appropriate\n  memory limit?\nWhy is my MPI job  failing due to the locked memory\n  (memlock) limit being too low?\nFor AdministratorsTest Environments\nCan multiple Slurm systems be run in\n  parallel for testing purposes?\nCan Slurm emulate a larger cluster?\nCan Slurm emulate nodes with more\n  resources than physically exist on the node?\nBuild and Install\nWhy aren't pam_slurm.so, auth_none.so, or other components in a\n  Slurm RPM?\nHow can I build Slurm with debugging symbols?\nHow can a patch file be generated from a Slurm commit\n  in GitHub?\nHow can I apply a patch to my Slurm source?\nWhy am I being offered an automatic update for Slurm?\nCluster Management\n How should I relocate the primary or backup\n  controller?\nDo I need to maintain synchronized clocks\n  on the cluster?\nHow can I stop Slurm from scheduling jobs?\nHow can I dry up the workload for a maintenance\n  period?\nWhat should I be aware of when upgrading Slurm?\nIs there anything exceptional to be aware of when\n  upgrading my database server?\nWhen adding a new cluster, how can the Slurm cluster\n  configuration be copied from an existing cluster to the new cluster?\nHow could some jobs submitted immediately before the\n  slurmctld daemon crashed be lost?\nIs resource limit propagation\n  useful on a homogeneous cluster?\nWhy are the resource limits set in the database\n  not being enforced?\nCan Slurm be configured to manage licenses?\nHow easy is it to switch from PBS or Torque to Slurm?\nWhat might account for MPI performance being below the\n  expected level?\nHow do I safely remove partitions?\nHow can a routing queue be configured?\nAccounting Database\nWhy should I use the slurmdbd instead of the\n  regular database plugins?\nHow can I rebuild the database hierarchy?\nHow critical is configuring high availability for my\n  database?\nHow can I use double quotes in MySQL queries?\nCompute Nodes (slurmd)\nWhy is a node shown in state DOWN when the node\n  has registered for service?\nWhat happens when a node crashes?\nHow can I control the execution of multiple\n  jobs per node?\nWhy are jobs allocated nodes and then unable to initiate\n  programs on some nodes?\n Why does slurmctld log that some nodes\n  are not responding even if they are not in any partition?\nHow can I easily preserve drained node\n  information between major Slurm updates?\nDoes anyone have an example node health check\n  script for Slurm?\nWhy doesn't the HealthCheckProgram\n  execute on DOWN nodes?\nHow can I prevent the slurmd and\n  slurmstepd daemons from being killed when a node's memory\n  is exhausted?\nI see the host of my calling node as 127.0.1.1\n  instead of the correct IP address.  Why is that?\nHow should I add nodes to Slurm?\nHow should I remove nodes from Slurm?\nWhy is a compute node down with the reason set to\n  \"Node unexpectedly rebooted\"?\nHow do I convert my nodes to Control Group (cgroup)\n  v2?\nCan Slurm be used to run jobs on Amazon's EC2?\nUser Management\nHow can PAM be used to control a user's limits on or\n  access to compute nodes?\nHow can I exclude some users from pam_slurm?\nCan a user's account be changed in the database?\nI had to change a user's UID and now they cannot submit\n  jobs. How do I get the new UID to take effect?\nHow can I get SSSD to work with Slurm?\nJobs\nHow is job suspend/resume useful?\nHow can I suspend, resume, hold or release all\n  of the jobs belonging to a specific user, partition, etc?\nAfter manually setting a job priority value,\n  how can its priority value be returned to being managed by the\n  priority/multifactor plugin?\nCan I update multiple jobs with a single\n  scontrol command?\nHow could I automatically print a job's\n  Slurm job ID to its standard output?\nIs it possible to write to user stdout?\nWhy are user processes and srun\n  running even though the job is supposed to be completed?\nHow can a job which has exited with a specific exit code\n  be requeued?\nWhy is Slurm unable to set the CPU frequency for jobs?\nCan the salloc command be configured to\n  launch a shell on a node in the job's allocation?\nHow can I set up a private /tmp and /dev/shm for\n  jobs on my machine?\nHow do I configure Slurm to work with System V IPC\n  enabled applications?\nGeneral Troubleshooting\nIf a Slurm daemon core dumps, where can I find the\n  core file?\nHow can I get a backtrace from a core file?\nError Messages\n\"Cannot resolve X plugin operations\" on\n  daemon startup\n\"Credential replayed\" in\n  SlurmdLogFile\n\"Invalid job credential\"\n\"Task launch failed on node ... Job credential\n  replayed\"\n\"Unable to accept new connection: Too many open\n  files\"\nSlurmdDebug fails to log job step information\n  at the appropriate level\n\"Batch JobId=# missing from batch node <node>\n  (not found BatchStartTime after startup)\"\nMulti-Instance GPU not working with Slurm and\n  PMIx; GPUs are \"In use by another client\"\n\"srun: error: Unable to accept connection:\n  Resources temporarily unavailable\"\n\"Warning: Note very large processing time\"\n  in SlurmctldLogFile\n\"Duplicate entry\" causes slurmdbd to\n  fail\n\"Unable to find plugin: serializer/json\"\nThird Party Integrations\nCan Slurm be used with Globus?\nHow can TotalView be configured to operate with\n  Slurm?\nFor ManagementIs Slurm really free?\nYes, Slurm is free and open source:\n\nSlurm is free as defined by the\n  Free Software\n  Foundation\nSlurm\u2019s source code and\n  documentation are\n  publicly available under the GNU GPL v2\nSlurm can be \n  downloaded, used, modified, and redistributed at no monetary cost\nWhy should I use Slurm or other free software?\nFree software, as with proprietary software, varies widely in quality, but the\nmechanism itself has proven to be capable of producing high-quality software\nthat is trusted by companies around the world. The Linux kernel is a prominent\nexample, which is often trusted on web servers, infrastructure servers,\nsupercomputers, and mobile devices.Likewise, Slurm has become a trusted tool in the supercomputing world since\nits initial release in 2002 and the founding of SchedMD in 2010 to continue\ndeveloping Slurm. Today, Slurm powers a majority of the\nTOP500 supercomputers. Customers switching\nfrom commercial workload managers to Slurm typically report higher scalability,\nbetter performance and lower costs.Why should I pay for free software?\nFree software does not mean that it is without cost. Software requires\nsignificant time and expertise to write, test, distribute, and maintain. If the\nsoftware is large and complex, like Slurm or the Linux kernel, these costs can\nbecome very substantial.Slurm is often used for highly important tasks at major computing clusters\naround the world. Due to the extensive features available and the complexity of\nthe code required to provide those features, many organizations prefer to have\nexperts available to provide tailored recommendations and troubleshooting\nassistance. While Slurm has a global development community incorporating leading\nedge technology, SchedMD personnel have\ndeveloped most of the code and can provide competitively priced commercial\nsupport and on-site training.What does \"Slurm\" stand for?\nNothing.Originally, \"SLURM\" (completely capitalized) was an acronym for\n\"Simple Linux Utility for Resource Management\". In 2012 the preferred\ncapitalization was changed to Slurm, and the acronym was dropped \u2014 the\ndevelopers preferred to think of Slurm as \"sophisticated\" rather than \"Simple\"\nby this point. And, as Slurm continued to expand it's scheduling capabilities,\nthe \"Resource Management\" label was also viewed as outdated.For ResearchersHow should I cite work involving Slurm?\nWe recommend citing the peer-reviewed paper from JSSPP 2023:\n\nArchitecture of the Slurm Workload Manager.Jette, M.A., Wickberg, T. (2023). Architecture of the Slurm Workload Manager.\nIn: Klus\u00e1\u010dek, D., Corbal\u00e1n, J., Rodrigo, G.P. (eds) Job Scheduling Strategies\nfor Parallel Processing. JSSPP 2023. Lecture Notes in Computer Science,\nvol 14283. Springer, Cham. https://doi.org/10.1007/978-3-031-43943-8_1\nFor UsersDesigning JobsHow can I run multiple jobs from within a\nsingle script?\nA Slurm job is just a resource allocation. You can execute many\njob steps within that allocation, either in parallel or sequentially.\nSome jobs actually launch thousands of job steps this way. The job\nsteps will be allocated nodes that are not already allocated to\nother job steps. This essentially provides a second level of resource\nmanagement within the job for the job steps.How can I run a job within an existing\njob allocation?\nThere is an srun option --jobid that can be used to specify\na job's ID.\nFor a batch job or within an existing resource allocation, the\nenvironment variable SLURM_JOB_ID has already been defined,\nso all job steps will run within that job allocation unless\notherwise specified.\nThe one exception to this is when submitting batch jobs.\nWhen a batch job is submitted from within an existing batch job,\nit is treated as a new job allocation request and will get a\nnew job ID unless explicitly set with the --jobid option.\nIf you specify that a batch job should use an existing allocation,\nthat job allocation will be released upon the termination of\nthat batch job.Slurm documentation refers to CPUs, cores and threads.\nWhat exactly is considered a CPU?\nIf your nodes are configured with hyperthreading, then a CPU is equivalent\nto a hyperthread.\nOtherwise a CPU is equivalent to a core.\nYou can determine if your nodes have more than one thread per core\nusing the command \"scontrol show node\" and looking at the values of\n\"ThreadsPerCore\".Note that even on systems with hyperthreading enabled, the resources will\ngenerally be allocated to jobs at the level of a core (see NOTE below).\nTwo different jobs will not share a core except through the use of a partition\nOverSubscribe configuration parameter.\nFor example, a job requesting resources for three tasks on a node with\nThreadsPerCore=2 will be allocated two full cores.\nNote that Slurm commands contain a multitude of options to control\nresource allocation with respect to base boards, sockets, cores and threads.(NOTE: An exception to this would be if the system administrator\nconfigured SelectTypeParameters=CR_CPU and each node's CPU count without its\nsocket/core/thread specification. In that case, each thread would be\nindependently scheduled as a CPU. This is not a typical configuration.)How do I run specific tasks on certain nodes\nin my allocation?\nOne of the distribution methods for srun '-m\nor --distribution' is 'arbitrary'.  This means you can tell Slurm to\nlayout your tasks in any fashion you want.  For instance if I had an\nallocation of 2 nodes and wanted to run 4 tasks on the first node and\n1 task on the second and my nodes allocated from SLURM_JOB_NODELIST\nwhere tux[0-1] my srun line would look like this:\nsrun -n5 -m arbitrary -w tux[0,0,0,0,1] hostname\nIf I wanted something similar but wanted the third task to be on tux 1\nI could run this:\nsrun -n5 -m arbitrary -w tux[0,0,1,0,0] hostname\nHere is a simple Perl script named arbitrary.pl that can be ran to easily lay\nout tasks on nodes as they are in SLURM_JOB_NODELIST.\n#!/usr/bin/perl\nmy @tasks = split(',', $ARGV[0]);\nmy @nodes = `scontrol show hostnames $SLURM_JOB_NODELIST`;\nmy $node_cnt = $#nodes + 1;\nmy $task_cnt = $#tasks + 1;\n\nif ($node_cnt < $task_cnt) {\n  print STDERR \"ERROR: You only have $node_cnt nodes, but requested layout on $task_cnt nodes.\\n\";\n  $task_cnt = $node_cnt;\n}\n\nmy $cnt = 0;\nmy $layout;\nforeach my $task (@tasks) {\n  my $node = $nodes[$cnt];\n  last if !$node;\n  chomp($node);\n  for(my $i=0; $i < $task; $i++) {\n    $layout .= \",\" if $layout;\n    $layout .= \"$node\";\n  }\n  $cnt++;\n}\nprint $layout;\nWe can now use this script in our srun line in this fashion.\nsrun -m arbitrary -n5 -w `arbitrary.pl 4,1` -l hostname\nThis will layout 4 tasks on the first node in the allocation and 1\ntask on the second node.How can I get the task ID in the output\nor error file name for a batch job?\nIf you want separate output by task, you will need to build a script\ncontaining this specification. For example:\n$ cat test\n#!/bin/sh\necho begin_test\nsrun -o out_%j_%t hostname\n\n$ sbatch -n7 -o out_%j test\nsbatch: Submitted batch job 65541\n\n$ ls -l out*\n-rw-rw-r--  1 jette jette 11 Jun 15 09:15 out_65541\n-rw-rw-r--  1 jette jette  6 Jun 15 09:15 out_65541_0\n-rw-rw-r--  1 jette jette  6 Jun 15 09:15 out_65541_1\n-rw-rw-r--  1 jette jette  6 Jun 15 09:15 out_65541_2\n-rw-rw-r--  1 jette jette  6 Jun 15 09:15 out_65541_3\n-rw-rw-r--  1 jette jette  6 Jun 15 09:15 out_65541_4\n-rw-rw-r--  1 jette jette  6 Jun 15 09:15 out_65541_5\n-rw-rw-r--  1 jette jette  6 Jun 15 09:15 out_65541_6\n\n$ cat out_65541\nbegin_test\n\n$ cat out_65541_2\ntdev2\nHow does Slurm establish the environment\nfor my job?\nSlurm processes are not run under a shell, but directly exec'ed\nby the slurmd daemon (assuming srun is used to launch\nthe processes).\nThe environment variables in effect at the time the srun command\nis executed are propagated to the spawned processes.\nThe ~/.profile and ~/.bashrc scripts are not executed\nas part of the process launch. You can also look at the --export option of\nsrun and sbatch. See man pages for details.Can the make command\nutilize the resources allocated to a Slurm job?\nYes. There is a patch available for GNU make version 3.81\navailable as part of the Slurm distribution in the file\ncontribs/make-3.81.slurm.patch.  For GNU make version 4.0 you\ncan use the patch in the file contribs/make-4.0.slurm.patch.\nThis patch will use Slurm to launch tasks across a job's current resource\nallocation. Depending upon the size of modules to be compiled, this may\nor may not improve performance. If most modules are thousands of lines\nlong, the use of additional resources should more than compensate for the\noverhead of Slurm's task launch. Use with make's -j option within an\nexisting Slurm allocation. Outside of a Slurm allocation, make's behavior\nwill be unchanged.How can I run an Ansys program with Slurm?\nIf you are talking about an interactive run of the Ansys app, then you can use\nthis simple script (it is for Ansys Fluent):\n$ cat ./fluent-srun.sh\n#!/usr/bin/env bash\nHOSTSFILE=.hostlist-job$SLURM_JOB_ID\nif [ \"$SLURM_PROCID\" == \"0\" ]; then\n    srun hostname -f > $HOSTSFILE\n    fluent -t $SLURM_NTASKS -cnf=$HOSTSFILE -ssh 3d\n    rm -f $HOSTSFILE\nfi\nexit 0\nTo run an interactive session, use srun like this:\n$ srun -n <tasks> ./fluent-srun.sh\nSubmitting JobsWhy are my srun options ignored?\nEverything after the command srun is\nexamined to determine if it is a valid option for srun. The first\ntoken that is not a valid option for srun is considered the command\nto execute and everything after that is treated as an option to\nthe command. For example:\nsrun -N2 uptime -pdebug\nsrun processes \"-N2\" as an option to itself. \"uptime\" is the command to\nexecute and \"-pdebug\" is treated as an option to the uptime command. Depending\non the command and options provided, you may get an invalid option message or\nunexpected behavior if the options happen to be valid.Options for srun should appear before the command to be run:\nsrun -N2 -pdebug uptime\nWhy does the srun --overcommit option not permit multiple jobs\nto run on nodes?\nThe --overcommit option is a means of indicating that a job or job step is willing\nto execute more than one task per processor in the job's allocation. For example,\nconsider a cluster of two processor nodes. The srun execute line may be something\nof this sort\nsrun --ntasks=4 --nodes=1 a.out\nThis will result in not one, but two nodes being allocated so that each of the four\ntasks is given its own processor. Note that the srun --nodes option specifies\na minimum node count and optionally a maximum node count. A command line of\nsrun --ntasks=4 --nodes=1-1 a.out\nwould result in the request being rejected. If the --overcommit option\nis added to either command line, then only one node will be allocated for all\nfour tasks to use.More than one job can execute simultaneously on the same compute resource\n(e.g. CPU) through the use of srun's --oversubscribe option in\nconjunction with the OverSubscribe parameter in Slurm's partition\nconfiguration. See the man pages for srun and slurm.conf for more information.Why is the srun --u/--unbuffered option adding\n  a carriage character return to my output?\nThe libc library used by many programs internally buffers output rather than\nwriting it immediately. This is done for performance reasons.\nThe only way to disable this internal buffering is to configure the program to\nwrite to a pseudo terminal (PTY) rather than to a regular file.\nThis configuration causes some implementations of libc to prepend the\ncarriage return character before all line feed characters.\nRemoving the carriage return character would result in desired formatting\nin some instances, while causing bad formatting in other cases.\nIn any case, Slurm is not adding the carriage return character, but displaying\nthe actual program's output.What is the difference between the sbatch\n  and srun commands?\nThe srun command has two different modes of operation. First, if not run within\nan existing job (i.e. not within a Slurm job allocation created by salloc or\nsbatch), then it will create a job allocation and spawn an application.\nIf run within an existing allocation, the srun command only spawns the\napplication.\nFor this question, we will only address the first mode of operation and compare\ncreating a job allocation using the sbatch and srun commands.The srun command is designed for interactive use, with someone monitoring\nthe output.\nThe output of the application is seen as output of the srun command,\ntypically at the user's terminal.\nThe sbatch command is designed to submit a script for later execution and its\noutput is written to a file.\nCommand options used in the job allocation are almost identical.\nThe most noticeable difference in options is that the sbatch command supports\nthe concept of job arrays, while srun does not.\nAnother significant difference is in fault tolerance.\nFailures involving sbatch jobs typically result in the job being requeued\nand executed again, while failures involving srun typically result in an\nerror message being generated with the expectation that the user will respond\nin an appropriate fashion.Can tasks be launched with a remote (pseudo)\nterminal?\nThe best method is to use salloc with\nuse_interactive_step set in the LaunchParameters option in\nslurm.conf. See\ngetting shell prompts in interactive mode.How can I get shell prompts in interactive\nmode?\nStarting in 20.11, the recommended way to get an interactive shell prompt is\nto configure use_interactive_step in slurm.conf:\nLaunchParameters=use_interactive_step\nThis configures salloc to automatically launch an interactive\nshell via srun on a node in the allocation whenever\nsalloc is called without a program to execute.By default, use_interactive_step creates an interactive step on\na node in the allocation and runs the shell in that step. An interactive step\nis to an interactive shell what a batch step is to a batch script - both have\naccess to all resources in the allocation on the node they are running on, but\ndo not \"consume\" them.Note that beginning in 20.11, steps created by srun are now exclusive. This\nmeans that the previously-recommended way to get an interactive shell,\nsrun --pty $SHELL, will no longer work, as the\nshell's step will now consume all resources on the node and cause subsequent\nsrun calls to pend.An alternative but not recommended method is to make use of srun's\n--pty option, (e.g. srun --pty bash -i).\nSrun's --pty option runs task zero in pseudo terminal mode. Bash's\n-i option instructs it to run in interactive mode (with prompts).\nHowever, unlike the batch or interactive steps, this launches a step which\nconsumes all resources in the job. This means that subsequent steps cannot be\nlaunched in the job unless they use the --overlap option. If task plugins\nare configured, the shell is limited to CPUs of the first task. Subsequent\nsteps (which must be launched with --overlap) may be limited to fewer\nresources than expected or may fail to launch tasks altogether if multiple\nnodes were requested.  Therefore, this alternative should rarely be used;\nsalloc should be used instead.\nCan Slurm export an X11 display on an allocated compute node?\nYou can use the X11 builtin feature starting at version 17.11.\nIt is enabled by setting PrologFlags=x11 in slurm.conf.\nOther X11 plugins must be deactivated.\n\nRun it as shown:\n\n$ ssh -X user@login1\n$ srun -n1 --pty --x11 xclock\n\nAn alternative for older versions is to build and install an optional SPANK\nplugin for that functionality. Instructions to build and install the plugin\nfollow. This SPANK plugin will not work if used in combination with native X11\nsupport so you must disable it compiling Slurm with --disable-x11. This\nplugin relies on openssh library and it provides features such as GSSAPI\nsupport. Update the Slurm installation path as needed:\n# It may be obvious, but don't forget the -X on ssh\n$ ssh -X alex@testserver.com\n\n# Get the plugin\n$ mkdir git\n$ cd git\n$ git clone https://github.com/hautreux/slurm-spank-x11.git\n$ cd slurm-spank-x11\n\n# Manually edit the X11_LIBEXEC_PROG macro definition\n$ vi slurm-spank-x11.c\n$ vi slurm-spank-x11-plug.c\n$ grep \"define X11_\" slurm-spank-x11.c\n#define X11_LIBEXEC_PROG \"/opt/slurm/17.02/libexec/slurm-spank-x11\"\n$ grep \"define X11_LIBEXEC_PROG\" slurm-spank-x11-plug.c\n#define X11_LIBEXEC_PROG \"/opt/slurm/17.02/libexec/slurm-spank-x11\"\n\n\n# Compile\n$ gcc -g -o slurm-spank-x11 slurm-spank-x11.c\n$ gcc -g -I/opt/slurm/17.02/include -shared -fPIC -o x11.so slurm-spank-x11-plug.c\n\n# Install\n$ mkdir -p /opt/slurm/17.02/libexec\n$ install -m 755 slurm-spank-x11 /opt/slurm/17.02/libexec\n$ install -m 755 x11.so /opt/slurm/17.02/lib/slurm\n\n# Configure\n$ echo -e \"optional x11.so\" >> /opt/slurm/17.02/etc/plugstack.conf\n$ cd ~/tests\n\n# Run\n$ srun -n1 --pty --x11 xclock\nalex@node1's password:\nSchedulingWhy is my job not running?\nThe answer to this question depends on a lot of factors. The main one is which\nscheduler is used by Slurm. Executing the command\n scontrol show config | grep SchedulerType\n will supply this information. If the scheduler type is builtin, then\njobs will be executed in the order of submission for a given partition. Even if\nresources are available to initiate your job immediately, it will be deferred\nuntil no previously submitted job is pending. If the scheduler type is backfill,\nthen jobs will generally be executed in the order of submission for a given partition\nwith one exception: later submitted jobs will be initiated early if doing so does\nnot delay the expected execution time of an earlier submitted job. In order for\nbackfill scheduling to be effective, users' jobs should specify reasonable time\nlimits. If jobs do not specify time limits, then all jobs will receive the same\ntime limit (that associated with the partition), and the ability to backfill schedule\njobs will be limited. The backfill scheduler does not alter job specifications\nof required or excluded nodes, so jobs which specify nodes will substantially\nreduce the effectiveness of backfill scheduling. See the \nbackfill section for more details. For any scheduler, you can check priorities\nof jobs using the command scontrol show job.\nOther reasons can include waiting for resources, memory, qos, reservations, etc.\nAs a guideline, issue an scontrol show job <jobid>\nand look at the field State and Reason to investigate the cause.\nA full list and explanation of the different Reasons can be found in the\nresource limits page.Why is the Slurm backfill scheduler not starting my job?\n\nThe most common problem is failing to set job time limits. If all jobs have\nthe same time limit (for example the partition's time limit), then backfill\nwill not be effective. Note that partitions can have both default and maximum\ntime limits, which can be helpful in configuring a system for effective\nbackfill scheduling.In addition, there are a multitude of backfill scheduling parameters\nwhich can impact which jobs are considered for backfill scheduling, such\nas the maximum number of jobs tested per user. For more information see\nthe slurm.conf man page and check the configuration of SchedulerParameters\non your system.Killed JobsWhy is my job killed prematurely?\nSlurm has a job purging mechanism to remove inactive jobs (resource allocations)\nbefore reaching its time limit, which could be infinite.\nThis inactivity time limit is configurable by the system administrator.\nYou can check its value with the command\nscontrol show config | grep InactiveLimit\nThe value of InactiveLimit is in seconds.\nA zero value indicates that job purging is disabled.\nA job is considered inactive if it has no active job steps or if the srun\ncommand creating the job is not responding.\nIn the case of a batch job, the srun command terminates after the job script\nis submitted.\nTherefore batch job pre- and post-processing is limited to the InactiveLimit.\nContact your system administrator if you believe the InactiveLimit value\nshould be changed.Why is my batch job that launches no\njob steps being killed?\nSlurm has a configuration parameter InactiveLimit intended\nto kill jobs that do not spawn any job steps for a configurable\nperiod of time. Your system administrator may modify the InactiveLimit\nto satisfy your needs. Alternately, you can just spawn a job step\nat the beginning of your script to execute in the background. It\nwill be purged when your script exits or your job otherwise terminates.\nA line of this sort near the beginning of your script should suffice:\nsrun -N1 -n1 sleep 999999 &What does \"srun: Force Terminated job\"\nindicate?\nThe srun command normally terminates when the standard output and\nerror I/O from the spawned tasks end. This does not necessarily\nhappen at the same time that a job step is terminated. For example,\na file system problem could render a spawned task non-killable\nat the same time that I/O to srun is pending. Alternately a network\nproblem could prevent the I/O from being transmitted to srun.\nIn any event, the srun command is notified when a job step is\nterminated, either upon reaching its time limit or being explicitly\nkilled. If the srun has not already terminated, the message\n\"srun: Force Terminated job\" is printed.\nIf the job step's I/O does not terminate in a timely fashion\nthereafter, pending I/O is abandoned and the srun command\nexits.What does this mean:\n\"srun: First task exited 30s ago\"\nfollowed by \"srun Job Failed\"?\nThe srun command monitors when tasks exit. By default, 30 seconds\nafter the first task exits, the job is killed.\nThis typically indicates some type of job failure and continuing\nto execute a parallel job when one of the tasks has exited is\nnot normally productive. This behavior can be changed using srun's\n--wait=<time> option to either change the timeout\nperiod or disable the timeout altogether. See srun's man page\nfor details.Managing JobsHow can I temporarily prevent a job from running\n(e.g. place it into a hold state)?\nThe easiest way to do this is to change a job's earliest begin time\n(optionally set at job submit time using the --begin option).\nThe example below places a job into hold state (preventing its initiation\nfor 30 days) and later permitting it to start now.\n$ scontrol update JobId=1234 StartTime=now+30days\n... later ...\n$ scontrol update JobId=1234 StartTime=now\nCan I change my job's size after it has started\nrunning?\nSlurm supports the ability to decrease the size of jobs.\nRequesting fewer hardware resources, and changing partition, qos,\nreservation, licenses, etc. is only allowed for pending jobs.Use the scontrol command to change a job's size either by specifying\na new node count (NumNodes=) for the job or identify the specific nodes\n(NodeList=) that you want the job to retain.\nAny job steps running on the nodes which are relinquished by the job will be\nkilled unless initiated with the --no-kill option.\nAfter the job size is changed, some environment variables created by Slurm\ncontaining information about the job's environment will no longer be valid and\nshould either be removed or altered (e.g. SLURM_JOB_NUM_NODES,\nSLURM_JOB_NODELIST and SLURM_NTASKS).\nThe scontrol command will generate a script that can be executed to\nreset local environment variables.\nYou must retain the SLURM_JOB_ID environment variable in order for the\nsrun command to gather information about the job's current state and\nspecify the desired node and/or task count in subsequent srun invocations.\nA new accounting record is generated when a job is resized, showing the job to\nhave been resubmitted and restarted at the new size.\nAn example is shown below.\n#!/bin/bash\nsrun my_big_job\nscontrol update JobId=$SLURM_JOB_ID NumNodes=2\n. slurm_job_${SLURM_JOB_ID}_resize.sh\nsrun -N2 my_small_job\nrm slurm_job_${SLURM_JOB_ID}_resize.*\nWhy does squeue (and \"scontrol show\njobid\") sometimes not display a job's  estimated start time?\nWhen the backfill scheduler is configured, it provides an estimated start time\nfor jobs that are candidates for backfill. Pending jobs with dependencies\nwill not have an estimate as it is difficult to predict what resources will\nbe available when the jobs they are dependent on terminate. Also note that\nthe estimate is better for jobs expected to start soon, as most running jobs\nend before their estimated time. There are other restrictions on backfill that\nmay apply. See the backfill section for more details.\nCan squeue output be color coded?\nThe squeue command output is not color coded, but other tools can be used to\nadd color. One such tool is ColorWrapper\n(https://github.com/rrthomas/cw).\nA sample ColorWrapper configuration file and output are shown below.\npath /bin:/usr/bin:/sbin:/usr/sbin:<env>\nusepty\nbase green+\nmatch red:default (Resources)\nmatch black:default (null)\nmatch black:cyan N/A\nregex cyan:default  PD .*$\nregex red:default ^\\d*\\s*C .*$\nregex red:default ^\\d*\\s*CG .*$\nregex red:default ^\\d*\\s*NF .*$\nregex white:default ^JOBID.*\nWhy is my job/node in a COMPLETING state?\nWhen a job is terminating, both the job and its nodes enter the COMPLETING state.\nAs the Slurm daemon on each node determines that all processes associated with\nthe job have terminated, that node changes state to IDLE or some other appropriate\nstate for use by other jobs.\nWhen every node allocated to a job has determined that all processes associated\nwith it have terminated, the job changes state to COMPLETED or some other\nappropriate state (e.g. FAILED).\nNormally, this happens within a second.\nHowever, if the job has processes that cannot be terminated with a SIGKILL\nsignal, the job and one or more nodes can remain in the COMPLETING state\nfor an extended period of time.\nThis may be indicative of processes hung waiting for a core file\nto complete I/O or operating system failure.\nIf this state persists, the system administrator should check for processes\nassociated with the job that cannot be terminated then use the\nscontrol command to change the node's\nstate to DOWN (e.g. \"scontrol update NodeName=name State=DOWN Reason=hung_completing\"),\nreboot the node, then reset the node's state to IDLE\n(e.g. \"scontrol update NodeName=name State=RESUME\").\nNote that setting the node DOWN will terminate all running or suspended\njobs associated with that node.\nAn alternative is to set the node's state to DRAIN until all jobs\nassociated with it terminate before setting it DOWN and re-booting.Note that Slurm has two configuration parameters that may be used to\nautomate some of this process.\nUnkillableStepProgram specifies a program to execute when\nnon-killable processes are identified.\nUnkillableStepTimeout specifies how long to wait for processes\nto terminate.\nSee the \"man slurm.conf\" for more information about these parameters.How can a job in a complete or failed state be requeued?\n\nSlurm supports requeuing jobs in a done or failed state. Use the\ncommand:scontrol requeue job_idThe job will then be requeued back in the PENDING state and scheduled again.\nSee man(1) scontrol.\nConsider a simple job like this:\n$cat zoppo\n#!/bin/sh\necho \"hello, world\"\nexit 10\n\n$sbatch -o here ./zoppo\nSubmitted batch job 10\n\nThe job finishes in FAILED state because it exits with\na non zero value. We can requeue the job back to\nthe PENDING state and the job will be dispatched again.\n\n$ scontrol requeue 10\n$ squeue\n      JOBID PARTITION  NAME     USER   ST   TIME  NODES NODELIST(REASON)\n      10      mira    zoppo    david  PD   0:00    1   (NonZeroExitCode)\n$ squeue\n    JOBID PARTITION   NAME     USER ST     TIME  NODES NODELIST(REASON)\n      10      mira    zoppo    david  R    0:03    1      alanz1\nSlurm supports requeuing jobs in a hold state with the command:scontrol requeuehold job_idThe job can be in state RUNNING, SUSPENDED, COMPLETED or FAILED\nbefore being requeued.\n$ scontrol requeuehold 10\n$ squeue\n    JOBID PARTITION  NAME     USER ST       TIME  NODES NODELIST(REASON)\n    10      mira    zoppo    david PD       0:00      1 (JobHeldUser)\nWhy is sview not coloring/highlighting nodes\n    properly?\nsview color-coding is affected by the GTK theme. The node status grid\nis made up of button widgets and certain GTK themes don't show the color\nsetting as desired. Changing GTK themes can restore proper color-coding.Why is my MPICH2 or MVAPICH2 job not running with\nSlurm? Why does the DAKOTA program not run with Slurm?\nThe Slurm library used to support MPICH2 or MVAPICH2 references a variety of\nsymbols. If those symbols resolve to functions or variables in your program\nrather than the appropriate library, the application will fail. For example\nDAKOTA, versions 5.1 and\nolder, contains a function named regcomp, which will get used rather\nthan the POSIX regex functions. Rename DAKOTA's function and\nreferences from regcomp to something else to make it work properly.Resource LimitsWhy are my resource limits not propagated?\nWhen the srun command executes, it captures the\nresource limits in effect at submit time on the node where srun executes.\nThese limits are propagated to the allocated nodes before initiating the\nuser's job.\nThe Slurm daemons running on the allocated nodes then try to establish\nidentical resource limits for the job being initiated.\nThere are several possible reasons for not being able to establish those\nresource limits.\nThe hard resource limits applied to Slurm's slurmd daemon are lower\nthan the user's soft resources limits on the submit host. Typically\nthe slurmd daemon is initiated by the init daemon with the operating\nsystem default limits. This may be addressed either through use of the\nulimit command in the /etc/sysconfig/slurm file or enabling\nPAM in Slurm.\nThe user's hard resource limits on the allocated node are lower than\nthe same user's soft hard resource limits on the node from which the\njob was submitted. It is recommended that the system administrator\nestablish uniform hard resource limits for users on all nodes\nwithin a cluster to prevent this from occurring.\nPropagateResourceLimits or PropagateResourceLimitsExcept parameters are\nconfigured in slurm.conf and avoid propagation of specified limits.\nNOTE: This may produce the error message\n\"Can't propagate RLIMIT_...\".\nThe error message is printed only if the user explicitly specifies that\nthe resource limit should be propagated or the srun command is running\nwith verbose logging of actions from the slurmd daemon (e.g. \"srun -d6 ...\").Why are jobs not getting the appropriate\nmemory limit?\nThis is probably a variation on the locked memory limit\nproblem described above.\nUse the same solution for the AS (Address Space), RSS (Resident Set Size),\nor other limits as needed.Why is my MPI job  failing due to the\nlocked memory (memlock) limit being too low?\nBy default, Slurm propagates all of your resource limits at the\ntime of job submission to the spawned tasks.\nThis can be disabled by specifically excluding the propagation of\nspecific limits in the slurm.conf file. For example\nPropagateResourceLimitsExcept=MEMLOCK might be used to\nprevent the propagation of a user's locked memory limit from a\nlogin node to a dedicated node used for his parallel job.\nIf the user's resource limit is not propagated, the limit in\neffect for the slurmd daemon will be used for the spawned job.\nA simple way to control this is to ensure that user root has a\nsufficiently large resource limit and ensuring that slurmd takes\nfull advantage of this limit. For example, you can set user root's\nlocked memory limit ulimit to be unlimited on the compute nodes (see\n\"man limits.conf\") and ensuring that slurmd takes\nfull advantage of this limit (e.g. by adding \"LimitMEMLOCK=infinity\"\nto your systemd's slurmd.service file). It may also be desirable to lock\nthe slurmd daemon's memory to help ensure that it keeps responding if memory\nswapping begins. A sample /etc/sysconfig/slurm which can be read from\nsystemd is shown below.\nRelated information about PAM is also available.\n#\n# Example /etc/sysconfig/slurm\n#\n# Memlocks the slurmd process's memory so that if a node\n# starts swapping, the slurmd will continue to respond\nSLURMD_OPTIONS=\"-M\"\nFor AdministratorsTest EnvironmentsCan multiple Slurm systems be run in\nparallel for testing purposes?\nYes, this is a great way to test new versions of Slurm.\nJust install the test version in a different location with a different\nslurm.conf.\nThe test system's slurm.conf should specify different\npathnames and port numbers to avoid conflicts.\nThe only problem is if more than one version of Slurm is configured\nwith burst_buffer/* plugins or others that may interact with external\nsystem APIs.\nIn that case, there can be conflicting API requests from\nthe different Slurm systems.\nThis can be avoided by configuring the test system with burst_buffer/none.Can Slurm emulate a larger cluster?\nYes, this can be useful for testing purposes.\nIt has also been used to partition \"fat\" nodes into multiple Slurm nodes.\nThere are two ways to do this.\nThe best method for most conditions is to run one slurmd\ndaemon per emulated node in the cluster as follows.\nWhen executing the configure program, use the option\n--enable-multiple-slurmd (or add that option to your ~/.rpmmacros\nfile).\nBuild and install Slurm in the usual manner.\nIn slurm.conf define the desired node names (arbitrary\nnames used only by Slurm) as NodeName along with the actual\naddress of the physical node in NodeHostname. Multiple\nNodeName values can be mapped to a single\nNodeHostname.  Note that each NodeName on a single\nphysical node needs to be configured to use a different port number\n(set Port to a unique value on each line for each node).  You\nwill also want to use the \"%n\" symbol in slurmd related path options in\nslurm.conf (SlurmdLogFile and SlurmdPidFile). \nWhen starting the slurmd daemon, include the NodeName\nof the node that it is supposed to serve on the execute line (e.g.\n\"slurmd -N hostname\").\n This is an example of the slurm.conf file with the  emulated nodes\nand ports configuration. Any valid value for the CPUs, memory or other\nvalid node resources can be specified.\n\nNodeName=dummy26[1-100] NodeHostName=achille Port=[6001-6100] NodeAddr=127.0.0.1 CPUs=4 RealMemory=6000\nPartitionName=mira Default=yes Nodes=dummy26[1-100]\nSee the\nProgrammers Guide\nfor more details about configuring multiple slurmd support.In order to emulate a really large cluster, it can be more\nconvenient to use a single slurmd daemon.\nThat daemon will not be able to launch many tasks, but can\nsuffice for developing or testing scheduling software.\nDo not run job steps with more than a couple of tasks each\nor execute more than a few jobs at any given time.\nDoing so may result in the slurmd daemon exhausting its\nmemory and failing.\nUse this method with caution.\nExecute the configure program with your normal options\nplus --enable-front-end (this will define HAVE_FRONT_END in\nthe resulting config.h file.\nBuild and install Slurm in the usual manner.\nIn slurm.conf define the desired node names (arbitrary\nnames used only by Slurm) as NodeName along with the actual\nname and address of the one physical node in NodeHostName\nand NodeAddr.\nUp to 64k nodes can be configured in this virtual cluster.\nStart your slurmctld and one slurmd daemon.\nIt is advisable to use the \"-c\" option to start the daemons without\ntrying to preserve any state files from previous executions.\nBe sure to use the \"-c\" option when switching from this mode too.\nCreate job allocations as desired, but do not run job steps\nwith more than a couple of tasks.\n\n$ ./configure --enable-debug --enable-front-end --prefix=... --sysconfdir=...\n$ make install\n$ grep NodeHostName slurm.conf\nNodeName=dummy[1-1200] NodeHostName=localhost NodeAddr=127.0.0.1\n$ slurmctld -c\n$ slurmd -c\n$ sinfo\nPARTITION AVAIL  TIMELIMIT NODES  STATE NODELIST\npdebug*      up      30:00  1200   idle dummy[1-1200]\n$ cat tmp\n#!/bin/bash\nsleep 30\n$ srun -N200 -b tmp\nsrun: jobid 65537 submitted\n$ srun -N200 -b tmp\nsrun: jobid 65538 submitted\n$ srun -N800 -b tmp\nsrun: jobid 65539 submitted\n$ squeue\nJOBID PARTITION  NAME   USER  ST  TIME  NODES NODELIST(REASON)\n65537    pdebug   tmp  jette   R  0:03    200 dummy[1-200]\n65538    pdebug   tmp  jette   R  0:03    200 dummy[201-400]\n65539    pdebug   tmp  jette   R  0:02    800 dummy[401-1200]\nCan Slurm emulate nodes with more\nresources than physically exist on the node?\nYes. In the slurm.conf file, configure SlurmdParameters=config_overrides\nand specify\nany desired node resource specifications (CPUs, Sockets,\nCoresPerSocket, ThreadsPerCore, and/or TmpDisk).\nSlurm will use the resource specification for each node that is\ngiven in slurm.conf and will not check these specifications\nagainst those actually found on the node. The system would best be configured\nwith TaskPlugin=task/none, so that launched tasks can run on any\navailable CPU under operating system control.Build and InstallWhy aren't pam_slurm.so, auth_none.so, or other components in a\nSlurm RPM?\nIt is possible that at build time the required dependencies for building the\nlibrary are missing. If you want to build the library then install pam-devel\nand compile again. See the file slurm.spec in the Slurm distribution for a list\nof other options that you can specify at compile time with rpmbuild flags\nand your rpmmacros file.The auth_none plugin is in a separate RPM and not built by default.\nUsing the auth_none plugin means that Slurm communications are not\nauthenticated, so you probably do not want to run in this mode of operation\nexcept for testing purposes. If you want to build the auth_none RPM then\nadd --with auth_none on the rpmbuild command line or add\n%_with_auth_none to your ~/rpmmacros file. See the file slurm.spec\nin the Slurm distribution for a list of other options.How can I build Slurm with debugging symbols?\nWhen configuring, run the configure script with --enable-developer option.\nThat will provide asserts, debug messages and the -Werror flag, that\nwill in turn activate --enable-debug.\nWith the --enable-debug flag, the code will be compiled with\n-ggdb3 and -g -O1 -fno-strict-aliasing flags that will produce\nextra debugging information. Another possible option to use is\n--disable-optimizations that will set -O0.\nSee also auxdir/x_ac_debug.m4 for more details.How can a patch file be generated from a Slurm\ncommit in GitHub?\nFind and open the commit in GitHub then append \".patch\" to the URL and save\nthe resulting file. For an example, see:\n\nhttps://github.com/SchedMD/slurm/commit/91e543d433bed11e0df13ce0499be641774c99a3.patch\nHow can I apply a patch to my Slurm source?\n\nIf you have a patch file that you need to apply to your source, such as a\nsecurity or bug fix patch supplied by SchedMD's support, you can do\nso with the patch command. You would first extract the contents of the\nsource tarball for the version you are using. You can then apply the patch\nto the extracted source. Below is an example of how to do this with the\nsource for Slurm 23.11.1:\n\n$ tar xjvf slurm-23.11.1.tar.bz2 > /dev/null\n$ patch -p1 -d slurm-23.11.1/ < example.patch\npatching file src/slurmctld/step_mgr.c\n\nOnce the patch has been applied to the source code, you can proceed to\nbuild Slurm as you would normally if you build with make. If you use\nrpmbuild to build Slurm, you will have to create a tarball with the\npatched files. The filename of the tarball must match the original filename\nto avoid errors.\n\n$ tar cjvf slurm-23.11.1.tar.bz2 slurm-23.11.1/ > /dev/null\n$ rpmbuild -ta slurm-23.11.1.tar.bz2 > /dev/null\n\nWhy am I being offered an automatic update for Slurm?\n\nEPEL has added Slurm packages to their repository to make them more widely\navailable to the Linux community. However, this packaged version is not\nsupported or maintained by SchedMD, and is not recommend for customers at this\ntime. If you are using the EPEL repo you could be offered an update for Slurm\nthat you may not anticipate. In order to prevent Slurm from being upgraded\nunintentionally, we recommend you modify the EPEL repository configuration file\nto exclude all Slurm packages from automatic updates.\nexclude=slurm*\nCluster ManagementHow should I relocate the primary or\nbackup controller?\nIf the cluster's computers used for the primary or backup controller\nwill be out of service for an extended period of time, it may be desirable\nto relocate them. In order to do so, follow this procedure:\n(Slurm 23.02 and older) Drain the cluster of running jobs\nStop all Slurm daemons\nModify the SlurmctldHost values in the slurm.conf file\nDistribute the updated slurm.conf file to all nodes\nCopy the StateSaveLocation directory to the new host and\nmake sure the permissions allow the SlurmUser to read and write it.\nRestart all Slurm daemons\nStarting with Slurm 23.11, jobs that were started by the old controller will\nreceive the updated controller address and will continue and finish normally.\nOn older versions, jobs started by the old controller will still try to report\nback to the older controller.\nIn both cases, there should be no loss of any pending jobs.\nEnsure that any nodes added to the cluster have a current slurm.conf\nfile installed.CAUTION: If two nodes are simultaneously configured as the primary\ncontroller (two nodes on which SlurmctldHost specify the local host\nand the slurmctld daemon is executing on each), system behavior will be\ndestructive. If a compute node has an incorrect SlurmctldHost parameter,\nthat node may be rendered unusable, but no other harm will result.Do I need to maintain synchronized\nclocks on the cluster?\nIn general, yes. Having inconsistent clocks may cause nodes to\nbe unusable. Slurm log files should contain references to\nexpired credentials. For example:\nerror: Munge decode failed: Expired credential\nENCODED: Wed May 12 12:34:56 2008\nDECODED: Wed May 12 12:01:12 2008\nHow can I stop Slurm from scheduling jobs?\nYou can stop Slurm from scheduling jobs on a per partition basis by setting\nthat partition's state to DOWN. Set its state UP to resume scheduling.\nFor example:\n$ scontrol update PartitionName=foo State=DOWN\n$ scontrol update PartitionName=bar State=UP\nHow can I dry up the workload for a\nmaintenance period?\nCreate a resource reservation as described in Slurm's\nResource Reservation Guide.What should I be aware of when upgrading Slurm?\nRefer to the Upgrade Guide for details.Is there anything exceptional to be aware of when\nupgrading my database server?\nGenerally, no. Special cases are noted in the \nDatabase server section of the Upgrade Guide.When adding a new cluster, how can the Slurm cluster\nconfiguration be copied from an existing cluster to the new cluster?\nAccounts need to be configured for the cluster. An easy way to copy information from\nan existing cluster is to use the sacctmgr command to dump that cluster's information,\nmodify it using some editor, the load the new information using the sacctmgr\ncommand. See the sacctmgr man page for details, including an example.How could some jobs submitted immediately before\nthe slurmctld daemon crashed be lost?\nAny time the slurmctld daemon or hardware fails before state information reaches\ndisk can result in lost state.\nSlurmctld writes state frequently (every five seconds by default), but with\nlarge numbers of jobs, the formatting and writing of records can take seconds\nand recent changes might not be written to disk.\nAnother example is if the state information is written to file, but that\ninformation is cached in memory rather than written to disk when the node fails.\nThe interval between state saves being written to disk can be configured at\nbuild time by defining SAVE_MAX_WAIT to a different value than five.Is resource limit propagation\nuseful on a homogeneous cluster?\nResource limit propagation permits a user to modify resource limits\nand submit a job with those limits.\nBy default, Slurm automatically propagates all resource limits in\neffect at the time of job submission to the tasks spawned as part\nof that job.\nSystem administrators can utilize the PropagateResourceLimits\nand PropagateResourceLimitsExcept configuration parameters to\nchange this behavior.\nUsers can override defaults using the srun --propagate\noption.\nSee \"man slurm.conf\" and \"man srun\" for more information\nabout these options.Why are the resource limits set in the\ndatabase not being enforced?\nIn order to enforce resource limits, set the value of\nAccountingStorageEnforce in each cluster's slurm.conf configuration\nfile appropriately. If AccountingStorageEnforce does not contains\nan option of \"limits\", then resource limits will not be enforced on that cluster.\nSee Resource Limits for more information.Can Slurm be configured to manage licenses?\nSlurm is not currently integrated with FlexLM, but it does provide for the\nallocation of global resources called licenses. Use the Licenses configuration\nparameter in your slurm.conf file (e.g. \"Licenses=foo:10,bar:20\").\nJobs can request licenses and be granted exclusive use of those resources\n(e.g. \"sbatch --licenses=foo:2,bar:1 ...\").\nIt is not currently possible to change the total number of licenses on a system\nwithout restarting the slurmctld daemon, but it is possible to dynamically\nreserve licenses and remove them from being available to jobs on the system\n(e.g. \"scontrol update reservation=licenses_held licenses=foo:5,bar:2\").How easy is it to switch from PBS or Torque to Slurm?\nA lot of users don't even notice the difference.\nSlurm has wrappers available for the mpiexec, pbsnodes, qdel, qhold, qrls,\nqstat, and qsub commands (see contribs/torque in the distribution and the\n\"slurm-torque\" RPM).\nThere is also a wrapper for the showq command at\n\nhttps://github.com/pedmon/slurm_showq.Slurm recognizes and translates the \"#PBS\" options in batch scripts.\nMost, but not all options are supported.Slurm also includes a SPANK plugin that will set all of the PBS environment\nvariables based upon the Slurm environment (e.g. PBS_JOBID, PBS_JOBNAME,\nPBS_WORKDIR, etc.).\nOne environment not set by PBS_ENVIRONMENT, which if set would result in the\nfailure of some MPI implementations.\nThe plugin will be installed in\n<install_directory>/lib/slurm/spank_pbs.so\nSee the SPANK man page for configuration details.What might account for MPI performance being below\nthe expected level?\nStarting the slurmd daemons with limited locked memory can account for this.\nAdding the line \"ulimit -l unlimited\" to the /etc/sysconfig/slurm file can\nfix this.How do I safely remove partitions?\n\nPartitions should be removed using the\n\"scontrol delete PartitionName=<partition>\" command. This is because\nscontrol will prevent any partitions from being removed that are in use.\nPartitions need to be removed from the slurm.conf after being removed using\nscontrol or they will return after a restart.\nAn existing job's partition(s) can be updated with the \"scontrol update\nJobId=<jobid> Partition=<partition(s)>\" command.\nRemoving a partition from the slurm.conf and restarting will cancel any existing\njobs that reference the removed partitions.\nHow can a routing queue be configured?\nA job submit plugin is designed to have access to a job request from a user,\nplus information about all of the available system partitions/queue.\nAn administrator can write a C plugin or LUA script to set an incoming job's\npartition based upon its size, time limit, etc.\nSee the  Job Submit Plugin API\nguide for more information.\nAlso see the available job submit plugins distributed with Slurm for examples\n(look in the \"src/plugins/job_submit\" directory).Accounting DatabaseWhy should I use the slurmdbd instead of the\nregular database plugins?\nWhile the normal storage plugins will work fine without the added\nlayer of the slurmdbd there are some great benefits to using the\nslurmdbd.\nAdded security.  Using the slurmdbd you can have an authenticated\nconnection to the database.\nOffloading processing from the controller. With the slurmdbd there is no\nslowdown to the controller due to a slow or overloaded database.\nKeeping enterprise wide accounting from all Slurm clusters in one database.\nThe slurmdbd is multi-threaded and designed to handle all the\naccounting for the entire enterprise.\nWith the database plugins you can query with sacct accounting stats from\nany node Slurm is installed on. With the slurmdbd you can also query any\ncluster using the slurmdbd from any other cluster's nodes. Other tools like\nsreport are also available.\nHow can I rebuild the database hierarchy?\nIf you see errors of this sort:\nerror: Can't find parent id 3358 for assoc 1504, this should never happen.\nin the slurmctld log file, this is indicative that the database hierarchy\ninformation has been corrupted, typically due to a hardware failure or\nadministrator error in directly modifying the database. In order to rebuild\nthe database information, start the slurmdbd daemon with the \"-R\" option\nfollowed by an optional comma separated list of cluster names to operate on.How critical is configuring high availability for my\ndatabase?\nConsider if you really need a high-availability MySQL setup. A short outage\nof slurmdbd is not a problem, because slurmctld will store all data in memory\nand send it to slurmdbd when it resumes operations. The slurmctld daemon will\nalso cache all user limits and fair share information.\nYou cannot use NDB, since SlurmDBD's MySQL implementation uses keys on BLOB\nvalues (and potentially other features on the incompatibility list).\nYou can set up \"classical\" Linux HA, with heartbeat/corosync to migrate IP\nbetween primary/backup mysql servers and:\n\nConfigure one way replication of mysql, and change primary/backup roles on\nfailure\nUse shared storage for primary/backup mysql servers database, and start\nbackup on primary mysql failure.\n\n\nHow can I use double quotes in MySQL queries?\nExecute:\nSET session sql_mode='ANSI_QUOTES';\nThis will allow double quotes in queries like this:\nshow columns from \"tux_assoc_table\" where Field='is_def';\nCompute Nodes (slurmd)Why is a node shown in state\nDOWN when the node has registered for service?\nThe configuration parameter ReturnToService in slurm.conf\ncontrols how DOWN nodes are handled.\nSet its value to one in order for DOWN nodes to automatically be\nreturned to service once the slurmd daemon registers\nwith a valid node configuration.\nA value of zero is the default and results in a node staying DOWN\nuntil an administrator explicitly returns it to service using\nthe command \"scontrol update NodeName=whatever State=RESUME\".\nSee \"man slurm.conf\" and \"man scontrol\" for more\ndetails.What happens when a node crashes?\nA node is set DOWN when the slurmd daemon on it stops responding\nfor SlurmdTimeout as defined in slurm.conf.\nThe node can also be set DOWN when certain errors occur or the\nnode's configuration is inconsistent with that defined in slurm.conf.\nAny active job on that node will be killed unless it was submitted\nwith the srun option --no-kill.\nAny active job step on that node will be killed.\nSee the slurm.conf and srun man pages for more information.How can I control the execution of multiple\njobs per node?\nThere are two mechanisms to control this.\nIf you want to allocate individual processors on a node to jobs,\nconfigure SelectType=select/cons_tres.\nSee Consumable Resources in Slurm\nfor details about this configuration.\nIf you want to allocate whole nodes to jobs, configure\nconfigure SelectType=select/linear.\nEach partition also has a configuration parameter OverSubscribe\nthat enables more than one job to execute on each node.\nSee man slurm.conf for more information about these\nconfiguration parameters.Why are jobs allocated nodes and then unable\nto initiate programs on some nodes?\nThis typically indicates that the time on some nodes is not consistent\nwith the node on which the slurmctld daemon executes. In order to\ninitiate a job step (or batch job), the slurmctld daemon generates\na credential containing a time stamp. If the slurmd daemon\nreceives a credential containing a time stamp later than the current\ntime or more than a few minutes in the past, it will be rejected.\nIf you check in the SlurmdLogFile on the nodes of interest, you\nwill likely see messages of this sort: \"Invalid job credential from\n<some IP address>: Job credential expired.\" Make the times\nconsistent across all of the nodes and all should be well.Why does slurmctld log that some nodes\nare not responding even if they are not in any partition?\nThe slurmctld daemon periodically pings the slurmd\ndaemon on every configured node, even if not associated with any\npartition. You can control the frequency of this ping with the\nSlurmdTimeout configuration parameter in slurm.conf.How can I easily preserve drained node\ninformation between major Slurm updates?\nMajor Slurm updates generally have changes in the state save files and\ncommunication protocols, so a cold-start (without state) is generally\nrequired. If you have nodes in a DRAIN state and want to preserve that\ninformation, you can easily build a script to preserve that information\nusing the sinfo command. The following command line will report the\nReason field for every node in a DRAIN state and write the output\nin a form that can be executed later to restore state.\nsinfo -t drain -h -o \"scontrol update nodename='%N' state=drain reason='%E'\"\nDoes anyone have an example node\nhealth check script for Slurm?\nProbably the most comprehensive and lightweight health check tool out\nthere is\nNode Health Check.\nIt has integration with Slurm as well as Torque resource managers.Why doesn't the HealthCheckProgram\nexecute on DOWN nodes?\nHierarchical communications are used for sending this message. If there\nare DOWN nodes in the communications hierarchy, messages will need to\nbe re-routed. This limits Slurm's ability to tightly synchronize the\nexecution of the HealthCheckProgram across the cluster, which\ncould adversely impact performance of parallel applications.\nThe use of CRON or node startup scripts may be better suited to ensure\nthat HealthCheckProgram gets executed on nodes that are DOWN\nin Slurm.How can I prevent the slurmd and\nslurmstepd daemons from being killed when a node's memory\nis exhausted?\nYou can set the value in the /proc/self/oom_adj for\nslurmd and slurmstepd by initiating the slurmd\ndaemon with the SLURMD_OOM_ADJ and/or SLURMSTEPD_OOM_ADJ\nenvironment variables set to the desired values.\nA value of -17 typically will disable killing.I see the host of my calling node as 127.0.1.1\n    instead of the correct IP address.  Why is that?\nSome systems by default will put your host in the /etc/hosts file as\nsomething like\n127.0.1.1\tsnowflake.llnl.gov\tsnowflake\nThis will cause srun and Slurm commands to use the 127.0.1.1 address\ninstead of the correct address and prevent communications between nodes.\nThe solution is to either remove this line or configure a different NodeAddr\nthat is known by your other nodes.The CommunicationParameters=NoInAddrAny configuration parameter is subject to\nthis same problem, which can also be addressed by removing the actual node\nname from the \"127.0.1.1\" as well as the \"127.0.0.1\"\naddresses in the /etc/hosts file.  It is ok if they point to\nlocalhost, but not the actual name of the node.How should I add nodes to Slurm?\nThe slurmctld daemon has many bitmaps to track state of nodes and cores in the\ncluster. Adding nodes to a running cluster would require the slurmctld daemon\nto rebuild all of those bitmaps, which the developers feel would be safer to do\nby restarting the daemon. Communications from the slurmd daemons on the compute\nnodes to the slurmctld daemon include a configuration file checksum, so you\nshould maintain the same slurm.conf file on all nodes. The following procedure\nis recommended:\nStop the slurmctld daemon (e.g. systemctl stop slurmctld\n  on the head node)\nUpdate the slurm.conf file on all nodes in the cluster\nRestart the slurmd daemons on all nodes (e.g.\n  systemctl restart slurmd on all nodes)\nRestart the slurmctld daemon (e.g. systemctl start slurmctld\n  on the head node)\nNOTE: Jobs submitted with srun, and that are waiting for an\nallocation, prior to new nodes being added to the slurm.conf can fail if the\njob is allocated one of the new nodes.How should I remove nodes from Slurm?\nTo safely remove a node from a cluster, it's best to drain the node of all jobs.\nThis ensures that job processes aren't running on the node after removal. On\nrestart of the controller, if a node is removed from a running job the\ncontroller will kill the job on any remaining allocated nodes and attempt to\nrequeue the job if possible. The following procedure is recommended:\nDrain node of all jobs (e.g.\n  scontrol update nodename='%N' state=drain reason='removing nodes'\n  )\nStop the slurmctld daemon (e.g. systemctl stop slurmctld\n  on the head node)\nUpdate the slurm.conf file on all nodes in the cluster\nRestart the slurmd daemons on all nodes (e.g.\n  systemctl restart slurmd on all nodes)\nRestart the slurmctld daemon (e.g. systemctl start slurmctld\n  on the head node)\nNOTE: Removing nodes from the cluster may cause some errors in the\nlogs. Verify that any errors in the logs are for nodes that you intended to\nremove.Why is a compute node down with the reason set to\n\"Node unexpectedly rebooted\"?\nThis is indicative of the slurmctld daemon running on the cluster's head node\nas well as the slurmd daemon on the compute node when the compute node reboots.\nIf you want to prevent this condition from setting the node into a DOWN state\nthen configure ReturnToService to 2. See the slurm.conf man page for details.\nOtherwise use scontrol or sview to manually return the node to service.How do I convert my nodes to Control Group (cgroup)\nv2?\nRefer to the cgroup v2 documentation\nfor the conversion procedure.Can Slurm be used to run jobs on\nAmazon's EC2?\nYes, here is a description of Slurm use with\nAmazon's EC2 courtesy of\nAshley Pittman:I do this regularly and have no problem with it, the approach I take is to\nstart as many instances as I want and have a wrapper around\nec2-describe-instances that builds a /etc/hosts file with fixed hostnames\nand the actual IP addresses that have been allocated.  The only other step\nthen is to generate a slurm.conf based on how many node you've chosen to boot\nthat day.  I run this wrapper script on my laptop and it generates the files\nand they rsyncs them to all the instances automatically.One thing I found is that Slurm refuses to start if any nodes specified in\nthe slurm.conf file aren't resolvable, I initially tried to specify cloud[0-15]\nin slurm.conf, but then if I configure less than 16 nodes in /etc/hosts this\ndoesn't work so I dynamically generate the slurm.conf as well as the hosts\nfile.As a comment about EC2 I run just run generic AMIs and have a persistent EBS\nstorage device which I attach to the first instance when I start up.  This\ncontains a /usr/local which has my software like Slurm, pdsh and MPI installed\nwhich I then copy over the /usr/local on the first instance and NFS export to\nall other instances.  This way I have persistent home directories and a very\nsimple first-login script that configures the virtual cluster for me.User ManagementHow can PAM be used to control a user's limits on\nor access to compute nodes?\nTo control a user's limits on a compute node:First, enable Slurm's use of PAM by setting UsePAM=1 in\nslurm.conf.Second, establish PAM configuration file(s) for Slurm in /etc/pam.conf\nor the appropriate files in the /etc/pam.d directory (e.g.\n/etc/pam.d/sshd by adding the line \"account required pam_slurm.so\".\nA basic configuration you might use is:\naccount  required  pam_unix.so\naccount  required  pam_slurm.so\nauth     required  pam_localuser.so\nsession  required  pam_limits.so\nThird, set the desired limits in /etc/security/limits.conf.\nFor example, to set the locked memory limit to unlimited for all users:\n*   hard   memlock   unlimited\n*   soft   memlock   unlimited\nFinally, you need to disable Slurm's forwarding of the limits from the\nsession from which the srun initiating the job ran. By default\nall resource limits are propagated from that session. For example, adding\nthe following line to slurm.conf will prevent the locked memory\nlimit from being propagated:PropagateResourceLimitsExcept=MEMLOCK.To control a user's access to a compute node:The pam_slurm_adopt and pam_slurm modules prevent users from\nlogging into nodes that they have not been allocated (except for user\nroot, which can always login).\nThey are both included with the Slurm distribution.The pam_slurm_adopt module is highly recommended for most installations,\nand is documented in its own guide.pam_slurm is older and less functional.\nThese modules are built by default for RPM packages, but can be disabled using\nthe .rpmmacros option \"%_without_pam 1\" or by entering the command line\noption \"--without pam\" when the configure program is executed.\nTheir source code is in the \"contribs/pam\" and \"contribs/pam_slurm_adopt\"\ndirectories respectively.The use of either pam_slurm_adopt or pam_slurm does not require\nUsePAM being set. The two uses of PAM are independent.How can I exclude some users from pam_slurm?\nCAUTION: Please test this on a test machine/VM before you actually do\nthis on your Slurm computers.Step 1. Make sure pam_listfile.so exists on your system.\nThe following command is an example on Redhat 6:\nls -la /lib64/security/pam_listfile.so\nStep 2. Create user list (e.g. /etc/ssh/allowed_users):\n# /etc/ssh/allowed_users\nroot\nmyadmin\nAnd, change file mode to keep it secret from regular users(Optional):\nchmod 600 /etc/ssh/allowed_users\nNOTE: root is not necessarily listed on the allowed_users, but I\nfeel somewhat safe if it's on the list.Step 3. On /etc/pam.d/sshd, add pam_listfile.so with sufficient flag\nbefore pam_slurm.so (e.g. my /etc/pam.d/sshd looks like this):\n#%PAM-1.0\nauth       required     pam_sepermit.so\nauth       include      password-auth\naccount    sufficient   pam_listfile.so item=user sense=allow file=/etc/ssh/allowed_users onerr=fail\naccount    required     pam_slurm.so\naccount    required     pam_nologin.so\naccount    include      password-auth\npassword   include      password-auth\n# pam_selinux.so close should be the first session rule\nsession    required     pam_selinux.so close\nsession    required     pam_loginuid.so\n# pam_selinux.so open should only be followed by sessions to be executed in the user context\nsession    required     pam_selinux.so open env_params\nsession    optional     pam_keyinit.so force revoke\nsession    include      password-auth\n(Information courtesy of Koji Tanaka, Indiana University)Can a user's account be changed in the database?\nA user's account can not be changed directly. A new association needs to be\ncreated for the user with the new account. Then the association with the old\naccount can be deleted.\n# Assume user \"adam\" is initially in account \"physics\"\nsacctmgr create user name=adam cluster=tux account=physics\nsacctmgr delete user name=adam cluster=tux account=chemistry\nI had to change a user's UID and now they cannot submit\n  jobs. How do I get the new UID to take effect?\nWhen changing UIDs, you will also need to restart the slurmctld for the changes to\ntake effect. Normally, when adding a new user to the system, the UID is filled in\nautomatically and immediately. If the user isn't known on the system yet, there is a\nthread that runs every hour that fills in those UIDs when they become known, but it\ndoesn't recognize UID changes of preexisting users. But you can simply restart the\nslurmctld for those changes to be recognized.How can I get SSSD to work with Slurm?\nSSSD or System Security Services Daemon does not allow enumeration of\ngroup members by default. Note that enabling enumeration in large\nenvironments might not be feasible. However, Slurm does not need enumeration\nexcept for some specific quirky configurations (multiple groups with the same\nGID), so it's probably safe to leave enumeration disabled.\nSSSD is also case sensitive by default for some configurations, which could\npossibly raise other issues. Add the following lines\nto /etc/sssd/sssd.conf on your head node to address these issues:\nenumerate = True\ncase_sensitive = False\nJobsHow is job suspend/resume useful?\nJob suspend/resume is most useful to get particularly large jobs initiated\nin a timely fashion with minimal overhead. Say you want to get a full-system\njob initiated. Normally you would need to either cancel all running jobs\nor wait for them to terminate. Canceling jobs results in the loss of\ntheir work to that point from their beginning.\nWaiting for the jobs to terminate can take hours, depending upon your\nsystem configuration. A more attractive alternative is to suspend the\nrunning jobs, run the full-system job, then resume the suspended jobs.\nThis can easily be accomplished by configuring a special queue for\nfull-system jobs and using a script to control the process.\nThe script would stop the other partitions, suspend running jobs in those\npartitions, and start the full-system partition.\nThe process can be reversed when desired.\nOne can effectively gang schedule (time-slice) multiple jobs\nusing this mechanism, although the algorithms to do so can get quite\ncomplex.\nSuspending and resuming a job makes use of the SIGSTOP and SIGCONT\nsignals respectively, so swap and disk space should be sufficient to\naccommodate all jobs allocated to a node, either running or suspended.How can I suspend, resume, hold or release all\n  of the jobs belonging to a specific user, partition, etc?\nThere isn't any filtering by user, partition, etc. available in the scontrol\ncommand; however the squeue command can be used to perform the filtering and\nbuild a script which you can then execute. For example:\n$ squeue -u adam -h -o \"scontrol hold %i\" >hold_script\nAfter manually setting a job priority\nvalue, how can its priority value be returned to being managed by the\npriority/multifactor plugin?\nHold and then release the job as shown below.\n$ scontrol hold <jobid>\n$ scontrol release <jobid>\nCan I update multiple jobs with a\nsingle scontrol command?\nNo, but you can probably use squeue to build the script taking\nadvantage of its filtering and formatting options. For example:\n$ squeue -tpd -h -o \"scontrol update jobid=%i priority=1000\" >my.script\nHow could I automatically print a job's\nSlurm job ID to its standard output?\nThe configured TaskProlog is the only thing that can write to\nthe job's standard output or set extra environment variables for a job\nor job step. To write to the job's standard output, precede the message\nwith \"print \". To export environment variables, output a line of this\nform \"export name=value\". The example below will print a job's Slurm\njob ID and allocated hosts for a batch job only.\n#!/bin/sh\n#\n# Sample TaskProlog script that will print a batch job's\n# job ID and node list to the job's stdout\n#\n\nif [ X\"$SLURM_STEP_ID\" = \"X\" -a X\"$SLURM_PROCID\" = \"X\"0 ]\nthen\n  echo \"print ==========================================\"\n  echo \"print SLURM_JOB_ID = $SLURM_JOB_ID\"\n  echo \"print SLURM_JOB_NODELIST = $SLURM_JOB_NODELIST\"\n  echo \"print ==========================================\"\nfi\nIs it possible to write to user stdout?\nThe way user I/O is handled by Slurm makes it impossible to write to the\nuser process as an admin after the user process is executed (execve is called).\nThis happens right after the call to\nTaskProlog, which is the last moment we can\nwrite to the stdout of the user process. Slurm assumes that this file\ndescriptor is only owned by the user process while running. The file descriptor\nis opened as specified and passed to the task so it makes use of the file\ndescriptor directly. Slumstepd is able to log error messages to the error file\nby duplicating the standard error of the process.It is possible to write to standard error from SPANK plugins, but this\ncan't be used to append a job summary, since the file descriptors are opened\nwith a close-on-exec flag and are closed by the operating system right after\nthe user process completes. In theory, a central place that could be used to\nprepare some kind of job summary is EpilogSlurmctld. However, using it to\nwrite to a file where user output is stored may be problematic. The script is\nrunning as SlurmUser, so intensive validation of the file name may be required\n(e.g. to prevent users from specifying something like /etc/passwd as the\noutput file). It's also possible that a job could have multiple output files\n(see filename pattern in the srun\nman page).Why are user processes and srun\nrunning even though the job is supposed to be completed?\nSlurm relies upon a configurable process tracking plugin to determine\nwhen all of the processes associated with a job or job step have completed.\nThose plugins relying upon a kernel patch can reliably identify every process.\nThose plugins dependent upon process group IDs or parent process IDs are not\nreliable. See the ProctrackType description in the slurm.conf\nman page for details. We rely upon the cgroup plugin for most systems.How can a job which has exited with a specific exit\n  code be requeued?\nSlurm supports requeue in hold with a SPECIAL_EXIT state using the\ncommand:scontrol requeuehold State=SpecialExit job_idThis is useful when users want to requeue and flag a job which has exited\nwith a specific error case. See man scontrol(1) for more details.\n$ scontrol requeuehold State=SpecialExit 10\n$ squeue\n   JOBID PARTITION  NAME     USER  ST       TIME  NODES NODELIST(REASON)\n    10      mira    zoppo    david SE       0:00      1 (JobHeldUser)\n\nThe job can be later released and run again.\n\nThe requeuing of jobs which exit with a specific exit code can be\nautomated using an EpilogSlurmctld, see man(5) slurm.conf.\nThis is an example of a script which exit code depends on the existence\nof a file.\n\n$ cat exitme\n#!/bin/sh\n#\necho \"hi! `date`\"\nif [ ! -e \"/tmp/myfile\" ]; then\n  echo \"going out with 8\"\n  exit 8\nfi\nrm /tmp/myfile\necho \"going out with 0\"\nexit 0\n\nThis is an example of an EpilogSlurmctld that checks the job exit value\nlooking at the SLURM_JOB_EXIT2 environment variable and requeues a job if\nit exited with value 8. The SLURM_JOB_EXIT2 has the format \"exit:sig\", the first\nnumber is the exit code, typically as set by the exit() function.\nThe second number of the signal that caused the process to terminate if\nit was terminated by a signal.\n\n$ cat slurmctldepilog\n#!/bin/sh\n\nexport PATH=/bin:/home/slurm/linux/bin\nLOG=/home/slurm/linux/log/logslurmepilog\n\necho \"Start `date`\" >> $LOG 2>&1\necho \"Job $SLURM_JOB_ID exitcode $SLURM_JOB_EXIT_CODE2\" >> $LOG 2>&1\nexitcode=`echo $SLURM_JOB_EXIT_CODE2|awk '{split($0, a, \":\"); print a[1]}'` >> $LOG 2>&1\nif [ \"$exitcode\" == \"8\" ]; then\n   echo \"Found REQUEUE_EXIT_CODE: $REQUEUE_EXIT_CODE\" >> $LOG 2>&1\n   scontrol requeuehold state=SpecialExit $SLURM_JOB_ID >> $LOG 2>&1\n   echo $? >> $LOG 2>&1\nelse\n   echo \"Job $SLURM_JOB_ID exit all right\" >> $LOG 2>&1\nfi\necho \"Done `date`\" >> $LOG 2>&1\n\nexit 0\n\nUsing the exitme script as an example, we have it exit with a value of 8 on\nthe first run, then when it gets requeued in hold with SpecialExit state\nwe touch the file /tmp/myfile, then release the job which will finish\nin a COMPLETE state.\nWhy is Slurm unable to set the CPU frequency for\n    jobs?\nFirst check that Slurm is configured to bind jobs to specific CPUs by\nmaking sure that TaskPlugin is configured to either affinity or cgroup.\nNext check that your processor is configured to permit frequency\ncontrol by examining the values in the file\n/sys/devices/system/cpu/cpu0/cpufreq where \"cpu0\" represents a CPU ID 0.\nOf particular interest is the file scaling_available_governors,\nwhich identifies the CPU governors available.\nIf \"userspace\" is not an available CPU governor, this may well be due to the\nintel_pstate driver being installed.\nInformation about disabling the intel_pstate driver is available\nfrom\n\nhttps://bugzilla.kernel.org/show_bug.cgi?id=57141 and\n\nhttp://unix.stackexchange.com/questions/121410/setting-cpu-governor-to-on-demand-or-conservative.Can the salloc command be configured to\nlaunch a shell on a node in the job's allocation?\nYes, just set \"use_interactive_step\" as part of the LaunchParameters\nconfiguration option in slurm.conf.How can I set up a private /tmp and /dev/shm for\n  jobs on my machine?\n\nTmpfs job container plugin can be used by including\nJobContainerType=job_container/tmpfs\nin your slurm.conf file. It additionally requires a\njob_container.conf file to be\nset up which is further described in the man page.\nTmpfs plugin creates a private mount namespace inside of which it mounts a\nprivate /tmp to a location that is configured in job_container.conf. The basepath\nis used to construct the mount path, by creating a job specific directory inside it\nand mounting /tmp to it. Since all the mounts are created inside of a mount\nnamespace which is private, they are only visible inside the job. Hence this\nproves to be a useful solution for jobs that are on shared nodes, since each\njob can only view mounts created in their own mount namespace. A private\n/dev/shm is also mounted to isolate it between different jobs.\nMount namespace construction also happens before job's spank environment is\nset up. Hence all spank related job steps will view only private /tmp the\nplugin creates. The plugin also provides an optional initialization script that\nis invoked before the job's namespace is constructed. This can be useful for\nany site specific customization that may be necessary.\nparallels@linux_vb:~$ echo $SLURM_JOB_ID\n7\nparallels@linux_vb:~$ findmnt -o+PROPAGATION | grep /tmp\n\u2514\u2500/tmp  /dev/sda1[/storage/7/.7] ext4  rw,relatime,errors=remount-ro,data=ordered   private\nIn the example above, BasePath points to /storage and a slurm job with\njob id 7 is set up to mount /tmp on /storage/7/.7. When user from inside a job\ntries to look up mounts, they can see that their /tmp is mounted. However\nthey are prevented from mistakenly accessing the backing directory directly.\nparallels@linux_vb:~$ cd /storage/7/\nbash: cd: /storage/7/: Permission denied\nThey are allowed to access (read/write) /tmp only.\nAdditionally pam_slurm_adopt has also been extended to support this functionality.\nIf a user starts an ssh session which is managed by pam_slurm_adopt, then\nthe user's process joins the namespace that is constructed by tmpfs plugin.\nHence in ssh sessions, user has the same view of /tmp and /dev/shm as\ntheir job. This functionality is enabled by default in pam_slurm_adopt\nbut can be disabled explicitly by appending join_container=false as shown:\naccount\tsufficient  pam_slurm_adopt.so join_container=false\nHow do I configure Slurm to work with System V IPC\n  enabled applications?\nSlurm is generally agnostic to\n\nSystem V IPC (a.k.a. \"sysv ipc\" in the Linux kernel).\nMemory accounting of processes using sysv ipc changes depending on the value\nof \nsysctl kernel.shm_rmid_forced (added in Linux kernel 3.1):\n\nshm_rmid_forced = 1\n\nForces all shared memory usage of processes to be accounted and reported by the\nkernel to Slurm. This breaks the separate namespace of sysv ipc and may cause\nunexpected application issues without careful planning. Processes that share\nthe same sysv ipc namespaces across jobs may end up getting OOM killed when\nanother job ends and their allocation percentage increases.\n\nshm_rmid_forced = 0 (default in most Linux distributions)\n\nSystem V memory usage will not be reported by Slurm for jobs.\nIt is generally suggested to configure the\n\nsysctl kernel.shmmax parameter. The value of kernel.shmmax times the\nmaximum number of job processes should be deducted from each node's\nconfigured RealMemory in your slurm.conf. Most Linux distributions set the\ndefault to what is effectively unlimited, which can cause the OOM killer\nto activate for unrelated new jobs or even for the slurmd process. If any\nprocesses use sysv memory mechanisms, the Linux kernel OOM killer will never\nbe able to free the used memory. A Slurm job epilog script will be needed to\nfree any of the user memory. Setting kernel.shmmax=0 will disable sysv ipc\nmemory allocations but may cause application issues.\n\nGeneral TroubleshootingIf a Slurm daemon core dumps, where can I find the\ncore file?\nIf slurmctld is started with the -D option, then the core file will be\nwritten to the current working directory. If SlurmctldLogFile is an\nabsolute path, the core file will be written to this directory. Otherwise the\ncore file will be written to the StateSaveLocation, or \"/var/tmp/\" as a\nlast resort.\nSlurmUser must have write permission on the directories. If none of the above\ndirectories have write permission for SlurmUser, no core file will be produced.If slurmd is started with the -D option, then the core file will also be\nwritten to the current working directory. If SlurmdLogFile is an\nabsolute path, the core file will be written to the this directory.\nOtherwise the core file will be written to the SlurmdSpoolDir, or\n\"/var/tmp/\" as a last resort.\nIf none of the above directories can be written, no core file will be produced.\nFor slurmstepd, the core file will depend upon when the failure\noccurs. If it is running in a privileged phase, it will be in the same location\nas that described above for the slurmd daemon. If it is running in an\nunprivileged phase, it will be in the spawned job's working directory.Nevertheless, in some operating systems this can vary:\n\nI.e. in RHEL the event\nmay be captured by abrt daemon and generated in the defined abrt configured\ndump location (i.e. /var/spool/abrt).\n\nNormally, distributions need some more tweaking in order to allow the core\nfiles to be generated correctly.slurmstepd uses the setuid() (set user ID) function to escalate\nprivileges. It is possible that in certain systems and for security policies,\nthis causes the core files not to be generated.\nTo allow the generation in such systems you usually must enable the\nsuid_dumpable kernel parameter:The value of 2, \"suidsafe\", makes any binary which normally not be dumped is\ndumped readable by root only.This allows the end user to remove such a dump\nbut not access it directly. For security reasons core dumps in this mode will\nnot overwrite one another or other files. This mode is appropriate when\nadministrators are attempting to debug problems in a normal environment.Then you must also set the core pattern to an absolute pathname:sysctl kernel.core_pattern=/tmp/core.%e.%pWe recommend reading your distribution's documentation about the\nconfiguration of these parameters.It is also usually needed to configure the system core limits, since it can be\nset to 0.\n$ grep core /etc/security/limits.conf\n#        - core - limits the core file size (KB)\n*               hard    core            unlimited\n*               soft    core            unlimited\nIn some systems it is not enough to set a hard limit, you must set also a\nsoft limit.Also, for generating the limits in userspace, the\nPropagateResourceLimits=CORE parameter in slurm.conf could be needed.Be also sure to give SlurmUser the appropriate permissions to write in the\ncore location directories.NOTE: On a diskless node depending on the core_pattern or if\n/var/spool/abrt is pointing to an in-memory filespace like tmpfs, if the job\ncaused an OOM, then the generation of the core may fill up your machine's\nmemory and hang it. It is encouraged then to make coredumps go to a persistent\nstorage. Be careful of multiple nodes writing a core dump to a shared\nfilesystem since it may significantly impact it.\nOther exceptions:On Centos 6, also set \"ProcessUnpackaged = yes\" in the file\n/etc/abrt/abrt-action-save-package-data.conf.\n\nOn RHEL6, also set \"DAEMON_COREFILE_LIMIT=unlimited\" in the file\nrc.d/init.d/functions.\nOn a SELinux enabled system, or on a distribution with similar security\nsystem, get sure it is allowing to dump cores:\n$ getsebool allow_daemons_dump_core\ncoredumpctl can also give valuable information:\n$ coredumpctl info\nHow can I get a backtrace from a core file?\nIf you do have a crash that generates a core file, you will want to get a\nbacktrace of that crash to send to SchedMD for evaluation.\nNOTE: Core files must be analyzed by the same binary that was used\nwhen they were generated. Compile time differences make it almost impossible\nfor SchedMD to use a core file from a different system. You should always\nsend a backtrace rather than a core file when sumitting a support request.\nIn order to generate a backtrace you must use gdb, specify the\npath to the slurm* binary that generated the crash, and specify the\npath to the core file. Below is an example of how to get a backtrace of a\ncore file generated by slurmctld:\n\ngdb -ex 't a a bt full' -batch /path/to/slurmctld <core_file>\n\n\nYou can also use gdb to generate a backtrace without a core file.\nThis can be useful if you are experiencing a crash on startup and aren't\ngetting a core file for some reason. You would want to start the binary\nfrom inside of gdb, wait for it to crash, and generate the backtrace.\nBelow is an example, using slurmctld as the example binary:\n\n(gdb) /path/to/slurmctld\n(gdb) set print pretty\n(gdb) r -d\n(gdb) t a a bt full\n\n\nYou may also need to get a backtrace of a running daemon if it is stuck\nor hung. To do this you would point gdb at the running binary and\nhave it generate the backtrace. Below is an example, again using\nslurmctld as the example:\n\ngdb -ex 't a a bt' -batch -p $(pidof slurmctld)\n\n\nError Messages\n\"Cannot resolve X plugin operations\" on\n  daemon startup\nThis means that symbols expected in the plugin were\nnot found by the daemon. This typically happens when the\nplugin was built or installed improperly or the configuration\nfile is telling the plugin to use an old plugin (say from the\nprevious version of Slurm). Restart the daemon in verbose mode\nfor more information (e.g. \"slurmctld -Dvvvvv\").\n\"Credential replayed\" in\n  SlurmdLogFile\nThis error is indicative of the slurmd daemon not being able\nto respond to job initiation requests from the srun command\nin a timely fashion (a few seconds).\nSrun responds by resending the job initiation request.\nWhen the slurmd daemon finally starts to respond, it\nprocesses both requests.\nThe second request is rejected and the event is logged with\nthe \"credential replayed\" error.\nIf you check the SlurmdLogFile and SlurmctldLogFile,\nyou should see signs of the slurmd daemon's non-responsiveness.\nA variety of factors can be responsible for this problem\nincluding\n\nDiskless nodes encountering network problems\nVery slow Network Information Service (NIS)\nThe Prolog script taking a long time to complete\n\nConfigure MessageTimeout in slurm.conf to a value higher than the\ndefault 10 seconds.\n\"Invalid job credential\"\nThis error is indicative of Slurm's job credential files being inconsistent across\nthe cluster. All nodes in the cluster must have the matching public and private\nkeys as defined by JobCredPrivateKey and JobCredPublicKey in the\nSlurm configuration file slurm.conf.\n\"Task launch failed on node ... Job credential\n  replayed\"\nThis error indicates that a job credential generated by the slurmctld daemon\ncorresponds to a job that the slurmd daemon has already revoked.\nThe slurmctld daemon selects job ID values based upon the configured\nvalue of FirstJobId (the default value is 1) and each job gets\na value one larger than the previous job.\nOn job termination, the slurmctld daemon notifies the slurmd on each\nallocated node that all processes associated with that job should be\nterminated.\nThe slurmd daemon maintains a list of the jobs which have already been\nterminated to avoid replay of task launch requests.\nIf the slurmctld daemon is cold-started (with the \"-c\" option\nor \"/etc/init.d/slurm startclean\"), it starts job ID values\nover based upon FirstJobId.\nIf the slurmd is not also cold-started, it will reject job launch requests\nfor jobs that it considers terminated.\nThis solution to this problem is to cold-start all slurmd daemons whenever\nthe slurmctld daemon is cold-started.\n\"Unable to accept new connection: Too many open\n  files\"\nThe srun command automatically increases its open file limit to\nthe hard limit in order to process all of the standard input and output\nconnections to the launched tasks. It is recommended that you set the\nopen file hard limit to 8192 across the cluster.\nSlurmdDebug fails to log job step information\n  at the appropriate level\nThere are two programs involved here. One is slurmd, which is\na persistent daemon running at the desired debug level. The second\nprogram is slurmstepd, which executes the user job and its\ndebug level is controlled by the user. Submitting the job with\nan option of --debug=# will result in the desired level of\ndetail being logged in the SlurmdLogFile plus the output\nof the program.\n\"Batch JobId=# missing from batch node <node>\n  (not found BatchStartTime after startup)\"\nA shell is launched on node zero of a job's allocation to execute\nthe submitted program. The slurmd daemon executing on each compute\nnode will periodically report to the slurmctld what programs it\nis executing. If a batch program is expected to be running on some\nnode (i.e. node zero of the job's allocation) and is not found, the\nmessage above will be logged and the job canceled. This typically is\nassociated with exhausting memory on the node or some other critical\nfailure that cannot be recovered from.\nMulti-Instance GPU not working with Slurm and PMIx;\n  GPUs are \"In use by another client\"\nPMIx uses the hwloc API for different purposes, including\nOS device features like querying sysfs folders (such as\n/sys/class/net and /sys/class/infiniband) to get the names of\nInfiniband HCAs. With the above mentioned features, hwloc defaults to\nquerying the OpenCL devices, which creates handles on /dev/nvidia* files.\nThese handles are kept by slurmstepd and will result in the following error\ninside a job:\n\n\n$ nvidia-smi mig --id 1 --create-gpu-instance FOO,FOO --default-compute-instance\nUnable to create a GPU instance on GPU 1 using profile FOO: In use by another client\n\n\nIn order to use Multi-Instance GPUs with Slurm and PMIx you can instruct hwloc\nto not query OpenCL devices by setting the\nHWLOC_COMPONENTS=-opencl environment\nvariable for slurmd, i.e. setting this variable in systemd unit file for slurmd.\n\n\"srun: error: Unable to accept connection:\n  Resources temporarily unavailable\"\nThis has been reported on some larger clusters running SUSE Linux when\na user's resource limits are reached. You may need to increase limits\nfor locked memory and stack size to resolve this problem.\n\"Warning: Note very large processing time\"\n  in SlurmctldLogFile\nThis error is indicative of some operation taking an unexpectedly\nlong time to complete, over one second to be specific.\nSetting the value of the SlurmctldDebug configuration parameter\nto debug2 or higher should identify which operation(s) are\nexperiencing long delays.\nThis message typically indicates long delays in file system access\n(writing state information or getting user information).\nAnother possibility is that the node on which the slurmctld\ndaemon executes has exhausted memory and is paging.\nTry running the program top to check for this possibility.\n\"Duplicate entry\" causes slurmdbd to\n  fail\nThis problem has been rarely observed with MySQL, but not MariaDB.\nThe root cause of the failure seems to be reaching the upper limit on the auto increment field.\nUpgrading to MariaDB is recommended.\nIf that is not possible then: backup the database, remove the duplicate record(s),\nand restart the slurmdbd daemon as shown below.\n\n$ slurmdbd -Dvv\n...\nslurmdbd: debug:  Table \"cray_job_table\" has changed.  Updating...\nslurmdbd: error: mysql_query failed: 1062 Duplicate entry '2711-1478734628' for key 'id_job'\n...\n\n$ mysqldump --single-transaction -u<user> -p<user> slurm_acct_db >/tmp/slurm_db_backup.sql\n\n$ mysql\nmysql> use slurm_acct_db;\nmysql> delete from cray_job_table where id_job='2711-1478734628';\nmysql> quit;\nBye\n\nIf necessary, you can edit the database dump and recreate the database as\nshown below.\n\n$ mysql\nmysql> drop database slurm_acct_db;\nmysql> create database slurm_acct_db;\nmysql> quit;\nBye\n\n$ mysql -u<user> -p<user> </tmp/slurm_db_backup.sql\n\n\"Unable to find plugin: serializer/json\"\n  \nSeveral parts of Slurm have swapped to using our centralized serializer\ncode. JSON or YAML plugins are only required if one of the functions that\nrequire it is executed. If one of the functions is executed it will fail to\ncreate the JSON/YAML output and the linker will fail with the following error:\n\n\nslurmctld: fatal: Unable to find plugin: serializer/json\n\n\nIn most cases, these are required for new functionality added after Slurm-20.02.\nHowever, with each release, we have been adding more places that use the\nserializer plugins. Because the list is evolving we do not plan on listing all\nthe commands that require the plugins but will instead provide the error\n(shown above). To correct the issue, please make sure that Slurm is configured,\ncompiled and installed with the relevant JSON or YAML library (or preferably\nboth). Configure can be made to explicitly request these libraries:\n\n\n./configure --with-json=PATH --with-yaml=PATH $@\n\n\nMost distributions include packages to make installation relatively easy.\nPlease make sure to install the 'dev' or 'devel' packages along with the\nlibrary packages. We also provide explicit instructions on how to install from\nsource: libyaml and\nlibjwt.\n\nThird Party Integrations\nCan Slurm be used with Globus?\nYes. Build and install Slurm's Torque/PBS command wrappers along with\nthe Perl APIs from Slurm's contribs directory and configure\nGlobus to use those PBS commands.\nNote there are RPMs available for both of these packages, named\ntorque and perlapi respectively.\n\nHow can TotalView be configured to operate with\n  Slurm?\nThe following lines should also be added to the global .tvdrc file\nfor TotalView to operate with Slurm:\n\n# Enable debug server bulk launch: Checked\ndset -set_as_default TV::bulk_launch_enabled true\n\n# Command:\n# Beginning with TV 7X.1, TV supports Slurm and %J.\n# Specify --mem-per-cpu=0 in case Slurm configured with default memory\n# value and we want TotalView to share the job's memory limit without\n# consuming any of the job's memory so as to block other job steps.\ndset -set_as_default TV::bulk_launch_string {srun --mem-per-cpu=0 -N%N -n%N -w`awk -F. 'BEGIN {ORS=\",\"} {if (NR==%N) ORS=\"\"; print $1}' %t1` -l --input=none %B/tvdsvr%K -callback_host %H -callback_ports %L -set_pws %P -verbosity %V -working_directory %D %F}\n\n# Temp File 1 Prototype:\n# Host Lines:\n# Slurm NodeNames need to be unadorned hostnames. In case %R returns\n# fully qualified hostnames, list the hostnames in %t1 here, and use\n# awk in the launch string above to strip away domain name suffixes.\ndset -set_as_default TV::bulk_launch_tmpfile1_host_lines {%R}\n\n\nLast modified 11 October 2024\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/related_software.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Related Software",
                "content": "Slurm source can be downloaded from\n\nhttps://www.schedmd.com/download-slurm/.Note that the following related software is not written or maintained by\nSchedMD. Some of the software is required for certain functionality\n(e.g. MySQL or MariaDB are required to use slurmdbd) while other software\nwas written to provide additional functionality for users or\nadministrators.\nAuthentication plugins identifies the user originating\na message.\n\nMUNGE (recommended)\nIn order to compile the \"auth/munge\" authentication plugin for Slurm,\nyou will need to build and install MUNGE, available from\nhttps://dun.github.io/munge/ and\nDebian and\nFedora and\nUbuntu.\n\nAuthentication tools for users that work with Slurm.\n\nAUKS\nAUKS is an utility designed to ease Kerberos V credential support addition\nto non-interactive applications, like batch systems (Slurm, LSF, Torque, etc.).\nIt includes a plugin for the Slurm workload manager. AUKS is not used as\nan authentication plugin by the Slurm code itself, but provides a mechanism\nfor the application to manage Kerberos V credentials.\n\nDatabases can be used to store accounting information.\nSee our Accounting web page for more information.\n\n\nMySQL\nMariaDB\n\nDRMAA (Distributed Resource Management Application API)\nPSNC DRMAA for Slurm\nis an implementation of Open Grid Forum\nDRMAA 1.0 (Distributed Resource Management Application API)\nspecification for submission\nand control of jobs to Slurm.\nUsing DRMAA, grid applications builders, portal developers and ISVs can use\nthe same high-level API to link their software with different cluster/resource\nmanagement systems.\nThere is a variant of PSNC DRMAA providing support for Slurm's --cluster option\navailable from\nhttps://github.com/natefoo/slurm-drmaa.\nPerl 6 DRMAA bindings are available from\nhttps://github.com/scovit/Scheduler-DRMAA.\n\nHardware topology\n\n\nPortable Hardware Locality (hwloc)\nNOTE: If you build Slurm or any MPI stack component with hwloc, note\nthat versions 2.5.0 through 2.7.0 (inclusive) of hwloc have a bug that pushes an\nuntouchable value into the environ array, causing a segfault when accessing it.\nIt is advisable to build with hwloc version 2.7.1 or later.\n\nUsed by slurmd and PMIx client to get hardware topology information.\n\n\nHostlist\nA Python program used for manipulation of Slurm hostlists including\nfunctions such as intersection and difference. Download the code from:\n\nhttp://www.nsc.liu.se/~kent/python-hostlist\n\nLua bindings for hostlist functions are also available here:\n\nhttps://github.com/grondo/lua-hostlist\nNOTE: The Lua  hostlist functions do not support the bracketed numeric\nranges anywhere except at the end of the name (i.e. \"tux[0001-0100]\"\nand \"rack[0-3]_blade[0-63]\" are not supported).\nMPI versions supported\n\nIntel MPI\nMPICH (a.k.a. MPICH2 / MPICH2)\nMVAPICH (a.k.a MVAPICH2)\nOpen MPI\n\nCommand wrappers\nThere is a wrapper for Maui/Moab's showq command\nhere.\nScripting interfaces\n\nA Perl interface is included in the Slurm distribution in the\ncontribs/perlapi directory and packaged in the perlapi RPM.\nPySlurm is a\nPython/Cython module to interface with Slurm.\nThere is also a Python module to expand and collect hostlist expressions\navailable \nhere.\n\nSPANK Plugins\nSPANK provides a very generic interface for stackable plug-ins which\nmay be used to dynamically modify the job launch code in Slurm. SPANK\nplugins may be built without access to Slurm source code. They need\nonly be compiled against Slurm\u2018s spank.h header file, added to the\nSPANK config file plugstack.conf, and they will be loaded at runtime\nduring the next job launch. Thus, the SPANK infrastructure provides\nadministrators and other developers a low cost, low effort ability to\ndynamically modify the runtime behavior of Slurm job launch.\nAdditional documentation can be found\nhere.\nNode Health Check\nProbably the most comprehensive and lightweight health check tool out\nthere is\nLBNL Node Health Check.\nIt has integration with Slurm as well as Torque resource managers.\nAccounting Tools\n\nUBMoD is a web based tool for displaying accounting data from various\nresource managers. It aggregates the accounting data from sacct into a MySQL\ndata warehouse and provide a front end web interface for browsing the data.\nFor more information, see the\nUDMod home page and\nsource code.\nXDMoD (XD Metrics on Demand)\nis an NSF-funded open source tool designed to audit and facilitate the utilization\nof the XSEDE cyberinfrastructure by providing a wide range of metrics on XSEDE\nresources, including resource utilization, resource performance, and impact on\nscholarship and research.\n\nSTUBL (Slurm Tools and UBiLities)\nSTUBL is a collection of supplemental tools and utility scripts for Slurm.\nSTUBL home page.\npestat\nPrints a consolidated compute node status line, with one line per node\nincluding a list of jobs.\n\nHome page\n\nGraphical Sdiag\nThe sdiag utility is a diagnostic tool that maintains statistics on Slurm's\nscheduling performance. You can run sdiag periodically or as you modify\nSlurm's configuration. However if you want a historical view of these\nstatistics, you could save them in a time-series database and graph them over\ntime as performed with this tool:\n\n\n  A collection of custom diamond collectors to gather various Slurm statistics\nCollectd\n  (for use with jobmetrics)\n\nJSON\nSome Slurm plugins (slurmrestd,\n\tburst_buffer/datawarp,\n\tburst_buffer/lua,\n\tjobcomp/elasticsearch, and\n\tjobcomp/kafka) parse and/or\n\tserialize JSON format data. These plugins and slurmrestd are designed to\n\tmake use of the JSON-C library (>= v1.12.0) for this purpose.\n\tInstructions for the build are as follows:\n\ngit clone --depth 1 --single-branch -b json-c-0.15-20200726 https://github.com/json-c/json-c.git json-c\nmkdir json-c-build\ncd json-c-build\ncmake ../json-c\nmake\nsudo make install\n\tDeclare the package configuration path before compiling Slurm\n\t(example provided for /bin/sh):\n\t\nexport PKG_CONFIG_PATH=/usr/local/lib/pkgconfig/:$PKG_CONFIG_PATH\n\n\nHTTP Parser\nslurmrestd requires libhttp_parser\n\t(>= v2.6.0). Instructions for the build are as follows:\n\ngit clone --depth 1 --single-branch -b v2.9.4 https://github.com/nodejs/http-parser.git http_parser\ncd http_parser\nmake\nsudo make install\n\tAdd the following argument when running configure for Slurm:\n\t--with-http-parser=/usr/local/\n\n\nYAML Parser\nslurmrestd and commands that recognize a\n\t--yaml flag will be able to parse YAML if libyaml\n\t(>= v0.2.5) is present. Instructions for the build are as follows:\n\t\ngit clone --depth 1 --single-branch -b 0.2.5 https://github.com/yaml/libyaml libyaml\ncd libyaml\n./bootstrap\n./configure\nmake\nsudo make install\n\tAdd the following argument when running configure for Slurm:\n\t--with-yaml=/usr/local/\n\n\nJWT library\nJWT authentication requires libjwt\n\t(>= v1.10.0). Instructions for the build are as follows:\n\ngit clone --depth 1 --single-branch -b v1.12.0 https://github.com/benmcollins/libjwt.git libjwt\ncd libjwt\nautoreconf --force --install\n./configure --prefix=/usr/local\nmake -j\nsudo make install\n\tAdd the following argument when running configure for Slurm:\n\t--with-jwt=/usr/local/\n\nLast modified 13 March 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/mpi_guide.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "MPI Users Guide",
                "content": "MPI use depends upon the type of MPI being used.\nThere are three fundamentally different modes of operation used\nby these various MPI implementations.\n\nSlurm directly launches the tasks and performs initialization of\ncommunications through the PMI-1, PMI-2 or PMIx APIs. (Supported by most\nmodern MPI implementations.)\nSlurm creates a resource allocation for the job and then\nmpirun launches tasks using Slurm's infrastructure (srun).\nSlurm creates a resource allocation for the job and then\nmpirun launches tasks using some mechanism other than Slurm,\nsuch as SSH or RSH.\nThese tasks are initiated outside of Slurm's monitoring\nor control and require access to the nodes from the batch node (e.g. SSH).\nSlurm's epilog should be configured to purge\nthese tasks when the job's allocation is relinquished. The\nuse of pam_slurm_adopt is strongly recommended.\n\nNOTE: Slurm is not directly launching the user application in case 3,\nwhich may prevent the desired behavior of binding tasks to CPUs and/or\naccounting and is not a recommended way.\nTwo Slurm parameters control which PMI (Process Management Interface)\nimplementation will be supported. Proper configuration is essential for Slurm to\nestablish the proper environment for the MPI job, such as setting the\nappropriate environment variables. The MpiDefault configuration parameter\nin slurm.conf establishes the system's default PMI to be used.\nThe srun option --mpi= (or the equivalent environment\nvariable SLURM_MPI_TYPE) can be used to specify when a\ndifferent PMI implementation is to be used for an individual job.\nThere are parameters that can be set in the\nmpi.conf file that allow you to modify\nthe behavior of the PMI plugins.\nNOTE: Use of an MPI implementation without the appropriate Slurm\nplugin may result in application failure. If multiple MPI implementations\nare used on a system then some users may be required to explicitly specify\na suitable Slurm MPI plugin.\nNOTE: If installing Slurm with RPMs, the slurm-libpmi\npackage will conflict with the pmix-libpmi package if it is\ninstalled. If policies at your site allow you to install from source, this\nwill allow you to install these packages to different locations, so you can\nchoose which libraries to use.\nNOTE: If you build any MPI stack component with hwloc, note that\nversions 2.5.0 through 2.7.0 (inclusive) of hwloc have a bug that pushes an\nuntouchable value into the environ array, causing a segfault when accessing it.\nIt is advisable to build with hwloc version 2.7.1 or later.\nLinks to instructions for using several varieties of MPI/PMI\nwith Slurm are provided below.\n\nPMIx\nOpen MPI\nIntel-MPI\nMPICH\nMVAPICH2\nHPE Cray PMI Support\n\n\nPMIx\n\n\nBuilding PMIx\n\n\nBefore building PMIx, it is advisable to read these\nHow-To Guides. They\nprovide some details on\n\nbuilding dependencies and installation steps as well as some relevant notes\nwith regards to\nSlurm Support\n.\nThis section is intended to complement the PMIx FAQ with some notes on how to\nprepare Slurm and PMIx to work together. PMIx can be obtained from the official\nPMIx GitHub repository,\neither by cloning the repository or by downloading a packaged release.\nSlurm support for PMIx was first included in Slurm 16.05 based on the PMIx\nv1.2 release. It has since been updated to support up to version 5.x of the\nPMIx series, as per the following table:\n\nSlurm 20.11+ supports PMIx v1.2+, v2.x and v3.x.\nSlurm 22.05+ supports PMIx v2.x, v3.x., v4.x. and v5.x.\n\nIf running PMIx v1, it is recommended to run at least 1.2.5 since older\nversions may have some compatibility issues with support of pmi and pmi2 APIs.\n\nNote also that Intel MPI doesn't officially support PMIx. It may work since PMIx\noffers some compatibility with PMI-2, but there is no guarantee that it will.\n\nAdditional PMIx notes can be found in the SchedMD\nPublications and Presentations page.\nBuilding Slurm with PMIx support\n\n\nAt configure time, Slurm won't build with PMIx unless --with-pmix is\nset. Then it will look by default for a PMIx installation under:\n\n/usr\n/usr/local\n\nIf PMIx isn't installed in any of the previous locations, the Slurm configure\nscript can be requested to point to the non default location. Here's an example\nassuming the installation dir is /home/user/pmix/v4.1.2/:\n\n\nuser@testbox:~/slurm/22.05/build$ ../src/configure \\\n> --prefix=/home/user/slurm/22.05/inst \\\n> --with-pmix=/home/user/pmix/4.1.2\n\nOr the analogous with RPM based building:\n\nuser@testbox:~/slurm_rpm$ rpmbuild \\\n> --define '_prefix /home/user/slurm/22.05/inst' \\\n> --define '_slurm_sysconfdir /home/user/slurm/22.05/inst/etc' \\\n> --define '_with_pmix --with-pmix=/home/user/pmix/4.1.2' \\\n> -ta slurm-22.05.2.1.tar.bz2\n\nNOTE: It is also possible to build against multiple PMIx versions\nwith a ':' separator. For instance to build against 3.2 and 4.1:\n\n...\n> --with-pmix=/path/to/pmix/3.2.3:/path/to/pmix/4.1.2 \\\n...\n\nThen, when submitting a job, the desired version can then be selected\nusing any of the available from --mpi=list. The default for pmix will be the\nhighest version of the library:\n\n$ srun --mpi=list\nMPI plugin types are...\n\tcray_shasta\n\tnone\n\tpmi2\n\tpmix\nspecific pmix plugin versions available: pmix_v3,pmix_v4\n\nContinuing with the configuration, if Slurm is unable to locate the PMIx\ninstallation and/or finds it but considers it not usable, the configure output\nshould log something like this:\n\nchecking for pmix installation...\nconfigure: WARNING: unable to locate pmix installation\n\nInspecting the generated config.log in the Slurm build directory might\nprovide more detail for troubleshooting purposes. After configuration,\nwe can proceed to install Slurm (using make or rpm accordingly):\n\nuser@testbox:~/slurm/22.05/build$ make -j install\nuser@testbox:~/slurm/22.05/build$ cd /home/user/slurm/22.05/inst/lib/slurm/\nuser@testbox:~/slurm/22.05/inst/lib/slurm$ ls -l *pmix*\nlrwxrwxrwx 1 user user      16 jul  6 17:17 mpi_pmix.so -> ./mpi_pmix_v4.so\n-rw-r--r-- 1 user user 9387254 jul  6 17:17 mpi_pmix_v3.a\n-rwxr-xr-x 1 user user    1065 jul  6 17:17 mpi_pmix_v3.la\n-rwxr-xr-x 1 user user 1265840 jul  6 17:17 mpi_pmix_v3.so\n-rw-r--r-- 1 user user 9935358 jul  6 17:17 mpi_pmix_v4.a\n-rwxr-xr-x 1 user user    1059 jul  6 17:17 mpi_pmix_v4.la\n-rwxr-xr-x 1 user user 1286936 jul  6 17:17 mpi_pmix_v4.so\n\nIf support for PMI-1 or PMI-2 version is also needed, it can also be\ninstalled from the contribs directory:\n\nuser@testbox:~/slurm/22.05/build/$ cd contribs/pmi1\nuser@testbox:~/slurm/22.05/build/contribs/pmi1$ make -j install\n\nuser@testbox:~/slurm/22.05/build/$ cd contribs/pmi2\nuser@testbox:~/slurm/22.05/build/contribs/pmi2$ make -j install\n\nuser@testbox:~/$ ls -l /home/user/slurm/22.05/inst/lib/*pmi*\n-rw-r--r-- 1 user user 493024 jul  6 17:27 libpmi2.a\n-rwxr-xr-x 1 user user    987 jul  6 17:27 libpmi2.la\nlrwxrwxrwx 1 user user     16 jul  6 17:27 libpmi2.so -> libpmi2.so.0.0.0\nlrwxrwxrwx 1 user user     16 jul  6 17:27 libpmi2.so.0 -> libpmi2.so.0.0.0\n-rwxr-xr-x 1 user user 219712 jul  6 17:27 libpmi2.so.0.0.0\n-rw-r--r-- 1 user user 427768 jul  6 17:27 libpmi.a\n-rwxr-xr-x 1 user user   1039 jul  6 17:27 libpmi.la\nlrwxrwxrwx 1 user user     15 jul  6 17:27 libpmi.so -> libpmi.so.0.0.0\nlrwxrwxrwx 1 user user     15 jul  6 17:27 libpmi.so.0 -> libpmi.so.0.0.0\n-rwxr-xr-x 1 user user 241640 jul  6 17:27 libpmi.so.0.0.0\n\nNOTE: Since Slurm and PMIx lower than 4.x both provide libpmi[2].so\nlibraries, we recommend you install both pieces of software in\ndifferent locations. Otherwise, these same libraries might end up being\ninstalled under standard locations like /usr/lib64 and the\npackage manager would error out, reporting the conflict.\nNOTE: Any application compiled against PMIx should use the same PMIx\nor at least a PMIx with the same security domain than the one Slurm is using,\notherwise there could be authentication issues. E.g. one PMIx compiled\n--with-munge while another compiled --without-munge (the default since PMIx\n4.2.4). A workaround which might work is to specify the desired security method\nadding \"--mca psec native\" to the cli or exporting PMIX_MCA_psec=native\nenvironment variable.\n\nNOTE: If you are setting up a test environment using multiple-slurmd,\nthe TmpFS option in your slurm.conf needs to be specified and the number of\ndirectory paths created needs to equal the number of nodes. These directories\nare used by the Slurm PMIx plugin to create temporal files and/or UNIX sockets.\nHere is an example setup for two nodes named compute[1-2]:\n\nslurm.conf:\nTmpFS=/home/user/slurm/22.05/inst/tmp/slurmd-tmpfs-%n\n\n$ mkdir /home/user/slurm/22.05/inst/tmp/slurmd-tmpfs-compute1\n$ mkdir /home/user/slurm/22.05/inst/tmp/slurmd-tmpfs-compute2\n\nTesting Slurm and PMIx\n\n\nIt is possible to directly test Slurm and PMIx without needing to have an\nMPI implementation installed. Here is an example demonstrating that\nboth components work properly:\n\n$ srun --mpi=list\nMPI plugin types are...\n\tcray_shasta\n\tnone\n\tpmi2\n\tpmix\nspecific pmix plugin versions available: pmix_v3,pmix_v4\n\n$ srun --mpi=pmix_v4 -n2 -N2 \\\n> /home/user/git/pmix/test/pmix_client -n 2 --job-fence -c\n==141756== OK\n==141774== OK\n\n\nOpenMPI\n\n\nThe current versions of Slurm and Open MPI support task launch using the\nsrun command.\nIf OpenMPI is configured with --with-pmi= pointing to either Slurm's\nPMI-1 libpmi.so or PMI-2 libpmi2.so libraries, the OMPI jobs can then be\nlaunched directly using the srun command. This is the preferred mode of\noperation since accounting features and affinity done by Slurm will become\navailable. If pmi2 support is enabled, the option '--mpi=pmi2' must be\nspecified on the srun command line.\nAlternately configure 'MpiDefault=pmi' or 'MpiDefault=pmi2' in slurm.conf.\nStarting with Open MPI version 3.1, PMIx is natively supported. To launch\nOpen MPI applications using PMIx the '--mpi=pmix' option must be specified on\nthe srun command line or 'MpiDefault=pmix' must be configured in slurm.conf.\nIt is also possible to build OpenMPI using an external PMIx installation.\nRefer to the OpenMPI documentation for a detailed procedure but it basically\nconsists of specifying --with-pmix=PATH when configuring OpenMPI.\nNote that if building OpenMPI using an external PMIx installation, both OpenMPI\nand PMIx need to be built against the same libevent/hwloc installations.\nOpenMPI configure script provides the options\n--with-libevent=PATH  and/or --with-hwloc=PATH to make OpenMPI\nmatch what PMIx was built against.\nA set of parameters are available to control the behavior of the\nSlurm PMIx plugin, read mpi.conf for more\ninformation.\nNOTE: OpenMPI has a limitation that does not support calls to\nMPI_Comm_spawn() from within a Slurm allocation. If you need to\nuse the MPI_Comm_spawn() function you will need to use another MPI\nimplementation combined with PMI-2 since PMIx doesn't support it either.\nNOTE: Some kernels and system configurations have resulted in a locked\nmemory too small for proper OpenMPI functionality, resulting in application\nfailure with a segmentation fault. This may be fixed by configuring the slurmd\ndaemon to execute with a larger limit. For example, add \"LimitMEMLOCK=infinity\"\nto your slurmd.service file.\n\nIntel MPI\n\n\nIntel\u00ae MPI Library for Linux OS supports the following methods of\nlaunching the MPI jobs under the control of the Slurm job manager:\n\nThe mpirun command over the Hydra PM\n\nThe srun command (Slurm, recommended)\n\n\nThis description provides detailed information on these two methods.\nThe mpirun Command over the Hydra Process Manager\n\n\nSlurm is supported by the mpirun command of the Intel\u00ae MPI Library\nthrough the Hydra Process Manager by default. When launched within an allocation\nthe mpirun command will automatically read the environment variables set\nby Slurm such as nodes, cpus, tasks, etc, in order to start the required\nhydra daemons on every node. These daemons will be started using srun and\nwill subsequently start the user application. Since Intel\u00ae MPI supports\nonly PMI-1 and PMI-2 (not PMIx), it is highly recommended to configure this mpi\nimplementation to use Slurm's PMI-2, which offers better scalability than PMI-1.\nPMI-1 is not recommended and should be deprecated soon.\nBelow is an example of how a user app can be launched within an exclusive\nallocation of 10 nodes using Slurm's PMI-2 library installed from contribs:\n\n$ salloc -N10 --exclusive\n$ export I_MPI_PMI_LIBRARY=/path/to/slurm/lib/libpmi2.so\n$ mpirun -np <num_procs> user_app.bin\n\nNote that by default Slurm will inject two environment variables to ensure\nmpirun or mpiexec will use the Slurm bootstrap mechanism (srun) to launch Hydra.\nWith these, Hydra will also pass the argument '--external-launcher' to srun in\norder to not consider these hydra processes as regular steps. This automatic\nvariable injection mechanism can be disabled by setting\n\"MpiParams=disable_slurm_hydra_bootstrap\" in slurm.conf, with the exception\nof the cases where \"slurm\" is already explicitly set as a bootstrap.\nIt is possible to run Intel MPI using a different bootstrap mechanism. To do\nso, explicitly set the following environment variable prior to submitting the\njob with sbatch or salloc:\n\n$ export I_MPI_HYDRA_BOOTSTRAP=ssh\n$ salloc -N10\n$ mpirun -np <num_procs> user_app.bin\n\nThe srun Command (Slurm, recommended)\n\n\nThis method is also supported by the Intel\u00ae MPI Library.\nThis method is the best integrated with Slurm and supports process tracking,\naccounting, task affinity, suspend/resume and other features.\nAs in the previous case, we show an example of how a user app can be\nlaunched within an exclusive allocation of 10 nodes using Slurm's PMI-2 library\ninstalled from contribs, allowing it to take advantage of of all the Slurm\nfeatures. This can be done with sbatch or salloc commands:\n\n$ salloc -N10 --exclusive\n$ export I_MPI_PMI_LIBRARY=/path/to/slurm/lib/libpmi2.so\n$ srun user_app.bin\n\nNOTE: The reason we're pointing manually to Slurm's PMI-1 or PMI-2\nlibrary is for licensing reasons. IMPI doesn't link directly to any external\nPMI implementations so, unlike other stacks (OMPI, MPICH, MVAPICH...), Intel is\nnot built against Slurm libs. Pointing to this library will cause Intel to\ndlopen and use this PMI library.\nNOTE: There is no official support provided by Intel against PMIx\nlibraries. Since IMPI is based on MPICH, using PMIx with Intel may work due to\nPMIx maintaining compatibility with pmi2 (which are the libraries used in MPICH)\nbut it is not guaranteed to run in all cases and PMIx could break this\ncompatibility in future versions.\n For more information see:\nIntel MPI Library\n.\n\nMPICH\n\n\nMPICH was formerly known as MPICH2.\nMPICH jobs can be launched using srun or mpiexec.\nBoth modes of operation are described below. The MPICH implementation supports\nPMI-1, PMI-2 and PMIx (starting with MPICH v4).\n\nNote that by default Slurm will inject two environment variables to ensure\nmpirun or mpiexec will use the Slurm bootstrap mechanism (srun) to launch Hydra.\nWith these, Hydra will also pass the argument '--external-launcher' to srun in\norder to not consider these hydra processes as regular steps. This automatic\nvariable injection mechanism can be disabled by setting\n\"MpiParams=disable_slurm_hydra_bootstrap\" in slurm.conf, with the exception\nof the cases where \"slurm\" is already explicitly set as a bootstrap.\nIt is possible to run MPICH using a different bootstrap mechanism. To do\nso, explicitly set the following environment variable prior to submitting the\njob with sbatch or salloc:\n\n$ export HYDRA_BOOTSTRAP=ssh\n$ salloc -N10\n$ mpirun -np <num_procs> user_app.bin\n\n\nMPICH with srun and linked with Slurm's PMI-1 or PMI-2 libs\n\n\nMPICH can be built specifically for use with Slurm and its PMI-1 or PMI-2\nlibraries using a configure line similar to that shown below. Building this way\nwill force the use of this library on every execution. Note that the\nLD_LIBRARY_PATH may not be necessary depending on your Slurm installation path:\n\n For PMI-2:\n\nuser@testbox:~/mpich-4.0.2/build$ LD_LIBRARY_PATH=~/slurm/22.05/inst/lib/ \\\n> ../configure --prefix=/home/user/bin/mpich/ --with-pmilib=slurm \\\n> --with-pmi=pmi2 --with-slurm=/home/lipi/slurm/master/inst\n\n\nor for PMI-1:\n\n\nuser@testbox:~/mpich-4.0.2/build$ LD_LIBRARY_PATH=~/slurm/22.05/inst/lib/ \\\n> ../configure --prefix=/home/user/bin/mpich/ --with-pmilib=slurm \\\n> --with-slurm=/home/user/slurm/22.05/inst\n\n These configure lines will detect the Slurm's installed PMI libraries and\nlink against them, but will not install the mpiexec commands. Since PMI-1\nis already old and doesn't scale well we don't recommend you link against it.\nIt is preferable to use PMI-2. You can follow this example to run a job with\nPMI-2:\n\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmi2 ./hello_world\n\nA Slurm upgrade will not affect this MPICH installation. There is only one\nunlikely scenario where a recompile of the MPI stack would be needed after an\nupgrade, which is when we forcibly link against Slurm's PMI-1 and/or PMI-2\nlibraries and if their APIs ever changed. These should not change often but\nif it were to happen, it would be noted in Slurm's RELEASE_NOTES file.\nMPICH with PMIx and integrated with Slurm\n\n\n You can also build MPICH using an external PMIx library which should be the\nsame one used when building Slurm:\n\n$ LD_LIBRARY_PATH=~/slurm/22.05/inst/lib/ ../configure \\\n> --prefix=/home/user/bin/mpich/ \\\n> --with-pmix=/home/user/bin/pmix_4.1.2/ \\\n> --with-pmi=pmix \\\n> --with-slurm=/home/user/slurm/master/inst\n\nAfter building this way, any execution must be made with Slurm (srun) since\nthe Hydra process manager is not installed, as it was in previous examples.\nCompile and run a process with:\n\n\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmix ./hello_world\n\nMPICH with its internal PMI and integrated with Slurm\n\n\nAnother option is to just compile MPICH but not set --with-pmilib,\n--with-pmix or --with-pmi, and only keep --with-slurm.\nIn that case, MPICH will not forcibly link against any PMI libraries and it will\ninstall the mpiexec.hydra command by default. This will cause it to use its\ninternal PMI implementation (based on PMI-1) and Slurm API functions to detect\nthe job environment and launch processes accordingly:\n\n\nuser@testbox:~/mpich-4.0.2/build$ ../configure \\\n> --prefix=/home/user/bin/mpich/ \\\n> --with-slurm=/home/user/slurm/22.05/inst\n\nThen the app can be run with srun or mpiexec:\n\n$ mpicc -o hello_world hello_world.c\n$ srun ./hello_world\n\nor\n\n$ mpiexec.hydra ./hello_world\n\nmpiexec.hydra will spawn its daemons using Slurm steps launched with srun and\nwill use its internal PMI implementation.\nNOTE: In this case, compiling with the --with-slurm option\ncreated the Hydra bootstrap commands (mpiexec.hydra and others) and linked them\nagainst the versioned Slurm's main public API (libslurm.so.X.0.0). That is\nbecause these commands use some Slurm functions to detect the job environment.\nBe aware then that upgrading Slurm would need a recompile of the MPICH stack.\nIt is usually enough to symlink the name of the linked library to the new one,\nbut this is not guaranteed to work.\n\nMPICH without Slurm integration\n\n\nFinally, it is possible to compile MPICH without integrating it with Slurm.\nIn that case it will not identify the job and will just run the processes as if\nit were on a local machine. We recommend reading MPICH documentation and the\nconfigure scripts for more information on the existing possibilities.\n\n\n\nMVAPICH2\n\n\n\nMVAPICH2 has support for Slurm. To enable it you need to build MVAPICH2\nwith a command similar to this:\n\n$ ./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/22.05/inst/\n\nNOTE:In certain MVAPICH2 versions and when building with GCC > 10.x,\nit is possible that these flags must be prepended to the configure line:\n\nFFLAGS=\"-std=legacy\" FCFLAGS=\"-std=legacy\" ./configure ...\n\nWhen MVAPICH2 is built with Slurm support it will detect that it is\nwithin a Slurm allocation, and will use the 'srun' command to spawn its hydra\ndaemons. It does not link to the Slurm API, which means that during an upgrade\nof Slurm there is no need to recompile MVAPICH2. By default it will use the\ninternal PMI implementation.\nNote that by default Slurm will inject two environment variables to ensure\nmpirun or mpiexec will use the Slurm bootstrap mechanism (srun) to launch Hydra.\nWith these, Hydra will also pass the argument '--external-launcher' to srun in\norder to not consider these hydra processes as regular steps. This automatic\nvariable injection mechanism can be disabled by setting\n\"MpiParams=disable_slurm_hydra_bootstrap\" in slurm.conf, with the exception\nof the cases where \"slurm\" is already explicitly set as a bootstrap.\nIt is possible to run MVAPICH2 using a different bootstrap mechanism. To do\nso, explicitly set the following environment variable prior to submitting the\njob with sbatch or salloc:\n\n$ export HYDRA_BOOTSTRAP=ssh\n$ salloc -N10\n$ mpirun -np <num_procs> user_app.bin\n\n\nMVAPICH2 with srun and linked with Slurm's PMI-1 or PMI-2 libs\n\n\nIt is possible to force MVAPICH2 to use one of the Slurm's PMI-1\n(libpmi.so.0.0.0)or PMI-2 (libpmi2.so.0.0.0) libraries. Building with this mode\nwill cause all the executions to use Slurm and its PMI libraries.\nThe Hydra process manager binaries (mpiexec) won't be installed. In fact the\nmpiexec command will exist as a symbolic link to Slurm's srun command. It is\nrecommended not to use PMI-1, but to use at least PMI-2 libs.\nSee below for an example of how to configure and usage:\n\nFor PMI-2:\n\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/  \\\n> --with-pm=slurm --with-pmi=pmi2\n\nand for PMI-1:\n\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/ \\\n> --with-pm=slurm --with-pmi=pmi1\n\nTo compile and run a user application in Slurm:\n\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmi2 ./hello_world\n\nFor more information, please see the MVAPICH2 documentation on their\nwebpage\n\nMVAPICH2 with Slurm support and linked with external PMIx\n\n\nIt is possible to use PMIx within MVAPICH2 and integrated with Slurm. This\nway no Hydra Process Manager will be installed and the user apps will need to\nrun with srun, assuming Slurm has been compiled against the same or a compatible\nPMIx version as the one used when building MVAPICH2.\nTo build MVAPICH2 to use PMIx and integrated with Slurm, a configuration line\nsimilar to this is needed:\n\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/ \\\n> --with-pm=slurm \\\n> --with-pmix=/home/user/bin/pmix_4.1.2/ \\\n> --with-pmi=pmix\n\nRunning a job looks similar to previous examples:\n\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmix ./hello_world\n\nNOTE: In the MVAPICH2 case, compiling with integration with Slurm\n(--with-slurm) doesn't add any dependency to commands or libraries, so\nupgrading Slurm should be safe without any need to recompile MVAPICH2. There\nis only one unlikely scenario where a recompile of the MPI stack would be\nneeded after an upgrade, which is when we forcibly link against Slurm's PMI-1\nand/or PMI-2 libraries and if their APIs ever changed. These should not change\noften but if it were to happen, it would be noted in Slurm's RELEASE_NOTES\nfile.\n\nHPE Cray PMI support\n\n\nSlurm comes by default with a Cray PMI vendor-specific plugin which provides\ncompatibility with the HPE Cray Programming Environment's PMI. It is intended to\nbe used in applications built with this environment on HPE Cray machines.\nThe plugin is named cray_shasta (Shasta was the first Cray\narchitecture this plugin supported) and built by default in all Slurm\ninstallations. Its availability is shown by running the following command:\n\n$ srun --mpi=list\nMPI plugin types are...\n\tcray_shasta\n\tnone\n\nThe Cray PMI plugin will use some reserved ports for its communication. These\nports are configurable by using --resv-ports option on the command line\nwith srun, or by setting MpiParams=ports=[port_range]\nin your slurm.conf. The first port listed in this option will be used as the\nPMI control port, defined by Cray as the PMI_CONTROL_PORT environment\nvariable. There cannot be more than one application launched in the same node\nusing the same PMI_CONTROL_PORT.\nThis plugin does not support MPMD/heterogeneous jobs and it requires\nlibpals >= 0.2.8.\nLast modified 21 June 2024\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "PMIx\n\n",
                "content": "Building PMIx\n\nBefore building PMIx, it is advisable to read these\nHow-To Guides. They\nprovide some details on\n\nbuilding dependencies and installation steps as well as some relevant notes\nwith regards to\nSlurm Support\n.This section is intended to complement the PMIx FAQ with some notes on how to\nprepare Slurm and PMIx to work together. PMIx can be obtained from the official\nPMIx GitHub repository,\neither by cloning the repository or by downloading a packaged release.Slurm support for PMIx was first included in Slurm 16.05 based on the PMIx\nv1.2 release. It has since been updated to support up to version 5.x of the\nPMIx series, as per the following table:\n\nSlurm 20.11+ supports PMIx v1.2+, v2.x and v3.x.\nSlurm 22.05+ supports PMIx v2.x, v3.x., v4.x. and v5.x.\n\nIf running PMIx v1, it is recommended to run at least 1.2.5 since older\nversions may have some compatibility issues with support of pmi and pmi2 APIs.\n\nNote also that Intel MPI doesn't officially support PMIx. It may work since PMIx\noffers some compatibility with PMI-2, but there is no guarantee that it will.\nAdditional PMIx notes can be found in the SchedMD\nPublications and Presentations page.Building Slurm with PMIx support\n\nAt configure time, Slurm won't build with PMIx unless --with-pmix is\nset. Then it will look by default for a PMIx installation under:\n/usr\n/usr/local\nIf PMIx isn't installed in any of the previous locations, the Slurm configure\nscript can be requested to point to the non default location. Here's an example\nassuming the installation dir is /home/user/pmix/v4.1.2/:\n\nuser@testbox:~/slurm/22.05/build$ ../src/configure \\\n> --prefix=/home/user/slurm/22.05/inst \\\n> --with-pmix=/home/user/pmix/4.1.2\nOr the analogous with RPM based building:\nuser@testbox:~/slurm_rpm$ rpmbuild \\\n> --define '_prefix /home/user/slurm/22.05/inst' \\\n> --define '_slurm_sysconfdir /home/user/slurm/22.05/inst/etc' \\\n> --define '_with_pmix --with-pmix=/home/user/pmix/4.1.2' \\\n> -ta slurm-22.05.2.1.tar.bz2\nNOTE: It is also possible to build against multiple PMIx versions\nwith a ':' separator. For instance to build against 3.2 and 4.1:\n...\n> --with-pmix=/path/to/pmix/3.2.3:/path/to/pmix/4.1.2 \\\n...\nThen, when submitting a job, the desired version can then be selected\nusing any of the available from --mpi=list. The default for pmix will be the\nhighest version of the library:\n$ srun --mpi=list\nMPI plugin types are...\n\tcray_shasta\n\tnone\n\tpmi2\n\tpmix\nspecific pmix plugin versions available: pmix_v3,pmix_v4\nContinuing with the configuration, if Slurm is unable to locate the PMIx\ninstallation and/or finds it but considers it not usable, the configure output\nshould log something like this:\nchecking for pmix installation...\nconfigure: WARNING: unable to locate pmix installation\nInspecting the generated config.log in the Slurm build directory might\nprovide more detail for troubleshooting purposes. After configuration,\nwe can proceed to install Slurm (using make or rpm accordingly):\nuser@testbox:~/slurm/22.05/build$ make -j install\nuser@testbox:~/slurm/22.05/build$ cd /home/user/slurm/22.05/inst/lib/slurm/\nuser@testbox:~/slurm/22.05/inst/lib/slurm$ ls -l *pmix*\nlrwxrwxrwx 1 user user      16 jul  6 17:17 mpi_pmix.so -> ./mpi_pmix_v4.so\n-rw-r--r-- 1 user user 9387254 jul  6 17:17 mpi_pmix_v3.a\n-rwxr-xr-x 1 user user    1065 jul  6 17:17 mpi_pmix_v3.la\n-rwxr-xr-x 1 user user 1265840 jul  6 17:17 mpi_pmix_v3.so\n-rw-r--r-- 1 user user 9935358 jul  6 17:17 mpi_pmix_v4.a\n-rwxr-xr-x 1 user user    1059 jul  6 17:17 mpi_pmix_v4.la\n-rwxr-xr-x 1 user user 1286936 jul  6 17:17 mpi_pmix_v4.so\nIf support for PMI-1 or PMI-2 version is also needed, it can also be\ninstalled from the contribs directory:\nuser@testbox:~/slurm/22.05/build/$ cd contribs/pmi1\nuser@testbox:~/slurm/22.05/build/contribs/pmi1$ make -j install\n\nuser@testbox:~/slurm/22.05/build/$ cd contribs/pmi2\nuser@testbox:~/slurm/22.05/build/contribs/pmi2$ make -j install\n\nuser@testbox:~/$ ls -l /home/user/slurm/22.05/inst/lib/*pmi*\n-rw-r--r-- 1 user user 493024 jul  6 17:27 libpmi2.a\n-rwxr-xr-x 1 user user    987 jul  6 17:27 libpmi2.la\nlrwxrwxrwx 1 user user     16 jul  6 17:27 libpmi2.so -> libpmi2.so.0.0.0\nlrwxrwxrwx 1 user user     16 jul  6 17:27 libpmi2.so.0 -> libpmi2.so.0.0.0\n-rwxr-xr-x 1 user user 219712 jul  6 17:27 libpmi2.so.0.0.0\n-rw-r--r-- 1 user user 427768 jul  6 17:27 libpmi.a\n-rwxr-xr-x 1 user user   1039 jul  6 17:27 libpmi.la\nlrwxrwxrwx 1 user user     15 jul  6 17:27 libpmi.so -> libpmi.so.0.0.0\nlrwxrwxrwx 1 user user     15 jul  6 17:27 libpmi.so.0 -> libpmi.so.0.0.0\n-rwxr-xr-x 1 user user 241640 jul  6 17:27 libpmi.so.0.0.0\nNOTE: Since Slurm and PMIx lower than 4.x both provide libpmi[2].so\nlibraries, we recommend you install both pieces of software in\ndifferent locations. Otherwise, these same libraries might end up being\ninstalled under standard locations like /usr/lib64 and the\npackage manager would error out, reporting the conflict.NOTE: Any application compiled against PMIx should use the same PMIx\nor at least a PMIx with the same security domain than the one Slurm is using,\notherwise there could be authentication issues. E.g. one PMIx compiled\n--with-munge while another compiled --without-munge (the default since PMIx\n4.2.4). A workaround which might work is to specify the desired security method\nadding \"--mca psec native\" to the cli or exporting PMIX_MCA_psec=native\nenvironment variable.\nNOTE: If you are setting up a test environment using multiple-slurmd,\nthe TmpFS option in your slurm.conf needs to be specified and the number of\ndirectory paths created needs to equal the number of nodes. These directories\nare used by the Slurm PMIx plugin to create temporal files and/or UNIX sockets.\nHere is an example setup for two nodes named compute[1-2]:\nslurm.conf:\nTmpFS=/home/user/slurm/22.05/inst/tmp/slurmd-tmpfs-%n\n\n$ mkdir /home/user/slurm/22.05/inst/tmp/slurmd-tmpfs-compute1\n$ mkdir /home/user/slurm/22.05/inst/tmp/slurmd-tmpfs-compute2\nTesting Slurm and PMIx\n\nIt is possible to directly test Slurm and PMIx without needing to have an\nMPI implementation installed. Here is an example demonstrating that\nboth components work properly:\n$ srun --mpi=list\nMPI plugin types are...\n\tcray_shasta\n\tnone\n\tpmi2\n\tpmix\nspecific pmix plugin versions available: pmix_v3,pmix_v4\n\n$ srun --mpi=pmix_v4 -n2 -N2 \\\n> /home/user/git/pmix/test/pmix_client -n 2 --job-fence -c\n==141756== OK\n==141774== OK\n\nOpenMPI\n\nThe current versions of Slurm and Open MPI support task launch using the\nsrun command.If OpenMPI is configured with --with-pmi= pointing to either Slurm's\nPMI-1 libpmi.so or PMI-2 libpmi2.so libraries, the OMPI jobs can then be\nlaunched directly using the srun command. This is the preferred mode of\noperation since accounting features and affinity done by Slurm will become\navailable. If pmi2 support is enabled, the option '--mpi=pmi2' must be\nspecified on the srun command line.\nAlternately configure 'MpiDefault=pmi' or 'MpiDefault=pmi2' in slurm.conf.Starting with Open MPI version 3.1, PMIx is natively supported. To launch\nOpen MPI applications using PMIx the '--mpi=pmix' option must be specified on\nthe srun command line or 'MpiDefault=pmix' must be configured in slurm.conf.It is also possible to build OpenMPI using an external PMIx installation.\nRefer to the OpenMPI documentation for a detailed procedure but it basically\nconsists of specifying --with-pmix=PATH when configuring OpenMPI.\nNote that if building OpenMPI using an external PMIx installation, both OpenMPI\nand PMIx need to be built against the same libevent/hwloc installations.\nOpenMPI configure script provides the options\n--with-libevent=PATH  and/or --with-hwloc=PATH to make OpenMPI\nmatch what PMIx was built against.A set of parameters are available to control the behavior of the\nSlurm PMIx plugin, read mpi.conf for more\ninformation.NOTE: OpenMPI has a limitation that does not support calls to\nMPI_Comm_spawn() from within a Slurm allocation. If you need to\nuse the MPI_Comm_spawn() function you will need to use another MPI\nimplementation combined with PMI-2 since PMIx doesn't support it either.NOTE: Some kernels and system configurations have resulted in a locked\nmemory too small for proper OpenMPI functionality, resulting in application\nfailure with a segmentation fault. This may be fixed by configuring the slurmd\ndaemon to execute with a larger limit. For example, add \"LimitMEMLOCK=infinity\"\nto your slurmd.service file.Intel MPI\n\nIntel\u00ae MPI Library for Linux OS supports the following methods of\nlaunching the MPI jobs under the control of the Slurm job manager:\n\nThe mpirun command over the Hydra PM\n\nThe srun command (Slurm, recommended)\n\nThis description provides detailed information on these two methods.The mpirun Command over the Hydra Process Manager\n\nSlurm is supported by the mpirun command of the Intel\u00ae MPI Library\nthrough the Hydra Process Manager by default. When launched within an allocation\nthe mpirun command will automatically read the environment variables set\nby Slurm such as nodes, cpus, tasks, etc, in order to start the required\nhydra daemons on every node. These daemons will be started using srun and\nwill subsequently start the user application. Since Intel\u00ae MPI supports\nonly PMI-1 and PMI-2 (not PMIx), it is highly recommended to configure this mpi\nimplementation to use Slurm's PMI-2, which offers better scalability than PMI-1.\nPMI-1 is not recommended and should be deprecated soon.Below is an example of how a user app can be launched within an exclusive\nallocation of 10 nodes using Slurm's PMI-2 library installed from contribs:\n$ salloc -N10 --exclusive\n$ export I_MPI_PMI_LIBRARY=/path/to/slurm/lib/libpmi2.so\n$ mpirun -np <num_procs> user_app.bin\nNote that by default Slurm will inject two environment variables to ensure\nmpirun or mpiexec will use the Slurm bootstrap mechanism (srun) to launch Hydra.\nWith these, Hydra will also pass the argument '--external-launcher' to srun in\norder to not consider these hydra processes as regular steps. This automatic\nvariable injection mechanism can be disabled by setting\n\"MpiParams=disable_slurm_hydra_bootstrap\" in slurm.conf, with the exception\nof the cases where \"slurm\" is already explicitly set as a bootstrap.It is possible to run Intel MPI using a different bootstrap mechanism. To do\nso, explicitly set the following environment variable prior to submitting the\njob with sbatch or salloc:\n$ export I_MPI_HYDRA_BOOTSTRAP=ssh\n$ salloc -N10\n$ mpirun -np <num_procs> user_app.bin\nThe srun Command (Slurm, recommended)\n\nThis method is also supported by the Intel\u00ae MPI Library.\nThis method is the best integrated with Slurm and supports process tracking,\naccounting, task affinity, suspend/resume and other features.\nAs in the previous case, we show an example of how a user app can be\nlaunched within an exclusive allocation of 10 nodes using Slurm's PMI-2 library\ninstalled from contribs, allowing it to take advantage of of all the Slurm\nfeatures. This can be done with sbatch or salloc commands:\n$ salloc -N10 --exclusive\n$ export I_MPI_PMI_LIBRARY=/path/to/slurm/lib/libpmi2.so\n$ srun user_app.bin\nNOTE: The reason we're pointing manually to Slurm's PMI-1 or PMI-2\nlibrary is for licensing reasons. IMPI doesn't link directly to any external\nPMI implementations so, unlike other stacks (OMPI, MPICH, MVAPICH...), Intel is\nnot built against Slurm libs. Pointing to this library will cause Intel to\ndlopen and use this PMI library.NOTE: There is no official support provided by Intel against PMIx\nlibraries. Since IMPI is based on MPICH, using PMIx with Intel may work due to\nPMIx maintaining compatibility with pmi2 (which are the libraries used in MPICH)\nbut it is not guaranteed to run in all cases and PMIx could break this\ncompatibility in future versions. For more information see:\nIntel MPI Library\n.\nMPICH\n\nMPICH was formerly known as MPICH2.MPICH jobs can be launched using srun or mpiexec.\nBoth modes of operation are described below. The MPICH implementation supports\nPMI-1, PMI-2 and PMIx (starting with MPICH v4).\nNote that by default Slurm will inject two environment variables to ensure\nmpirun or mpiexec will use the Slurm bootstrap mechanism (srun) to launch Hydra.\nWith these, Hydra will also pass the argument '--external-launcher' to srun in\norder to not consider these hydra processes as regular steps. This automatic\nvariable injection mechanism can be disabled by setting\n\"MpiParams=disable_slurm_hydra_bootstrap\" in slurm.conf, with the exception\nof the cases where \"slurm\" is already explicitly set as a bootstrap.It is possible to run MPICH using a different bootstrap mechanism. To do\nso, explicitly set the following environment variable prior to submitting the\njob with sbatch or salloc:\n$ export HYDRA_BOOTSTRAP=ssh\n$ salloc -N10\n$ mpirun -np <num_procs> user_app.bin\n\nMPICH with srun and linked with Slurm's PMI-1 or PMI-2 libs\n\nMPICH can be built specifically for use with Slurm and its PMI-1 or PMI-2\nlibraries using a configure line similar to that shown below. Building this way\nwill force the use of this library on every execution. Note that the\nLD_LIBRARY_PATH may not be necessary depending on your Slurm installation path:\n For PMI-2:\nuser@testbox:~/mpich-4.0.2/build$ LD_LIBRARY_PATH=~/slurm/22.05/inst/lib/ \\\n> ../configure --prefix=/home/user/bin/mpich/ --with-pmilib=slurm \\\n> --with-pmi=pmi2 --with-slurm=/home/lipi/slurm/master/inst\n\nor for PMI-1:\n\nuser@testbox:~/mpich-4.0.2/build$ LD_LIBRARY_PATH=~/slurm/22.05/inst/lib/ \\\n> ../configure --prefix=/home/user/bin/mpich/ --with-pmilib=slurm \\\n> --with-slurm=/home/user/slurm/22.05/inst\n These configure lines will detect the Slurm's installed PMI libraries and\nlink against them, but will not install the mpiexec commands. Since PMI-1\nis already old and doesn't scale well we don't recommend you link against it.\nIt is preferable to use PMI-2. You can follow this example to run a job with\nPMI-2:\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmi2 ./hello_world\nA Slurm upgrade will not affect this MPICH installation. There is only one\nunlikely scenario where a recompile of the MPI stack would be needed after an\nupgrade, which is when we forcibly link against Slurm's PMI-1 and/or PMI-2\nlibraries and if their APIs ever changed. These should not change often but\nif it were to happen, it would be noted in Slurm's RELEASE_NOTES file.MPICH with PMIx and integrated with Slurm\n\n You can also build MPICH using an external PMIx library which should be the\nsame one used when building Slurm:\n$ LD_LIBRARY_PATH=~/slurm/22.05/inst/lib/ ../configure \\\n> --prefix=/home/user/bin/mpich/ \\\n> --with-pmix=/home/user/bin/pmix_4.1.2/ \\\n> --with-pmi=pmix \\\n> --with-slurm=/home/user/slurm/master/inst\nAfter building this way, any execution must be made with Slurm (srun) since\nthe Hydra process manager is not installed, as it was in previous examples.\nCompile and run a process with:\n\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmix ./hello_world\nMPICH with its internal PMI and integrated with Slurm\n\nAnother option is to just compile MPICH but not set --with-pmilib,\n--with-pmix or --with-pmi, and only keep --with-slurm.\nIn that case, MPICH will not forcibly link against any PMI libraries and it will\ninstall the mpiexec.hydra command by default. This will cause it to use its\ninternal PMI implementation (based on PMI-1) and Slurm API functions to detect\nthe job environment and launch processes accordingly:\n\nuser@testbox:~/mpich-4.0.2/build$ ../configure \\\n> --prefix=/home/user/bin/mpich/ \\\n> --with-slurm=/home/user/slurm/22.05/inst\nThen the app can be run with srun or mpiexec:\n$ mpicc -o hello_world hello_world.c\n$ srun ./hello_world\nor\n$ mpiexec.hydra ./hello_world\nmpiexec.hydra will spawn its daemons using Slurm steps launched with srun and\nwill use its internal PMI implementation.\nNOTE: In this case, compiling with the --with-slurm option\ncreated the Hydra bootstrap commands (mpiexec.hydra and others) and linked them\nagainst the versioned Slurm's main public API (libslurm.so.X.0.0). That is\nbecause these commands use some Slurm functions to detect the job environment.\nBe aware then that upgrading Slurm would need a recompile of the MPICH stack.\nIt is usually enough to symlink the name of the linked library to the new one,\nbut this is not guaranteed to work.\n\nMPICH without Slurm integration\n\n\nFinally, it is possible to compile MPICH without integrating it with Slurm.\nIn that case it will not identify the job and will just run the processes as if\nit were on a local machine. We recommend reading MPICH documentation and the\nconfigure scripts for more information on the existing possibilities.\n\n\n\nMVAPICH2\n\n\n\nMVAPICH2 has support for Slurm. To enable it you need to build MVAPICH2\nwith a command similar to this:\n\n$ ./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/22.05/inst/\n\nNOTE:In certain MVAPICH2 versions and when building with GCC > 10.x,\nit is possible that these flags must be prepended to the configure line:\n\nFFLAGS=\"-std=legacy\" FCFLAGS=\"-std=legacy\" ./configure ...\n\nWhen MVAPICH2 is built with Slurm support it will detect that it is\nwithin a Slurm allocation, and will use the 'srun' command to spawn its hydra\ndaemons. It does not link to the Slurm API, which means that during an upgrade\nof Slurm there is no need to recompile MVAPICH2. By default it will use the\ninternal PMI implementation.\nNote that by default Slurm will inject two environment variables to ensure\nmpirun or mpiexec will use the Slurm bootstrap mechanism (srun) to launch Hydra.\nWith these, Hydra will also pass the argument '--external-launcher' to srun in\norder to not consider these hydra processes as regular steps. This automatic\nvariable injection mechanism can be disabled by setting\n\"MpiParams=disable_slurm_hydra_bootstrap\" in slurm.conf, with the exception\nof the cases where \"slurm\" is already explicitly set as a bootstrap.\nIt is possible to run MVAPICH2 using a different bootstrap mechanism. To do\nso, explicitly set the following environment variable prior to submitting the\njob with sbatch or salloc:\n\n$ export HYDRA_BOOTSTRAP=ssh\n$ salloc -N10\n$ mpirun -np <num_procs> user_app.bin\n\n\nMVAPICH2 with srun and linked with Slurm's PMI-1 or PMI-2 libs\n\n\nIt is possible to force MVAPICH2 to use one of the Slurm's PMI-1\n(libpmi.so.0.0.0)or PMI-2 (libpmi2.so.0.0.0) libraries. Building with this mode\nwill cause all the executions to use Slurm and its PMI libraries.\nThe Hydra process manager binaries (mpiexec) won't be installed. In fact the\nmpiexec command will exist as a symbolic link to Slurm's srun command. It is\nrecommended not to use PMI-1, but to use at least PMI-2 libs.\nSee below for an example of how to configure and usage:\n\nFor PMI-2:\n\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/  \\\n> --with-pm=slurm --with-pmi=pmi2\n\nand for PMI-1:\n\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/ \\\n> --with-pm=slurm --with-pmi=pmi1\n\nTo compile and run a user application in Slurm:\n\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmi2 ./hello_world\n\nFor more information, please see the MVAPICH2 documentation on their\nwebpage\n\nMVAPICH2 with Slurm support and linked with external PMIx\n\n\nIt is possible to use PMIx within MVAPICH2 and integrated with Slurm. This\nway no Hydra Process Manager will be installed and the user apps will need to\nrun with srun, assuming Slurm has been compiled against the same or a compatible\nPMIx version as the one used when building MVAPICH2.\nTo build MVAPICH2 to use PMIx and integrated with Slurm, a configuration line\nsimilar to this is needed:\n\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/ \\\n> --with-pm=slurm \\\n> --with-pmix=/home/user/bin/pmix_4.1.2/ \\\n> --with-pmi=pmix\n\nRunning a job looks similar to previous examples:\n\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmix ./hello_world\n\nNOTE: In the MVAPICH2 case, compiling with integration with Slurm\n(--with-slurm) doesn't add any dependency to commands or libraries, so\nupgrading Slurm should be safe without any need to recompile MVAPICH2. There\nis only one unlikely scenario where a recompile of the MPI stack would be\nneeded after an upgrade, which is when we forcibly link against Slurm's PMI-1\nand/or PMI-2 libraries and if their APIs ever changed. These should not change\noften but if it were to happen, it would be noted in Slurm's RELEASE_NOTES\nfile.\n\nHPE Cray PMI support\n\n\nSlurm comes by default with a Cray PMI vendor-specific plugin which provides\ncompatibility with the HPE Cray Programming Environment's PMI. It is intended to\nbe used in applications built with this environment on HPE Cray machines.\nThe plugin is named cray_shasta (Shasta was the first Cray\narchitecture this plugin supported) and built by default in all Slurm\ninstallations. Its availability is shown by running the following command:\n\n$ srun --mpi=list\nMPI plugin types are...\n\tcray_shasta\n\tnone\n\nThe Cray PMI plugin will use some reserved ports for its communication. These\nports are configurable by using --resv-ports option on the command line\nwith srun, or by setting MpiParams=ports=[port_range]\nin your slurm.conf. The first port listed in this option will be used as the\nPMI control port, defined by Cray as the PMI_CONTROL_PORT environment\nvariable. There cannot be more than one application launched in the same node\nusing the same PMI_CONTROL_PORT.\nThis plugin does not support MPMD/heterogeneous jobs and it requires\nlibpals >= 0.2.8.\nLast modified 21 June 2024\n"
            },
            {
                "title": "\n\nMVAPICH2\n\n\n",
                "content": "MVAPICH2 has support for Slurm. To enable it you need to build MVAPICH2\nwith a command similar to this:\n$ ./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/22.05/inst/\nNOTE:In certain MVAPICH2 versions and when building with GCC > 10.x,\nit is possible that these flags must be prepended to the configure line:\nFFLAGS=\"-std=legacy\" FCFLAGS=\"-std=legacy\" ./configure ...\nWhen MVAPICH2 is built with Slurm support it will detect that it is\nwithin a Slurm allocation, and will use the 'srun' command to spawn its hydra\ndaemons. It does not link to the Slurm API, which means that during an upgrade\nof Slurm there is no need to recompile MVAPICH2. By default it will use the\ninternal PMI implementation.Note that by default Slurm will inject two environment variables to ensure\nmpirun or mpiexec will use the Slurm bootstrap mechanism (srun) to launch Hydra.\nWith these, Hydra will also pass the argument '--external-launcher' to srun in\norder to not consider these hydra processes as regular steps. This automatic\nvariable injection mechanism can be disabled by setting\n\"MpiParams=disable_slurm_hydra_bootstrap\" in slurm.conf, with the exception\nof the cases where \"slurm\" is already explicitly set as a bootstrap.It is possible to run MVAPICH2 using a different bootstrap mechanism. To do\nso, explicitly set the following environment variable prior to submitting the\njob with sbatch or salloc:\n$ export HYDRA_BOOTSTRAP=ssh\n$ salloc -N10\n$ mpirun -np <num_procs> user_app.bin\n\nMVAPICH2 with srun and linked with Slurm's PMI-1 or PMI-2 libs\n\nIt is possible to force MVAPICH2 to use one of the Slurm's PMI-1\n(libpmi.so.0.0.0)or PMI-2 (libpmi2.so.0.0.0) libraries. Building with this mode\nwill cause all the executions to use Slurm and its PMI libraries.\nThe Hydra process manager binaries (mpiexec) won't be installed. In fact the\nmpiexec command will exist as a symbolic link to Slurm's srun command. It is\nrecommended not to use PMI-1, but to use at least PMI-2 libs.\nSee below for an example of how to configure and usage:\nFor PMI-2:\n\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/  \\\n> --with-pm=slurm --with-pmi=pmi2\n\nand for PMI-1:\n\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/ \\\n> --with-pm=slurm --with-pmi=pmi1\nTo compile and run a user application in Slurm:\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmi2 ./hello_world\nFor more information, please see the MVAPICH2 documentation on their\nwebpage\nMVAPICH2 with Slurm support and linked with external PMIx\n\nIt is possible to use PMIx within MVAPICH2 and integrated with Slurm. This\nway no Hydra Process Manager will be installed and the user apps will need to\nrun with srun, assuming Slurm has been compiled against the same or a compatible\nPMIx version as the one used when building MVAPICH2.To build MVAPICH2 to use PMIx and integrated with Slurm, a configuration line\nsimilar to this is needed:\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/ \\\n> --with-pm=slurm \\\n> --with-pmix=/home/user/bin/pmix_4.1.2/ \\\n> --with-pmi=pmix\nRunning a job looks similar to previous examples:\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmix ./hello_world\nNOTE: In the MVAPICH2 case, compiling with integration with Slurm\n(--with-slurm) doesn't add any dependency to commands or libraries, so\nupgrading Slurm should be safe without any need to recompile MVAPICH2. There\nis only one unlikely scenario where a recompile of the MPI stack would be\nneeded after an upgrade, which is when we forcibly link against Slurm's PMI-1\nand/or PMI-2 libraries and if their APIs ever changed. These should not change\noften but if it were to happen, it would be noted in Slurm's RELEASE_NOTES\nfile.\nHPE Cray PMI support\n\nSlurm comes by default with a Cray PMI vendor-specific plugin which provides\ncompatibility with the HPE Cray Programming Environment's PMI. It is intended to\nbe used in applications built with this environment on HPE Cray machines.The plugin is named cray_shasta (Shasta was the first Cray\narchitecture this plugin supported) and built by default in all Slurm\ninstallations. Its availability is shown by running the following command:\n$ srun --mpi=list\nMPI plugin types are...\n\tcray_shasta\n\tnone\nThe Cray PMI plugin will use some reserved ports for its communication. These\nports are configurable by using --resv-ports option on the command line\nwith srun, or by setting MpiParams=ports=[port_range]\nin your slurm.conf. The first port listed in this option will be used as the\nPMI control port, defined by Cray as the PMI_CONTROL_PORT environment\nvariable. There cannot be more than one application launched in the same node\nusing the same PMI_CONTROL_PORT.This plugin does not support MPMD/heterogeneous jobs and it requires\nlibpals >= 0.2.8.Last modified 21 June 2024"
            },
            {
                "title": "MPICH without Slurm integration\n\n",
                "content": "Finally, it is possible to compile MPICH without integrating it with Slurm.\nIn that case it will not identify the job and will just run the processes as if\nit were on a local machine. We recommend reading MPICH documentation and the\nconfigure scripts for more information on the existing possibilities.\n\nMVAPICH2\n\n\nMVAPICH2 has support for Slurm. To enable it you need to build MVAPICH2\nwith a command similar to this:\n$ ./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/22.05/inst/\nNOTE:In certain MVAPICH2 versions and when building with GCC > 10.x,\nit is possible that these flags must be prepended to the configure line:\nFFLAGS=\"-std=legacy\" FCFLAGS=\"-std=legacy\" ./configure ...\nWhen MVAPICH2 is built with Slurm support it will detect that it is\nwithin a Slurm allocation, and will use the 'srun' command to spawn its hydra\ndaemons. It does not link to the Slurm API, which means that during an upgrade\nof Slurm there is no need to recompile MVAPICH2. By default it will use the\ninternal PMI implementation.Note that by default Slurm will inject two environment variables to ensure\nmpirun or mpiexec will use the Slurm bootstrap mechanism (srun) to launch Hydra.\nWith these, Hydra will also pass the argument '--external-launcher' to srun in\norder to not consider these hydra processes as regular steps. This automatic\nvariable injection mechanism can be disabled by setting\n\"MpiParams=disable_slurm_hydra_bootstrap\" in slurm.conf, with the exception\nof the cases where \"slurm\" is already explicitly set as a bootstrap.It is possible to run MVAPICH2 using a different bootstrap mechanism. To do\nso, explicitly set the following environment variable prior to submitting the\njob with sbatch or salloc:\n$ export HYDRA_BOOTSTRAP=ssh\n$ salloc -N10\n$ mpirun -np <num_procs> user_app.bin\n\nMVAPICH2 with srun and linked with Slurm's PMI-1 or PMI-2 libs\n\nIt is possible to force MVAPICH2 to use one of the Slurm's PMI-1\n(libpmi.so.0.0.0)or PMI-2 (libpmi2.so.0.0.0) libraries. Building with this mode\nwill cause all the executions to use Slurm and its PMI libraries.\nThe Hydra process manager binaries (mpiexec) won't be installed. In fact the\nmpiexec command will exist as a symbolic link to Slurm's srun command. It is\nrecommended not to use PMI-1, but to use at least PMI-2 libs.\nSee below for an example of how to configure and usage:\nFor PMI-2:\n\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/  \\\n> --with-pm=slurm --with-pmi=pmi2\n\nand for PMI-1:\n\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/ \\\n> --with-pm=slurm --with-pmi=pmi1\nTo compile and run a user application in Slurm:\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmi2 ./hello_world\nFor more information, please see the MVAPICH2 documentation on their\nwebpage\nMVAPICH2 with Slurm support and linked with external PMIx\n\nIt is possible to use PMIx within MVAPICH2 and integrated with Slurm. This\nway no Hydra Process Manager will be installed and the user apps will need to\nrun with srun, assuming Slurm has been compiled against the same or a compatible\nPMIx version as the one used when building MVAPICH2.To build MVAPICH2 to use PMIx and integrated with Slurm, a configuration line\nsimilar to this is needed:\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/ \\\n> --with-pm=slurm \\\n> --with-pmix=/home/user/bin/pmix_4.1.2/ \\\n> --with-pmi=pmix\nRunning a job looks similar to previous examples:\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmix ./hello_world\nNOTE: In the MVAPICH2 case, compiling with integration with Slurm\n(--with-slurm) doesn't add any dependency to commands or libraries, so\nupgrading Slurm should be safe without any need to recompile MVAPICH2. There\nis only one unlikely scenario where a recompile of the MPI stack would be\nneeded after an upgrade, which is when we forcibly link against Slurm's PMI-1\nand/or PMI-2 libraries and if their APIs ever changed. These should not change\noften but if it were to happen, it would be noted in Slurm's RELEASE_NOTES\nfile.\nHPE Cray PMI support\n\nSlurm comes by default with a Cray PMI vendor-specific plugin which provides\ncompatibility with the HPE Cray Programming Environment's PMI. It is intended to\nbe used in applications built with this environment on HPE Cray machines.The plugin is named cray_shasta (Shasta was the first Cray\narchitecture this plugin supported) and built by default in all Slurm\ninstallations. Its availability is shown by running the following command:\n$ srun --mpi=list\nMPI plugin types are...\n\tcray_shasta\n\tnone\nThe Cray PMI plugin will use some reserved ports for its communication. These\nports are configurable by using --resv-ports option on the command line\nwith srun, or by setting MpiParams=ports=[port_range]\nin your slurm.conf. The first port listed in this option will be used as the\nPMI control port, defined by Cray as the PMI_CONTROL_PORT environment\nvariable. There cannot be more than one application launched in the same node\nusing the same PMI_CONTROL_PORT.This plugin does not support MPMD/heterogeneous jobs and it requires\nlibpals >= 0.2.8.Last modified 21 June 2024"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/user_permissions.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "User Permissions",
                "content": "Slurm supports several special user permissions as described below.OperatorThese users can add, modify, and remove any database object\n(user, account, etc), and add other operators.\nOn a SlurmDBD served cluster, these users can\n\nView information that is blocked to regular uses by a PrivateData flag\nCreate/Alter/Delete Reservations\n\nSet using an AdminLevel option in the user's database record.\nFor configuration information, see\nAccounting and Resource Limits.\nAdmin\nThese users have the same level of privileges as an operator in the database.\nThey can also alter anything on a served slurmctld as if they were the SlurmUser\nor root.\nAn AdminLevel option in the user's database record.\nFor configuration information, see\nAccounting and Resource Limits.\nCoordinator\nA special privileged user, usually an account manager, that can add\nusers or sub-accounts to the account they are coordinator over.\nThis should be a trusted person since they can change limits on account and\nuser associations, as well as cancel, requeue or reassign accounts of jobs\ninside their realm. Note that a coordinator may not increase job limits above\nthe parent ones.\nSet using a table in Slurm's database defining users and accounts for\nwhich they can serve as coordinators.\nFor configuration information, see the\nsacctmgr man page.\nLast modified 07 April 2020\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Admin",
                "content": "These users have the same level of privileges as an operator in the database.\nThey can also alter anything on a served slurmctld as if they were the SlurmUser\nor root.An AdminLevel option in the user's database record.\nFor configuration information, see\nAccounting and Resource Limits.CoordinatorA special privileged user, usually an account manager, that can add\nusers or sub-accounts to the account they are coordinator over.\nThis should be a trusted person since they can change limits on account and\nuser associations, as well as cancel, requeue or reassign accounts of jobs\ninside their realm. Note that a coordinator may not increase job limits above\nthe parent ones.Set using a table in Slurm's database defining users and accounts for\nwhich they can serve as coordinators.\nFor configuration information, see the\nsacctmgr man page.Last modified 07 April 2020"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/site_factor.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Slurm Priority Site Factor Plugin API",
                "content": "Overview This document describes Slurm site_factor plugins and the API that defines\nthem. It is intended as a resource to programmers wishing to write their own\nSlurm site_factor plugins.Slurm site_factor plugins are Slurm plugins that implement the Slurm\nsite_factor API described herein. They are designed to provide the site a\nway to build a custom multifactor priority factor, and will only be loaded\nand operation alongside\nPriorityType=priority/multifactor.The plugins are meant to set and update the\nsite_factor value in the\njob_record_t corresponding to a given job.\nNote that the site_factor itself is an\nunsigned integer, but uses NICE_OFFSET as\nan offset to allow the value to be positive or negative. The plugin itself\nmust add NICE_OFFSET to the value stored to\nsite_factor for proper operation, otherwise\nthe value itself will be extremely negative, and the job priority will likely\ndrop to 1. (The lowest value that does not correspond to a held job.)Plugins must conform to the Slurm Plugin API with the following\nspecifications:const char plugin_type[]=\"major/minor\"\nThe major type must be \"site_factor\" The minor type can be any\nrecognizable abbreviation for the specific plugin.const char plugin_name[]\nSome descriptive name for the plugin.\nThere is no requirement with respect to its format.const uint32_t plugin_version\nIf specified, identifies the version of Slurm used to build this plugin and\nany attempt to load the plugin from a different version of Slurm will result\nin an error.\nIf not specified, then the plugin may be loaded by Slurm commands and\ndaemons from any version, however this may result in difficult to diagnose\nfailures due to changes in the arguments to plugin functions or changes\nin other Slurm functions used by the plugin.API FunctionsThe following functions must appear. Functions which are not implemented\nmust be stubbed, or the plugin will fail to load.int init(void)\nDescription:\n  Called when the plugin is loaded, before any other functions are\n  called. Put global initialization here.\nReturns: \nSLURM_SUCCESS on success, or\nSLURM_ERROR on failure.\nvoid fini(void)\nDescription:\n  Called when the plugin is removed. Clear any allocated storage here.\nReturns: None.\nNote: These init and fini functions are not the same as those\ndescribed in the dlopen (3) system library.\nThe C run-time system co-opts those symbols for its own initialization.\nThe system _init() is called before the Slurm\ninit(), and the Slurm\nfini() is called before the system's\n_fini().\nvoid site_factor_p_reconfig(void)\nDescription: Refresh the plugin's\nconfiguration. Called whenever slurmctld is reconfigured.\nReturns: void\nvoid site_factor_p_set(job_record_t *job_ptr)\nDescription: Sets the site_priority of the job, if desired.\nArguments:\njob_ptr (input) pointer to the job record.\nReturns: void\nvoid site_factor_p_update(void)\nDescription: Handle periodic updates to\nall site_priority values in the job_list. Called every \nReturns: void\nParameters\nThese parameters can be used in the slurm.conf to configure the\nplugin and the frequency at which to gather information about running jobs.\n\nPrioritySiteFactorParameters\nOptional parameters for the site_factor plugin. Interpretation of any\nvalue is left to the site_factor plugin itself.\nPrioritySiteFactorPlugin\nSpecifies which plugin should be used.\nPriorityCalcPeriod\nInterval between calls to\nsite_factor_p_update()\n\nLast modified 23 October 2019\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Parameters",
                "content": "These parameters can be used in the slurm.conf to configure the\nplugin and the frequency at which to gather information about running jobs.\nPrioritySiteFactorParameters\nOptional parameters for the site_factor plugin. Interpretation of any\nvalue is left to the site_factor plugin itself.\nPrioritySiteFactorPlugin\nSpecifies which plugin should be used.\nPriorityCalcPeriod\nInterval between calls to\nsite_factor_p_update()\nLast modified 23 October 2019"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/tres.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Trackable RESources (TRES)",
                "content": "A TRES is a resource that can be tracked for usage or used to enforce\n  limits against.  A TRES is a combination of a Type and a Name.\n  Types are predefined.\n\nCurrent TRES Types are:\n\nBB (burst buffers)\nBilling\nCPU\nEnergy\nFS (filesystem)\nGRES\nIC (interconnect)\nLicense\nMem (Memory)\nNode\nPages\nVMem (Virtual Memory/Size)\n\n  The Billing TRES is calculated from a partition's TRESBillingWeights. Though\n  TRES weights on a partition may be defined as doubles, the Billing TRES values\n  for a job are stored as integers. This is not the case when calculating a\n  job's fairshare where the value is treated as a double.\n\n  Valid 'FS' TRES are 'disk' (local disk) and 'lustre'.  These are primarily\n  there for reporting usage, not limiting access.\n\n  Valid 'IC' TRES is 'ofed'.  These are primarily there for reporting usage, not\n  limiting access.\nslurm.conf settings\nAccountingStorageTRES\nUsed to define which TRES are\n  to be tracked on the system. By default Billing, CPU, Energy, Memory, Node,\n  FS/Disk, Pages and VMem are tracked. These default TRES cannot be disabled,\n  but only appended to. The following example:\n\nAccountingStorageTRES=gres/gpu,license/iop1\n\n  will track billing, cpu, energy, memory, nodes, fs/disk, pages and vmem along\n  with a GRES called gpu, as well as a license called iop1. Whenever these\n  resources are used on the cluster they are recorded. TRES are automatically\n  set up in the database on the start of the slurmctld.\n\n The TRES that require associated names are BB, GRES, and\n  License.  As seen in the above example, GRES and License are typically\n  different on each system.  The BB TRES is named the same as\n  the burst buffer plugin being used. In the above example we are using the\n  Cray burst buffer plugin.\n\n When including a specific GRES with a subtype, it is also recommended to\ninclude its generic type, otherwise a request with only the generic one won't\nbe accounted for. For example, if we want to account for gres/gpu:tesla,\nwe would also include gres/gpu for accounting gpus in requests like\nsrun --gres=gpu:1.\n\nAccountingStorageTRES=gres/gpu,gres/gpu:tesla\n\nNOTE: Setting gres/gpu will also set gres/gpumem and gres/gpuutil.\ngres/gpumem and gres/gpuutil can be set individually when gres/gpu is not set.\n\nPriorityWeightTRES\nA comma separated list of TRES Types and weights that sets the\n  degree that each TRES Type contributes to the job's priority.\nPriorityWeightTRES=CPU=1000,Mem=2000,GRES/gpu=3000\nApplicable only if PriorityType=priority/multifactor and if\nAccountingStorageTRES is configured with each TRES Type.\nThe default values are 0.\nThe Billing TRES is not available for priority calculations because the\nnumber isn't generated until after the job has been allocated resources \u2014\nsince the number can change for different partitions.\n\nTRESBillingWeights\nFor each partition this option is used to define the billing\n  weights of each TRES type that will be used in calculating the usage\n  of a job.\n\nBilling weights are specified as a comma-separated list of\nTRES=Weight pairs.\n\nAny TRES Type is available for billing. Note that the base unit for memory\nand burst buffers is megabytes.\n\nBy default the billing of TRES is calculated as the sum of all TRES types\nmultiplied by their corresponding billing weight.\n\nThe weighted amount of a resource can be adjusted by adding a suffix of\nK,M,G,T or P after the billing weight. For example, a memory weight of \"mem=.25\"\non a job allocated 8GB will be billed 2048 (8192MB *.25) units. A memory weight\nof \"mem=.25G\" on the same job will be billed 2 (8192MB * (.25/1024)) units.\n\nWhen a job is allocated 1 CPU and 8 GB of memory on a partition configured\nwith:\n\nTRESBillingWeights=\"CPU=1.0,Mem=0.25G,GRES/gpu=2.0,license/licA=1.5\"\nthe billable TRES will be:\n\n(1*1.0) + (8*0.25) + (0*2.0) + (0*1.5) = 3.0\nIf PriorityFlags=MAX_TRES is configured, the billable TRES is\n  calculated as the MAX of individual TRESs on a node (e.g. cpus, mem,\n  gres) plus the sum of all global TRESs (e.g. licenses). Using the\n  same example above, the billable TRES will be:\n\nMAX(1*1.0, 8*0.25, 0*2.0) + (0*1.5) = 2.0\nIf TRESBillingWeights is not defined then the job is billed against the total\nnumber of allocated CPUs.\n\nNOTE: TRESBillingWeights is only used when calculating fairshare and\ndoesn't affect job priority directly as it is currently not used for the size of\nthe job. If you want TRESs to play a role in the job's priority then refer to\nthe PriorityWeightTRES option.\n\nNOTE: As with PriorityWeightTRES only TRES defined in\n  AccountingStorageTRES are available for TRESBillingWeights.\n\nNOTE: Jobs can be limited based off of the calculated TRES billing\nvalue. See Resource Limits documentation for\nmore information.\n\nNOTE: If a Billing TRES is defined as a weight, it is ignored.\n\nsacctsacct can be used to view the TRES of each job by adding \"tres\" to the\n  --format option.\nsacctmgrsacctmgr is used to view the various TRES available globally in the\n  system. sacctmgr show tres will do this.\nsreportsreport reports on different TRES. Simply using the comma separated input\n  option --tres= will have sreport generate reports available\n  for the requested TRES types.  More information about these reports\n  can be found on the sreport manpage.\n\n  In sreport, the \"Reported\" Billing TRES is calculated from the largest\n  Billing TRES of each node multiplied by the time frame. For example, if a node\n  is part of multiple partitions and each has a different TRESBillingWeights\n  defined the Billing TRES for the node will be the highest of the partitions.\n  If TRESBillingWeights is not defined on any partition for a node then the\n  Billing TRES will be equal to the number of CPUs on the node.\nLast modified 16 August 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/gres_design.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Generic Resource (GRES) Design Guide",
                "content": "OverviewGeneric Resources (GRES) are resources associated with a specific node\nthat can be allocated to jobs and steps. The most obvious example of\nGRES use would be GPUs. GRES are identified by a specific name and use an\noptional plugin to provide device-specific support. This document is meant\nto provide details about Slurm's implementation of GRES support including the\nrelevant data structures. For an overview of GRES configuration and use, see\nGeneric Resource (GRES) Scheduling.\n\nData Structures\nGRES are associated with Slurm nodes, jobs and job steps. You will find\na string variable named gres in those data structures which\nis used to store the GRES configured on a node or required by a job or step\n(e.g. \"gpu:2,nic:1\"). This string is also visible to various Slurm commands\nviewing information about those data structures (e.g. \"scontrol show job\").\nThere is a second variable associated with each of those data structures on\nthe slurmctld daemon\nnamed gres_list that is intended for program use only. Each element\nin the list gres_list provides information about a specific GRES type\n(e.g. one data structure for \"gpu\" and a second structure with information\nabout \"nic\"). The structures on gres_list contain an ID number\n(which is faster to compare than a string) plus a pointer to another structure.\nThis second structure differs somewhat for nodes, jobs, and steps (see\ngres_node_state_t, gres_job_state_t, and gres_step_state_t in\nsrc/common/gres.h for details), but contains various counters and bitmaps.\nSince these data structures differ for various entity types, the functions\nused to work with them are also different. If no GRES are associated with a\nnode, job or step, then both gres and gres_list will be NULL.\n\n------------------------\n|   Job Information    |\n|----------------------|\n| gres = \"gpu:2,nic:1\" |\n|      gres_list       |\n------------------------\n           |\n           +---------------------------------\n           |                                |\n   ------------------               ------------------\n   | List Struct    |               | List Struct    |\n   |----------------|               |----------------|\n   | id = 123 (gpu) |               | id = 124 (nic) |\n   |   gres_data    |               |   gres_data    |\n   ------------------               ------------------\n           |                                |\n           |                              ....\n           |\n           |\n------------------------------------------------\n| gres_job_state_t                             |\n|----------------------------------------------|\n| gres_count = 2                               |\n| node_count = 3                               |\n| gres_bitmap(by node) = 0,1;                  |\n|                        2,3;                  |\n|                        0,2                   |\n| gres_count_allocated_to_steps(by node) = 1;  |\n|                                          1;  |\n|                                          1   |\n| gres_bitmap_allocated_to_steps(by node) = 0; |\n|                                           2; |\n|                                           0  |\n------------------------------------------------\n\nMode of Operation\nAfter the slurmd daemon reads the configuration files, it calls the function\nnode_config_load() for each configured plugin. This can be used to\nvalidate the configuration, for example validate that the appropriate devices\nactually exist. If no GRES plugin exists for that resource type, the information\nin the configuration file is assumed correct. Each node's GRES information is\nreported by slurmd to the slurmctld daemon at node registration time.\nThe slurmctld daemon maintains GRES information in the data structures\ndescribed above for each node, including the number of configured and allocated\nresources. If those resources are identified with a specific device file\nrather than just a count, bitmaps are used record which specific resources have\nbeen allocated to jobs.\nThe slurmctld daemon's GRES information about jobs includes several arrays\nequal in length to the number of allocated nodes. The index into each of the\narrays is the sequence number of the node in that's job's allocation (e.g.\nthe first element is node zero of the job allocation). The job step's\nGRES information is similar to that of a job including the design where the\nindex into arrays is based upon the job's allocation. This means when a job\nstep is allocated or terminates, the required bitmap operations are very\neasy to perform without computing different index values for job and step\ndata structures.\nThe most complex operation on the GRES data structures happens when a job\nchanges size (has nodes added or removed). In that case, the array indexed by\nnode index must be rebuilt, with records shifting as appropriate. Note that\nthe current software is not compatible with having different GRES counts by\nnode (a job can not have 2 GPUs on one node and 1 GPU on a second node),\nalthough that might be addressed at a later time.\nWhen a job or step is initiated, its credential includes allocated GRES information.\nThis can be used by the slurmd daemon to associate those resources with that\njob. Our plan is to use the Linux cgroups logic to bind a job and/or its\ntasks with specific GRES devices, however that logic does not currently exist.\nWhat does exist today is a pair of plugin APIs, job_set_env() and\nstep_set_env() which can be used to set environment variables for the\nprogram directing it to GRES which have been allocated for its use (the CUDA\nlibraries base their GPU selection upon environment variables, so this logic\nshould work for CUDA today if users do not attempt to manipulate the\nenvironment variables reserved for CUDA use).\nIf you want to see how GRES logic is allocating resources, configure\nDebugFlags=GRES to log GRES state changes. Note the resulting output can\nbe quite verbose, especially for larger clusters.\nLast modified 6 August 2021\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Data Structures",
                "content": "GRES are associated with Slurm nodes, jobs and job steps. You will find\na string variable named gres in those data structures which\nis used to store the GRES configured on a node or required by a job or step\n(e.g. \"gpu:2,nic:1\"). This string is also visible to various Slurm commands\nviewing information about those data structures (e.g. \"scontrol show job\").\nThere is a second variable associated with each of those data structures on\nthe slurmctld daemon\nnamed gres_list that is intended for program use only. Each element\nin the list gres_list provides information about a specific GRES type\n(e.g. one data structure for \"gpu\" and a second structure with information\nabout \"nic\"). The structures on gres_list contain an ID number\n(which is faster to compare than a string) plus a pointer to another structure.\nThis second structure differs somewhat for nodes, jobs, and steps (see\ngres_node_state_t, gres_job_state_t, and gres_step_state_t in\nsrc/common/gres.h for details), but contains various counters and bitmaps.\nSince these data structures differ for various entity types, the functions\nused to work with them are also different. If no GRES are associated with a\nnode, job or step, then both gres and gres_list will be NULL.\n------------------------\n|   Job Information    |\n|----------------------|\n| gres = \"gpu:2,nic:1\" |\n|      gres_list       |\n------------------------\n           |\n           +---------------------------------\n           |                                |\n   ------------------               ------------------\n   | List Struct    |               | List Struct    |\n   |----------------|               |----------------|\n   | id = 123 (gpu) |               | id = 124 (nic) |\n   |   gres_data    |               |   gres_data    |\n   ------------------               ------------------\n           |                                |\n           |                              ....\n           |\n           |\n------------------------------------------------\n| gres_job_state_t                             |\n|----------------------------------------------|\n| gres_count = 2                               |\n| node_count = 3                               |\n| gres_bitmap(by node) = 0,1;                  |\n|                        2,3;                  |\n|                        0,2                   |\n| gres_count_allocated_to_steps(by node) = 1;  |\n|                                          1;  |\n|                                          1   |\n| gres_bitmap_allocated_to_steps(by node) = 0; |\n|                                           2; |\n|                                           0  |\n------------------------------------------------\nMode of OperationAfter the slurmd daemon reads the configuration files, it calls the function\nnode_config_load() for each configured plugin. This can be used to\nvalidate the configuration, for example validate that the appropriate devices\nactually exist. If no GRES plugin exists for that resource type, the information\nin the configuration file is assumed correct. Each node's GRES information is\nreported by slurmd to the slurmctld daemon at node registration time.The slurmctld daemon maintains GRES information in the data structures\ndescribed above for each node, including the number of configured and allocated\nresources. If those resources are identified with a specific device file\nrather than just a count, bitmaps are used record which specific resources have\nbeen allocated to jobs.The slurmctld daemon's GRES information about jobs includes several arrays\nequal in length to the number of allocated nodes. The index into each of the\narrays is the sequence number of the node in that's job's allocation (e.g.\nthe first element is node zero of the job allocation). The job step's\nGRES information is similar to that of a job including the design where the\nindex into arrays is based upon the job's allocation. This means when a job\nstep is allocated or terminates, the required bitmap operations are very\neasy to perform without computing different index values for job and step\ndata structures.The most complex operation on the GRES data structures happens when a job\nchanges size (has nodes added or removed). In that case, the array indexed by\nnode index must be rebuilt, with records shifting as appropriate. Note that\nthe current software is not compatible with having different GRES counts by\nnode (a job can not have 2 GPUs on one node and 1 GPU on a second node),\nalthough that might be addressed at a later time.When a job or step is initiated, its credential includes allocated GRES information.\nThis can be used by the slurmd daemon to associate those resources with that\njob. Our plan is to use the Linux cgroups logic to bind a job and/or its\ntasks with specific GRES devices, however that logic does not currently exist.\nWhat does exist today is a pair of plugin APIs, job_set_env() and\nstep_set_env() which can be used to set environment variables for the\nprogram directing it to GRES which have been allocated for its use (the CUDA\nlibraries base their GPU selection upon environment variables, so this logic\nshould work for CUDA today if users do not attempt to manipulate the\nenvironment variables reserved for CUDA use).If you want to see how GRES logic is allocating resources, configure\nDebugFlags=GRES to log GRES state changes. Note the resulting output can\nbe quite verbose, especially for larger clusters.Last modified 6 August 2021"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/api.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Slurm APIs",
                "content": "OverviewAll of the Slurm commands utilize a collection of Application Programming\nInterfaces (APIs) exposed through the \"slurm.h\" and \"slurmdb.h\" headers.\nSlurm's APIs are fixed within each major release. They do change \u2014\nsignificantly \u2014 between each major release.Developers are encouraged to target Slurm's REST API\n which provides for broader cross-version compatibility.\nLast modified 12 February 2024\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/job_array.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Job Array Support",
                "content": "OverviewJob arrays offer a mechanism for submitting and managing collections of\nsimilar jobs quickly and easily; job arrays with millions of tasks can be\nsubmitted in milliseconds (subject to configured size limits).\nAll jobs must have the same initial options (e.g. size, time limit, etc.),\nhowever it is possible to change some of these options after the job has begun\nexecution using the scontrol command specifying the JobID of the array or individual\nArrayJobID.\n$ scontrol update job=101 ...\n$ scontrol update job=101_1 ...\nJob arrays are only supported for batch jobs and the array index values are\nspecified using the --array or -a option of the sbatch\ncommand. The option argument can be specific array index values, a range of\nindex values, and an optional step size as shown in the examples below.\nNote that the minimum index value is zero and the maximum value is a Slurm\nconfiguration parameter (MaxArraySize minus one).\nJobs which are part of a job array will have the environment variable\nSLURM_ARRAY_TASK_ID set to its array index value.\n# Submit a job array with index values between 0 and 31\n$ sbatch --array=0-31    -N1 tmp\n\n# Submit a job array with index values of 1, 3, 5 and 7\n$ sbatch --array=1,3,5,7 -N1 tmp\n\n# Submit a job array with index values between 1 and 7\n# with a step size of 2 (i.e. 1, 3, 5 and 7)\n$ sbatch --array=1-7:2   -N1 tmp\nA maximum number of simultaneously running tasks from the job array may be\nspecified using a \"%\" separator.\nFor example \"--array=0-15%4\" will limit the number of simultaneously\nrunning tasks from this job array to 4.Job ID and Environment Variables\n\nJob arrays will have additional environment variables set.\nSLURM_ARRAY_JOB_ID will be set to the first job ID of the array.\nSLURM_ARRAY_TASK_ID will be set to the job array index value.\nSLURM_ARRAY_TASK_COUNT will be set to the number of tasks in the job\narray.\nSLURM_ARRAY_TASK_MAX will be set to the highest job array index\nvalue.\nSLURM_ARRAY_TASK_MIN will be set to the lowest job array index value.Under normal circumstances, array jobs will have the first task of the array\nbe a place holder for the rest of the array, causing it to be the last to run.\nAs a result, the task with the lowest SLURM_JOB_ID will have the highest\nSLURM_ARRAY_TASK_ID.\nFor example a job submission of this sort:\nsbatch --array=1-3 -N1 tmp\nwill generate a job array containing three jobs. If the sbatch command\nresponds with:\nSubmitted batch job 36\nthen the environment variables will be set as follows:\nSLURM_JOB_ID=36\nSLURM_ARRAY_JOB_ID=36\nSLURM_ARRAY_TASK_ID=3\nSLURM_ARRAY_TASK_COUNT=3\nSLURM_ARRAY_TASK_MAX=3\nSLURM_ARRAY_TASK_MIN=1\n\nSLURM_JOB_ID=37\nSLURM_ARRAY_JOB_ID=36\nSLURM_ARRAY_TASK_ID=1\nSLURM_ARRAY_TASK_COUNT=3\nSLURM_ARRAY_TASK_MAX=3\nSLURM_ARRAY_TASK_MIN=1\n\nSLURM_JOB_ID=38\nSLURM_ARRAY_JOB_ID=36\nSLURM_ARRAY_TASK_ID=2\nSLURM_ARRAY_TASK_COUNT=3\nSLURM_ARRAY_TASK_MAX=3\nSLURM_ARRAY_TASK_MIN=1\nOrdering of the tasks as shown above is not guaranteed. For example, there\ncan be cases where individual tasks are created out of order when tasks are\nrequeued. The task with the lowest JOB_ID may not have the highest TASK_ID if\nthe tasks are not created sequentially due to the tasks being updated/modified\nbefore they start. Other edge cases may cause similar behavior.All Slurm commands and APIs recognize the SLURM_JOB_ID value.\nMost commands also recognize the SLURM_ARRAY_JOB_ID plus SLURM_ARRAY_TASK_ID\nvalues separated by an underscore as identifying an element of a job array.\nUsing the example above, \"37\" or \"36_1\" would be equivalent ways to identify\nthe second array element of job 36.\nA set of APIs has been developed to operate on an entire job array or select\ntasks of a job array in a single function call.\nThe function response consists of an array identifying the various error codes\nfor various tasks of a job ID.\nFor example the job_resume2() function might return an array of error\ncodes indicating that tasks 1 and 2 have already completed; tasks 3 through 5\nare resumed successfully, and tasks 6 through 99 have not yet started.File NamesTwo additional options are available to specify a job's stdin, stdout, and\nstderr file names:\n%A will be replaced by the value of SLURM_ARRAY_JOB_ID (as defined above)\nand\n%a will be replaced by the value of SLURM_ARRAY_TASK_ID (as defined above).\nThe default output file format for a job array is \"slurm-%A_%a.out\".\nAn example of explicit use of the formatting is:\nsbatch -o slurm-%A_%a.out --array=1-3 -N1 tmp\nwhich would generate output files names of this sort\n\"slurm-36_1.out\", \"slurm-36_2.out\" and \"slurm-36_3.out\".\nIf these file name options are used without being part of a job array then\n\"%A\" will be replaced by the current job ID and \"%a\" will be replaced by\n4,294,967,294 (equivalent to 0xfffffffe or NO_VAL).Scancel Command Use\n\nIf the job ID of a job array is specified as input to the scancel command\nthen all elements of that job array will be cancelled.\nAlternately an array ID, optionally using regular expressions, may be specified\nfor job cancellation.\n# Cancel array ID 1 to 3 from job array 20\n$ scancel 20_[1-3]\n\n# Cancel array ID 4 and 5 from job array 20\n$ scancel 20_4 20_5\n\n# Cancel all elements from job array 20\n$ scancel 20\n\n# Cancel the current job or job array element (if job array)\nif [[-z $SLURM_ARRAY_JOB_ID]]; then\n  scancel $SLURM_JOB_ID\nelse\n  scancel ${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}\nfi\nSqueue Command UseWhen a job array is submitted to Slurm, only one job record is created.\nAdditional job records will only be created when the state of a task in the\njob array changes, typically when a task is allocated resources or its state\nis modified using the scontrol command.\nBy default, the squeue command will report all of the tasks associated with\na single job record on one line and use a regular expression to indicate the\n\"array_task_id\" values as shown below.\n$ squeue\n JOBID     PARTITION  NAME  USER  ST  TIME  NODES NODELIST(REASON)\n1080_[5-1024]  debug   tmp   mac  PD  0:00      1 (Resources)\n1080_1         debug   tmp   mac   R  0:17      1 tux0\n1080_2         debug   tmp   mac   R  0:16      1 tux1\n1080_3         debug   tmp   mac   R  0:03      1 tux2\n1080_4         debug   tmp   mac   R  0:03      1 tux3\nAn option of \"--array\" or \"-r\" has also been added to the squeue command\nto print one job array element per line as shown below.\nThe environment variable \"SQUEUE_ARRAY\" is equivalent to including the \"--array\"\noption on the squeue command line.\n$ squeue -r\n JOBID PARTITION  NAME  USER  ST  TIME  NODES NODELIST(REASON)\n1082_3     debug   tmp   mac  PD  0:00      1 (Resources)\n1082_4     debug   tmp   mac  PD  0:00      1 (Priority)\n  1080     debug   tmp   mac   R  0:17      1 tux0\n  1081     debug   tmp   mac   R  0:16      1 tux1\n1082_1     debug   tmp   mac   R  0:03      1 tux2\n1082_2     debug   tmp   mac   R  0:03      1 tux3\nThe squeue --step/-s and --job/-j options can accept job or step\nspecifications of the same format.\n$ squeue -j 1234_2,1234_3\n...\n$ squeue -s 1234_2.0,1234_3.0\n...\nTwo additional job output format field options have been added to squeue:\n%F prints the array_job_id value\n%K prints the array_task_id value\n(all of the obvious letters to use were already assigned to other job fields).Scontrol Command Use\n\nUse of the scontrol show job option shows two new fields related to\njob array support.\nThe JobID is a unique identifier for the job.\nThe ArrayJobID is the JobID of the first element of the job\narray.\nThe ArrayTaskID is the array index of this particular entry, either a\nsingle number of an expression identifying the entries represented by this\njob record (e.g. \"5-1024\").\nNeither field is displayed if the job is not part of a job array.\nThe optional job ID specified with the scontrol show job or\nscontrol show step commands can identify job array elements by\nspecifying ArrayJobId and ArrayTaskId with an underscore between\nthem (e.g. <ArrayJobID>_<ArrayTaskId>). \n\nThe scontrol command will operate on all elements of a job array if the\njob ID specified is ArrayJobID.\nIndividual job array tasks can be modified using the\nArrayJobID_ArrayTaskID as shown below.\n\n$ sbatch --array=1-4 -J array ./sleepme 86400\nSubmitted batch job 21845\n\n$ squeue\n JOBID   PARTITION     NAME     USER  ST  TIME NODES NODELIST\n 21845_1    canopo    array    david  R  0:13  1     dario\n 21845_2    canopo    array    david  R  0:13  1     dario\n 21845_3    canopo    array    david  R  0:13  1     dario\n 21845_4    canopo    array    david  R  0:13  1     dario\n\n$ scontrol update JobID=21845_2 name=arturo\n$ squeue\n JOBID   PARTITION     NAME     USER  ST   TIME  NODES NODELIST\n 21845_1    canopo    array    david  R   17:03   1    dario\n 21845_2    canopo   arturo    david  R   17:03   1    dario\n 21845_3    canopo    array    david  R   17:03   1    dario\n 21845_4    canopo    array    david  R   17:03   1    dario\n\n\n\nThe scontrol hold, holdu, release, requeue, requeuehold, suspend and resume\ncommands can also either operate on all elements of a job array or individual\nelements as shown below.\n\n\n$ scontrol suspend 21845\n$ squeue\n JOBID PARTITION      NAME     USER  ST TIME  NODES NODELIST\n21845_1    canopo    array    david  S 25:12  1     dario\n21845_2    canopo   arturo    david  S 25:12  1     dario\n21845_3    canopo    array    david  S 25:12  1     dario\n21845_4    canopo    array    david  S 25:12  1     dario\n$ scontrol resume 21845\n$ squeue\n JOBID PARTITION      NAME     USER  ST TIME  NODES NODELIST\n21845_1    canopo    array    david  R 25:14  1     dario\n21845_2    canopo   arturo    david  R 25:14  1     dario\n21845_3    canopo    array    david  R 25:14  1     dario\n21845_4    canopo    array    david  R 25:14  1     dario\n\nscontrol suspend 21845_3\n$ squeue\n JOBID PARTITION      NAME     USER  ST TIME  NODES NODELIST\n21845_1    canopo    array    david  R 25:14  1     dario\n21845_2    canopo   arturo    david  R 25:14  1     dario\n21845_3    canopo    array    david  S 25:14  1     dario\n21845_4    canopo    array    david  R 25:14  1     dario\nscontrol resume 21845_3\n$ squeue\n JOBID PARTITION      NAME     USER  ST TIME  NODES NODELIST\n21845_1    canopo    array    david  R 25:14  1     dario\n21845_2    canopo   arturo    david  R 25:14  1     dario\n21845_3    canopo    array    david  R 25:14  1     dario\n21845_4    canopo    array    david  R 25:14  1     dario\n\n\nJob Dependencies\n\n\nA job which is to be dependent upon an entire job array should specify\nitself dependent upon the ArrayJobID.\nSince each array element can have a different exit code, the interpretation of\nthe afterok and afternotok clauses will be based upon the highest\nexit code from any task in the job array.\nWhen a job dependency specifies the job ID of a job array:\nThe after clause is satisfied after all tasks in the job array start.\nThe afterany clause is satisfied after all tasks in the job array\ncomplete.\nThe aftercorr clause is satisfied after the corresponding task ID in the\nspecified job has completed successfully (ran to completion with an exit code of\nzero).\nThe afterok clause is satisfied after all tasks in the job array\ncomplete successfully.\nThe afternotok clause is satisfied after all tasks in the job array\ncomplete with at least one tasks not completing successfully.\nExamples of use are shown below:\n\n# Wait for specific job array elements\nsbatch --depend=after:123_4 my.job\nsbatch --depend=afterok:123_4:123_8 my.job2\n\n# Wait for entire job array to complete\nsbatch --depend=afterany:123 my.job\n\n# Wait for corresponding job array elements\nsbatch --depend=aftercorr:123 my.job\n\n# Wait for entire job array to complete successfully\nsbatch --depend=afterok:123 my.job\n\n# Wait for entire job array to complete and at least one task fails\nsbatch --depend=afternotok:123 my.job\n\nOther Command Use\n\n\nThe following Slurm commands do not currently recognize job arrays and their\nuse requires the use of Slurm job IDs, which are unique for each array element:\nsbcast, sprio, sreport, sshare and sstat.\nThe sacct, sattach and strigger commands have been modified to permit\nspecification of either job IDs or job array elements.\nThe sview command has been modified to permit display of a job's ArrayJobId\nand ArrayTaskId fields. Both fields are displayed with a value of \"N/A\" if\nthe job is not part of a job array.\nSystem Administration\n\n\nA new configuration parameter has been added to control the maximum\njob array size: MaxArraySize. The smallest index that can be specified\nby a user is zero and the maximum index is MaxArraySize minus one.\nThe default value of MaxArraySize is 1001.\nThe maximum MaxArraySize supported in Slurm is 4000001.\n Be mindful about the value of MaxArraySize as job arrays offer an easy way\nfor users to submit large numbers of jobs very quickly.\nThe sched/backfill plugin has been modified to improve performance with\njob arrays. Once one element of a job array is discovered to not be runnable\nor impact the scheduling of pending jobs, the remaining elements of that job\narray will be quickly skipped.\nSlurm creates a single job record when a job array is submitted.\nAdditional job records are only created as needed, typically when a task\nof a job array is started, which provides a very scalable mechanism to\nmanage large job counts.\nEach task of the job array will share the same ArrayJobId but will have their\nown unique ArrayTaskId. In addition to the ArrayJobId, each job will have a\nunique JobId that gets assigned as the tasks are started.\nLast modified 01 January 2024\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Job Dependencies\n\n",
                "content": "A job which is to be dependent upon an entire job array should specify\nitself dependent upon the ArrayJobID.\nSince each array element can have a different exit code, the interpretation of\nthe afterok and afternotok clauses will be based upon the highest\nexit code from any task in the job array.When a job dependency specifies the job ID of a job array:\nThe after clause is satisfied after all tasks in the job array start.\nThe afterany clause is satisfied after all tasks in the job array\ncomplete.\nThe aftercorr clause is satisfied after the corresponding task ID in the\nspecified job has completed successfully (ran to completion with an exit code of\nzero).\nThe afterok clause is satisfied after all tasks in the job array\ncomplete successfully.\nThe afternotok clause is satisfied after all tasks in the job array\ncomplete with at least one tasks not completing successfully.Examples of use are shown below:\n# Wait for specific job array elements\nsbatch --depend=after:123_4 my.job\nsbatch --depend=afterok:123_4:123_8 my.job2\n\n# Wait for entire job array to complete\nsbatch --depend=afterany:123 my.job\n\n# Wait for corresponding job array elements\nsbatch --depend=aftercorr:123 my.job\n\n# Wait for entire job array to complete successfully\nsbatch --depend=afterok:123 my.job\n\n# Wait for entire job array to complete and at least one task fails\nsbatch --depend=afternotok:123 my.job\nOther Command Use\n\nThe following Slurm commands do not currently recognize job arrays and their\nuse requires the use of Slurm job IDs, which are unique for each array element:\nsbcast, sprio, sreport, sshare and sstat.\nThe sacct, sattach and strigger commands have been modified to permit\nspecification of either job IDs or job array elements.\nThe sview command has been modified to permit display of a job's ArrayJobId\nand ArrayTaskId fields. Both fields are displayed with a value of \"N/A\" if\nthe job is not part of a job array.System Administration\n\nA new configuration parameter has been added to control the maximum\njob array size: MaxArraySize. The smallest index that can be specified\nby a user is zero and the maximum index is MaxArraySize minus one.\nThe default value of MaxArraySize is 1001.\nThe maximum MaxArraySize supported in Slurm is 4000001.\n Be mindful about the value of MaxArraySize as job arrays offer an easy way\nfor users to submit large numbers of jobs very quickly.The sched/backfill plugin has been modified to improve performance with\njob arrays. Once one element of a job array is discovered to not be runnable\nor impact the scheduling of pending jobs, the remaining elements of that job\narray will be quickly skipped.Slurm creates a single job record when a job array is submitted.\nAdditional job records are only created as needed, typically when a task\nof a job array is started, which provides a very scalable mechanism to\nmanage large job counts.\nEach task of the job array will share the same ArrayJobId but will have their\nown unique ArrayTaskId. In addition to the ArrayJobId, each job will have a\nunique JobId that gets assigned as the tasks are started.Last modified 01 January 2024"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/resource_binding.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Resource Binding",
                "content": "\nOverview\nSrun --cpu-bind option\nNode CpuBind Configuration\nPartition CpuBind Configuration\nTaskPluginParam Configuration\nOverviewSlurm has a rich set of options to control the default\nbinding of tasks to resources.\nFor example, tasks can be bound to individual threads, cores, sockets, NUMA\nor boards.\nSee the slurm.conf and srun man pages for more information about how these\noptions work.\nThis document focuses on how default binding configuration can be configured.\nDefault binding can be configured on a per-node, per-partition or global\nbasis. The highest priority will be that specified using the srun\n--cpu-bind option.\nThe next highest priority binding will be the node-specific binding, if any\nnode in the job allocation has some \nCpuBind configuration parameter and all other nodes in the job\nallocation either have the same or no CpuBind configuration parameter.\nThe next highest priority binding will be the partition-specific\nCpuBind configuration\nparameter (if any).\nThe lowest priority binding will be that specified by the\nTaskPluginParam\nconfiguration parameter.Summary of the order of enforcement:\n\nSrun --cpu-bind option\nNode CpuBind configuration parameter (if all nodes match)\nPartition CpuBind configuration parameter\nTaskPluginParam configuration parameter\n\nSrun --cpu-bind optionThe srun --cpu-bind option will always\nbe used to control task binding. If the --cpu-bind option only includes\n\"verbose\" rather than identifying the entities to be bound to, then the verbose\noption will be used together with the default entity based upon Slurm\nconfiguration parameters as described below.Node CpuBind Configuration\n\nThe next possible source of the resource binding information is the node's\nconfigured CpuBind value, but only\nif every node has the same CpuBind value (or no configured CpuBind value).\nThe node's CpuBind value is configured in the slurm.conf file.\nIts value may be viewed or modified using the scontrol command.\nTo clear a node's CpuBind value use the command:\n\nscontrol update NodeName=node01 CpuBind=off\n\nIf a node_features plugin is configured, typically to support booting Intel\nKNL nodes into different NUMA and/or MCDRAM modes, the plugin can be configured\nto modify the node's CpuBind option based upon the NUMA mode.\nThis is accomplished by specifying the NumaCpuBind parameter in the knl.conf\nconfiguration file with pairs of NUMA modes and CpuBind options.\nAs soon as the node is booted into a new NUMA mode, the node's CpuBind option\nis automatically modified.\nFor example, a line like the following in the knl.conf file\n(NumaCpuBind=a2a=core;snc2=thread),\nwill set a node's CpuBind field to \"core\" when booted into \"a2a\" (all to all)\nNUMA mode and to \"thread\" when booted into \"snc2 NUMA mode.\nAny NUMA mode not specified in the NumaCpuBind configuration file will result\nin no change to the node's CpuBind field.Partition CpuBind Configuration\n\nThe next possible source of the resource binding information is the\npartition's configured CpuBind\nvalue. The partition's CpuBind value is configured in the slurm.conf file.\nIts value may be viewed or modified using the scontrol command, similar to how\na node's CpuBind value is changed:\n\nscontrol update PartitionName=debug CpuBind=cores\n\nTaskPluginParam Configuration\n\nThe last possible source of the resource binding information is the\nTaskPluginParam\nconfiguration parameter from the slurm.conf file.Last modified 10 July 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/publications.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Presentations",
                "content": "Note that older presentations may contain outdated information.Presentations from Cray User Group, May 2024\nSlurm 24.08 24.05, and Beyond,\nTim Wickberg, SchedMD\nPresentations from SC23, November 2023\nSlurm and/or/vs Kubernetes,\nTim Wickberg, SchedMD\nSlurm 23.02, 23.11, and Beyond,\nTim Wickberg, SchedMD\nPresentations from Slurm User Group Meeting, September 2023\n\nKeynote: Improving quinoa through the development of genetic and genomic resources,\nDavid Jarvis, Brigham Young University\nNever use Slurm HA again: Solve all your problems with Kubernetes,\nChris Samuel and Doug Jacobsen, NERSC\n\nContainers in Slurm,\nScott Hilton, SchedMD\n\nOptimizing Diverse Workloads and Resource Usage with Slurm,\nChansup Byun et al, LLSC\nSlurm's REST API,\nNathan Rini, SchedMD\nBuild a flexible and powerful High Performance Computing foundation with\u00a0Google Cloud,\nVolker Eyrich (Google) and Joshua Fryer (Recursion)\nDemand Driven Cluster Elasticity,\nMike Fazio, Dow\nField Notes 7 \u2013 How to make the most of Slurm and avoid common issues,\nJason Booth, SchedMD\n\nAccelerating Genomics Research Machine Learning with Slurm,\nWilly Markuske, San Diego Supercomputing Center (SDSC)\nSaving Power with Slurm,\nOle Nielsen, Technical University of Denmark (DTU)\nRunning Flux in Slurm,\nRyan Day, LLNL\nSite Report: CINECA experience with Slurm,\nAlessandro Marani, CINECA\nStep Management Enhancements,\nBrian Christiansen, SchedMD\nSystem and Job Scheduling Simulation For Enhancing Production HPC,\nVivian Hafener, LANL\nSite Update: Georgia Institute of Technology,\nMarian Zvada and Aaron Jezghani, Georgia Tech\nBuilding Blocks in the Cloud: Scaling LEGO Engineering with AWS High-Performance Computing,\nBrian Skjerven and Matt Vaughn, AWS\nSlurm 23.02, 23.11, and Beyond (Roadmap),\nTim Wickberg, SchedMD\nPresentations from Dell HPC Community, September 2023\nSlurm and/or/vs Kubernetes,\nTim Wickberg, SchedMD\nPresentations from Cray User Group, May 2023\nSlurm 23.02, 23.11, and Beyond,\nTim Wickberg, SchedMD\nPresentations from SC22, November 2022\nSlurm \u2665 Containers,\nNate Rini & Tim Wickberg, SchedMD\nDoing More with Slurm Advanced Capabilities,\nShawn Hoopes, SchedMD\nSlurm 22.05, 23.02, and Beyond,\nTim Wickberg, SchedMD\nSlurm and/or/versus Kubernetes,\nTim Wickberg, SchedMD\nAccelerating HPC and AI with Slurm and SchedMD,\nNick Ihli, SchedMD\nPresentations from the HPC Containers Advisory Working Group, November 2022\nSlurm \u2665 Containers, Nathan Rini, SchedMD\nPresentations from CNCF Research End User Group, October 2022\nSlurm Container Support,\nVideo,\nNathan Rini, SchedMD\n\nPresentations from Slurm User Group Meeting, September 2022\nField Notes 6: From The Frontlines of Slurm Support,\nVideo,\nJason Booth, SchedMD\nPathfinding into the clouds,\nVideo,\nOle Nielsen, Technical University of Denmark (DTU)\nOCI Containers with scrun\nVideo,\nNathan Rini, SchedMD\nLBNL Site Report,\nVideo,\nWei Feinstein, Lawrence Berkeley National Laboratory\nCloudy, With a Chance of Dynamic Nodes,\nVideo,\nNick Ihli, SchedMD\nBurst Buffer Lua Plugin For Lustre,\nVideo,\nKota Tsuyuzaki / Rikimaru Honjo / Yusuke Kaneko / Kohei Tahara, NTT Computer and Data Science Laboratory / NTT TechnoCross Corporation\n22.05, 23.02, and Beyond,\nVideo,\nTim Wickberg, SchedMD\nEDA Slurm Cluster on AWS,\nVideo,\nAllan Carter, AWS\nPresentations from NHR Container Workshop, December 2021\nContainers and Slurm,\nVideo,\nNathan Rini, SchedMD\nPresentations from SC21, November 2021\nBOF: Slurm Birds of a Feather,\nVideo,\nTim Wickberg, SchedMD\nPresentations from Slurm User Group Meeting, September 2021\nField Notes 5: From The Frontlines of Slurm Support,\nVideo,\nJason Booth, SchedMD\nREST API and also Containers,\nVideo,\nNathan Rini, SchedMD\nburst_buffer/lua and slurmscriptd,\nVideo,\nMarshall Garey, SchedMD\nSlurm in the Clouds,\nVideo,\nNick Ihli, SchedMD\nSlurm 21.08 and Beyond,\nVideo,\nTim Wickberg, SchedMD\nPresentations from SC20, November 2020\nBOF: Slurm Birds of a Feather,\nTim Wickberg, SchedMD\nPresentations from Slurm User Group Meeting, September 2020\nField Notes 4: From The Frontlines of Slurm Support,\nVideo,\nJason Booth, SchedMD\nCloud and Stuff,\nVideo,\nBrian Christiansen, SchedMD\nREST API,\nVideo,\nNathan Rini, SchedMD\nSlurm 20.11 and Beyond,\nVideo,\nTim Wickberg, SchedMD\nPresentations from PEARC HPCSYSPROS Workshop, August 2020\nREST API, Nathan Rini, SchedMD\nPresentations from Slurm User Group Meeting, September 2019\n\nWelcome,\nDanny Auble, SchedMD\nTutorial: TRES and Banking,\nBrian Christiansen, SchedMD\nTechnical: GPU Scheduling and the cons_tres plugin,\nChad Vizino and Morris Jette, SchedMD\nSite Report: LANL,\nJoseph 'Joshi' Fullop, LANL\nTutorial: Cgroups and pam_slurm_adopt,\nMarshall Garey, SchedMD\nSite Report: Enabling and Scaling Diverse Work Loads Efficiently With Slurm,\nChansup Byun et al., MIT Lincoln Laboratory\nTutorial: Priority and Fair Trees,\nShawn Hoopes, SchedMD\nTechnical: Slurm: Seamless Integration With Unprivileged Containers,\nLuke Yeager et al., NVIDIA\nTechnical: REST API,\nNathan Rini, SchedMD\nTechnical: Job Container plugin for managing node local namespaces,\nAditi Gaur, NERSC\n\nTechnical: VMs and containers for a Slurm-based development cluster,\nFran\u00e7ois Daikhat\u00e9, CEA\nTechnical: High Throughput Computing,\nBroderick Gardner, SchedMD\nSite Report: Slurm on Sherlock,\nKilian Cavalotti, Stanford Research Computing Center\nTechnical: Slurm + GCP,\nBrian Christiansen (SchedMD) and Keith Binder (Google)\nSite Report: ORNL,\nMatt Ezell, ORNL\nTechnical: Monitoring Slurm with a Splunk App,\nNicole Dobson, LANL\nSite Report: NERSC,\nChris Samuel, NERSC\nTutorial: Troubleshooting,\nAlbert Gil and Jason Booth, SchedMD\nTechnical: Slurm Account Synchronization with UNIX Groups and Users,\nOle Nielsen, Technical University of Denmark (DTU)\nTechnical: A fully configurable HPC web portal for managing Slurm jobs,\nPatrice Calegari, Atos\nTechnical: Slurm 19.05,\nTim Wickberg, SchedMD\nTechnical: Slurm 20.02 and Beyond,\nTim Wickberg, SchedMD\nTechnical: Field Notes From A MadMan,\nTim Wickberg, SchedMD\nPresentations from Slurm User Group Meeting, September 2018\n\n\nTutorial: Slurm Overview,\nFelip Moll Marqu\u00e8s, SchedMD\nTechnical: Workload Management Requirements for an Interactive Computing e-Infrastructure,\nSadaf Alam (CSCS) and the ICEI team (BSC, CEA, CINECA, CSCS, J\u00fclich)\nTechnical: Slurm in a Container Only World \u2014 Are We Crazy?,\nPaul Peltz and Lowell Wofford (LANL)\nTechnical: Kraken - A stateful approach to cluster management,\nPaul Peltz and Lowell Wofford (LANL)\nTechnical: A Declarative Programming Style Job Submission Filter,\nDouglas Jacobsen, NERSC\nTechnical: Generalized Hypercube (GHC) \u2014 A Topology Plugin,\nM. Clayer and A. Faure, Atos\nTechnical: Keeping Accounts Consistent Across Clusters Using LDAP and YAML,\nChristian Cl\u00e9men\u00e7on, Ewan Roche, Ricardo Silva (EPFL)\nTechnical: Real-Time Job Monitoring Using An Extended slurmctld Generic Plugin \u2014 Introducing the plugin architecture SPACE,\nMike Arnhold, Ulf Markwardt, and Danny Rotscher (Dresden)\nTechnical: Scheduling by Trackable Resource (cons_tres),\nMorris Jette and Dominik Bartkiewicz, SchedMD\n\nTechnical: Slurm 18.08 Overview,\nBrian Christiansen, SchedMD\n\nTechnical: Layout For Checkpoint Restart on Specialized Blades,\nBill Brophy, Martin Perry, Doug Parisek, and Steve Mehlberg (Atos)\nSite Report: CEA Site Report,\nRegine Gaudin, CEA\nSite Report: Colliding High Energy Physics With HPC, Cloud, and Parallel Filesystems,\nCarolina Lindqvist, Pablo Llopis, and Nils H\u00f8imyr (CERN)\nTechnical: Slurm Simulator Improvements and Evaluation,\nMarco D'Amico, Ana Jokanovic, Julita Corbalan (BSC)\nSite Report: CETA-CIEMAT Site Report,\nAlfonso Pardo, CETA-CIEMAT\nSite Report: Tuning Slurm the CSCS Way,\nMiguel Gila, CSCS\nTechnical: Workload Scheduling and Power Management,\nMorris Jette and Alejandro Sanchez, SchedMD\nSite Report: LANL Site Report \u2014 One Year Post Migration,\nJoseph 'Joshi' Fullop, LANL\nTechnical: Field Notes Mark 2: Random Musings From Under A New Hat,\nTim Wickberg, SchedMD\nPresentations from Slurm Booth and Birds of a Feather, SC17, November 2017\nBooth: Slurm Overview,\nBrian Christiansen, Marshall Garey, Isaac Hartung (SchedMD)\nBooth: Heterogeneous Job Support,\nMorris Jette, Tim Wickberg (SchedMD)\nBooth: From Moab to Slurm: 12 HPC Systems in 2 Months,\nPaul Peltz, Los Alamost National Laboratory\nBooth: PMIx Multi-Cluster Operations,\nRalph H. Castain\nBooth: Federated Cluster Support,\nBrian Christiansen, SchedMD\nBooth: PMIx Plugin with UCX Support,\nArtem Polyakov, Mellanox\nBOF: Slurm Birds of a Feather,\nTim Wickberg, SchedMD\nPresentations from Slurm User Group Meeting, September 2017\n\nKeynote: Supernova Cosmology & Supercomputing,\nAlex Kim, Lawrence Berkeley National Laboratory\nTutorial: Introduction to Slurm,\nTim Wickberg, SchedMD\nTechnical: SLURMFS \u2014 Resource Manager File System for Slurm,\nSteven Senator, Los Alamos National Laboratory\nTechnical: Federated Cluster Support,\nBrian Christiansen and Danny Auble, SchedMD\nTechnical: Utilizing Slurm and Passive Nagios Plugins for Scalable KNL Compute Node Monitoring,\nTony Quan and Basil Lalli, NERSC/LBNL\nTechnical: Field Notes From the Frontlines of Slurm Support,\nTim Wickberg, SchedMD\n\nTechnical: Towards Modular Supercomputing with Slurm\nDorian Krause et al., JSC\nTechnical: Heterogeneous Job Support,\nMorris Jette, SchedMD\nTechnical: cli_filter \u2014 command line filtration, manipulation, and introspection of job submissions\n(PDF version),\nDouglas Jacobsen, NERSC\nTechnical: Slurm \u2014 Some Slightly Unconventional Use Cases,\nChris Hill (MIT, Rajul Kumar (Northeastern), Evan Weinberg and Naved Ansari (BU), Tim Donahue\nTechnical: Managing Diversity in Complex Workloads in a Complex Environment,\nNicholas Cardo, CSCS\n\nTechnical: SELinux policy for Slurm,\nGilles Wiber and Mathieu Blanc (CEA), M'hamed Bouaziz and Liana Bozga (Atos)\nSite Report: From Moab to Slurm: 12 HPC Systems in 2 Months,\nPeltz, Fullop, Jennings, Senator, Grunau (Los Alamost National Laboratory)\nSite Report: NERSC Site Report,\nJames Botts and Douglas Jacobsen\nTechnical: Slurm Roadmap - 17.11, 18.08, and Beyond,\nDanny Auble, Morris Jette, Tim Wickberg (SchedMD)\nTechnical: New Statistics Using TRES,\nBill Brophy, Martin Perry, Thomas Cadeau (Atos)\nTechnical: Enabling web-based interactive notebooks on geographically distributed HPC resources,\nAlexandre Beche, EPFL\nTechnical: Slurm Singularity Spank Plugin,\nMartin Perry, Steve Mehlberg, Thomas Cadeau (Atos)\nSite Report: A Slurm Odyssey: Slurm at Harvard FAS Research Computing,\nPaul Edmond\nSite Report: LLSC Adoption of Slurm for Managing Diverse Resources and Workloads,\nChansup Byun et al., MIT Lincoln Laboratory\nSite Report: Cyfronet Site Report \u2014 Improving Slurm Usability and Monitoring,\nM Pawlik, J. Budzowski, L. Flis, P Laso\u0144, M. Magry\u015b\nTechnical: When you have a hammer, everything looks like a nail \u2014 Checkpoint/restart in Slurm\nManuel Rodr\u00edguez-Pascual, J.A. Mor\u00ed\u00f1igo, and Rafael Mayo-Garc\u00eda, CIEMAT\nPresentations from Slurm Booth and Birds of a Feather, SC16, November 2016\nBooth: Process Management Interface - Exascale (PMIx),\nRalph H. Castain\nBooth: Bull Slurm Related Developments,\nJob Packs demo video,\nYiannis Georgiou, Bull AtoS\nBooth: Transition Hangout (a.k.a. how we converted to Slurm),\nRyan Cox (BYU), Bruce Pfaff (NASA)\nBooth: Expanding Serial Analysis with Slurm Arrays,\nChristopher Coffey, Northern Arizona University\nBooth: Intel HPC Orchestrator,\nTom Krueger, Intel\nBooth: Slurm Overview,\nMoe Jette, SchedMD\nBOF: Slurm State of the Union; v16.05, v17.02 and Beyond,\nTim Wickberg, SchedMD\nPresentations from Slurm User Group Meeting, September 2016\n\nKeynote: Computer-aided drug design for novel anti-cancer agents,\nDr. Zoe Cournia (Biomedical Research Foundation, Academy of Athens)\nTechnical: Overview of Slurm Version 16.05,\nDanny Auble (SchedMD), Yiannis Georgiou (Bull)\nTechnical: MCS (Multi-Category Security) Plugin,\nAline Roy, CEA\nTechnical: Slurm Burst Buffer Integration,\nDavid Paul, NERSC\nTechnical: Slurm Configuration Impact on Benchmarking,\nJos\u00e9 Mor\u00ed\u00f1igo, Manuel Rodr\u00edguez-Pascual, and Rafael Mayo-Garc\u00eda, CIEMAT\nTechnical: Real-time monitoring Slurm jobs with InfluxDB,\nCarlos Fenoy Garc\u00eda\nTechnical: Optimising HPC resource allocation through monitoring,\nAlexandre Beche, EPFL\nTechnical: Simunix, a large scale platform simulator,\nDavid Glesser and Adrien Faure, Bull AtoS\nSite Report: Swiss National Supercomputer Centre (CSCS),\nNicholas Cardo\nTechnical: Configure a Slurm cluster with Ansible,\nJohan Guldmyr, CSC\nTechnical: Checkpoint/restart in Slurm: current status and new developments,\nManuel Rodr\u00edguez-Pascual, J.A. Mor\u00ed\u00f1igo, and Rafael Mayo-Garc\u00eda, CIEMAT\nTechnical: Intel Knights Landing (KNL),\nMorris Jette and Tim Wickberg, SchedMD\nTechnical: Job Packs - A New Slurm Feature For Enhanced Support of Heterogeneuous Resources,\nAndry Razafinjatovo, Martin Perry, and Yiannis Georgiou (Bull AtoS), Matthieu Hautreux (CEA)\nTechnical: Improving system utilization under strict power budget using the layouts,\nDineshkumar Rajagopal, Yiannis Georgiou, and David Glesser, Bull AtoS\nTechnical: High definition power and energy monitoring support,\nThomas Cadeau and Yiannis Georgiou, Bull AtoS\nTechnical: Federated Cluster Scheduling,\nDominik Bartkiewicz and Brian Christiansen, SchedMD\nTechnical: Slurm Roadmap - SchedMD,\nDanny Auble, SchedMD\nTechnical: Slurm Roadmap - Bull,\nYiannis Georgiou and Andry Razafinjatovo, Bull AtoS\nSite Report: Electricit\u00e9 de France(EDF),\nC\u00e9cile Yoshikawa\nSite Report: Leibniz-Rechenzentrum (LRZ),\nJuan Pancorbo Armada\nSite Report: NERSC Site Report - One Year Of Slurm,\nDouglas Jacobsen\nSite Report: Experience using Slurm on ARIS HPC System,\nNikos Nikoloutsakos, GRNET\nPresentations from Slurm Booth and Birds of a Feather, SC15, November 2015\nBooth: PMIx - Enabling Application-driven Execution at Exascale ,\nRalph H. Castain\nBooth: NASA NCCS Site Update,\nBruce Pfaff, NASA\nBooth: Brigham Young University - Site Report,\nRyan Cox, BYU\nBooth: Slurm Overview\nBrian Christiansen and Danny Auble, SchedMD\nBooth: Never Port Your Code Again - Docker functionality with Shifter using SLURM,\nShane Canon, NERSC\nBooth: Slurm Burst Buffer Support,\nTim Wickberg, SchedMD\nBooth: Slurm Overview and Elasticsearch Plugin\nAlejandro Sanchez, SchedMD\nBooth: All Things TRES,\nBrian Christiansen, SchedMD\nBOF: Slurm Version 15.08,\nDanny Auble, SchedMD\nBOF: Improving Backfilling by using Machine Learning to Predict Running Times in SLURM,\nDavid Glesser, Bull\nPresentations from Slurm User Group Meeting, September 2015\n\nKeynote: 10-years of Computing and Atmospheric Research at NASA: 1 day per day,\nBill Putnam, NASA\nTechnical: Overview of Slurm Version 15.08,\nMorris Jette and Danny Auble (SchedMD), Yiannis Georgiou (Bull)\nTechnical: Trackable Resources (TRES),\nBrian Christiansen and Danny Auble, SchedMD\nTechnical: Message Aggregation,\nDanny Auble (SchedMD), Yiannis Georgiou and Martin Perry (Bull)\nTechnical:  Slurm Burst Buffer Support,\nMorris Jette (SchedMD), Tim Wickberg (GW)\nTechnical: Partition QOS,\nDanny Auble, SchedMD\nTechnical: Slurm Power Management Support,\nMorris Jette, SchedMD\nTechnical: Slurm Layouts Framework,\nMatthieu Hautreux, CEA\nTechnical: Power Adaptive Scheduling,\nYiannis Georgiou and David Glesser (Bull),\nMatthieu Hautreux (CEA),\nDenis Trystram (LIG)\nTechnical: Never Port Your Code Again - Docker functionality with Shifter using SLURM,\nDouglas Jacobsen, James Botts, and Shane Canon, NERSC\nTechnical: Increasing cluster throughput with Slurm and rCUDA,\nFederico Silla, Technical University of Valencia Spain\nTechnical: Running Virtual Machines in a Slurm Batch System,\nUlf Markwardt, Technische Universit\u00e4t Dresden\nTechnical: Supporting SR-IOV and IVSHMEM in MVAPICH2 on Slurm,\nXiaoyi Lu, Jie Zhang, et. al., The Ohio State University \nTechnical: Heterogeneous Resources and MPMD (aka Job Pack),\nRod Schultz and Martin Perry (Atos), Matthieu Hautreaux (CEA), Yiannis Georgiou (Atos)\nTechnical: Towards multi-objective resource selection,\nDineshkumar Rajagopal, David Glesser, Yiannis Georgiou, BULL\nTechnical: Enhancing Startup Performance of Parallel Applications with SLURM,\nSourav Chakraborty, et. al., OSU / LLNL\nTechnical: Adaptable Profile-Driven TestBed (\"Apt\"),\nBrian Haymore, The University of Utah\nTechnical: Using and Modifying the BSC Slurm Workload Simulator,\nStephen Trofinoff and Massimo Benini, CSCS\nTechnical: Improving Job Scheduling by using Machine Learning,\nDavid Glesser, Yiannis Georgiou (BULL) and Denis Trystram (LIG)\nTechnical: Federated Cluster Scheduling,\nBrian Christiansen and Danny Auble, SchedMD\nTechnical: Native SLURM on the XC30,\nDoug Jacobsen, James Botts, NERSC\nTechnical: Slurm Roadmap - Versions 16.05 and beyond,\nMorris Jette and Danny Auble (SchedMD), Yiannis Georgiou (Bull)\nTechnical: Exascale Process Management Interface,\nRalph Castain (Intel), Joshua Ladd (Mellanox), Artem Polyakov (Mellanox), David Bigagli (SchedMD), Gary Brown (Adaptive Computing)\nSite Report: Brigham Young University,\nRyan Cox, BYU\nSite Report: University of South Florida,\nJohn DeSantis, USF\nSite Report: NASA Center for Climate Simulation,\nBruce Pfaff, NASA\nSite Report: J\u00fclich Supercomputing Centre,\nDorian Krause, JSC\nSite Report: The George Washington University,\nTim Wickberg, GW\nPresentations from Slurm Birds of a Feather and the Slurm booth, SC14, November 2014\nSlurm Overview,\nDanny Auble and Brian Christiansen, SchedMD\nSlurm Version 14.11,\nJacob Jenson, SchedMD\nSlurm Version 15.08 Roadmap,\nJacob Jenson, SchedMD\nSlurm on Cray systems,\nDavid Wallace, Cray\nFair Tree: Fairshare Algorithm for Slurm\nRyan Cox and Levi Morrison (Brigham Young University)\nVLSCI Site Report,\nChris Samuel (VLSCI)\nPresentations from Slurm User Group Meeting, September 2014\nGroup photo\nPaul Hsi (MIT Kavli Institute for Astrophysics and Space Research)\nWelcoming Address\nColin McMurtrie (Swiss National Supercomputing Centre, CSCS)\n\nOverview of Slurm Versions 14.03 and 14.11\nJacob Jenson (SchedMD) and Yiannis Georgiou (Bull)\nWarewulf Node Health Check\nJacqueline Scoggins and Michael Jennings (Lawrence Berkeley National Lab)\nSlurm Process Isolation\nBill Brophy, Martin Perry and Yiannis Georgiou (Bull),\nMorris Jette (SchedMD),\nMatthieu Hautreux (CEA)\nImproving message forwarding logic in Slurm\nRod Schultz, Martin Perry and Yiannis Georgiou (Bull),\nMatthieu Hautreux (CEA),\nDanny Auble and Morris Jette (SchedMD)\nTuning Slurm Scheduling for Optimal Responsiveness and Utilization\nMorris Jette (SchedMD)\nImproving HPC applications scheduling with predictions\nbased on automatically-collected historical data\nCarlos Fenoy Garc\u00eda (Barcelona  Supercomputing Centre)\nOStrich: Fair Scheduler for Burst Submissions of Parallel Job\nKrzysztof Rzadca (University of Warsaw) and Filip Skalski ((University of Warsaw / Google)\nAdaptive Resource and Job Management for limited power consumption\nYiannis Georgiou and David Glesser (Bull),\nMatthieu Hautreux (CEA),\nDenis Trystram (University Grenoble-Alpes)\nIntroducing Energy based fair-share scheduling\nYiannis Georgiou and David Glesser (Bull),\nKrzysztof Rzadca (University of Warsaw),\nDenis Trystram (University Grenoble-Alpes)\nHigh Performance Data movement between Lustre and Enterprise storage systems\nAamir Rashid (Terascala)\nExtending Slurm with Support for Remote GPU Virtualization\nSergio Iserte, Adri\u00e1n Castell\u00f3, Rafael Mayo,\nEnrique S. Quintana-Ortl\u00ed, Federico Silla, Jose Duato\n(Universitat Jaume and Universitat Polit\u00e8cnica de Val\u00e8ncia)\nSLURM Migration Experience\nJacqueline Scoggins (Lawrence Berkeley National Lab)\nBudget Checking Plugin for SLURM\nHuub Stoffers (SURF sara)\nFair Tree: Fairshare Algorithm for Slurm\nRyan Cox and Levi Morrison (Brigham Young University)\nIntegrating Layouts Framework in Slurm\nThomas Cadeau and Yiannis Georgiou (Bull),\nMatthieu Hautreux (CEA)\nTopology-Aware Resource Selectiont\nEmmanuel Jeannot, Guillaume Mercier, and Ad\u00e8le Villiermet (Inria)\nSlurm Inter-Cluster Project (presentation),\n(paper)\nStephen Trofinoff (CSCS)\nSlurm Native Workload Management on Cray Systems\nMorris Jette (SchedMD)\nSlurm on Cray Systems\nJason Coverston (Cray)\nSLURM Roadmap\nYiannis Georgiou (Bull), Morris Jette and Jacob Jenson (SchedMD)\nPrivate /tmp for each job using SPANK\nMagnus Jonsson (Ume\u00e5 Universitet)\nICM Warsaw University Site Report\nDominik Bartkiewicz and Marcin Stolarek (ICM Warsaw University)\niVEC Site Report\nAndrew Elwell (iVEC)\nCEA Site Report\nMatthieu Hautreux (CEA)\nSwiss National Supercomputing Centre site report\nMassimo Benini (Swiss National Supercomputing Centre, CSCS)\nAalto University Site Report\nJanne Blomqvist, Ivan Degtyarenko and Mikko Hakala (Aalto University)\nThe George Washington University site report\nTim Wickberg (George Washington University)\nPresentations from Slurm Birds of a Feather, SC13, November 2013\nSlurm Workload Manager Project Report,\nMorris Jette and Danny Auble, SchedMD\nBull's Slurm Roadmapt,\nEric Monchalin, Bull\nNative Slurm on Cray XC30,\nDavid Wallace, Cray\nPresentations from Slurm User Group Meeting, September 2013\nGroup photo\nWelcome: Welcome\nMorris Jette (SchedMD)\nKeynote: Future Outlook for Advanced Computing\nDona Crawford (LLNL)\nTechnical: Overview of Slurm version 2.6,\nMorris Jette and Danny Auble (SchedMD), Yiannis Georgiou (Bull) \nTutorial: Energy Accounting and External Sensor Plugins,\nYiannis Georgiou, Martin Perry, Thomas Cadeau (Bull), Danny Auble (SchedMD)\n\nTechnical: Debugging Large Machines,\nMatthieu Hautreux (CEA)\nTechnical: Creating easy to use HPC portals with NICE EnginFrame and Slurm,\nAlberto Falzone, Paolo Maggi (Nice Software)\n\nTutorial: Usage of new profiling functionalities,\n Rod Schultz, Yiannis Georgiou (Bull) Danny Auble (SchedMD)\n\nTechnical: Fault Tolerant Workload Management,\nDavid Bigagli, Morris Jette (SchedMD)\nTechnical: Slurm Layouts Framework,\nYiannis Georgiou (Bull) Matthieu Hautreux (CEA)\nTechnical: License Management,\nBill Brophy (Bull)\nTechnical: Multi-Cluster Management,\nJuan Pancorbo Armada (IRZ)\n\nTechnical: \nDepth Oblivious Hierarchical Fairshare Priority Factor,\nFrancois Daikhate, Matthieu Hautreux (CEA)\nTechnical: Refactoring ALPS,\nDave Wallace (Cray)\nSite Report: CEA,\nFrancois Diakhate, Francis Belot, Matthieu Hautreux (CEA)\nSite Report: George Washington University,\nTim Wickberg (George Washington University)\nSite Report: Brigham Young University,\nRyan Cox (BYU)\n\nSite Report: Technische Universitat Dresden,\nDr. Ulf Markwardt (Technische Universit\u00e4t Dresden)\nTechnical: Slurm Roadmap,\nMorris Jette, Danny Auble (SchedMD), Yiannis Georgiou (Bull)\nPresentations from Slurm Birds of a Feather, SC12, November 2012\nSlurm Workload Manager Project Report,\nMorris Jette and Danny Auble, SchedMD\nUsing Slurm for Data Aware Scheduling in the Cloud,\nMartijn de Vries, BrightComputing\nSlurm Roadmap,\nEric Monchalin, Bull\nMapReduce Support in Slurm: Releasing the Elephant,\nRalph H. Castain, Wangda Tan, Jimmy Cao and Michael Lv, Greenplum/EMC\nSlurm at Rensselaer,\nTim Wickberg, Rensselaer Polytechnic Institute\nPresentations from Slurm User Group Meeting, October 2012\nGroup photo\nKeynote: The OmSs Programming Model and its links to resource managers,\nJesus Labarta, BSC\nSlurm Status Report,\nMorris Jette and Danny Auble, SchedMD\nSite Report: BSC/RES,\nAlejandro Lucero and Carles Fenoy, BSC\nSite Report: CSCS,\nStephen Trofinoff, CSCS\nSite Report: CEA,\nMatthieu Hautreux, CEA\nSite Report: CETA/CIEMAT,\nAlfonso Pardo Diaz, CIEMAT\nPorting Slurm to Bluegene/Q,\nDon Lipari, LLNL\nTutorial: Slurm Database Use, Accounting and Limits,\nDanny Auble, SchedMD\nTutorial: The Slurm Scheduler Design,\nDon Lipari, LLNL\nTutorial: Cgroup Support on Slurm,\nMartin Perry and Yiannis Georgiou (Bull), Matthieu Hautreux (CEA)\nTutorial: Kerberos and Slurm using Auks,\nMatthieu Hautreux, CEA\n\nKeynote: Challenges in Evaluating Parallel Job Schedulers,\nDror Feitelson, Hebrew University\nIntegration of Slurm with IBM's Parallel Environment,\nMorris Jette and Danny Auble, SchedMD\nSlurm Bank,\nJimmy Tang and Paddy Doyle, Trinity College, Dublin\nUsing Slurm for Data Aware Scheduling in the Cloud,\nMartijn de Vries, Bright Computing\nEnhancing Slurm with Energy Consumption Monitoring and Control Features,\nYiannis Georgiou, Bull\nMapReduce Support in SLURM:\nReleasing the Elephant,\nRalph H. Castain, et. al., Greenplum/EMC\nUsing Slurm via Python,\nMark Roberts (AWE) and Stephan Gorget (EDF)\nHigh Throughput Computing with Slurm,\nMorris Jette and Danny Auble, SchedMD\nEvaluating Scalability and Efficiency of the Resource and Job Management System on large HPC clusters,\nYiannis Georgiou (Bull) and Matthieu Hautreux (CEA)\nInteger Programming Based Herogeneous CPU-GPU Clusters,\nSeren Soner, Bogazici University\nJob Resource Utilization as a Metric for Clusters Comparison and Optimization,\nJoseph Emeras, INRIA/LIG\nPresentations from the Sixth Linux Collaboration Summit, April 2012\nResource Management with Linux Control Groups in HPC Clusters\nYiannis Georgiou, Bull\nPresentations from Slurm Birds Of a Feather, SC11, November 2011\nSlurm Version 2.3 and Beyond\nMorris Jette, SchedMD LLC\nBull's Slurm Roadmap\nEric Monchalin, Bull\nCloud Bursting with Slurm and Bright Cluster Manager\nMartijn de Vries, Bright Computing\nPresentations from Slurm User Group Meeting, September 2011\nGroup photo\nBasic Configuration and Usage,\nRod Schultz, Groupe Bull\nSLURM: Advanced Usage,\nRod Schultz, Groupe Bull\nCPU Management Allocation and Binding,\nMartin Perry, Groupe Bull\nConfiguring Slurm for HA,\nDavid Egolf and Bill Brophy, Groupe Bull\nSlurm Resources isolation through cgroups,\nYiannis Georgiou (Groupe Bull), Matthieu Hautreux (CEA)\nSlurm Operation on Cray XT and XE,\nMoe Jette, SchedMD LLC\n\nChallenges and Opportunities for Exascale\nResource Management and How Today's Petascale Systems are Guiding the Way,\nWilliam Kramer, NCSA\nCEA Site report,\nMatthieu Hautreux, CEA\nLLNL Site Report,\nDon Lipari, LLNL\nSlurm Version 2.3 and Beyond,\nMoe Jette, SchedMD LLC\nSlurm Simulator,\nAlejandro Lucero, BSC\nProposed Design for Enhanced Enterprise-wide Scheduling,\nDon Lipari, LLNL\nBright Cluster Manager & SLURM,\nRobert Stober, Bright Computing\nJob Step Management in User Space,\nMoe Jette, SchedMD LLC\nSlurm Operation IBM BlueGene/Q,\nDanny Auble, SchedMD LLC\nPresentations from Slurm Birds Of a Feather, SC10, November 2010\n\nSlurm Version 2.2: Features and Release Plans,\nMorris Jette, Danny Auble and Donald Lipari, Lawrence Livermore National Laboratory\nPresentations from Slurm User Group Meeting, October 2010\nGroup photo\n\nSlurm: Resource Management from the Simple to the Sophisticated,\nMorris Jette and Danny Auble, Lawrence Livermore National Laboratory\n\nSlurm at CEA,\nMatthieu Hautreux, CEA/DAM/DIF\n\nSlurm Support for Linux Control Groups,\nMartin Perry, Bull Information Systems\n\nSlurm at BSC,\nCarles Fenoy and Alejandro Lucero, Barcelona Supercomputing Center\n\nPorting Slurm to the Cray XT and XE,\nNeil Stringfellow and Gerrit Renker, Swiss National Supercomputer Centre\n\nReal Scale Experimentations of Slurm Resource and Job Management System,\nYiannis Georgiou, Bull Information Systems\n\nSlurm Version 2.2: Features and Release Plans,\nMorris Jette and Danny Auble, Lawrence Livermore National Laboratory\nPresentations from Slurm Birds of a Feather, SC09, November 2009\n\nSlurm Community Meeting,\nMorris Jette, Danny Auble and Don Lipari, Lawrence Livermore National Laboratory\nPresentations from Slurm Birds of a Feather, SC08, November 2008\n\nHigh Scalability Resource Management with SLURM,\nMorris Jette, Lawrence Livermore National Laboratory\n\nSlurm Status Report,\nMorris Jette and Danny Auble, Lawrence Livermore National Laboratory\nOther Presentations\n\nSlurm Version 1.3,\nMorris Jette and Danny Auble, Lawrence Livermore National Laboratory\n(May 2008)\n\nManaging Clusters with Moab and Slurm,\nMorris Jette and Donald Lipari, Lawrence Livermore National Laboratory\n(May 2008)\n\nResource Management at LLNL, Slurm Version 1.2,\nMorris Jette, Danny Auble and Chris Morrone, Lawrence Livermore National Laboratory\n(April 2007)\n\nResource Management Using Slurm,\nMorris Jette, Lawrence Livermore National Laboratory\n(Tutorial, The 7th International Conference on Linux Clusters, May 2006)\nPublications\nEnergy Accounting and Control with Slurm Resource and Job Management System,\nYiannis Georgiou, et. al.\n(ICDCN 2014, January 2014)\n\nEvaluating scalability and efficiency of the Resource and Job Management System on large HPC Clusters,\nYiannis Georgiou (BULL S.A.S, France); Matthieu Hautreux (CEA-DAM, France)\n(16th Workshop on Job Scheduling Strategies for Parallel Processing, May 2012)\nGreenSlot: Scheduling Energy Consumption in Green Datacenters,\nInigo Goiri, et. al.\n(SuperComputing 2011, November 2011)\n\nContributions for Resource and Job Management in High Performance Computing,\nYiannis Georgiou, Universite Joseph Fourier\n(Thesis, December 2010)\n\nCaos NSA and Perceus: All-in-one Cluster Software Stack,\nJeffrey B. Layton,\nLinux Magazine,\n5 February 2009.\nEnhancing an Open Source Resource Manager with Multi-Core/Multi-threaded Support,\nS. M. Balle and D. Palermo,\nJob Scheduling Strategies for Parallel Processing,\n2007.\n\n\nSlurm: Simple Linux Utility for Resource Management [PDF],\nM. Jette and M. Grondona,\nProceedings of ClusterWorld Conference and Expo,\nSan Jose, California, June 2003.\nSlurm: Simple Linux Utility for Resource Management,\nA. Yoo, M. Jette, and M. Grondona,\nJob Scheduling Strategies for Parallel Processing,\nvolume 2862 of Lecture Notes in Computer Science,\npages 44-60,\nSpringer-Verlag, 2003.\nInterview\nRCE 10: Slurm (podcast):\nBrock Palen and Jeff Squyres speak with Morris Jette and\nDanny Auble of LLNL about Slurm.Other Resources\nLearning Chef: Compute Cluter with Slurm\nA Slurm Cookbook by Adam DeConinckLast modified 07 May 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/reservations.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Advanced Resource Reservation Guide",
                "content": "Slurm has the ability to reserve resources for jobs\nbeing executed by select users and/or select bank accounts.\nA resource reservation identifies the resources in that reservation\nand a time period during which the reservation is available.\nThe resources which can be reserved include cores, nodes, licenses and/or\nburst buffers.\nA reservation that contains nodes or cores is associated with one partition,\nand can't span resources over multiple partitions.\nThe only exception from this is when\nthe reservation is created with explicitly requested nodes.\nNote that resource reservations are not compatible with Slurm's\ngang scheduler plugin since the termination time of running jobs\ncannot be accurately predicted.Note that reserved burst buffers and licenses are treated somewhat\ndifferently than reserved cores or nodes.\nWhen cores or nodes are reserved, then jobs using that reservation can use only\nthose resources (this behavior can be change using FLEX flag) and no other jobs can use those resources.\nReserved burst buffers and licenses can only be used by jobs associated with\nthat reservation, but licenses not explicitly reserved are available to any job.\nThis eliminates the need to explicitly put licenses into every advanced\nreservation created.Reservations can be created, updated, or destroyed only by user root\nor the configured SlurmUser using the scontrol command.\nThe scontrol and sview commands can be used\nto view reservations. Additionally, root and the configured SlurmUser\nhave access to all reservations, even if they would normally not have access.\nThe man pages for the various commands contain details.Reservation Creation\n\nOne common mode of operation for a reservation would be to reserve\nan entire computer at a particular time for a system down time.\nThe example below shows the creation of a full-system reservation\nat 16:00 hours on 6 February and lasting for 120 minutes.\nThe \"maint\" flag is used to identify the reservation for accounting\npurposes as system maintenance.\nThe \"ignore_jobs\" flag is used to indicate that we can ignore currently\nrunning jobs when creating this reservation.\nBy default, only resources which are not expected to have a running job\nat the start time can be reserved (the time limit of all running\njobs will have been reached).\nIn this case we can manually cancel the running jobs as needed\nto perform system maintenance.\nAs the reservation time approaches,\nonly jobs that can complete by the reservation time will be initiated.\n$ scontrol create reservation starttime=2009-02-06T16:00:00 \\\n   duration=120 user=root flags=maint,ignore_jobs nodes=ALL\nReservation created: root_3\n\n$ scontrol show reservation\nReservationName=root_3 StartTime=2009-02-06T16:00:00\n   EndTime=2009-02-06T18:00:00 Duration=120\n   Nodes=ALL NodeCnt=20\n   Features=(null) PartitionName=(null)\n   Flags=MAINT,SPEC_NODES,IGNORE_JOBS Licenses=(null)\n   BurstBuffers=(null)\n   Users=root Accounts=(null)\nA variation of this would be to configure licenses to represent system\nresources, such as a global file system.\nThe system resource may not require an actual license for use, but\nSlurm licenses can be used to prevent jobs needing the resource from being\nstarted when that resource is unavailable.\nOne could create a reservation for all of those licenses in order to perform\nmaintenance on that resource.\nIn the example below, we create a reservation for 1000 licenses with the name\nof \"lustre\". \nIf there are a total of 1000 lustre licenses configured in this cluster,\nthis reservation will prevent any job specifying the need for a lustre\nlicense from being scheduled on this cluster during this reservation.\n$ scontrol create reservation starttime=2009-04-06T16:00:00 \\\n   duration=120 user=root flags=license_only \\\n   licenses=lustre:1000\nReservation created: root_4\n\n$ scontrol show reservation\nReservationName=root_4 StartTime=2009-04-06T16:00:00\n   EndTime=2009-04-06T18:00:00 Duration=120\n   Nodes= NodeCnt=0\n   Features=(null) PartitionName=(null)\n   Flags=LICENSE_ONLY Licenses=lustre*1000\n   BurstBuffers=(null)\n   Users=root Accounts=(null)\nAnother mode of operation would be to reserve specific nodes\nfor an indefinite period in order to study problems on those\nnodes. This could also be accomplished using a Slurm partition\nspecifically for this purpose, but that would fail to capture\nthe maintenance nature of their use.\n$ scontrol create reservation user=root starttime=now \\\n   duration=infinite flags=maint nodes=sun000\nReservation created: root_5\n\n$ scontrol show res\nReservationName=root_5 StartTime=2009-02-04T16:22:57\n   EndTime=2009-02-04T16:21:57 Duration=4294967295\n   Nodes=sun000 NodeCnt=1\n   Features=(null) PartitionName=(null)\n   Flags=MAINT,SPEC_NODES Licenses=(null)\n   BurstBuffers=(null)\n   Users=root Accounts=(null)\nOur next example is to reserve ten nodes in the default\nSlurm partition starting at noon and with a duration of 60\nminutes occurring daily. The reservation will be available\nonly to users \"alan\" and \"brenda\".\n$ scontrol create reservation user=alan,brenda \\\n   starttime=noon duration=60 flags=daily nodecnt=10\nReservation created: alan_6\n\n$ scontrol show res\nReservationName=alan_6 StartTime=2009-02-05T12:00:00\n   EndTime=2009-02-05T13:00:00 Duration=60\n   Nodes=sun[000-003,007,010-013,017] NodeCnt=10\n   Features=(null) PartitionName=pdebug\n   Flags=DAILY Licenses=(null) BurstBuffers=(null)\n   Users=alan,brenda Accounts=(null)\nOur next example is to reserve 100GB of burst buffer space\nstarting at noon today and with a duration of 60 minutes.\nThe reservation will be available only to users \"alan\" and \"brenda\".\n$ scontrol create reservation user=alan,brenda \\\n   starttime=noon duration=60 flags=any_nodes burstbuffer=100GB\nReservation created: alan_7\n\n$ scontrol show res\nReservationName=alan_7 StartTime=2009-02-05T12:00:00\n   EndTime=2009-02-05T13:00:00 Duration=60\n   Nodes= NodeCnt=0\n   Features=(null) PartitionName=(null)\n   Flags=ANY_NODES Licenses=(null) BurstBuffer=100GB\n   Users=alan,brenda Accounts=(null)\nNote that specific nodes to be associated with the reservation are\nidentified immediately after creation of the reservation. This permits\nusers to stage files to the nodes in preparation for use during the\nreservation. Note that the reservation creation request can also\nidentify the partition from which to select the nodes or _one_\nfeature that every selected node must contain.On a smaller system, one might want to reserve cores rather than\nwhole nodes.\nThis capability permits the administrator to identify the core count to be\nreserved on each node as shown in the examples below.\nNOTE: Core reservations are not available when the system is configured\nto use the select/linear plugin.\n# Create a two core reservation for user alan\n$ scontrol create reservation StartTime=now Duration=60 \\\n  NodeCnt=1 CoreCnt=2 User=alan\n\n# Create a reservation for user brenda with two cores on\n# node tux8 and 4 cores on node tux9\n$ scontrol create reservation StartTime=now Duration=60 \\\n  Nodes=tux8,tux9 CoreCnt=2,4 User=brenda\nReservations can not only be created for the use of specific accounts and\nusers, but specific accounts and/or users can be prevented from using them.\nIn the following example, a reservation is created for account \"foo\", but user\n\"alan\" is prevented from using the reservation even when using the account\n\"foo\".\n$ scontrol create reservation account=foo \\\n   user=-alan partition=pdebug \\\n   starttime=noon duration=60 nodecnt=2k,2k\nReservation created: alan_9\n\n$ scontrol show res\nReservationName=alan_9 StartTime=2011-12-05T13:00:00\n   EndTime=2011-12-05T14:00:00 Duration=60\n   Nodes=bgp[000x011,210x311] NodeCnt=4096\n   Features=(null) PartitionName=pdebug\n   Flags= Licenses=(null) BurstBuffers=(null)\n   Users=-alan Accounts=foo\nWhen creating a reservation, you can request that Slurm include all the\nnodes in a partition by specifying the PartitionName option.\nIf you only want a certain number of nodes or CPUs from that partition\nyou can combine PartitionName with the CoreCnt, NodeCnt\nor TRES options to specify how many of a resource you want.\nIn the following example, a reservation is created in the 'gpu' partition\nthat uses the TRES option to limit the reservation to 24 processors,\ndivided among 4 nodes.\n$ scontrol create reservationname=test start=now duration=1 \\\n   user=user1 partition=gpu tres=cpu=24,node=4\nReservation created: test\n\n$ scontrol show res\nReservationName=test StartTime=2020-08-28T11:07:09\n   EndTime=2020-08-28T11:08:09 Duration=00:01:00\n   Nodes=node[01-04] NodeCnt=4 CoreCnt=24\n   Features=(null) PartitionName=gpu\n     NodeName=node01 CoreIDs=0-5\n     NodeName=node02 CoreIDs=0-5\n     NodeName=node03 CoreIDs=0-5\n     NodeName=node04 CoreIDs=0-5\n   TRES=cpu=24\n   Users=user1 Accounts=(null) Licenses=(null)\n   State=ACTIVE BurstBuffer=(null)\n   MaxStartDelay=(null)\nReservation UseThe reservation create response includes the reservation's name.\nThis name is automatically generated by Slurm based upon the first\nuser or account name and a numeric suffix. In order to use the\nreservation, the job submit request must explicitly specify that\nreservation name. The job must be contained completely within the\nnamed reservation. The job will be canceled after the reservation\nreaches its EndTime. If letting the job continue execution after\nthe reservation EndTime, a configuration option ResvOverRun\nin slurm.conf can be set to control how long the job can continue execution.\n$ sbatch --reservation=alan_6 -N4 my.script\nsbatch: Submitted batch job 65540\nNote that use of a reservation does not alter a job's priority, but it\ndoes act as an enhancement to the job's priority.\nAny job with a reservation is considered for scheduling to resources \nbefore any other job in the same Slurm partition (queue) not associated\nwith a reservation.Reservation Modification\n\nReservations can be modified by user root as desired.\nFor example their duration could be altered or the users\ngranted access changed as shown below:\n$ scontrol update ReservationName=root_3 \\\n   duration=150 users=admin\nReservation updated.\n\nbash-3.00$ scontrol show ReservationName=root_3\nReservationName=root_3 StartTime=2009-02-06T16:00:00\n   EndTime=2009-02-06T18:30:00 Duration=150\n   Nodes=ALL NodeCnt=20 Features=(null)\n   PartitionName=(null) Flags=MAINT,SPEC_NODES\n   Licenses=(null) BurstBuffers=(null)\n   Users=admin Accounts=(null)\nReservation Deletion\n\nReservations are automatically purged after their end time.\nThey may also be manually deleted as shown below.\nNote that a reservation can not be deleted while there are\njobs running in it.\n$ scontrol delete ReservationName=alan_6\n\nNOTE: By default, when a reservation ends the reservation request will be\nremoved from any pending jobs submitted to the reservation and will be put into\na held state.  Use the NO_HOLD_JOBS_AFTER_END reservation flag to let jobs run\noutside of the reservation after the reservation is gone.\nOverlapping Reservations\n\nBy default, reservations must not overlap. They must either include\ndifferent nodes or operate at different times. If specific nodes\nare not specified when a reservation is created, Slurm will\nautomatically select nodes to avoid overlap and ensure that\nthe selected nodes are available when the reservation begins.There is very limited support for overlapping reservations\nwith two specific modes of operation available.\nFor ease of system maintenance, you can create a reservation\nwith the \"maint\" flag that overlaps existing reservations.\nThis permits an administrator to easily create a maintenance\nreservation for an entire cluster without needing to remove\nor reschedule pre-existing reservations. Users requesting access\nto one of these pre-existing reservations will be prevented from\nusing resources that are also in this maintenance reservation.\nFor example, users alan and brenda might have a reservation for\nsome nodes daily from noon until 1PM. If there is a maintenance\nreservation for all nodes starting at 12:30PM, the only jobs they\nmay start in their reservation would have to be completed by 12:30PM,\nwhen the maintenance reservation begins.The second exception operates in the same manner as a maintenance\nreservation except that it is not logged in the accounting system as nodes\nreserved for maintenance.\nIt requires the use of the \"overlap\" flag when creating the second\nreservation.\nThis might be used to ensure availability of resources for a specific\nuser within a group having a reservation.\nUsing the previous example of alan and brenda having a 10 node reservation\nfor 60 minutes, we might want to reserve 4 nodes of that for brenda\nduring the first 30 minutes of the time period.\nIn this case, the creation of one overlapping reservation (for a total of\ntwo reservations) may be simpler than creating three separate reservations,\npartly since the use of any reservation requires the job specification\nof the reservation name.\n\nA six node reservation for both alan and brenda that lasts the full\n60 minutes\nA four node reservation for brenda for the first 30 minutes\nA four node reservation for both alan and brenda that lasts for the\nfinal 30 minutes\nIf the \"maint\" or \"overlap\" flag is used when creating reservations,\none could create a reservation within a reservation within a third\nreservation.\nNote a reservation having a \"maint\" or \"overlap\" flag will not have\nresources removed from it by a subsequent reservation also having a\n\"maint\" or \"overlap\" flag, so nesting of reservations only works to a\ndepth of two.Reservations Floating Through Time\n\nSlurm can be used to create an advanced reservation with a start time that\nremains a fixed period of time in the future.\nThese reservation are not intended to run jobs, but to prevent long running\njobs from being initiated on specific nodes.\nThat node might be placed in a DRAINING state to prevent any new jobs\nfrom being started there.\nAlternately, an advanced reservation might be placed on the node to prevent\njobs exceeding some specific time limit from being started.\nAttempts by users to make use of a reservation with a floating start time will\nbe rejected.\nWhen ready to perform the maintenance, place the node in DRAINING state and\ndelete the previously created advanced reservation.Create the reservation by using the flag value of TIME_FLOAT and a\nstart time that is relative to the current time (use the keyword now).\nThe reservation duration should generally be a value which is large relative\nto typical job run times in order to not adversely impact backfill scheduling\ndecisions.\nAlternately the reservation can have a specific end time, in which case the\nreservation's start time will increase through time until the reservation's\nend time is reached.\nWhen the current time passes the reservation end time then the reservation will\nbe purged.\nIn the example below, node tux8 is prevented from starting any jobs exceeding\na 60 minute time limit.\nThe duration of this reservation is 100 (minutes).\n$ scontrol create reservation user=operator nodes=tux8 \\\n  starttime=now+60minutes duration=100 flags=time_float\nReservations that Replace Allocated Resources\n\nBy default, nodes in a reservation that are DOWN or DRAINED will be replaced,\nbut not nodes that are allocated to jobs. This behavior may be explicitly\nrequested with the REPLACE_DOWN flag.However, you may instruct Slurm to also replace nodes which are allocated to\njobs with new idle nodes. This is done using the REPLACE flag as shown in\nthe example below.\nThe effect of this is to always maintain a constant size pool of resources.\nThis option is not supported for reservations specifying cores which\nspan more than one node, rather than full nodes. (E.g. a 1 core reservation on\nnode \"tux1\" will be moved if node \"tux1\" goes down, but a reservation\ncontaining 2 cores on node \"tux1\" and 3 cores on \"tux2\" will not be moved if\n\"tux1\" goes down.)\n$ scontrol create reservation starttime=now duration=60 \\\n  users=foo nodecnt=2 flags=replace\nReservation created: foo_82\n\n$ scontrol show res\nReservationName=foo_82 StartTime=2014-11-20T16:21:11\n   EndTime=2014-11-20T17:21:11 Duration=01:00:00\n   Nodes=tux[0-1] NodeCnt=2 CoreCnt=12 Features=(null)\n   PartitionName=debug Flags=REPLACE\n   Users=jette Accounts=(null) Licenses=(null) State=ACTIVE\n\n$ sbatch -n4 --reservation=foo_82 tmp\nSubmitted batch job 97\n\n$ scontrol show res\nReservationName=foo_82 StartTime=2014-11-20T16:21:11\n   EndTime=2014-11-20T17:21:11 Duration=01:00:00\n   Nodes=tux[1-2] NodeCnt=2 CoreCnt=12 Features=(null)\n   PartitionName=debug Flags=REPLACE\n   Users=jette Accounts=(null) Licenses=(null) State=ACTIVE\n\n$ sbatch -n4 --reservation=foo_82 tmp\nSubmitted batch job 98\n\n$ scontrol show res\nReservationName=foo_82 StartTime=2014-11-20T16:21:11\n   EndTime=2014-11-20T17:21:11 Duration=01:00:00\n   Nodes=tux[2-3] NodeCnt=2 CoreCnt=12 Features=(null)\n   PartitionName=debug Flags=REPLACE\n   Users=jette Accounts=(null) Licenses=(null) State=ACTIVE\n\n$ squeue\nJOBID PARTITION  NAME  USER ST  TIME  NODES NODELIST(REASON)\n   97     debug   tmp   foo  R  0:09      1 tux0\n   98     debug   tmp   foo  R  0:07      1 tux1\nFLEX ReservationsBy default, jobs that run in reservations must fit within the time and\nsize constraints of the reserved resources. With the FLEX flag jobs\nare able to start before the reservation begins or continue after it ends.\nThey are also able to use the reserved node(s) along with additional nodes if\nrequired and available.\n\nThe default behavior for jobs that request a reservation is that they must\nbe able to run within the confines (time and space) of that reservation.\nThe following example shows that the FLEX flag allows the job to run\nbefore the reservation starts, after it ends, and on a node outside\nof the reservation.\n\n$ scontrol create reservation user=user1 nodes=node01 starttime=now+10minutes duration=10 flags=flex\nReservation created: user1_831\n\n$ sbatch -wnode0[1-2] -t30:00 --reservation=user1_831 test.job\nSubmitted batch job 57996\n\n$ squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n             57996     debug sleepjob    user1  R       0:08      2 node[01-02]\n\nMagnetic Reservations\n\n\nThe default behavior for reservations is that jobs must request a\nreservation in order to run in it. The MAGNETIC flag allows you to\ncreate a reservation that will allow jobs to run in it without requiring that\nthey specify the name of the reservation. The reservation will only \"attract\"\njobs that meet the access control requirements.\nNOTE: Magnetic reservations cannot \"attract\" heterogeneous jobs -\nheterogeneous jobs will only run in magnetic reservations if they explicitly\nrequest the reservation.\nThe following example shows a reservation created on node05. The user\nspecified as being able to access the reservation then submits a job and\nthe job starts on the reserved node.\n\n$ scontrol create reservation user=user1 nodes=node05 starttime=now duration=10 flags=magnetic\nReservation created: user1_850\n\n$ scontrol show res\nReservationName=user1_850 StartTime=2020-07-29T13:44:13 EndTime=2020-07-29T13:54:13 Duration=00:10:00\n   Nodes=node05 NodeCnt=1 CoreCnt=12 Features=(null) PartitionName=(null) Flags=SPEC_NODES,MAGNETIC\n   TRES=cpu=12\n   Users=user1 Accounts=(null) Licenses=(null) State=ACTIVE BurstBuffer=(null)\n   MaxStartDelay=(null)\n\n$ sbatch -N1 -t5:00 test.job\nSubmitted batch job 62297\n\n$ squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n             62297     debug sleepjob    user1  R       0:04      1 node05\n\nReservation Purging After Last Job\n\n\nA reservation may be automatically purged after the last associated job\ncompletes. This is accomplished by using a \"purge_comp\" flag.\nOnce the reservation has been created, it must be populated within 5 minutes\nof its start time or it will be purged before any jobs have been run.\nReservation Accounting\n\n\nJobs executed within a reservation are accounted for using the appropriate\nuser and bank account. If resources within a reservation are not used, those\nresources will be accounted for as being used by all users or bank accounts\nassociated with the reservation on an equal basis (e.g. if two users are\neligible to use a reservation and neither does, each user will be reported\nto have used half of the reserved resources).\nProlog and Epilog\n\n\nSlurm supports both a reservation prolog and epilog.\nThey may be configured using the ResvProlog and ResvEpilog\nconfiguration parameters in the slurm.conf file.\nThese scripts can be used to cancel jobs, modify partition configuration,\netc.\nFuture Work\nReservations made within a partition having gang scheduling assumes\nthe highest level rather than the actual level of time-slicing when\nconsidering the initiation of jobs.\nThis will prevent the initiation of some jobs which would complete execution\nbefore a reservation given fewer jobs to time-slice with.\nLast modified 02 August 2024\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Magnetic Reservations\n\n",
                "content": "The default behavior for reservations is that jobs must request a\nreservation in order to run in it. The MAGNETIC flag allows you to\ncreate a reservation that will allow jobs to run in it without requiring that\nthey specify the name of the reservation. The reservation will only \"attract\"\njobs that meet the access control requirements.NOTE: Magnetic reservations cannot \"attract\" heterogeneous jobs -\nheterogeneous jobs will only run in magnetic reservations if they explicitly\nrequest the reservation.The following example shows a reservation created on node05. The user\nspecified as being able to access the reservation then submits a job and\nthe job starts on the reserved node.\n$ scontrol create reservation user=user1 nodes=node05 starttime=now duration=10 flags=magnetic\nReservation created: user1_850\n\n$ scontrol show res\nReservationName=user1_850 StartTime=2020-07-29T13:44:13 EndTime=2020-07-29T13:54:13 Duration=00:10:00\n   Nodes=node05 NodeCnt=1 CoreCnt=12 Features=(null) PartitionName=(null) Flags=SPEC_NODES,MAGNETIC\n   TRES=cpu=12\n   Users=user1 Accounts=(null) Licenses=(null) State=ACTIVE BurstBuffer=(null)\n   MaxStartDelay=(null)\n\n$ sbatch -N1 -t5:00 test.job\nSubmitted batch job 62297\n\n$ squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n             62297     debug sleepjob    user1  R       0:04      1 node05\nReservation Purging After Last Job\n\nA reservation may be automatically purged after the last associated job\ncompletes. This is accomplished by using a \"purge_comp\" flag.\nOnce the reservation has been created, it must be populated within 5 minutes\nof its start time or it will be purged before any jobs have been run.Reservation Accounting\n\nJobs executed within a reservation are accounted for using the appropriate\nuser and bank account. If resources within a reservation are not used, those\nresources will be accounted for as being used by all users or bank accounts\nassociated with the reservation on an equal basis (e.g. if two users are\neligible to use a reservation and neither does, each user will be reported\nto have used half of the reserved resources).Prolog and Epilog\n\nSlurm supports both a reservation prolog and epilog.\nThey may be configured using the ResvProlog and ResvEpilog\nconfiguration parameters in the slurm.conf file.\nThese scripts can be used to cancel jobs, modify partition configuration,\netc.Future WorkReservations made within a partition having gang scheduling assumes\nthe highest level rather than the actual level of time-slicing when\nconsidering the initiation of jobs.\nThis will prevent the initiation of some jobs which would complete execution\nbefore a reservation given fewer jobs to time-slice with.Last modified 02 August 2024"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/configurator.easy.html",
        "sections": [
            {
                "title": "Slurm Version 24.05 Configuration Tool - Easy Version",
                "content": "This form can be used to create a Slurm configuration file with\nyou controlling many of the important configuration parameters.This is a simplified version of the Slurm configuration tool. This version\nhas fewer  options for creating a Slurm configuration file. The full version\nof the Slurm configuration tool is available at\nconfigurator.html.This tool supports Slurm version 24.05 only.\nConfiguration files for other versions of Slurm should be built\nusing the tool distributed with it in doc/html/configurator.html.\nSome parameters will be set to default values, but you can\nmanually edit the resulting slurm.conf as desired\nfor greater flexibility. See man slurm.conf for more\ndetails about the configuration parameters.Note the while Slurm daemons create log files and other files as needed,\nit treats the lack of parent directories as a fatal error.\nThis prevents the daemons from running if critical file systems are\nnot mounted and will minimize the risk of cold-starting (starting\nwithout preserving jobs).Note that this configuration file must be installed on all nodes\nin your cluster.After you have filled in the fields of interest, use the\n\"Submit\" button on the bottom of the page to build the slurm.conf\nfile. It will appear on your web browser. Save the file in text format\nas slurm.conf for use by Slurm.\n\nFor more information about Slurm, see\nhttps://slurm.schedmd.com/slurm.html\nCluster Name\n ClusterName:\nThe name of your cluster. Using different names for each of your clusters is\nimportant when using a single database to record information from multiple\nSlurm-managed clusters.\n\nControl Machines\nDefine the hostname of the computer on which the Slurm controller and\noptional backup controller will execute.\nHostname values should should not be the fully qualified domain\nname (e.g. use tux rather than tux.abc.com).\n\n SlurmctldHost:\nPrimary Controller Hostname\n\nCompute Machines\nDefine the machines on which user applications can run.\nYou can also specify addresses of these computers if desired\n(defaults to their hostnames).\nOnly a few of the possible parameters associated with the nodes will\nbe set by this tool, but many others are available.\nExecuting the command slurmd -C on each compute node will print its\nphysical configuration (sockets, cores, real memory size, etc.), which\ncan be used in constructing the slurm.conf file.\nAll of the nodes will be placed into a single partition (or queue)\nwith global access. Many options are available to group nodes into\npartitions with a wide variety of configuration parameters.\nManually edit the slurm.conf produced to exercise these options.\nNode names and addresses may be specified using a numeric range specification.\n\n\n NodeName:\nCompute nodes\n\n NodeAddr: Compute node addresses\n(optional)\n\n PartitionName:\nName of the one partition to be created\n\n MaxTime:\nMaximum time limit of jobs in minutes or INFINITE\n\nThe following parameters describe a node's configuration.\nSet a value for CPUs.\nThe other parameters are optional, but provide more control over scheduled resources:\n\n CPUs: Count of processors\non each compute node.\nIf CPUs is omitted, it will be inferred from:\nSockets, CoresPerSocket, and ThreadsPerCore.\n\n\nSockets:\nNumber of physical processor sockets/chips on the node.\nIf Sockets is omitted, it will be inferred from:\nCPUs, CoresPerSocket, and ThreadsPerCore.\n\n\nCoresPerSocket:\nNumber of cores in a single physical processor socket.\nThe CoresPerSocket value describes physical cores, not\nthe logical number of processors per socket.\n\n\nThreadsPerCore:\nNumber of logical threads in a single physical core.\n\n RealMemory: Amount\nof real memory. This parameter is required when specifying Memory as a\nconsumable resource with the select/cons_tres plug-in. See below\nunder Resource Selection.\n\nSlurm User\nThe Slurm controller (slurmctld) can run without elevated privileges,\nso it is recommended that a user \"slurm\" be created for it. For testing\npurposes any user name can be used.\n\n SlurmUser\n\nState Preservation\nDefine the location of a directory where the slurmctld daemon saves its state.\nThis should be a fully qualified pathname which can be read and written to\nby the Slurm user on both the control machine and backup controller (if configured).\nThe location of a directory where slurmd saves state should also be defined.\nThis must be a unique directory on each compute server (local disk).\nThe use of a highly reliable file system (e.g. RAID) is recommended.\n\n\nStateSaveLocation: Slurmctld state save directory\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\nhandling required (InfiniBand, Myrinet, Ethernet, etc.)\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
            },
            {
                "title": "Cluster Name",
                "content": "ClusterNameControl Machinestuxtux.abc.com\n SlurmctldHost:\nPrimary Controller Hostname\n\nCompute Machines\nDefine the machines on which user applications can run.\nYou can also specify addresses of these computers if desired\n(defaults to their hostnames).\nOnly a few of the possible parameters associated with the nodes will\nbe set by this tool, but many others are available.\nExecuting the command slurmd -C on each compute node will print its\nphysical configuration (sockets, cores, real memory size, etc.), which\ncan be used in constructing the slurm.conf file.\nAll of the nodes will be placed into a single partition (or queue)\nwith global access. Many options are available to group nodes into\npartitions with a wide variety of configuration parameters.\nManually edit the slurm.conf produced to exercise these options.\nNode names and addresses may be specified using a numeric range specification.\n\n\n NodeName:\nCompute nodes\n\n NodeAddr: Compute node addresses\n(optional)\n\n PartitionName:\nName of the one partition to be created\n\n MaxTime:\nMaximum time limit of jobs in minutes or INFINITE\n\nThe following parameters describe a node's configuration.\nSet a value for CPUs.\nThe other parameters are optional, but provide more control over scheduled resources:\n\n CPUs: Count of processors\non each compute node.\nIf CPUs is omitted, it will be inferred from:\nSockets, CoresPerSocket, and ThreadsPerCore.\n\n\nSockets:\nNumber of physical processor sockets/chips on the node.\nIf Sockets is omitted, it will be inferred from:\nCPUs, CoresPerSocket, and ThreadsPerCore.\n\n\nCoresPerSocket:\nNumber of cores in a single physical processor socket.\nThe CoresPerSocket value describes physical cores, not\nthe logical number of processors per socket.\n\n\nThreadsPerCore:\nNumber of logical threads in a single physical core.\n\n RealMemory: Amount\nof real memory. This parameter is required when specifying Memory as a\nconsumable resource with the select/cons_tres plug-in. See below\nunder Resource Selection.\n\nSlurm User\nThe Slurm controller (slurmctld) can run without elevated privileges,\nso it is recommended that a user \"slurm\" be created for it. For testing\npurposes any user name can be used.\n\n SlurmUser\n\nState Preservation\nDefine the location of a directory where the slurmctld daemon saves its state.\nThis should be a fully qualified pathname which can be read and written to\nby the Slurm user on both the control machine and backup controller (if configured).\nThe location of a directory where slurmd saves state should also be defined.\nThis must be a unique directory on each compute server (local disk).\nThe use of a highly reliable file system (e.g. RAID) is recommended.\n\n\nStateSaveLocation: Slurmctld state save directory\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\nhandling required (InfiniBand, Myrinet, Ethernet, etc.)\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
            },
            {
                "title": "Compute Machines",
                "content": "slurmd -Cslurm.confslurm.conf\n NodeName:\nCompute nodes\n\n NodeAddr: Compute node addresses\n(optional)\n\n PartitionName:\nName of the one partition to be created\n\n MaxTime:\nMaximum time limit of jobs in minutes or INFINITE\n\nThe following parameters describe a node's configuration.\nSet a value for CPUs.\nThe other parameters are optional, but provide more control over scheduled resources:\n\n CPUs: Count of processors\non each compute node.\nIf CPUs is omitted, it will be inferred from:\nSockets, CoresPerSocket, and ThreadsPerCore.\n\n\nSockets:\nNumber of physical processor sockets/chips on the node.\nIf Sockets is omitted, it will be inferred from:\nCPUs, CoresPerSocket, and ThreadsPerCore.\n\n\nCoresPerSocket:\nNumber of cores in a single physical processor socket.\nThe CoresPerSocket value describes physical cores, not\nthe logical number of processors per socket.\n\n\nThreadsPerCore:\nNumber of logical threads in a single physical core.\n\n RealMemory: Amount\nof real memory. This parameter is required when specifying Memory as a\nconsumable resource with the select/cons_tres plug-in. See below\nunder Resource Selection.\n\nSlurm User\nThe Slurm controller (slurmctld) can run without elevated privileges,\nso it is recommended that a user \"slurm\" be created for it. For testing\npurposes any user name can be used.\n\n SlurmUser\n\nState Preservation\nDefine the location of a directory where the slurmctld daemon saves its state.\nThis should be a fully qualified pathname which can be read and written to\nby the Slurm user on both the control machine and backup controller (if configured).\nThe location of a directory where slurmd saves state should also be defined.\nThis must be a unique directory on each compute server (local disk).\nThe use of a highly reliable file system (e.g. RAID) is recommended.\n\n\nStateSaveLocation: Slurmctld state save directory\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\nhandling required (InfiniBand, Myrinet, Ethernet, etc.)\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
            },
            {
                "title": "Slurm User",
                "content": "\n SlurmUser\n\nState Preservation\nDefine the location of a directory where the slurmctld daemon saves its state.\nThis should be a fully qualified pathname which can be read and written to\nby the Slurm user on both the control machine and backup controller (if configured).\nThe location of a directory where slurmd saves state should also be defined.\nThis must be a unique directory on each compute server (local disk).\nThe use of a highly reliable file system (e.g. RAID) is recommended.\n\n\nStateSaveLocation: Slurmctld state save directory\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\nhandling required (InfiniBand, Myrinet, Ethernet, etc.)\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
            },
            {
                "title": "State Preservation",
                "content": "\n\nStateSaveLocation: Slurmctld state save directory\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\nhandling required (InfiniBand, Myrinet, Ethernet, etc.)\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
            },
            {
                "title": "Scheduling",
                "content": "SchedulerTypeBackfillBuiltin\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\nhandling required (InfiniBand, Myrinet, Ethernet, etc.)\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
            },
            {
                "title": "Interconnect",
                "content": "SwitchTypeHPE\n  SlingshotNone\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
            },
            {
                "title": "Default MPI Type",
                "content": "MpiDefaultMPI-PMI2MPI-PMIxNone\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
            },
            {
                "title": "Process Tracking",
                "content": "ProctrackTypeCgroupcgroupcgroup.confLinuxProcPgid\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
            },
            {
                "title": "Resource Selection",
                "content": "SelectTypecons_tres\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
            },
            {
                "title": "Task Launch",
                "content": "TaskPluginNoneAffinityCgroup\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
            },
            {
                "title": "Event Logging",
                "content": "\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
            },
            {
                "title": "Job Accounting Gather",
                "content": "JobAcctGatherTypeNonecgroupLinuxJob Accounting StorageAccountingStorageTypeNoneSlurmDBD\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
            },
            {
                "title": "Process ID Logging",
                "content": "\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/gres.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Generic Resource (GRES) Scheduling",
                "content": "Contents\nOverview\nConfiguration\nRunning Jobs\nAutoDetect\nGPU Management\nMPS Management\nMIG Management\nSharding\nOverviewSlurm supports the ability to define and schedule arbitrary Generic RESources\n(GRES). Additional built-in features are enabled for specific GRES types,\nincluding Graphics Processing Units (GPUs), CUDA Multi-Process Service (MPS)\ndevices, and Sharding through an extensible plugin mechanism.Configuration\n\nBy default, no GRES are enabled in the cluster's configuration.\nYou must explicitly specify which GRES are to be managed in the\nslurm.conf configuration file. The configuration parameters of\ninterest are GresTypes and Gres.\n\nFor more details, see GresTypes and Gres in the slurm.conf man page.\nNote that the GRES specification for each node works in the same fashion\nas the other resources managed. Nodes which are found to have fewer resources\nthan configured will be placed in a DRAIN state.Snippet from an example slurm.conf file:\n# Configure four GPUs (with MPS), plus bandwidth\nGresTypes=gpu,mps,bandwidth\nNodeName=tux[0-7] Gres=gpu:tesla:2,gpu:kepler:2,mps:400,bandwidth:lustre:no_consume:4G\nEach compute node with generic resources typically contain a gres.conf\nfile describing which resources are available on the node, their count,\nassociated device files and cores which should be used with those resources.There are cases where you may want to define a Generic Resource on a node\nwithout specifying a quantity of that GRES. For example, the filesystem type\nof a node doesn't decrease in value as jobs run on that node.\nYou can use the no_consume flag to allow users to request a GRES\nwithout having a defined count that gets used as it is requested.\nTo view available gres.conf configuration parameters, see the\ngres.conf man page.Running Jobs\n\nJobs will not be allocated any generic resources unless specifically\nrequested at job submit time using the options:\n--gres\nGeneric resources required per node\n--gpus\nGPUs required per job\n--gpus-per-node\nGPUs required per node. Equivalent to the --gres option for GPUs.\n--gpus-per-socket\nGPUs required per socket. Requires the job to specify a task socket.\n--gpus-per-task\nGPUs required per task. Requires the job to specify a task count.\nAll of these options are supported by the salloc, sbatch and\nsrun commands.\nNote that all of the --gpu* options are only supported by Slurm's\nselect/cons_tres plugin.\nJobs requesting these options when the select/cons_tres plugin is not\nconfigured will be rejected.\nThe --gres option requires an argument specifying which generic resources\nare required and how many resources using the form name[:type:count]\nwhile all of the --gpu* options require an argument of the form\n [type]:count.\nThe name is the same name as\nspecified by the GresTypes and Gres configuration parameters.\ntype identifies a specific type of that generic resource (e.g. a\nspecific model of GPU).\ncount specifies how many resources are required and has a default\nvalue of 1. For example:\nsbatch --gres=gpu:kepler:2 ....Requests for typed vs non-typed generic resources must be consistent\nwithin a job. For example, if you request --gres=gpu:2 with\nsbatch, you would not be able to request --gres=gpu:tesla:2\nwith srun to create a job step. The same holds true in reverse,\nif you request a typed GPU to create a job allocation, you should request\na GPU of the same type to create a job step.Several additional resource requirement specifications are available\nspecifically for GPUs and detailed descriptions about these options are\navailable in the man pages for the job submission commands.\nAs for the --gpu* option, these options are only supported by Slurm's\nselect/cons_tres plugin.\n--cpus-per-gpu\nCount of CPUs allocated per GPU.\n--gpu-bind\nDefine how tasks are bound to GPUs.\n--gpu-freq\nSpecify GPU frequency and/or GPU memory frequency.\n--mem-per-gpu\nMemory allocated per GPU.\nJobs will be allocated specific generic resources as needed to satisfy\nthe request. If the job is suspended, those resources do not become available\nfor use by other jobs.Job steps can be allocated generic resources from those allocated to the\njob using the --gres option with the srun command as described\nabove. By default, a job step will be allocated all of the generic resources\nthat have been requested by the job, except those implicitly requested when a\njob is exclusive. If desired, the job step may explicitly specify a\ndifferent generic resource count than the job.\nThis design choice was based upon a scenario where each job executes many\njob steps. If job steps were granted access to all generic resources by\ndefault, some job steps would need to explicitly specify zero generic resource\ncounts, which we considered more confusing. The job step can be allocated\nspecific generic resources and those resources will not be available to other\njob steps. A simple example is shown below.\n#!/bin/bash\n#\n# gres_test.bash\n# Submit as follows:\n# sbatch --gres=gpu:4 -n4 -N1-1 gres_test.bash\n#\nsrun --gres=gpu:2 -n2 --exclusive show_device.sh &\nsrun --gres=gpu:1 -n1 --exclusive show_device.sh &\nsrun --gres=gpu:1 -n1 --exclusive show_device.sh &\nwait\nAutoDetect\n\nIf AutoDetect=nvml, AutoDetect=rsmi, AutoDetect=nrt,\nor AutoDetect=oneapi are set in gres.conf, configuration details\nwill automatically be filled in for any system-detected GPU. This removes the\nneed to explicitly configure GPUs in gres.conf, though the Gres= line in\nslurm.conf is still required in order to tell slurmctld how many GRES to expect.\nNote that AutoDetect=nvml, AutoDetect=rsmi, and\nAutoDetect=oneapi need their corresponding GPU management libraries\ninstalled on the node and found during Slurm configuration in order to work.\nBy default, all system-detected devices are added to the node.\nHowever, if Type and File in gres.conf match a GPU on\nthe system, any other properties explicitly specified (e.g.\nCores or Links) can be double-checked against it.\nIf the system-detected GPU differs from its matching GPU configuration, then the\nGPU is omitted from the node with an error.\nThis allows gres.conf to serve as an optional sanity check and notifies\nadministrators of any unexpected changes in GPU properties.\nIf not all system-detected devices are specified by the slurm.conf\nconfiguration, then the relevant slurmd will be drained. However, it is still\npossible to use a subset of the devices found on the system if they are\nspecified manually (with AutoDetect disabled) in gres.conf.\nExample gres.conf file:\n# Configure four GPUs (with MPS), plus bandwidth\nAutoDetect=nvml\nName=gpu Type=gp100  File=/dev/nvidia0 Cores=0,1\nName=gpu Type=gp100  File=/dev/nvidia1 Cores=0,1\nName=gpu Type=p6000  File=/dev/nvidia2 Cores=2,3\nName=gpu Type=p6000  File=/dev/nvidia3 Cores=2,3\nName=mps Count=200  File=/dev/nvidia0\nName=mps Count=200  File=/dev/nvidia1\nName=mps Count=100  File=/dev/nvidia2\nName=mps Count=100  File=/dev/nvidia3\nName=bandwidth Type=lustre Count=4G Flags=CountOnly\n In this example, since AutoDetect=nvml is specified, Cores\nfor each GPU will be checked against a corresponding GPU found on the system\nmatching the Type and File specified.\nSince Links is not specified, it will be automatically filled in\naccording to what is found on the system.\nIf a matching system GPU is not found, no validation takes place and the GPU is\nassumed to be as the configuration says.\nFor Type to match a system-detected device, it must either exactly\nmatch or be a substring of the GPU name reported by slurmd via the AutoDetect\nmechanism. This GPU name will have all spaces replaced with underscores. To see\nthe GPU name, set SlurmdDebug=debug2 in your slurm.conf and either\nrestart or reconfigure slurmd and check the slurmd log. For example,\nwith AutoDetect=nvml:\ndebug:  gpu/nvml: init: init: GPU NVML plugin loaded\ndebug2: gpu/nvml: _nvml_init: Successfully initialized NVML\ndebug:  gpu/nvml: _get_system_gpu_list_nvml: Systems Graphics Driver Version: 450.36.06\ndebug:  gpu/nvml: _get_system_gpu_list_nvml: NVML Library Version: 11.450.36.06\ndebug2: gpu/nvml: _get_system_gpu_list_nvml: Total CPU count: 6\ndebug2: gpu/nvml: _get_system_gpu_list_nvml: Device count: 1\ndebug2: gpu/nvml: _get_system_gpu_list_nvml: GPU index 0:\ndebug2: gpu/nvml: _get_system_gpu_list_nvml:     Name: geforce_rtx_2060\ndebug2: gpu/nvml: _get_system_gpu_list_nvml:     UUID: GPU-g44ef22a-d954-c552-b5c4-7371354534b2\ndebug2: gpu/nvml: _get_system_gpu_list_nvml:     PCI Domain/Bus/Device: 0:1:0\ndebug2: gpu/nvml: _get_system_gpu_list_nvml:     PCI Bus ID: 00000000:01:00.0\ndebug2: gpu/nvml: _get_system_gpu_list_nvml:     NVLinks: -1\ndebug2: gpu/nvml: _get_system_gpu_list_nvml:     Device File (minor number): /dev/nvidia0\ndebug2: gpu/nvml: _get_system_gpu_list_nvml:     CPU Affinity Range - Machine: 0-5\ndebug2: gpu/nvml: _get_system_gpu_list_nvml:     Core Affinity Range - Abstract: 0-5\nIn this example, the GPU's name is reported as\ngeforce_rtx_2060. So in your slurm.conf and\ngres.conf, the GPU Type can be set to \ngeforce, rtx, \n2060, geforce_rtx_2060, or any other\nsubstring, and slurmd should be able to match it to the system-detected\ndevice geforce_rtx_2060.\n\nGPU Management\n\n\nIn the case of Slurm's GRES plugin for GPUs, the environment variable\nCUDA_VISIBLE_DEVICES\nis set for each job step to determine which GPUs are\navailable for its use on each node. This environment variable is only set\nwhen tasks are launched on a specific compute node (no global environment\nvariable is set for the salloc command and the environment variable set\nfor the sbatch command only reflects the GPUs allocated to that job\non that node, node zero of the allocation).\nCUDA version 3.1 (or higher) uses this environment\nvariable in order to run multiple jobs or job steps on a node with GPUs\nand ensure that the resources assigned to each are unique. In the example\nabove, the allocated node may have four or more graphics devices. In that\ncase, CUDA_VISIBLE_DEVICES\nwill reference unique devices for each file and\nthe output might resemble this:\n\nJobStep=1234.0 CUDA_VISIBLE_DEVICES=0,1\nJobStep=1234.1 CUDA_VISIBLE_DEVICES=2\nJobStep=1234.2 CUDA_VISIBLE_DEVICES=3\n\nNOTE: Be sure to specify the File parameters in the\ngres.conf file and ensure they are in the increasing numeric order.\nThe CUDA_VISIBLE_DEVICES\nenvironment variable will also be set in the job's Prolog and Epilog programs.\nNote that the environment variable set for the job may differ from that set for\nthe Prolog and Epilog if Slurm is configured to constrain the device files\nvisible to a job using Linux cgroup.\nThis is because the Prolog and Epilog programs run outside of any Linux\ncgroup while the job runs inside of the cgroup and may thus have a\ndifferent set of visible devices.\nFor example, if a job is allocated the device \"/dev/nvidia1\", then\nCUDA_VISIBLE_DEVICES will be set to a value of\n\"1\" in the Prolog and Epilog while the job's value of\nCUDA_VISIBLE_DEVICES will be set to a\nvalue of \"0\" (i.e. the first GPU device visible to the job).\nFor more information see the\nProlog and Epilog Guide.\nWhen possible, Slurm automatically determines the GPUs on the system using\nNVML. NVML (which powers the\nnvidia-smi tool) numbers GPUs in order by their\nPCI bus IDs. For this numbering to match the numbering reported by CUDA, the\nCUDA_DEVICE_ORDER environmental variable must\nbe set to CUDA_DEVICE_ORDER=PCI_BUS_ID.\nGPU device files (e.g. /dev/nvidia1) are\nbased on the Linux minor number assignment, while NVML's device numbers are\nassigned via PCI bus ID, from lowest to highest. Mapping between these two is\nnondeterministic and system dependent, and could vary between boots after\nhardware or OS changes. For the most part, this assignment seems fairly stable.\nHowever, an after-bootup check is required to guarantee that a GPU device is\nassigned to a specific device file.\nPlease consult the\n\nNVIDIA CUDA documentation for more information about the\nCUDA_VISIBLE_DEVICES and\nCUDA_DEVICE_ORDER environmental variables.\nMPS Management\n\n\n\nCUDA Multi-Process Service (MPS) provides a mechanism where GPUs can be\nshared by multiple jobs, where each job is allocated some percentage of the\nGPU's resources.\nThe total count of MPS resources available on a node should be configured in\nthe slurm.conf file (e.g. \"NodeName=tux[1-16] Gres=gpu:2,mps:200\").\nSeveral options are available for configuring MPS in the gres.conf file\nas listed below with examples following that:\n\nNo MPS configuration: The count of gres/mps elements defined in the\nslurm.conf will be evenly distributed across all GPUs configured on the\nnode. For example, \"NodeName=tux[1-16] Gres=gpu:2,mps:200\" will configure\na count of 100 gres/mps resources on each of the two GPUs.\nMPS configuration includes only the Name and Count parameters:\nThe count of gres/mps elements will be evenly distributed across all GPUs\nconfigured on the node. This is similar to case 1, but places duplicate\nconfiguration in the gres.conf file.\nMPS configuration includes the Name, File and Count\nparameters: Each File parameter should identify the device file path of a\nGPU and the Count should identify the number of gres/mps resources\navailable for that specific GPU device.\nThis may be useful in a heterogeneous environment.\nFor example, some GPUs on a node may be more powerful than others and thus be\nassociated with a higher gres/mps count.\nAnother use case would be to prevent some GPUs from being used for MPS (i.e.\nthey would have an MPS count of zero).\n\nNote that Type and Cores parameters for gres/mps are ignored.\nThat information is copied from the gres/gpu configuration.\nNote the Count parameter is translated to a percentage, so the value\nwould typically be a multiple of 100.\nNote that if NVIDIA's NVML library is installed, the GPU configuration\n(i.e. Type, File, Cores and Links data) will be\nautomatically gathered from the library and need not be recorded in the\ngres.conf file.\nBy default, job requests for MPS are required to fit on a single gpu on\neach node. This can be overridden with a flag in the slurm.conf\nconfiguration file. See \nOPT_MULTIPLE_SHARING_GRES_PJ.\nNote the same GPU can be allocated either as a GPU type of GRES or as\nan MPS type of GRES, but not both.\nIn other words, once a GPU has been allocated as a gres/gpu resource it will\nnot be available as a gres/mps.\nLikewise, once a GPU has been allocated as a gres/mps resource it will\nnot be available as a gres/gpu.\nHowever the same GPU can be allocated as MPS generic resources to multiple jobs\nbelonging to multiple users, so long as the total count of MPS allocated to\njobs does not exceed the configured count.\nAlso, since shared GRES (MPS) cannot be allocated at the same time as a sharing\nGRES (GPU) this option only allocates all sharing GRES and no underlying shared\nGRES.\nSome example configurations for Slurm's gres.conf file are shown below.\n\n# Example 1 of gres.conf\n# Configure four GPUs (with MPS)\nAutoDetect=nvml\nName=gpu Type=gp100 File=/dev/nvidia0 Cores=0,1\nName=gpu Type=gp100 File=/dev/nvidia1 Cores=0,1\nName=gpu Type=p6000 File=/dev/nvidia2 Cores=2,3\nName=gpu Type=p6000 File=/dev/nvidia3 Cores=2,3\n# Set gres/mps Count value to 100 on each of the 4 available GPUs\nName=mps Count=400\n\n\n\n# Example 2 of gres.conf\n# Configure four different GPU types (with MPS)\nAutoDetect=nvml\nName=gpu Type=gtx1080 File=/dev/nvidia0 Cores=0,1\nName=gpu Type=gtx1070 File=/dev/nvidia1 Cores=0,1\nName=gpu Type=gtx1060 File=/dev/nvidia2 Cores=2,3\nName=gpu Type=gtx1050 File=/dev/nvidia3 Cores=2,3\nName=mps Count=1300   File=/dev/nvidia0\nName=mps Count=1200   File=/dev/nvidia1\nName=mps Count=1100   File=/dev/nvidia2\nName=mps Count=1000   File=/dev/nvidia3\n\nNOTE: gres/mps requires the use of the select/cons_tres\nplugin.\nJob requests for MPS will be processed the same as any other GRES except\nthat the request must be satisfied using only one GPU per node and only one\nGPU per node may be configured for use with MPS.\nFor example, a job request for \"--gres=mps:50\" will not be satisfied by using\n20 percent of one GPU and 30 percent of a second GPU on a single node.\nMultiple jobs from different users can use MPS on a node at the same time.\nNote that GRES types of GPU and MPS can not be requested within\na single job.\nAlso jobs requesting MPS resources can not specify a GPU frequency.\nA prolog program should be used to start and stop MPS servers as needed.\nA sample prolog script to do this is included with the Slurm distribution in\nthe location etc/prolog.example.\nIts mode of operation is if a job is allocated gres/mps resources then the\nProlog will have the CUDA_VISIBLE_DEVICES,\nCUDA_MPS_ACTIVE_THREAD_PERCENTAGE, and\nSLURM_JOB_UID environment variables set.\nThe Prolog should then make sure that an MPS server is started for that GPU\nand user (UID == User ID).\nIt also records the GPU device ID in a local file.\nIf a job is allocated gres/gpu resources then the Prolog will have the\nCUDA_VISIBLE_DEVICES and\nSLURM_JOB_UID environment variables set\n(no CUDA_MPS_ACTIVE_THREAD_PERCENTAGE).\nThe Prolog should then terminate any MPS server associated with that GPU.\nIt may be necessary to modify this script as needed for the local environment.\nFor more information see the\nProlog and Epilog Guide.\nJobs requesting MPS resources will have the\nCUDA_VISIBLE_DEVICES\nand CUDA_DEVICE_ORDER environment variables set.\nThe device ID is relative to those resources under MPS server control and will\nalways have a value of zero in the current implementation (only one GPU will be\nusable in MPS mode per node).\nThe job will also have the\nCUDA_MPS_ACTIVE_THREAD_PERCENTAGE\nenvironment variable set to that job's percentage of MPS resources available on\nthe assigned GPU.\nThe percentage will be calculated based upon the portion of the configured\nCount on the Gres is allocated to a job of step.\nFor example, a job requesting \"--gres=mps:200\" and using\nconfiguration example 2 above would be\nallocated\n15% of the gtx1080 (File=/dev/nvidia0, 200 x 100 / 1300 = 15), or\n16% of the gtx1070 (File=/dev/nvidia0, 200 x 100 / 1200 = 16), or\n18% of the gtx1060 (File=/dev/nvidia0, 200 x 100 / 1100 = 18), or\n20% of the gtx1050 (File=/dev/nvidia0, 200 x 100 / 1000 = 20).\nAn alternate mode of operation would be to permit jobs to be allocated whole\nGPUs then trigger the starting of an MPS server based upon comments in the job.\nFor example, if a job is allocated whole GPUs then search for a comment of\n\"mps-per-gpu\" or \"mps-per-node\" in the job (using the \"scontrol show job\"\ncommand) and use that as a basis for starting one MPS daemon per GPU or across\nall GPUs respectively.\nPlease consult the\n\nNVIDIA Multi-Process Service documentation for more information about MPS.\n\nNote that a vulnerability exists in previous versions of the NVIDIA driver that\nmay affect users when sharing GPUs. More information can be found in\n\nCVE-2018-6260 and in the\n\nSecurity Bulletin: NVIDIA GPU Display Driver - February 2019.\nNVIDIA MPS has a built-in limitation regarding GPU sharing among different\nusers. Only one user on a system may have an active MPS server, and the MPS\ncontrol daemon will queue MPS server activation requests from separate users,\nleading to serialized exclusive access of the GPU between users (see\n\nSection 2.3.1.1 - Limitations in the MPS docs). So different users cannot\ntruly run concurrently on the GPU with MPS; rather, the GPU will be time-sliced\nbetween the users (for a diagram depicting this process, see\n\nSection 3.3 - Provisioning Sequence in the MPS docs).\nMIG Management\n\n\nBeginning in version 21.08, Slurm now supports NVIDIA\nMulti-Instance GPU (MIG) devices. This feature allows some newer NVIDIA\nGPUs (like the A100) to split up a GPU into up to seven separate, isolated GPU\ninstances. Slurm can treat these MIG instances as individual GPUs, complete with\ncgroup isolation and task binding.\nTo configure MIGs in Slurm, specify\nAutoDetect=nvml in gres.conf for the\nnodes with MIGs, and specify Gres\nin slurm.conf as if the MIGs were regular GPUs, like this:\nNodeName=tux[1-16] gres=gpu:2. An optional\nGRES type can be specified to distinguish MIGs of different sizes from each\nother, as well as from other GPUs in the cluster. This type must be a substring\nof the \"MIG Profile\" string as reported by the node in its slurmd log under the\ndebug2 log level. Here is an example slurm.conf\nfor a system with 2 gpus, one of which is partitioned into 2 MIGs where the\n\"MIG Profile\" is nvidia_a100_3g.20gb:\n\nAccountingStorageTRES=gres/gpu,gres/gpu:a100,gres/gpu:a100_3g.20gb\nGresTypes=gpu\nNodeName=tux[1-16] gres=gpu:a100:1,gpu:a100_3g.20gb:2\n\nThe MultipleFiles parameter\nallows you to specify multiple device files for the GPU card.\nThe sanity-check AutoDetect mode is not supported for MIGs.\nSlurm expects MIG devices to already be partitioned, and does not support\ndynamic MIG partitioning.\nFor more information on NVIDIA MIGs (including how to partition them), see\n\nthe MIG user guide.\nSharding\n\n\n\nSharding provides a generic mechanism where GPUs can be\nshared by multiple jobs. While it does permit multiple jobs to run on a given\nGPU it does not fence the processes running on the GPU, it only allows the GPU\nto be shared. Sharding, therefore, works best with homogeneous workflows. It is\nrecommended to limit the number of shards on a node to equal the max possible\njobs that can run simultaneously on the node (i.e. cores).\nThe total count of shards available on a node should be configured in\nthe slurm.conf file (e.g. \"NodeName=tux[1-16] Gres=gpu:2,shard:64\").\nSeveral options are available for configuring shards in the gres.conf file\nas listed below with examples following that:\n\nNo Shard configuration: The count of gres/shard elements defined in the\nslurm.conf will be evenly distributed across all GPUs configured on the\nnode. For example, \"NodeName=tux[1-16] Gres=gpu:2,shard:64\" will configure\na count of 32 gres/shard resources on each of the two GPUs.\nShard configuration includes only the Name and Count parameters:\nThe count of gres/shard elements will be evenly distributed across all GPUs\nconfigured on the node. This is similar to case 1, but places duplicate\nconfiguration in the gres.conf file.\nShard configuration includes the Name, File and Count\nparameters: Each File parameter should identify the device file path of a\nGPU and the Count should identify the number of gres/shard resources\navailable for that specific GPU device.\nThis may be useful in a heterogeneous environment.\nFor example, some GPUs on a node may be more powerful than others and thus be\nassociated with a higher gres/shard count.\nAnother use case would be to prevent some GPUs from being used for sharding (i.e.\nthey would have a shard count of zero).\n\nNote that Type and Cores parameters for gres/shard are ignored.\nThat information is copied from the gres/gpu configuration.\nNote that if NVIDIA's NVML library is installed, the GPU configuration\n(i.e. Type, File, Cores and Links data) will be\nautomatically gathered from the library and need not be recorded in the\ngres.conf file.\nNote the same GPU can be allocated either as a GPU type of GRES or as\na shard type of GRES, but not both.\nIn other words, once a GPU has been allocated as a gres/gpu resource it will\nnot be available as a gres/shard.\nLikewise, once a GPU has been allocated as a gres/shard resource it will\nnot be available as a gres/gpu.\nHowever the same GPU can be allocated as shard generic resources to multiple jobs\nbelonging to multiple users, so long as the total count of SHARD allocated to\njobs does not exceed the configured count.\nBy default, job requests for shards are required to fit on a single gpu on\neach node. This can be overridden with a flag in the slurm.conf\nconfiguration file. See \nOPT_MULTIPLE_SHARING_GRES_PJ.\nIn order for this to be correctly configured, the appropriate nodes need\nto have the shard keyword added as a GRES for the relevant nodes as\nwell as being added to the GresTypes parameter. If you want the shards\nto be tracked in accounting then shard also needs to be added to\nAccountingStorageTRES.\nSee the relevant settings in an example slurm.conf:\n\nAccountingStorageTRES=gres/gpu,gres/shard\nGresTypes=gpu,shard\nNodeName=tux[1-16] Gres=gpu:2,shard:64\n\nSome example configurations for Slurm's gres.conf file are shown below.\n\n# Example 1 of gres.conf\n# Configure four GPUs (with Sharding)\nAutoDetect=nvml\nName=gpu Type=gp100 File=/dev/nvidia0 Cores=0,1\nName=gpu Type=gp100 File=/dev/nvidia1 Cores=0,1\nName=gpu Type=p6000 File=/dev/nvidia2 Cores=2,3\nName=gpu Type=p6000 File=/dev/nvidia3 Cores=2,3\n# Set gres/shard Count value to 8 on each of the 4 available GPUs\nName=shard Count=32\n\n\n\n# Example 2 of gres.conf\n# Configure four different GPU types (with Sharding)\nAutoDetect=nvml\nName=gpu Type=gtx1080 File=/dev/nvidia0 Cores=0,1\nName=gpu Type=gtx1070 File=/dev/nvidia1 Cores=0,1\nName=gpu Type=gtx1060 File=/dev/nvidia2 Cores=2,3\nName=gpu Type=gtx1050 File=/dev/nvidia3 Cores=2,3\nName=shard Count=8    File=/dev/nvidia0\nName=shard Count=8    File=/dev/nvidia1\nName=shard Count=8    File=/dev/nvidia2\nName=shard Count=8    File=/dev/nvidia3\n\nNOTE: gres/shard requires the use of the select/cons_tres\nplugin.\nJob requests for shards can not specify a GPU frequency.\nJobs requesting shards resources will have the\nCUDA_VISIBLE_DEVICES, ROCR_VISIBLE_DEVICES,\nor GPU_DEVICE_ORDINAL environment variable set\nwhich would be the same as if it were a GPU.\n\nSteps with shards haveSLURM_SHARDS_ON_NODE\nset indicating the number of shards allocated.\nLast modified 16 February 2024\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "GPU Management\n\n",
                "content": "In the case of Slurm's GRES plugin for GPUs, the environment variable\nCUDA_VISIBLE_DEVICES\nis set for each job step to determine which GPUs are\navailable for its use on each node. This environment variable is only set\nwhen tasks are launched on a specific compute node (no global environment\nvariable is set for the salloc command and the environment variable set\nfor the sbatch command only reflects the GPUs allocated to that job\non that node, node zero of the allocation).\nCUDA version 3.1 (or higher) uses this environment\nvariable in order to run multiple jobs or job steps on a node with GPUs\nand ensure that the resources assigned to each are unique. In the example\nabove, the allocated node may have four or more graphics devices. In that\ncase, CUDA_VISIBLE_DEVICES\nwill reference unique devices for each file and\nthe output might resemble this:\nJobStep=1234.0 CUDA_VISIBLE_DEVICES=0,1\nJobStep=1234.1 CUDA_VISIBLE_DEVICES=2\nJobStep=1234.2 CUDA_VISIBLE_DEVICES=3\nNOTE: Be sure to specify the File parameters in the\ngres.conf file and ensure they are in the increasing numeric order.The CUDA_VISIBLE_DEVICES\nenvironment variable will also be set in the job's Prolog and Epilog programs.\nNote that the environment variable set for the job may differ from that set for\nthe Prolog and Epilog if Slurm is configured to constrain the device files\nvisible to a job using Linux cgroup.\nThis is because the Prolog and Epilog programs run outside of any Linux\ncgroup while the job runs inside of the cgroup and may thus have a\ndifferent set of visible devices.\nFor example, if a job is allocated the device \"/dev/nvidia1\", then\nCUDA_VISIBLE_DEVICES will be set to a value of\n\"1\" in the Prolog and Epilog while the job's value of\nCUDA_VISIBLE_DEVICES will be set to a\nvalue of \"0\" (i.e. the first GPU device visible to the job).\nFor more information see the\nProlog and Epilog Guide.When possible, Slurm automatically determines the GPUs on the system using\nNVML. NVML (which powers the\nnvidia-smi tool) numbers GPUs in order by their\nPCI bus IDs. For this numbering to match the numbering reported by CUDA, the\nCUDA_DEVICE_ORDER environmental variable must\nbe set to CUDA_DEVICE_ORDER=PCI_BUS_ID.GPU device files (e.g. /dev/nvidia1) are\nbased on the Linux minor number assignment, while NVML's device numbers are\nassigned via PCI bus ID, from lowest to highest. Mapping between these two is\nnondeterministic and system dependent, and could vary between boots after\nhardware or OS changes. For the most part, this assignment seems fairly stable.\nHowever, an after-bootup check is required to guarantee that a GPU device is\nassigned to a specific device file.Please consult the\n\nNVIDIA CUDA documentation for more information about the\nCUDA_VISIBLE_DEVICES and\nCUDA_DEVICE_ORDER environmental variables.MPS Management\n\n\nCUDA Multi-Process Service (MPS) provides a mechanism where GPUs can be\nshared by multiple jobs, where each job is allocated some percentage of the\nGPU's resources.\nThe total count of MPS resources available on a node should be configured in\nthe slurm.conf file (e.g. \"NodeName=tux[1-16] Gres=gpu:2,mps:200\").\nSeveral options are available for configuring MPS in the gres.conf file\nas listed below with examples following that:\nNo MPS configuration: The count of gres/mps elements defined in the\nslurm.conf will be evenly distributed across all GPUs configured on the\nnode. For example, \"NodeName=tux[1-16] Gres=gpu:2,mps:200\" will configure\na count of 100 gres/mps resources on each of the two GPUs.\nMPS configuration includes only the Name and Count parameters:\nThe count of gres/mps elements will be evenly distributed across all GPUs\nconfigured on the node. This is similar to case 1, but places duplicate\nconfiguration in the gres.conf file.\nMPS configuration includes the Name, File and Count\nparameters: Each File parameter should identify the device file path of a\nGPU and the Count should identify the number of gres/mps resources\navailable for that specific GPU device.\nThis may be useful in a heterogeneous environment.\nFor example, some GPUs on a node may be more powerful than others and thus be\nassociated with a higher gres/mps count.\nAnother use case would be to prevent some GPUs from being used for MPS (i.e.\nthey would have an MPS count of zero).\nNote that Type and Cores parameters for gres/mps are ignored.\nThat information is copied from the gres/gpu configuration.Note the Count parameter is translated to a percentage, so the value\nwould typically be a multiple of 100.Note that if NVIDIA's NVML library is installed, the GPU configuration\n(i.e. Type, File, Cores and Links data) will be\nautomatically gathered from the library and need not be recorded in the\ngres.conf file.By default, job requests for MPS are required to fit on a single gpu on\neach node. This can be overridden with a flag in the slurm.conf\nconfiguration file. See \nOPT_MULTIPLE_SHARING_GRES_PJ.Note the same GPU can be allocated either as a GPU type of GRES or as\nan MPS type of GRES, but not both.\nIn other words, once a GPU has been allocated as a gres/gpu resource it will\nnot be available as a gres/mps.\nLikewise, once a GPU has been allocated as a gres/mps resource it will\nnot be available as a gres/gpu.\nHowever the same GPU can be allocated as MPS generic resources to multiple jobs\nbelonging to multiple users, so long as the total count of MPS allocated to\njobs does not exceed the configured count.\nAlso, since shared GRES (MPS) cannot be allocated at the same time as a sharing\nGRES (GPU) this option only allocates all sharing GRES and no underlying shared\nGRES.\nSome example configurations for Slurm's gres.conf file are shown below.\n# Example 1 of gres.conf\n# Configure four GPUs (with MPS)\nAutoDetect=nvml\nName=gpu Type=gp100 File=/dev/nvidia0 Cores=0,1\nName=gpu Type=gp100 File=/dev/nvidia1 Cores=0,1\nName=gpu Type=p6000 File=/dev/nvidia2 Cores=2,3\nName=gpu Type=p6000 File=/dev/nvidia3 Cores=2,3\n# Set gres/mps Count value to 100 on each of the 4 available GPUs\nName=mps Count=400\n\n# Example 2 of gres.conf\n# Configure four different GPU types (with MPS)\nAutoDetect=nvml\nName=gpu Type=gtx1080 File=/dev/nvidia0 Cores=0,1\nName=gpu Type=gtx1070 File=/dev/nvidia1 Cores=0,1\nName=gpu Type=gtx1060 File=/dev/nvidia2 Cores=2,3\nName=gpu Type=gtx1050 File=/dev/nvidia3 Cores=2,3\nName=mps Count=1300   File=/dev/nvidia0\nName=mps Count=1200   File=/dev/nvidia1\nName=mps Count=1100   File=/dev/nvidia2\nName=mps Count=1000   File=/dev/nvidia3\nNOTE: gres/mps requires the use of the select/cons_tres\nplugin.Job requests for MPS will be processed the same as any other GRES except\nthat the request must be satisfied using only one GPU per node and only one\nGPU per node may be configured for use with MPS.\nFor example, a job request for \"--gres=mps:50\" will not be satisfied by using\n20 percent of one GPU and 30 percent of a second GPU on a single node.\nMultiple jobs from different users can use MPS on a node at the same time.\nNote that GRES types of GPU and MPS can not be requested within\na single job.\nAlso jobs requesting MPS resources can not specify a GPU frequency.A prolog program should be used to start and stop MPS servers as needed.\nA sample prolog script to do this is included with the Slurm distribution in\nthe location etc/prolog.example.\nIts mode of operation is if a job is allocated gres/mps resources then the\nProlog will have the CUDA_VISIBLE_DEVICES,\nCUDA_MPS_ACTIVE_THREAD_PERCENTAGE, and\nSLURM_JOB_UID environment variables set.\nThe Prolog should then make sure that an MPS server is started for that GPU\nand user (UID == User ID).\nIt also records the GPU device ID in a local file.\nIf a job is allocated gres/gpu resources then the Prolog will have the\nCUDA_VISIBLE_DEVICES and\nSLURM_JOB_UID environment variables set\n(no CUDA_MPS_ACTIVE_THREAD_PERCENTAGE).\nThe Prolog should then terminate any MPS server associated with that GPU.\nIt may be necessary to modify this script as needed for the local environment.\nFor more information see the\nProlog and Epilog Guide.Jobs requesting MPS resources will have the\nCUDA_VISIBLE_DEVICES\nand CUDA_DEVICE_ORDER environment variables set.\nThe device ID is relative to those resources under MPS server control and will\nalways have a value of zero in the current implementation (only one GPU will be\nusable in MPS mode per node).\nThe job will also have the\nCUDA_MPS_ACTIVE_THREAD_PERCENTAGE\nenvironment variable set to that job's percentage of MPS resources available on\nthe assigned GPU.\nThe percentage will be calculated based upon the portion of the configured\nCount on the Gres is allocated to a job of step.\nFor example, a job requesting \"--gres=mps:200\" and using\nconfiguration example 2 above would be\nallocated\n15% of the gtx1080 (File=/dev/nvidia0, 200 x 100 / 1300 = 15), or\n16% of the gtx1070 (File=/dev/nvidia0, 200 x 100 / 1200 = 16), or\n18% of the gtx1060 (File=/dev/nvidia0, 200 x 100 / 1100 = 18), or\n20% of the gtx1050 (File=/dev/nvidia0, 200 x 100 / 1000 = 20).An alternate mode of operation would be to permit jobs to be allocated whole\nGPUs then trigger the starting of an MPS server based upon comments in the job.\nFor example, if a job is allocated whole GPUs then search for a comment of\n\"mps-per-gpu\" or \"mps-per-node\" in the job (using the \"scontrol show job\"\ncommand) and use that as a basis for starting one MPS daemon per GPU or across\nall GPUs respectively.Please consult the\n\nNVIDIA Multi-Process Service documentation for more information about MPS.\nNote that a vulnerability exists in previous versions of the NVIDIA driver that\nmay affect users when sharing GPUs. More information can be found in\n\nCVE-2018-6260 and in the\n\nSecurity Bulletin: NVIDIA GPU Display Driver - February 2019.NVIDIA MPS has a built-in limitation regarding GPU sharing among different\nusers. Only one user on a system may have an active MPS server, and the MPS\ncontrol daemon will queue MPS server activation requests from separate users,\nleading to serialized exclusive access of the GPU between users (see\n\nSection 2.3.1.1 - Limitations in the MPS docs). So different users cannot\ntruly run concurrently on the GPU with MPS; rather, the GPU will be time-sliced\nbetween the users (for a diagram depicting this process, see\n\nSection 3.3 - Provisioning Sequence in the MPS docs).MIG Management\n\nBeginning in version 21.08, Slurm now supports NVIDIA\nMulti-Instance GPU (MIG) devices. This feature allows some newer NVIDIA\nGPUs (like the A100) to split up a GPU into up to seven separate, isolated GPU\ninstances. Slurm can treat these MIG instances as individual GPUs, complete with\ncgroup isolation and task binding.To configure MIGs in Slurm, specify\nAutoDetect=nvml in gres.conf for the\nnodes with MIGs, and specify Gres\nin slurm.conf as if the MIGs were regular GPUs, like this:\nNodeName=tux[1-16] gres=gpu:2. An optional\nGRES type can be specified to distinguish MIGs of different sizes from each\nother, as well as from other GPUs in the cluster. This type must be a substring\nof the \"MIG Profile\" string as reported by the node in its slurmd log under the\ndebug2 log level. Here is an example slurm.conf\nfor a system with 2 gpus, one of which is partitioned into 2 MIGs where the\n\"MIG Profile\" is nvidia_a100_3g.20gb:\nAccountingStorageTRES=gres/gpu,gres/gpu:a100,gres/gpu:a100_3g.20gb\nGresTypes=gpu\nNodeName=tux[1-16] gres=gpu:a100:1,gpu:a100_3g.20gb:2\nThe MultipleFiles parameter\nallows you to specify multiple device files for the GPU card.The sanity-check AutoDetect mode is not supported for MIGs.\nSlurm expects MIG devices to already be partitioned, and does not support\ndynamic MIG partitioning.For more information on NVIDIA MIGs (including how to partition them), see\n\nthe MIG user guide.Sharding\n\n\nSharding provides a generic mechanism where GPUs can be\nshared by multiple jobs. While it does permit multiple jobs to run on a given\nGPU it does not fence the processes running on the GPU, it only allows the GPU\nto be shared. Sharding, therefore, works best with homogeneous workflows. It is\nrecommended to limit the number of shards on a node to equal the max possible\njobs that can run simultaneously on the node (i.e. cores).\nThe total count of shards available on a node should be configured in\nthe slurm.conf file (e.g. \"NodeName=tux[1-16] Gres=gpu:2,shard:64\").\nSeveral options are available for configuring shards in the gres.conf file\nas listed below with examples following that:\nNo Shard configuration: The count of gres/shard elements defined in the\nslurm.conf will be evenly distributed across all GPUs configured on the\nnode. For example, \"NodeName=tux[1-16] Gres=gpu:2,shard:64\" will configure\na count of 32 gres/shard resources on each of the two GPUs.\nShard configuration includes only the Name and Count parameters:\nThe count of gres/shard elements will be evenly distributed across all GPUs\nconfigured on the node. This is similar to case 1, but places duplicate\nconfiguration in the gres.conf file.\nShard configuration includes the Name, File and Count\nparameters: Each File parameter should identify the device file path of a\nGPU and the Count should identify the number of gres/shard resources\navailable for that specific GPU device.\nThis may be useful in a heterogeneous environment.\nFor example, some GPUs on a node may be more powerful than others and thus be\nassociated with a higher gres/shard count.\nAnother use case would be to prevent some GPUs from being used for sharding (i.e.\nthey would have a shard count of zero).\nNote that Type and Cores parameters for gres/shard are ignored.\nThat information is copied from the gres/gpu configuration.Note that if NVIDIA's NVML library is installed, the GPU configuration\n(i.e. Type, File, Cores and Links data) will be\nautomatically gathered from the library and need not be recorded in the\ngres.conf file.Note the same GPU can be allocated either as a GPU type of GRES or as\na shard type of GRES, but not both.\nIn other words, once a GPU has been allocated as a gres/gpu resource it will\nnot be available as a gres/shard.\nLikewise, once a GPU has been allocated as a gres/shard resource it will\nnot be available as a gres/gpu.\nHowever the same GPU can be allocated as shard generic resources to multiple jobs\nbelonging to multiple users, so long as the total count of SHARD allocated to\njobs does not exceed the configured count.By default, job requests for shards are required to fit on a single gpu on\neach node. This can be overridden with a flag in the slurm.conf\nconfiguration file. See \nOPT_MULTIPLE_SHARING_GRES_PJ.In order for this to be correctly configured, the appropriate nodes need\nto have the shard keyword added as a GRES for the relevant nodes as\nwell as being added to the GresTypes parameter. If you want the shards\nto be tracked in accounting then shard also needs to be added to\nAccountingStorageTRES.\nSee the relevant settings in an example slurm.conf:\n\nAccountingStorageTRES=gres/gpu,gres/shard\nGresTypes=gpu,shard\nNodeName=tux[1-16] Gres=gpu:2,shard:64\n\nSome example configurations for Slurm's gres.conf file are shown below.\n\n# Example 1 of gres.conf\n# Configure four GPUs (with Sharding)\nAutoDetect=nvml\nName=gpu Type=gp100 File=/dev/nvidia0 Cores=0,1\nName=gpu Type=gp100 File=/dev/nvidia1 Cores=0,1\nName=gpu Type=p6000 File=/dev/nvidia2 Cores=2,3\nName=gpu Type=p6000 File=/dev/nvidia3 Cores=2,3\n# Set gres/shard Count value to 8 on each of the 4 available GPUs\nName=shard Count=32\n\n\n\n# Example 2 of gres.conf\n# Configure four different GPU types (with Sharding)\nAutoDetect=nvml\nName=gpu Type=gtx1080 File=/dev/nvidia0 Cores=0,1\nName=gpu Type=gtx1070 File=/dev/nvidia1 Cores=0,1\nName=gpu Type=gtx1060 File=/dev/nvidia2 Cores=2,3\nName=gpu Type=gtx1050 File=/dev/nvidia3 Cores=2,3\nName=shard Count=8    File=/dev/nvidia0\nName=shard Count=8    File=/dev/nvidia1\nName=shard Count=8    File=/dev/nvidia2\nName=shard Count=8    File=/dev/nvidia3\n\nNOTE: gres/shard requires the use of the select/cons_tres\nplugin.\nJob requests for shards can not specify a GPU frequency.\nJobs requesting shards resources will have the\nCUDA_VISIBLE_DEVICES, ROCR_VISIBLE_DEVICES,\nor GPU_DEVICE_ORDINAL environment variable set\nwhich would be the same as if it were a GPU.\n\nSteps with shards haveSLURM_SHARDS_ON_NODE\nset indicating the number of shards allocated.\nLast modified 16 February 2024\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/selinux.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "SELinux",
                "content": "Starting with version 21.08, Slurm includes support for setting an SELinux\ncontext for jobs as a technology preview. The implementation may change in\nfuture releases, and support for it is not enabled by default.Architecture\n\nWhen enabled, the Slurm job submission commands \u2014 salloc, sbatch, and\nsrun \u2014 will automatically set a field with the current operating context.\nThis field can be overwritten by the --context\ncommand line option.It is important to note that this value can be directly manipulated by the\nend-user, and it is up to site-specific scripts to validate and control access\nto these contexts. At this time MUNGE, which Slurm users to security identify\nusers and hosts on the cluster, does not provide an SELinux context field, and\nas such there is no secure mechanism to send the current context to the Slurm\ncontroller. Thus the context, as provided at job submission time, must\nbe validated by a job_submit plugin running within the slurmctld.Without such a script, no context is set or managed for a user's job.Installation\n\nSource:SELinux support is disabled by default and must be enabled at configure time.\nIt requires the libselinux1 library and development headers to build.configure --enable-selinuxSetupOnce a version of Slurm that supports SELinux is installed, you will need to\nenable and create a job_submit plugin that will perform verification of the\nSELinux context, before passing it along to the slurmctld. At this time, there\nis not a reliable and secure way to get/verify contexts internally so you MUST\ncreate this script and perform verification in the job_submit plugin.Example:\nfunction slurm_job_submit(job_desc, part_list, submit_uid)\n  if job_desc.req_context then\n    local element = 0\n    for str in string.gmatch(job_desc.req_context, \"([^:]+)\") do\n      if element == 0 and str ~= \"unconfined_u\" then\n        slurm.log_user(\"Error: invalid SELinux context\")\n        return slurm.ERROR\n      elseif element == 1 and str ~= \"unconfined_r\" then\n        slurm.log_user(\"Error: %s is not a valid SELinux role\")\n        return slurm.ERROR\n      end\n      element = element + 1\n    end\n    job_desc.selinux_context = job_desc.req_context\n  else\n    -- Force a specific context if one wasn't requested\n    job_desc.selinux_context = unconfined_u:unconfined_r:slurm_t:s0\n  end\n  return slurm.SUCCESS\nend\nNote that job_desc.selinux_context is set based on the contents of\njob_desc.req_context if they are considered valid.\njob_desc.selinux_context is what set the context that will be used.Initial Testing\n\nid is very useful for showing what context a user is currently in. As a test\nto make sure that we are switching contexts, you can run a quick test with srun.\n\nmcmult@master:~$ srun id\nuid=1000(mcmult) gid=1000(mcmult) groups=1000(mcmult),27(sudo) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023\nmcmult@master:~$ srun --context=unconfined_u:unconfined_r:unconfined_t:s0 id\nuid=1000(mcmult) gid=1000(mcmult) groups=1000(mcmult),27(sudo) context=unconfined_u:unconfined_r:unconfined_t:s0\nAccountingThere is currently no support for tracking the SELinux context in Slurm's\naccounting. This may change as support evolves in future releases.\nIf you need to keep track of the SELinux Context, it is possible to store it in\nthe admin comment field as part of your job_submit plugin as is show in the\nexample below.Example:\nfunction slurm_job_submit(job_desc, part_list, submit_uid)\n  if job_desc.req_context then\n    local element = 0\n    for str in string.gmatch(job_desc.req_context, \"([^:]+)\") do\n      if element == 0 and str ~= \"unconfined_u\" then\n        slurm.log_user(\"Error: invalid SELinux context\")\n        return slurm.ERROR\n      elseif element == 1 and str ~= \"unconfined_r\" then\n        slurm.log_user(\"Error: %s is not a valid SELinux role\")\n        return slurm.ERROR\n      end\n      element = element + 1\n    end\n    job_desc.selinux_context = job_desc.req_context\n  else\n    -- Force a specific context if one wasn't requested\n    job_desc.selinux_context = unconfined_u:unconfined_r:slurm_t:s0\n  end\n  job_desc.admin_comment = \"SELinuxContext=\" .. job_desc.selinux_context\n  return slurm.SUCCESS\nend\nNote the addition of setting \"job_desc.admin_comment\" before returning. This\nwill set the admin comment to show what context we will try to set for the job.\nNotesIf you wish to use pam_slurm_adopt with SELinux, see the\npam_slurm_adopt documentation for hints on how\nto get this working. Note that that when using this feature and\npam_slurm_adopt at the same time that the ssh session may not land in the same\ncontext as the job.Last modified 22 September 2023"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/openapi_release_notes.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Slurm OpenAPI Plugin Release Notes",
                "content": "These release notes are maintained to provide OpenAPI client programmers a\nguide to where changes have taken place in the OpenAPI specifications from the\nOpenAPI plugins which are mainly used by slurmrestd\nbut are also used to generate JSON or YAML output for multiple CLI commands.All paths given are formatted for use with\njq. Make sure to always place the\npaths in a single quote string to avoid shell replacements.The OpenAPI specification should always be generated by slurmrestd once\nfully configured for normal operation. Query 'GET /openapi/v3' from slurmrestd\nto get the generated OpenAPI specification. The generated specification can\nchange depending on which plugins are loaded and how they are configured. Any\nclient must be careful to always use the current generated specification for\nany target slurmrestd daemon. Development of clients should always be designed\nto use the highest version of the plugins available to avoid needing to port\nclients sooner than would be otherwise required.Slurm 24.05.3data_parser/v0.0.41Modified Fields\n\nAdded\n.components.schemas[\"v0.0.41_kill_jobs_msg\"] = {};\n\n\nAdded\n.components.schemas[\"v0.0.41_kill_jobs_msg\"].required = [];\n\n\nAdded\n.components.schemas[\"v0.0.41_kill_jobs_msg\"].type = \"object\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].parameters = [];\n\n\nReplaced\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].requestBody = {};\n\n\nAdded\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].requestBody.content = {};\n\n\nAdded\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].requestBody.content[\"application/json\"] = {};\n\n\nAdded\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].requestBody.content[\"application/json\"].schema = {};\n\n\nAdded\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].requestBody.content[\"application/json\"].schema.$ref = \"#/components/schemas/v0.0.41_kill_jobs_msg\";\n\n\nAdded\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].requestBody.content[\"application/x-yaml\"] = {};\n\n\nAdded\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].requestBody.content[\"application/x-yaml\"].schema = {};\n\n\nAdded\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].requestBody.content[\"application/x-yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_kill_jobs_msg\";\n\n\nAdded\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].requestBody.description = \"Signal or cancel jobs\";\n\ndata_parser/v0.0.40Modified Fields\n\nAdded\n.components.schemas[\"v0.0.40_kill_jobs_msg\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].parameters = [];\n\n\nReplaced\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].requestBody = {};\n\n\nAdded\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].requestBody.content = {};\n\n\nAdded\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].requestBody.content[\"application/json\"] = {};\n\n\nAdded\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].requestBody.content[\"application/json\"].schema = {};\n\n\nAdded\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].requestBody.content[\"application/json\"].schema.$ref = \"#/components/schemas/v0.0.40_kill_jobs_msg\";\n\n\nAdded\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].requestBody.content[\"application/x-yaml\"] = {};\n\n\nAdded\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].requestBody.content[\"application/x-yaml\"].schema = {};\n\n\nAdded\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].requestBody.content[\"application/x-yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_kill_jobs_msg\";\n\n\nAdded\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].requestBody.description = \"Signal or cancel jobs\";\n\nSlurm 24.05.1data_parser/v0.0.41Removed mime types aliases\n\nRemoved\n.paths[\"/slurm/v0.0.41/diag/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/diag/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/diag/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_diag_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/diag/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/diag/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/diag/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_diag_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/diag/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/diag/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/diag/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_diag_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/diag/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/diag/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/diag/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_diag_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/allocate\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/allocate\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/allocate\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_job_alloc_req\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/allocate\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/allocate\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/allocate\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_job_alloc_req\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/allocate\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/allocate\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/allocate\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_alloc_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/allocate\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/allocate\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/allocate\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_alloc_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/allocate\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/allocate\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/allocate\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_alloc_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/allocate\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/allocate\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/allocate\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_alloc_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/submit\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/submit\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/submit\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_job_submit_req\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/submit\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/submit\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/submit\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_job_submit_req\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/submit\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/submit\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/submit\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_submit_response\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/submit\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/submit\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/submit\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_submit_response\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/submit\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/submit\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/submit\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_submit_response\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/submit\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/submit\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/submit\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_submit_response\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_job_desc_msg\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_job_desc_msg\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_post_response\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_post_response\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_post_response\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_post_response\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/job/{job_id}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_kill_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_kill_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_kill_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_kill_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/state/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/state/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/state/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/state/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/state/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/state/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/state/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/state/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/state/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/state/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/state/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/jobs/state/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/licenses/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/licenses/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/licenses/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_licenses_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/licenses/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/licenses/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/licenses/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_licenses_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/licenses/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/licenses/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/licenses/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_licenses_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/licenses/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/licenses/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/licenses/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_licenses_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_nodes_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_nodes_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_nodes_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_nodes_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_update_node_msg\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_update_node_msg\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/node/{node_name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/nodes/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/nodes/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/nodes/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_nodes_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/nodes/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/nodes/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/nodes/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_nodes_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/nodes/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/nodes/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/nodes/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_nodes_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/nodes/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/nodes/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/nodes/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_nodes_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partition/{partition_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partition/{partition_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partition/{partition_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_partition_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partition/{partition_name}\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partition/{partition_name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partition/{partition_name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_partition_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partition/{partition_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partition/{partition_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partition/{partition_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_partition_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partition/{partition_name}\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partition/{partition_name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partition/{partition_name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_partition_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partitions/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partitions/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partitions/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_partition_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partitions/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partitions/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partitions/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_partition_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partitions/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partitions/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partitions/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_partition_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partitions/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partitions/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/partitions/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_partition_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/ping/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/ping/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/ping/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_ping_array_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/ping/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/ping/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/ping/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_ping_array_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/ping/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/ping/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/ping/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_ping_array_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/ping/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/ping/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/ping/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_ping_array_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reconfigure/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reconfigure/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reconfigure/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reconfigure/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reconfigure/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reconfigure/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reconfigure/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reconfigure/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reconfigure/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reconfigure/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reconfigure/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reconfigure/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservation/{reservation_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservation/{reservation_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservation/{reservation_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_reservation_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservation/{reservation_name}\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservation/{reservation_name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservation/{reservation_name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_reservation_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservation/{reservation_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservation/{reservation_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservation/{reservation_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_reservation_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservation/{reservation_name}\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservation/{reservation_name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservation/{reservation_name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_reservation_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservations/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservations/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservations/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_reservation_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservations/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservations/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservations/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_reservation_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservations/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservations/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservations/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_reservation_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservations/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservations/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/reservations/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_reservation_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/shares\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/shares\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/shares\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_shares_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/shares\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/shares\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/shares\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_shares_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/shares\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/shares\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/shares\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_shares_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/shares\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/shares\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.41/shares\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_shares_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_accounts_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_accounts_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_accounts_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_accounts_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_accounts_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_accounts_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_accounts_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/account/{account_name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_accounts_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_accounts_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_accounts_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_accounts_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_accounts_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_accounts_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_accounts_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts/\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts_association/\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts_association/\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts_association/\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_accounts_add_cond_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts_association/\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts_association/\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts_association/\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_accounts_add_cond_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts_association/\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts_association/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts_association/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_accounts_add_cond_resp_str\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts_association/\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts_association/\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts_association/\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_accounts_add_cond_resp_str\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts_association/\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts_association/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts_association/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_accounts_add_cond_resp_str\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts_association/\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts_association/\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/accounts_association/\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_accounts_add_cond_resp_str\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_assocs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_assocs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_assocs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_assocs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_assocs_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_assocs_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_assocs_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/association/\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_assocs_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_assocs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_assocs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_assocs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_assocs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_assocs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_assocs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_assocs_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_assocs_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_assocs_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/associations/\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_assocs_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_clusters_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_clusters_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_clusters_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_clusters_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_clusters_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_clusters_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_clusters_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/cluster/{cluster_name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_clusters_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_clusters_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_clusters_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_clusters_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_clusters_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_clusters_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_clusters_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/clusters/\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_config_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_config_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_config_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_config_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_config_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_config_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/config\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/diag/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/diag/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/diag/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_stats_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/diag/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/diag/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/diag/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_stats_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/diag/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/diag/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/diag/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_stats_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/diag/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/diag/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/diag/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_stats_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instance/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instance/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instance/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_instances_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instance/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instance/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instance/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_instances_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instance/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instance/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instance/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_instances_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instance/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instance/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instance/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_instances_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instances/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instances/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instances/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_instances_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instances/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instances/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instances/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_instances_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instances/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instances/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instances/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_instances_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instances/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instances/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/instances/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_instances_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/job/{job_id}\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/job/{job_id}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/job/{job_id}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/job/{job_id}\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/job/{job_id}\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/job/{job_id}\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/job/{job_id}\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/job/{job_id}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/job/{job_id}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/job/{job_id}\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/job/{job_id}\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/job/{job_id}\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/jobs/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/jobs/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/jobs/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/jobs/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/jobs/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/jobs/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/jobs/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/jobs/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/jobs/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/jobs/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/jobs/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/jobs/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_qos_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_qos_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_qos_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_qos_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_qos_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_qos_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_qos_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_qos_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_qos_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_qos_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_qos_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_qos_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_qos_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/qos/{qos}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_slurmdbd_qos_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_tres_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_tres_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_tres_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_tres_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_tres_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_tres_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/tres/\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_users_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_users_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_users_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_users_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/user/{name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_users_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_users_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_users_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_users_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_users_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_users_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users/\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users_association/\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users_association/\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users_association/\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_users_add_cond_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users_association/\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users_association/\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users_association/\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_users_add_cond_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users_association/\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users_association/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users_association/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_users_add_cond_resp_str\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users_association/\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users_association/\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users_association/\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_users_add_cond_resp_str\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users_association/\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users_association/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users_association/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_users_add_cond_resp_str\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users_association/\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users_association/\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/users_association/\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_users_add_cond_resp_str\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_wckey_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_wckey_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_wckey_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_wckey_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_wckey_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_wckey_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_wckey_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckey/{id}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_wckey_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_wckey_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_wckey_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_wckey_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_wckey_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_wckey_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_wckey_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.41/wckeys/\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.41_openapi_resp\";\n\ndata_parser/v0.0.40Removed mime types aliases\n\nRemoved\n.components.schemas[\"v0.0.40_job_desc_msg\"].properties.cpu_binding_flags.items.enum[5] = \"CPU_BIND_RANK\";\n\n\nRemoved\n.components.schemas[\"v0.0.40_job_desc_msg\"].properties.cpu_binding_flags.items.enum[6] = \"CPU_BIND_MAP\";\n\n\nRemoved\n.components.schemas[\"v0.0.40_job_desc_msg\"].properties.cpu_binding_flags.items.enum[7] = \"CPU_BIND_MASK\";\n\n\nRemoved\n.components.schemas[\"v0.0.40_job_desc_msg\"].properties.cpu_binding_flags.items.enum[8] = \"CPU_BIND_LDRANK\";\n\n\nRemoved\n.components.schemas[\"v0.0.40_job_desc_msg\"].properties.cpu_binding_flags.items.enum[9] = \"CPU_BIND_LDMAP\";\n\n\nRemoved\n.components.schemas[\"v0.0.40_job_desc_msg\"].properties.cpu_binding_flags.items.enum[10] = \"CPU_BIND_LDMASK\";\n\n\nRemoved\n.components.schemas[\"v0.0.40_job_desc_msg\"].properties.cpu_binding_flags.items.enum[11] = \"VERBOSE\";\n\n\nReplaced\n.components.schemas[\"v0.0.40_job_desc_msg\"].properties.cpu_binding_flags.items.enum[5] = \"CPU_BIND_MAP\";\n\n\nAdded\n.components.schemas[\"v0.0.40_job_desc_msg\"].properties.cpu_binding_flags.items.enum[6] = \"CPU_BIND_MASK\";\n\n\nAdded\n.components.schemas[\"v0.0.40_job_desc_msg\"].properties.cpu_binding_flags.items.enum[7] = \"CPU_BIND_LDRANK\";\n\n\nAdded\n.components.schemas[\"v0.0.40_job_desc_msg\"].properties.cpu_binding_flags.items.enum[8] = \"CPU_BIND_LDMAP\";\n\n\nAdded\n.components.schemas[\"v0.0.40_job_desc_msg\"].properties.cpu_binding_flags.items.enum[9] = \"CPU_BIND_LDMASK\";\n\n\nAdded\n.components.schemas[\"v0.0.40_job_desc_msg\"].properties.cpu_binding_flags.items.enum[10] = \"VERBOSE\";\n\n\nAdded\n.components.schemas[\"v0.0.40_job_desc_msg\"].properties.cpu_binding_flags.items.enum[11] = \"CPU_BIND_ONE_THREAD_PER_CORE\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/diag/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/diag/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/diag/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_diag_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/diag/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/diag/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/diag/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_diag_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/diag/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/diag/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/diag/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_diag_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/diag/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/diag/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/diag/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_diag_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/submit\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/submit\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/submit\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_job_submit_req\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/submit\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/submit\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/submit\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_job_submit_req\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/submit\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/submit\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/submit\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_job_submit_response\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/submit\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/submit\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/submit\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_job_submit_response\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/submit\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/submit\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/submit\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_job_submit_response\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/submit\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/submit\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/submit\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_job_submit_response\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_job_desc_msg\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_job_desc_msg\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_job_post_response\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_job_post_response\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_job_post_response\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_job_post_response\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/job/{job_id}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_kill_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_kill_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_kill_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_kill_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/state/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/state/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/state/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/state/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/state/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/state/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/state/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/state/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/state/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/state/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/state/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/jobs/state/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_job_info_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/licenses/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/licenses/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/licenses/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_licenses_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/licenses/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/licenses/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/licenses/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_licenses_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/licenses/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/licenses/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/licenses/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_licenses_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/licenses/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/licenses/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/licenses/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_licenses_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_nodes_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_nodes_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_nodes_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_nodes_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_update_node_msg\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_update_node_msg\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/node/{node_name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/nodes/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/nodes/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/nodes/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_nodes_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/nodes/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/nodes/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/nodes/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_nodes_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/nodes/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/nodes/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/nodes/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_nodes_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/nodes/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/nodes/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/nodes/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_nodes_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partition/{partition_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partition/{partition_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partition/{partition_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_partition_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partition/{partition_name}\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partition/{partition_name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partition/{partition_name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_partition_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partition/{partition_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partition/{partition_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partition/{partition_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_partition_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partition/{partition_name}\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partition/{partition_name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partition/{partition_name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_partition_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partitions/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partitions/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partitions/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_partition_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partitions/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partitions/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partitions/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_partition_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partitions/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partitions/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partitions/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_partition_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partitions/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partitions/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/partitions/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_partition_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/ping/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/ping/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/ping/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_ping_array_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/ping/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/ping/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/ping/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_ping_array_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/ping/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/ping/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/ping/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_ping_array_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/ping/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/ping/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/ping/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_ping_array_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reconfigure/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reconfigure/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reconfigure/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reconfigure/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reconfigure/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reconfigure/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reconfigure/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reconfigure/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reconfigure/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reconfigure/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reconfigure/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reconfigure/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservation/{reservation_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservation/{reservation_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservation/{reservation_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_reservation_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservation/{reservation_name}\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservation/{reservation_name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservation/{reservation_name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_reservation_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservation/{reservation_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservation/{reservation_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservation/{reservation_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_reservation_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservation/{reservation_name}\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservation/{reservation_name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservation/{reservation_name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_reservation_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservations/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservations/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservations/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_reservation_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservations/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservations/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservations/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_reservation_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservations/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservations/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservations/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_reservation_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservations/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservations/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/reservations/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_reservation_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/shares\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/shares\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/shares\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_shares_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/shares\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/shares\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/shares\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_shares_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/shares\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/shares\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/shares\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_shares_resp\";\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/shares\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/shares\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurm/v0.0.40/shares\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_shares_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_accounts_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_accounts_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_accounts_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_accounts_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_accounts_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_accounts_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_accounts_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/account/{account_name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_accounts_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_accounts_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_accounts_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_accounts_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_accounts_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_accounts_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_accounts_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts/\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts_association/\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts_association/\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts_association/\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_accounts_add_cond_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts_association/\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts_association/\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts_association/\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_accounts_add_cond_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts_association/\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts_association/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts_association/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_accounts_add_cond_resp_str\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts_association/\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts_association/\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts_association/\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_accounts_add_cond_resp_str\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts_association/\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts_association/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts_association/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_accounts_add_cond_resp_str\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts_association/\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts_association/\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/accounts_association/\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_accounts_add_cond_resp_str\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_assocs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_assocs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_assocs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_assocs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_assocs_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_assocs_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_assocs_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/association/\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_assocs_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_assocs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_assocs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_assocs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_assocs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_assocs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_assocs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_assocs_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_assocs_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_assocs_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/associations/\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_assocs_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_clusters_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_clusters_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_clusters_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_clusters_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_clusters_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_clusters_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_clusters_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/cluster/{cluster_name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_clusters_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_clusters_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_clusters_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_clusters_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_clusters_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_clusters_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_clusters_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/clusters/\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_config_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_config_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_config_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_config_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_config_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_config_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/config\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/diag/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/diag/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/diag/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_stats_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/diag/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/diag/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/diag/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_stats_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/diag/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/diag/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/diag/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_stats_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/diag/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/diag/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/diag/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_stats_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instance/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instance/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instance/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_instances_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instance/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instance/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instance/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_instances_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instance/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instance/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instance/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_instances_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instance/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instance/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instance/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_instances_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instances/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instances/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instances/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_instances_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instances/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instances/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instances/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_instances_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instances/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instances/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instances/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_instances_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instances/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instances/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/instances/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_instances_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/job/{job_id}\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/job/{job_id}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/job/{job_id}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/job/{job_id}\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/job/{job_id}\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/job/{job_id}\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/job/{job_id}\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/job/{job_id}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/job/{job_id}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/job/{job_id}\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/job/{job_id}\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/job/{job_id}\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/jobs/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/jobs/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/jobs/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/jobs/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/jobs/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/jobs/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/jobs/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/jobs/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/jobs/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/jobs/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/jobs/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/jobs/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_jobs_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_qos_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_qos_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_qos_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_qos_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_qos_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_qos_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_qos_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_qos_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_qos_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_qos_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_qos_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_qos_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_qos_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/qos/{qos}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_slurmdbd_qos_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_tres_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_tres_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_tres_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_tres_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_tres_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_tres_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/tres/\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_users_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_users_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_users_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_users_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/user/{name}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_users_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_users_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_users_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_users_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_users_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_users_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users/\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users_association/\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users_association/\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users_association/\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_users_add_cond_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users_association/\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users_association/\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users_association/\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_users_add_cond_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users_association/\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users_association/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users_association/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_users_add_cond_resp_str\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users_association/\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users_association/\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users_association/\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_users_add_cond_resp_str\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users_association/\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users_association/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users_association/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_users_add_cond_resp_str\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users_association/\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users_association/\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/users_association/\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_users_add_cond_resp_str\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_wckey_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_wckey_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_wckey_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_wckey_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"][\"delete\"].responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_wckey_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"][\"delete\"].responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_wckey_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"][\"delete\"].responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_wckey_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckey/{id}\"][\"delete\"].responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_wckey_removed_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].get.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].get.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_wckey_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].get.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].get.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].get.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_wckey_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].get.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].get.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_wckey_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].get.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].get.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].get.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_wckey_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].post.requestBody.content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].post.requestBody.content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].post.requestBody.content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_wckey_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].post.requestBody.content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].post.requestBody.content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].post.requestBody.content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_wckey_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].post.responses[\"200\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].post.responses[\"200\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].post.responses[\"200\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].post.responses[\"200\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].post.responses[\"200\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].post.responses[\"default\"].content[\"application/jsonrequest\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].post.responses[\"default\"].content[\"application/jsonrequest\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].post.responses[\"default\"].content[\"text/yaml\"] = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].post.responses[\"default\"].content[\"text/yaml\"].schema = {};\n\n\nRemoved\n.paths[\"/slurmdb/v0.0.40/wckeys/\"].post.responses[\"default\"].content[\"text/yaml\"].schema.$ref = \"#/components/schemas/v0.0.40_openapi_resp\";\n\nSlurm 24.05.0openapi/slurmctlddata_parser/v0.0.41Only populate \"deprecated\" fields if true.\nremove/paths/\"/slurm/v0.0.41\"/jobs/get/parameters[0]/deprecated\nremove/paths/\"/slurm/v0.0.41\"/jobs/get/parameters[1]/deprecated\nremove/paths/\"/slurm/v0.0.41\"/node\"/{node_name}/delete/parameters[0]/deprecated\nremove/paths/\"/slurm/v0.0.41\"/node\"/{node_name}/get/parameters[0]/deprecated\nremove/paths/\"/slurm/v0.0.41\"/node\"/{node_name}/get/parameters[1]/deprecated\nremove/paths/\"/slurm/v0.0.41\"/node\"/{node_name}/get/parameters[2]/deprecated\nremove/paths/\"/slurm/v0.0.41\"/node\"/{node_name}/post/parameters[0]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/delete/parameters[0]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/delete/parameters[1]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/delete/parameters[2]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/delete/parameters[3]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/delete/parameters[4]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/delete/parameters[5]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/delete/parameters[6]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/delete/parameters[7]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/delete/parameters[8]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/delete/parameters[9]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/delete/parameters[10]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/delete/parameters[11]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/delete/parameters[12]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/delete/parameters[13]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/delete/parameters[14]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/delete/parameters[15]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/delete/parameters[16]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/delete/parameters[17]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/get/parameters[0]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/get/parameters[1]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/get/parameters[2]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/get/parameters[3]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/get/parameters[4]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/get/parameters[5]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/get/parameters[6]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/get/parameters[7]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/get/parameters[8]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/get/parameters[9]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/get/parameters[10]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/get/parameters[11]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/get/parameters[12]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/get/parameters[13]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/get/parameters[14]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/get/parameters[15]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/get/parameters[16]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/associations/get/parameters[17]/deprecated\nremove/paths/\"/slurm/v0.0.41\"/nodes/get/parameters[0]/deprecated\nremove/paths/\"/slurm/v0.0.41\"/nodes/get/parameters[1]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/clusters/get/parameters[0]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/clusters/get/parameters[1]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/clusters/get/parameters[2]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/clusters/get/parameters[3]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/clusters/get/parameters[4]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/clusters/get/parameters[5]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/clusters/get/parameters[6]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/clusters/get/parameters[7]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/clusters/get/parameters[8]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/clusters/get/parameters[9]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/qos/get/parameters[0]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/qos/get/parameters[1]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/qos/get/parameters[2]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/qos/get/parameters[3]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/qos/get/parameters[4]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/qos/get/parameters[5]/deprecated\nremove/components/schemas/v0.0.41_openapi_job_post_response/properties/step_id/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/user\"/{name}/delete/parameters[0]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/user\"/{name}/get/parameters[0]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/user\"/{name}/get/parameters[1]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/user\"/{name}/get/parameters[2]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/user\"/{name}/get/parameters[3]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/user\"/{name}/get/parameters[4]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/accounts/get/parameters[0]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/accounts/get/parameters[1]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/accounts/get/parameters[2]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/accounts/get/parameters[3]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/account\"/{account_name}/delete/parameters[0]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/account\"/{account_name}/get/parameters[0]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/account\"/{account_name}/get/parameters[1]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/account\"/{account_name}/get/parameters[2]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/account\"/{account_name}/get/parameters[3]/deprecated\nremove/components/schemas/v0.0.41_openapi_job_post_response/properties/job_id/deprecated\nremove/components/schemas/v0.0.41_openapi_job_post_response/properties/job_submit_user_msg/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/qos\"/{qos}/get/parameters[1]/deprecated\nremove/paths/\"/slurm/v0.0.41\"/shares/get/parameters[0]/deprecated\nremove/paths/\"/slurm/v0.0.41\"/shares/get/parameters[1]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/wckeys/get/parameters[0]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/wckeys/get/parameters[1]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/wckeys/get/parameters[2]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/wckeys/get/parameters[3]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/wckeys/get/parameters[4]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/wckeys/get/parameters[5]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/wckeys/get/parameters[6]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/wckeys/get/parameters[7]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/wckeys/get/parameters[8]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/wckeys/get/parameters[9]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/wckey\"/{id}/delete/parameters[0]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/wckey\"/{id}/get/parameters[0]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/users/get/parameters[0]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/users/get/parameters[1]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/users/get/parameters[2]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/users/get/parameters[3]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/users/get/parameters[4]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/users/get/parameters[5]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/users/get/parameters[6]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/users/get/parameters[7]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[0]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[1]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[2]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[3]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[4]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[5]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[6]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[7]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[8]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[9]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[10]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[11]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[12]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[13]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[14]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[15]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[16]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[17]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[18]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[19]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[20]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[21]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[22]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[23]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[24]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[25]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[26]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[27]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[28]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[29]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[30]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[31]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[32]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[33]/deprecated\nremove/paths/\"/slurm/v0.0.41\"/job\"/{job_id}/delete/parameters[0]/deprecated\nremove/paths/\"/slurm/v0.0.41\"/job\"/{job_id}/delete/parameters[1]/deprecated\nremove/paths/\"/slurm/v0.0.41\"/job\"/{job_id}/delete/parameters[2]/deprecated\nremove/paths/\"/slurm/v0.0.41\"/job\"/{job_id}/get/parameters[0]/deprecated\nremove/paths/\"/slurm/v0.0.41\"/job\"/{job_id}/get/parameters[1]/deprecated\nremove/paths/\"/slurm/v0.0.41\"/job\"/{job_id}/get/parameters[2]/deprecated\nremove/paths/\"/slurm/v0.0.41\"/job\"/{job_id}/post/parameters[0]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/delete/parameters[0]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/delete/parameters[1]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/delete/parameters[2]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/delete/parameters[3]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/delete/parameters[4]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/delete/parameters[5]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/delete/parameters[6]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/delete/parameters[7]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/delete/parameters[8]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/delete/parameters[9]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/delete/parameters[10]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/delete/parameters[11]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/delete/parameters[12]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/delete/parameters[13]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/delete/parameters[14]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/delete/parameters[15]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/delete/parameters[16]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/delete/parameters[17]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/get/parameters[0]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/get/parameters[1]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/get/parameters[2]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/get/parameters[3]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/get/parameters[4]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/get/parameters[5]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/get/parameters[6]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/get/parameters[7]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/get/parameters[8]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/get/parameters[9]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/get/parameters[10]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/get/parameters[11]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/get/parameters[12]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/get/parameters[13]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/get/parameters[14]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/get/parameters[15]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/get/parameters[16]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/association/get/parameters[17]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/delete/parameters[0]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/delete/parameters[1]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/delete/parameters[2]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/delete/parameters[3]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/delete/parameters[4]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/delete/parameters[5]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/delete/parameters[6]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/delete/parameters[7]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/delete/parameters[8]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/delete/parameters[9]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/delete/parameters[10]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/get/parameters[0]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/get/parameters[1]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/get/parameters[2]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/get/parameters[3]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/get/parameters[4]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/get/parameters[5]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/get/parameters[6]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/get/parameters[7]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/get/parameters[8]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/get/parameters[9]/deprecated\nremove/paths/\"/slurmdb/v0.0.41\"/cluster\"/{cluster_name}/get/parameters[10]/deprecated\nPopulate missing \"deprecated\" fields\nadd/components/schemas/v0.0.41_openapi_job_submit_response/properties/result/deprecated\nadd/components/schemas/v0.0.41_job_desc_msg/properties/exclusive/deprecated\nadd/components/schemas/v0.0.41_job_desc_msg/properties/oversubscribe/deprecated\nReduced number of \"$ref\" fields.\nreplace/components/schemas/v0.0.41_openapi_nodes_resp/properties/last_update/$ref\nreplace/components/schemas/v0.0.41_partition_info/properties/timeouts/properties/suspend/$ref\nreplace/components/schemas/v0.0.41_partition_info/properties/timeouts/properties/resume/$ref\nreplace/components/schemas/v0.0.41_partition_info/properties/suspend_time/$ref\nreplace/components/schemas/v0.0.41_partition_info/properties/maximums/properties/cpus_per_node/$ref\nreplace/components/schemas/v0.0.41_partition_info/properties/maximums/properties/cpus_per_socket/$ref\nreplace/components/schemas/v0.0.41_partition_info/properties/maximums/properties/nodes/$ref\nreplace/components/schemas/v0.0.41_partition_info/properties/maximums/properties/time/$ref\nreplace/components/schemas/v0.0.41_partition_info/properties/maximums/properties/partition_memory_per_cpu/$ref\nreplace/components/schemas/v0.0.41_partition_info/properties/maximums/properties/over_time_limit/$ref\nreplace/components/schemas/v0.0.41_partition_info/properties/maximums/properties/partition_memory_per_node/$ref\nreplace/components/schemas/v0.0.41_partition_info/properties/defaults/properties/time/$ref\nreplace/components/schemas/v0.0.41_partition_info/properties/defaults/properties/partition_memory_per_cpu/$ref\nreplace/components/schemas/v0.0.41_partition_info/properties/defaults/properties/partition_memory_per_node/$ref\nreplace/components/schemas/v0.0.41_assoc_rec_set/properties/grpwall/$ref\nreplace/components/schemas/v0.0.41_assoc_rec_set/properties/minpriothresh/$ref\nreplace/components/schemas/v0.0.41_assoc_rec_set/properties/grpsubmitjobs/$ref\nreplace/components/schemas/v0.0.41_assoc_rec_set/properties/maxjobsaccrue/$ref\nreplace/components/schemas/v0.0.41_assoc_rec_set/properties/maxwalldurationperjob/$ref\nreplace/components/schemas/v0.0.41_assoc_rec_set/properties/maxjobs/$ref\nreplace/components/schemas/v0.0.41_assoc_rec_set/properties/maxsubmitjobs/$ref\nreplace/components/schemas/v0.0.41_assoc_rec_set/properties/qoslevel/description\nreplace/components/schemas/v0.0.41_assoc_rec_set/properties/grpjobsaccrue/$ref\nreplace/components/schemas/v0.0.41_assoc_rec_set/properties/grpjobs/$ref\nreplace/components/schemas/v0.0.41_assoc_rec_set/properties/priority/$ref\nreplace/components/schemas/v0.0.41_process_exit_code_verbose/properties/signal/properties/id/$ref\nreplace/components/schemas/v0.0.41_process_exit_code_verbose/properties/return_code/$ref\nreplace/components/schemas/v0.0.41_acct_gather_energy/properties/current_watts/$ref\nreplace/components/schemas/v0.0.41_job_res/properties/nodes/type\nreplace/components/schemas/v0.0.41_openapi_partition_resp/properties/last_update/$ref\nreplace/components/schemas/v0.0.41_stats_msg_rpcs_by_type/description\nreplace/components/schemas/v0.0.41_stats_msg_rpcs_by_type/items/properties/count/format\nreplace/components/schemas/v0.0.41_stats_msg_rpcs_by_type/items/description\nreplace/components/schemas/v0.0.41_update_node_msg/properties/weight/$ref\nreplace/components/schemas/v0.0.41_update_node_msg/properties/resume_after/$ref\nreplace/components/schemas/v0.0.41_openapi_job_info_resp/properties/last_update/$ref\nreplace/components/schemas/v0.0.41_openapi_job_info_resp/properties/last_backfill/$ref\nreplace/components/schemas/v0.0.41_power_mgmt_data/properties/new_job_time/$ref\nreplace/components/schemas/v0.0.41_power_mgmt_data/properties/maximum_watts/$ref\nreplace/components/schemas/v0.0.41_openapi_reservation_resp/properties/last_update/$ref\nreplace/components/schemas/v0.0.41_job_desc_msg/properties/kill_warning_delay/$ref\nreplace/components/schemas/v0.0.41_job_desc_msg/properties/priority/$ref\nreplace/components/schemas/v0.0.41_job_desc_msg/properties/time_minimum/$ref\nreplace/components/schemas/v0.0.41_job_desc_msg/properties/memory_per_cpu/$ref\nreplace/components/schemas/v0.0.41_job_desc_msg/properties/rlimits/properties/cpu/$ref\nreplace/components/schemas/v0.0.41_job_desc_msg/properties/rlimits/properties/nofile/$ref\nreplace/components/schemas/v0.0.41_job_desc_msg/properties/rlimits/properties/nproc/$ref\nreplace/components/schemas/v0.0.41_job_desc_msg/properties/rlimits/properties/core/$ref\nreplace/components/schemas/v0.0.41_job_desc_msg/properties/rlimits/properties/rss/$ref\nreplace/components/schemas/v0.0.41_job_desc_msg/properties/rlimits/properties/data/$ref\nreplace/components/schemas/v0.0.41_job_desc_msg/properties/rlimits/properties/stack/$ref\nreplace/components/schemas/v0.0.41_job_desc_msg/properties/rlimits/properties/fsize/$ref\nreplace/components/schemas/v0.0.41_job_desc_msg/properties/rlimits/properties/memlock/$ref\nreplace/components/schemas/v0.0.41_job_desc_msg/properties/rlimits/properties/as/$ref\nreplace/components/schemas/v0.0.41_job_desc_msg/properties/begin_time/$ref\nreplace/components/schemas/v0.0.41_job_desc_msg/properties/time_limit/$ref\nreplace/components/schemas/v0.0.41_job_desc_msg/properties/required_switches/$ref\nreplace/components/schemas/v0.0.41_job_desc_msg/properties/memory_per_node/$ref\nreplace/components/schemas/v0.0.41_ext_sensors_data/properties/consumed_energy/$ref\nreplace/components/schemas/v0.0.41_ext_sensors_data/properties/temperature/$ref\nreplace/components/schemas/v0.0.41_stats_msg_rpcs_by_user/items/properties/count/format\nreplace/components/schemas/v0.0.41_stats_msg_rpcs_by_user/items/description\nreplace/components/schemas/v0.0.41_job_res_nodes/description\nreplace/components/schemas/v0.0.41_openapi_licenses_resp/properties/last_update/$ref\nreplace/components/schemas/v0.0.41_stats_msg/properties/job_states_ts/$ref\nreplace/components/schemas/v0.0.41_stats_msg/properties/req_time_start/$ref\nreplace/components/schemas/v0.0.41_stats_msg/properties/req_time/$ref\nreplace/components/schemas/v0.0.41_stats_msg/properties/bf_when_last_cycle/$ref\nadd/components/schemas/v0.0.41_stats_msg/properties/pending_rpcs_by_hostlist\nadd/components/schemas/v0.0.41_stats_msg/properties/pending_rpcs\nadd/components/schemas/v0.0.41_stats_msg/properties/schedule_cycle_sum\nremove/components/schemas/v0.0.41_stats_msg/properties/bf_cycle_max\nremove/components/schemas/v0.0.41_stats_msg/properties/bf_table_size_sum\nremove/components/schemas/v0.0.41_stats_msg/properties/schedule_cycle_depth\nreplace/components/schemas/v0.0.41_rollup_stats/type\nadd/components/schemas/v0.0.41_rollup_stats/properties\nadd/components/schemas/v0.0.41_rollup_stats/required\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[4]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[4]/allowEmptyValue\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[5]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[5]/allowEmptyValue\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[6]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[7]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[8]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[9]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[9]/allowEmptyValue\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[10]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[11]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[11]/allowEmptyValue\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[12]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[13]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[14]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[15]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[16]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[17]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[18]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[19]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[19]/allowEmptyValue\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[20]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[20]/allowEmptyValue\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[21]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[21]/description\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[22]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[22]/description\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[23]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[23]/description\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[24]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[24]/description\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[25]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[25]/description\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[26]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[26]/description\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[27]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[27]/description\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[28]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[28]/description\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[29]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[29]/description\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[30]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[30]/description\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[31]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[31]/description\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[32]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[32]/description\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[33]/name\nreplace/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[33]/description\nadd/components/schemas/v0.0.41_stats_msg_rpcs_by_type/items/properties/average_time/$ref\nadd/components/schemas/v0.0.41_stats_msg_rpcs_by_type/items/required\nremove/components/schemas/v0.0.41_stats_msg_rpcs_by_type/items/properties/average_time/format\nremove/components/schemas/v0.0.41_stats_msg_rpcs_by_type/items/properties/average_time/type\nadd/components/schemas/v0.0.41_stats_msg_rpcs_by_user/items/properties/average_time/$ref\nremove/components/schemas/v0.0.41_stats_msg_rpcs_by_user/items/properties/average_time/format\nremove/components/schemas/v0.0.41_stats_msg_rpcs_by_user/items/properties/average_time/type\nadd/components/schemas/v0.0.41_stats_msg_rpcs_by_user/items/required\nremove/components/schemas/v0.0.41_accounting_list/items/$ref\nadd/components/schemas/v0.0.41_accounting_list/items/properties\nadd/components/schemas/v0.0.41_accounting_list/items/required\nadd/components/schemas/v0.0.41_accounting_list/items/type\nremove/components/schemas/v0.0.41_account_list/items/$ref\nadd/components/schemas/v0.0.41_account_list/items/properties\nadd/components/schemas/v0.0.41_account_list/items/required\nadd/components/schemas/v0.0.41_account_list/items/type\nremove/components/schemas/v0.0.41_assoc_list/items/$ref\nadd/components/schemas/v0.0.41_assoc_list/items/properties\nadd/components/schemas/v0.0.41_assoc_list/items/required\nadd/components/schemas/v0.0.41_assoc_list/items/type\nremove/components/schemas/v0.0.41_assoc_shares_obj_list/items/$ref\nadd/components/schemas/v0.0.41_assoc_shares_obj_list/items/properties\nadd/components/schemas/v0.0.41_assoc_shares_obj_list/items/required\nadd/components/schemas/v0.0.41_assoc_shares_obj_list/items/type\nremove/components/schemas/v0.0.41_cluster_rec_list/items/$ref\nadd/components/schemas/v0.0.41_cluster_rec_list/items/properties\nadd/components/schemas/v0.0.41_cluster_rec_list/items/required\nadd/components/schemas/v0.0.41_cluster_rec_list/items/type\nremove/components/schemas/v0.0.41_controller_ping_array/items/$ref\nadd/components/schemas/v0.0.41_controller_ping_array/items/properties\nadd/components/schemas/v0.0.41_controller_ping_array/items/required\nadd/components/schemas/v0.0.41_controller_ping_array/items/type\nremove/components/schemas/v0.0.41_coord_list/items/$ref\nadd/components/schemas/v0.0.41_coord_list/items/properties\nadd/components/schemas/v0.0.41_coord_list/items/required\nadd/components/schemas/v0.0.41_coord_list/items/type\nremove/components/schemas/v0.0.41_instance_list/items/$ref\nadd/components/schemas/v0.0.41_instance_list/items/properties\nadd/components/schemas/v0.0.41_instance_list/items/required\nadd/components/schemas/v0.0.41_instance_list/items/type\nremove/components/schemas/v0.0.41_job_array_response_array/items/$ref\nadd/components/schemas/v0.0.41_job_array_response_array/items/properties\nadd/components/schemas/v0.0.41_job_array_response_array/items/required\nadd/components/schemas/v0.0.41_job_array_response_array/items/type\nremove/components/schemas/v0.0.41_job_info_msg/items/$ref\nadd/components/schemas/v0.0.41_job_info_msg/items/properties\nadd/components/schemas/v0.0.41_job_info_msg/items/required\nadd/components/schemas/v0.0.41_job_info_msg/items/type\nremove/components/schemas/v0.0.41_licenses/items/$ref\nadd/components/schemas/v0.0.41_licenses/items/properties\nadd/components/schemas/v0.0.41_licenses/items/required\nadd/components/schemas/v0.0.41_licenses/items/type\nremove/components/schemas/v0.0.41_nodes/items/$ref\nadd/components/schemas/v0.0.41_nodes/items/properties\nadd/components/schemas/v0.0.41_nodes/items/required\nadd/components/schemas/v0.0.41_nodes/items/type\nremove/components/schemas/v0.0.41_openapi_errors/items/$ref\nadd/components/schemas/v0.0.41_openapi_errors/items/properties\nadd/components/schemas/v0.0.41_openapi_errors/items/required\nadd/components/schemas/v0.0.41_openapi_errors/items/type\nremove/components/schemas/v0.0.41_openapi_slurmdbd_jobs_resp/properties/jobs/$ref\nadd/components/schemas/v0.0.41_openapi_slurmdbd_jobs_resp/properties/jobs/items\nadd/components/schemas/v0.0.41_openapi_slurmdbd_jobs_resp/properties/jobs/type\nremove/components/schemas/v0.0.41_openapi_warnings/items/$ref\nadd/components/schemas/v0.0.41_openapi_warnings/items/properties\nadd/components/schemas/v0.0.41_openapi_warnings/items/required\nadd/components/schemas/v0.0.41_openapi_warnings/items/type\nremove/components/schemas/v0.0.41_qos_list/items/$ref\nadd/components/schemas/v0.0.41_qos_list/items/properties\nadd/components/schemas/v0.0.41_qos_list/items/required\nadd/components/schemas/v0.0.41_qos_list/items/type\nremove/components/schemas/v0.0.41_reservation_info_core_spec/items/$ref\nadd/components/schemas/v0.0.41_reservation_info_core_spec/items/properties\nadd/components/schemas/v0.0.41_reservation_info_core_spec/items/required\nadd/components/schemas/v0.0.41_reservation_info_core_spec/items/type\nremove/components/schemas/v0.0.41_reservation_info_msg/items/$ref\nadd/components/schemas/v0.0.41_reservation_info_msg/items/properties\nadd/components/schemas/v0.0.41_reservation_info_msg/items/required\nadd/components/schemas/v0.0.41_reservation_info_msg/items/type\nremove/components/schemas/v0.0.41_shares_float128_tres_list/items/$ref\nadd/components/schemas/v0.0.41_shares_float128_tres_list/items/properties\nadd/components/schemas/v0.0.41_shares_float128_tres_list/items/required\nadd/components/schemas/v0.0.41_shares_float128_tres_list/items/type\nremove/components/schemas/v0.0.41_shares_uint64_tres_list/items/$ref\nadd/components/schemas/v0.0.41_shares_uint64_tres_list/items/properties\nadd/components/schemas/v0.0.41_shares_uint64_tres_list/items/required\nadd/components/schemas/v0.0.41_shares_uint64_tres_list/items/type\nremove/components/schemas/v0.0.41_stats_rpc_list/items/$ref\nadd/components/schemas/v0.0.41_stats_rpc_list/items/properties\nadd/components/schemas/v0.0.41_stats_rpc_list/items/required\nadd/components/schemas/v0.0.41_stats_rpc_list/items/type\nremove/components/schemas/v0.0.41_stats_user_list/items/$ref\nadd/components/schemas/v0.0.41_stats_user_list/items/properties\nadd/components/schemas/v0.0.41_stats_user_list/items/required\nadd/components/schemas/v0.0.41_stats_user_list/items/type\nremove/components/schemas/v0.0.41_tres_list/items/$ref\nadd/components/schemas/v0.0.41_tres_list/items/properties\nadd/components/schemas/v0.0.41_tres_list/items/required\nadd/components/schemas/v0.0.41_tres_list/items/type\nremove/components/schemas/v0.0.41_user_list/items/$ref\nadd/components/schemas/v0.0.41_user_list/items/properties\nadd/components/schemas/v0.0.41_user_list/items/required\nadd/components/schemas/v0.0.41_user_list/items/type\nremove/components/schemas/v0.0.41_wckey_list/items/$ref\nadd/components/schemas/v0.0.41_wckey_list/items/properties\nadd/components/schemas/v0.0.41_wckey_list/items/required\nadd/components/schemas/v0.0.41_wckey_list/items/type\nadd/components/schemas/v0.0.41_job_res_socket_array\nadd/components/schemas/v0.0.41_stats_msg_rpcs_dump\nadd/components/schemas/v0.0.41_stats_msg_rpcs_queue\nremove/components/schemas/v0.0.41_job_res/properties/allocated_nodes\nadd/components/schemas/v0.0.41_job_res/properties/cpus\nadd/components/schemas/v0.0.41_job_res/properties/nodes/properties\nadd/components/schemas/v0.0.41_job_res/properties/select_type\nadd/components/schemas/v0.0.41_job_res/properties/threads_per_core\nadd/components/schemas/v0.0.41_job_res/required[0]\nadd/components/schemas/v0.0.41_job_res/required[1]\nadd/components/schemas/v0.0.41_job_res/required[2]\nadd/components/schemas/v0.0.41_job_res/required[3]\nadd/components/schemas/v0.0.41_job_res/required[4]\nadd/components/schemas/v0.0.41_job_res/required[5]\nadd/components/schemas/v0.0.41_job_res/required[6]\nadd/components/schemas/v0.0.41_job_res/required[7]\nremove/components/schemas/v0.0.41_accounting\nremove/components/schemas/v0.0.41_account\nremove/components/schemas/v0.0.41_assoc_shares_obj_wrap\nremove/components/schemas/v0.0.41_assoc\nremove/components/schemas/v0.0.41_cluster_rec\nremove/components/schemas/v0.0.41_controller_ping\nremove/components/schemas/v0.0.41_coord\nremove/components/schemas/v0.0.41_instance\nremove/components/schemas/v0.0.41_job_array_response_msg_entry\nremove/components/schemas/v0.0.41_job_info\nremove/components/schemas/v0.0.41_job_list\nremove/components/schemas/v0.0.41_job\nremove/components/schemas/v0.0.41_license\nremove/components/schemas/v0.0.41_node\nremove/components/schemas/v0.0.41_openapi_error\nremove/components/schemas/v0.0.41_openapi_warning\nremove/components/schemas/v0.0.41_qos\nremove/components/schemas/v0.0.41_reservation_core_spec\nremove/components/schemas/v0.0.41_reservation_info\nremove/components/schemas/v0.0.41_rollup_stats/items\nremove/components/schemas/v0.0.41_shares_float128_tres\nremove/components/schemas/v0.0.41_shares_uint64_tres\nremove/components/schemas/v0.0.41_stats_rpc\nremove/components/schemas/v0.0.41_stats_user\nremove/components/schemas/v0.0.41_step_list\nremove/components/schemas/v0.0.41_step\nremove/components/schemas/v0.0.41_step_tres_req_max\nremove/components/schemas/v0.0.41_step_tres_req_min\nremove/components/schemas/v0.0.41_step_tres_usage_max\nremove/components/schemas/v0.0.41_step_tres_usage_min\nremove/components/schemas/v0.0.41_user\nremove/components/schemas/v0.0.41_wckey_tag_struct\nremove/components/schemas/v0.0.41_wckey\nadd/components/schemas/v0.0.41_job_res_core_array\nadd/components/schemas/v0.0.41_job_res_nodes/items/properties\nadd/components/schemas/v0.0.41_job_res_nodes/items/required\nadd/components/schemas/v0.0.41_job_res_nodes/items/type\nRename NO_VAL schemas to add correct integer sizes\nreplace/components/schemas/v0.0.41_uint16_no_val\n/components/schemas/v0.0.41_uint16_no_val_struct\nreplace/components/schemas/v0.0.41_float64_no_val\n/components/schemas/v0.0.41_float64_no_val_struct\nreplace/components/schemas/v0.0.41_uint64_no_val\n/components/schemas/v0.0.41_uint64_no_val_struct\nreplace/components/schemas/v0.0.41_uint32_no_val\n/components/schemas/v0.0.41_uint32_no_val_struct\nCorrect description generation\nadd/components/schemas/v0.0.41_acct_gather_energy/properties/current_watts/description\nadd/components/schemas/v0.0.41_ext_sensors_data/properties/consumed_energy/description\nadd/components/schemas/v0.0.41_ext_sensors_data/properties/temperature/description\nadd/components/schemas/v0.0.41_job_desc_msg/properties/begin_time/description\nadd/components/schemas/v0.0.41_job_desc_msg/properties/kill_warning_delay/description\nadd/components/schemas/v0.0.41_job_desc_msg/properties/memory_per_cpu/description\nadd/components/schemas/v0.0.41_job_desc_msg/properties/memory_per_node/description\nadd/components/schemas/v0.0.41_job_desc_msg/properties/priority/description\nadd/components/schemas/v0.0.41_job_desc_msg/properties/required_switches/description\nadd/components/schemas/v0.0.41_job_desc_msg/properties/time_limit/description\nadd/components/schemas/v0.0.41_job_desc_msg/properties/time_minimum/description\nadd/components/schemas/v0.0.41_job_res_nodes/items/description\nadd/components/schemas/v0.0.41_partition_info/properties/defaults/properties/partition_memory_per_cpu/description\nadd/components/schemas/v0.0.41_partition_info/properties/defaults/properties/partition_memory_per_node/description\nadd/components/schemas/v0.0.41_partition_info/properties/defaults/properties/time/description\nadd/components/schemas/v0.0.41_partition_info/properties/maximums/properties/cpus_per_node/description\nadd/components/schemas/v0.0.41_partition_info/properties/maximums/properties/cpus_per_socket/description\nadd/components/schemas/v0.0.41_partition_info/properties/maximums/properties/nodes/description\nadd/components/schemas/v0.0.41_partition_info/properties/maximums/properties/over_time_limit/description\nadd/components/schemas/v0.0.41_partition_info/properties/maximums/properties/partition_memory_per_cpu/description\nadd/components/schemas/v0.0.41_partition_info/properties/maximums/properties/partition_memory_per_node/description\nadd/components/schemas/v0.0.41_partition_info/properties/maximums/properties/time/description\nadd/components/schemas/v0.0.41_partition_info/properties/suspend_time/description\nadd/components/schemas/v0.0.41_partition_info/properties/timeouts/properties/resume/description\nadd/components/schemas/v0.0.41_partition_info/properties/timeouts/properties/suspend/description\nadd/components/schemas/v0.0.41_partition_info/properties/select_type\nadd/components/schemas/v0.0.41_power_mgmt_data/properties/maximum_watts/description\nadd/components/schemas/v0.0.41_power_mgmt_data/properties/new_job_time/description\nadd/components/schemas/v0.0.41_stats_msg/properties/bf_when_last_cycle/description\nadd/components/schemas/v0.0.41_stats_msg/properties/job_states_ts/description\nadd/components/schemas/v0.0.41_stats_msg/properties/req_time/description\nadd/components/schemas/v0.0.41_stats_msg/properties/req_time_start/description\nadd/components/schemas/v0.0.41_stats_msg/properties/rpcs_by_message_type/description\nadd/components/schemas/v0.0.41_stats_msg/properties/rpcs_by_user/description\nadd/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[19]/description\nadd/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[20]/description\nmove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[9]/description\nremove/components/schemas/v0.0.41_rollup_stats/description\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[4]/description\nremove/paths/\"/slurmdb/v0.0.41\"/jobs/get/parameters[5]/description\nAdd node fields\n\n\nnew fields\n\n\n.components.schemas.\"v0.0.41_node\".properties.res_cores_per_gpu\n.components.schemas.\"v0.0.41_node\".properties.gpu_spec\n\n\n\n\nSwitch integer to have NO_VAL tagging to allow for complex values.\n\n\nField modified\n\n\t\t.components.schemas.\"v0.0.41_job_desc_msg\".properties.distribution_plane_size\n\t\n\n\nFields tagged as deprecated\n\n\nDeprecated field\n\n\t\t.components.schemas.\"v0.0.41_job_submit_req\".properties.script\n\t\n\n\nModified Fields\n\n\nModified field\n.components.schemas.\"v0.0.41_kill_jobs_msg\".properties.flags.items.enum\n\n\nAdded STEPMGR_ENABLED flag\n.components.schemas.\"v0.0.41_openapi_job_info_resp\".properties.jobs.items.properties.flags\n\n\nNew enum values\n\n\nNew values\n.components.schemas.\"v0.0.41_openapi_slurmdbd_config_resp\".properties.accounts.items.properties.flags.items.enum\n\n\nNew values\n.components.schemas.\"v0.0.41_openapi_accounts_resp\".properties.accounts.items.properties.flags.items.enum\n\n\nNew values\n.components.schemas.\"v0.0.41_openapi_assocs_resp\".properties.associations.items.properties.flags.items.enum\n\n\nNew values\n.components.schemas.\"v0.0.41_job_desc_msg\".properties.shared.items.enum.topo\n\n\nNew values\n.components.schemas.\"v0.0.41_openapi_job_info_resp\".properties.shared.items.enum.topo\n\n\nNew values\n.components.schemas.\"v0.0.41_job_desc_msg\".properties.exclusive.items.enum.topo\n\n\nNew values\n.components.schemas.\"v0.0.41_openapi_job_info_resp\".properties.exclusive.items.enum.topo\n\n\nAdd fields to job description\n\n\nnew field\n\n\t\t.components.schemas.\"v0.0.41_job_desc_msg\".properties.segment_size\n\t\n\n\nnew field\n\n\t\t.components.schemas.\"v0.0.41_job_desc_msg\".properties.resv_mpi_ports\n\t\n\n\ndata_parser/v0.0.40\nModified Fields\n\n\nModified field\n.components.schemas.\"v0.0.40_kill_jobs_msg\".properties.flags.items.enum\n\n\nopenapi/v0.0.39 (src/plugins/openapi/v0.0.39/openapi.json)\nDeprecation notice\nThe v0.0.39 plugin has now been marked as deprecated.\n\nopenapi/v0.0.38 (src/plugins/openapi/v0.0.38/openapi.json)\nRemoval notice\nThe v0.0.38 plugin has now been removed.\n\nopenapi/slurmdbd\nGenerated openapi.json will only populate \"deprecated\" fields if true.\nReduced number of \"$ref\" fields. Where there was only 1 reference for the\n\tschema, the \"$ref\" schema will be directly populated in place. Content\n\tof the schemas will be relatively unchanged.\nopenapi/dbv0.0.39 (src/plugins/openapi/dbv0.0.39/openapi.json)\nDeprecation notice\nThe dbv0.0.39 plugin has now been marked as deprecated.\n\nopenapi/dbv0.0.38 (src/plugins/openapi/dbv0.0.38/openapi.json)\nRemoval notice\nThe dbv0.0.38 plugin has now been removed.\n\nSlurm 23.11.7\ndata_parser/v0.0.40\nAdd missing schema for job states query\n\n\nmodified field\n.paths.\"/slurm/v0.0.40/jobs/state\".get.responses.\"200\".content\n\n\nmodified field\n.paths.\"/slurm/v0.0.40/jobs/state\".get.responses.\"default\".content\n\n\nnew schema\n.components.schemas.\"v0.0.40_openapi_job_state_resp\"\n\n\nSlurm 23.11.6\ndata_parser/v0.0.40\nAdd new endpoint\n\n\nnew path\n.paths.\"/slurm/v0.0.40/jobs\".delete\n\n\nnew path\n.components.schemas.\"v0.0.40_openapi_kill_jobs_resp\"\n\n\nnew path\n.components.schemas.\"v0.0.40_kill_jobs_msg\"\n\n\nnew path\n.components.schemas.\"v0.0.40_kill_jobs_resp_msg\"\n\n\nModified field\n.components.schemas.\"v0.0.40_kill_jobs_msg\".properties.flags.items.enum\n\n\nSlurm 23.11.5\ndata_parser/v0.0.40\nAdd field\n\n\npath\n.components.schemas.\"v0.0.40_job_desc_msg\".properties.cores_per_socket\n\n\nSlurm 23.11.2\ndata_parser/v0.0.40\nAdded field\n\n\nbefore\n\n\n.components.schemas.\"v0.0.40_job\".properties.hold\n.components.schemas.\"v0.0.40_job\".properties.priority.description\n.components.schemas.\"v0.0.40_job_desc_msg\".properties.hold\n.components.schemas.\"v0.0.40_job_desc_msg\".properties.priority.description\n.components.schemas.\"v0.0.40_job_info\".properties.hold\n.components.schemas.\"v0.0.40_job_info\".properties.priority.description\n\n\n\n\nAdd GRES_ONE_TASK_PER_SHARING and GRES_MULT_TASKS_PER_SHARING flags to /job_info, /job_desc_msg output\n\n\nNew property\n\n\n.components.schemas.\"v0.0.40_job_info\".properties.flags\n.components.schemas.\"v0.0.40_job_desc_msg\".properties.flags\n\n\n\n\nSlurm 23.11.1\ndata_parser/v0.0.40\nRenamed field\n\n\nbefore\n\n\n.components.schemas.v0.0.39_job_mem_per_node\n\n\n\n\nafter\n\n\n.components.schemas.v0.0.39_mem_per_node\n\n\n\n\nRenamed field\n\n\nbefore\n\n\n.components.schemas.v0.0.39_job_mem_per_cpu\n\n\n\n\nafter\n\n\n.components.schemas.v0.0.39_mem_per_cpu\n\n\n\n\nopenapi/slurmctld\nAdd field\n\n\nadd\n\n\n.components.schemas.\"v0.0.40_openapi_meta\".properties.slurm.properties.cluster\n\n\n\n\nadd\n\n\n.components.schemas.\"v0.0.40_partition_info\".properties.defaults.properties.partition_memory_per_cpu\n\n\n\n\nadd\n\n\n.components.schemas.\"v0.0.40_partition_info\".properties.defaults.properties.partition_memory_per_node\n\n\n\n\nadd\n\n\n.components.schemas.\"v0.0.40_partition_info\".properties.maximums.properties.partition_memory_per_cpu\n\n\n\n\nadd\n\n\n.components.schemas.\"v0.0.40_partition_info\".properties.maximums.properties.oversubscribe\n\n\n\n\nadd\n\n\n.components.schemas.\"v0.0.40_partition_info\".properties.maximums.properties.partition_memory_per_node\n\n\n\n\nopenapi/slurmdbd\nAdd field\n\n\nadd\n\n\n.components.schemas.\"v0.0.40_openapi_meta\".properties.slurm.properties.cluster\n\n\n\n\nSlurm 23.11.0\nopenapi/slurmctld\nNew plugin\nThe openapi/slurmctld plugin forked from the openapi/v0.0.39 plugin.\n\nSwap job exit codes to verbose output\n\n\nField modified\n\n\t\t.components.schemas.\"v0.0.40_job_info\".properties.exit_code\n\t\n\n\nField modified\n\n\t\t.components.schemas.\"v0.0.40_job_info\".properties.derived_exit_code\n\t\n\n\nRemove the \"required/memory\" field. It would dump very large integers if\n    the job required per cpu memory but dump correct amounts for per node\n    memory.\n\n\nField removed\n\n\t\t.component.schemas.\"v0.0.40_job\".properties.required.properties.memory\n\t\n\n\nAdd timestamps for last change to data or generation times.\n\n\nFields added\n\n\t\t.components.schemas.\"v0.0.40_openapi_job_info_resp\".properties.last_backfill\n\t\t.components.schemas.\"v0.0.40_openapi_job_info_resp\".properties.last_update\n\t\t.components.schemas.\"v0.0.40_openapi_nodes_resp\".properties.last_update\n\t\t.components.schemas.\"v0.0.40_openapi_partition_resp\".properties.last_update\n\t\t.components.schemas.\"v0.0.40_openapi_reservation_resp\".properties.last_update\n\t\t.components.schemas.\"v0.0.40_openapi_licenses_resp\".properties.last_update\n\t\n\n\nopenapi/v0.0.38 (src/plugins/openapi/v0.0.38/openapi.json)\nDeprecation notice\nThe v0.0.38 plugin has now been marked as deprecated.\n\nopenapi/v0.0.37 (src/plugins/openapi/v0.0.37/openapi.json)\nRemoval notice\nThe v0.0.37 plugin has now been removed.\n\nopenapi/slurmdbd\nNew plugin\nThe openapi/slurmdbd plugin forked from the openapi/dbv0.0.39 plugin.\n\nSwap job exit codes to process exit codes\n\n\nField modified\n\n\t\t.components.schemas.\"v0.0.40_job\".properties.exit_code\n\t\n\n\nField modified\n\n\t\t.components.schemas.\"v0.0.40_job\".properties.derived_exit_code\n\t\n\n\nField modified\n\n\t\t.components.schemas.\"v0.0.40_step\".properties.exit_code\n\t\n\n\nSwitch StepID field to be string to match CLI format\n\n\nField modified\n\n\t\t.components.schemas.\"v0.0.40_step\".properties.step\n\t\n\n\nAdd fields to assocations\n\n\nAdded\n\n\t\t.components.schemas.\"v0.0.40_assoc\".properties.accounting\n\t\n\n\nAdded\n\n\t\t.components.schemas.\"v0.0.40_assoc\".properties.id\n\t\n\n\nAdded\n\n\t\t.components.schemas.\"v0.0.40_assoc\".properties.comment\n\t\n\n\nRemoved field from assocations\n\n\nRemoved\n\n\t\t.components.schemas.\"v0.0.40_assoc\".properties.usage\n\t\n\n\nAdd new /accounts_association endpoint\n\n\nadd\n\n\n.components.schemas.\"v0.0.40_openapi_accounts_add_cond_resp_str\"\n.components.schemas.\"v0.0.40_openapi_accounts_add_cond_resp\"\n.paths.\"/slurmdb/v0.0.40/accounts_association\"\n\n\n\n\nAdd new /users_association endpoint\n\n\nadd\n\n\n.components.schemas.\"v0.0.40_openapi_users_add_cond_resp_str\"\n.components.schemas.\"v0.0.40_openapi_users_add_cond_resp\"\n.paths.\"/slurmdb/v0.0.40/users_association\"\n\n\n\n\nopenapi/dbv0.0.38 (src/plugins/openapi/dbv0.0.38/openapi.json)\nDeprecation notice\nThe dbv0.0.38 plugin has now been marked as deprecated.\n\nopenapi/dbv0.0.37 (src/plugins/openapi/dbv0.0.37/openapi.json)\nRemoval notice\nThe dbv0.0.37 plugin has now been removed.\n\nSlurm 23.02.6\nopenapi/v0.0.39\nCorrect path for responses types\n\n\nUpdated paths\n\n\t\t.paths.\"/slurm/v0.0.39/licenses\".get.responses.default\n\t\t.paths.\"/slurm/v0.0.39/job/{job_id}\".get.responses.default\n\t\n\n\nopenapi/dbv0.0.39\nSwitch integer to have NO_VAL tagging to allow for complex values.\n\n\nField modified\n\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.per.properties.count\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.active\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.accruing\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.total\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.grace_time\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.active_jobs.properties.accruing\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.active_jobs.properties.count\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.wall_clock.properties.per.properties.qos\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.wall_clock.properties.per.properties.job\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.jobs.properties.active_jobs.properties.account\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.jobs.properties.active_jobs.properties.user\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.jobs.properties.per.properties.account\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.accruing.properties.per.properties.account\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.accruing.properties.per.properties.user\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.min.properties.priority_threshold\n\t\n\n\nSlurm 23.02.5\nopenapi/v0.0.39\nAdd missing fields\n\n\npath\n\n\t\t.components.schemas.\"v0.0.39_job_desc_msg\".properties.hold\n\t\t.components.schemas.\"v0.0.39_job_desc_msg\".properties.priority\n\t\t.components.schemas.\"v0.0.39_job_desc_msg\".properties.required\n\t\t.components.schemas.\"v0.0.39_cron_entry\".properties.line.properties.start\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.nodes.properties.allowed_allocation\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.nodes.properties.configured\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.accounts.properties.allowed\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.qos.properties.allowed\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.qos.properties.deny\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.tres.properties.billing_weights\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.cpus.properties.task_binding\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.default.properties.memory_per_cpu\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.default.properties.time\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.maximums.properties.cpus_per_node\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.maximums.properties.cpus_per_socket\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.maximums.properties.memory_per_cpu\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.maximums.properties.nodes\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.maximums.properties.shares\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.maximums.properties.time\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.priority.properties.job_factor\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.timeouts.properties.resume\n\t\t.components.schemas.\"v0.0.39_job_info\".properties.hold\n\t\n\n\nAdd required/memory_per_cpu and required/memory_per_node to v0.0.39_job to\n\tdifferientate between jobs that require memory per node or per CPU.\n\tJobs that required per cpu memory will dump very large integers due to\n\tinternal bit packing required/memory but dump correct amounts for per\n\tnode memory.\n\n\nField added\n\n\t\t\t.components.schemas.\"v0.0.39_job\".properties.required.properties.mem_per_cpu\n\t\t\n\n\nField added\n\n\t\t\t.components.schemas.\"v0.0.39_job\".properties.required.properties.mem_per_node\n\t\t\n\n\nopenapi/dbv0.0.39\nAdd missing fields\n\n\npath\n\n\t\t.components.schemas.\"v0.0.39_stats_rpc\".properties.time.properties.average\n\t\t.components.schemas.\"v0.0.39_stats_user\".properties.time.properties.average\n\t\t.components.schemas.\"v0.0.39_user\".properties.default.properties.account\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.tres\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.active\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.accruing\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.total\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.per.properties.count\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.per.properties.accruing\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.per.properties.submitted\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.grace_time\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.jobs\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.factor\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.min.properties.priority_threshold\n\t\t.components.schemas.\"v0.0.39_qos\".properties.preempt.properties.list\n\t\t.components.schemas.\"v0.0.39_qos\".properties.preempt.properties.mode\n\t\t.components.schemas.\"v0.0.39_job\".properties.hold\n\t\t.components.schemas.\"v0.0.39_job\".properties.comment.properties.administrator\n\t\t.components.schemas.\"v0.0.39_job\".properties.comment.properties.job\n\t\t.components.schemas.\"v0.0.39_job\".properties.array.properties.job_id\n\t\t.components.schemas.\"v0.0.39_job\".properties.array.properties.limits\n\t\t.components.schemas.\"v0.0.39_job\".properties.array.properties.task_id\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.elapsed\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.eligible\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.end\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.start\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.submission\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.suspended\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.system\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.limit\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.total\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.user.properties.seconds\n\t\t.components.schemas.\"v0.0.39_job\".properties.het.properties.job_id\n\t\t.components.schemas.\"v0.0.39_job\".properties.required.properties.CPUs\n\t\t.components.schemas.\"v0.0.39_job\".properties.reservation.properties.id\n\t\t.components.schemas.\"v0.0.39_job\".properties.state.properties.current\n\t\t.components.schemas.\"v0.0.39_job\".properties.tres.properties.allocated\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.elapsed\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.end\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.start\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.suspended\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.system\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.total\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.user.properties.seconds\n\t\t.components.schemas.\"v0.0.39_step\".properties.nodes.properties.count\n\t\t.components.schemas.\"v0.0.39_step\".properties.nodes.properties.range\n\t\t.components.schemas.\"v0.0.39_step\".properties.CPU.properties.requested_frequency\n\t\t.components.schemas.\"v0.0.39_step\".properties.statistics.properties.CPU\n\t\t.components.schemas.\"v0.0.39_step\".properties.step.properties.id\n\t\t.components.schemas.\"v0.0.39_step\".properties.tres.properties.requested\n\t\t.components.schemas.\"v0.0.39_step\".properties.tres.properties.consumed\n\t\t.components.schemas.\"v0.0.39_cluster_rec\".properties.controller.properties.host\t\n\n\nSwitch integer to have NO_VAL tagging to allow for complex values.\n\n\nField modified\n\n\t\t.components.schemas.\"v0.0.39_acct_gather_energy\".properties.current_watts\n\t\n\n\nField modified\n\n\t\t.components.schemas.\"v0.0.39_qos\".properties.priority\n\t\n\n\nSlurm 23.02.4\nopenapi/v0.0.39\nTag job description environment field as required\n\n\nField add\n\n\t\t.components.schemas.\"v0.0.39_job_desc_msg\".required\n\t\n\n\nAdd status schema to default\n\n\nField added\n\n\t\t.paths.\"/licenses/\".get.responses.default\n\t\n\n\nField added\n\n\t\t.paths.\"/job/{job_id}\".get.responses.default\n\t\n\n\nTag derived_exit_code and exit_code as UINT32_NO_VAL to avoid 4294967295 on still running jobs.\n\n\nField changed\n\n\t\t.components.schemas.\"v0.0.39_job_info\".properties.derived_exit_code\n\t\n\n\nField changed\n\n\t\t.components.schemas.\"v0.0.39_job_info\".properties.exit_code\n\t\n\n\n\nSlurm 23.02.3\nopenapi/v0.0.39\nCorrect invalid reference in status schema\n\n\nField modified\n\n\t\t.components.schemas.status.properties.errors\n\t\n\n\nRevert removal of Job description \"oversubscribe\" field\n\n\nNew field\n\n\t\t       .components.schemas.\"v0.0.39_job_desc_msg\".properties.oversubscribe\n       \n\n\nNew field\n\n\t\t       .components.schemas.\"v0.0.39_job_info\".properties.oversubscribe\n       \n\n\nSlurm 23.02.2\nopenapi/v0.0.39\nRevert format change for job updates\n\n\nModify field\n\n\t\t\t.paths.\"/job/{job_id}\".post.requestBody.content\n\t\n\n\nRevert removal of Job description \"exclusive\" field\n\n\nNew field\n\n                       .components.schemas.\"v0.0.39_job_desc_msg\".properties.exclusive\n       \n\n\nNew field\n\n                       .components.schemas.\"v0.0.39_job_info\".properties.exclusive\n       \n\n\nopenapi/dbv0.0.39\nRevert removal of Job description \"exclusive\" field\n\n\nNew field\n\n                       .components.schemas.\"v0.0.39_job_desc_msg\".properties.exclusive\n       \n\n\nNew field\n\n                       .components.schemas.\"v0.0.39_job_info\".properties.exclusive\n       \n\n\nSlurm 23.02.0\nopenapi/v0.0.39\nNew plugin\nThe v0.0.39 plugin forked from the v0.0.38 plugin.\n\nNew Schema\nPlugin has been converted to new data_parser/v0.0.39 plugin for handling\nparsing and dumping of all data structures and automatic data structure schema\ngeneration. There have been significant changes to all schemas and care needs\nto be take to update data sent and expected from requests while porting to new\nplugin. Developers are advised to review the generated OpenAPI output directly\nwhile updating.\n\nNew methods for node queries\nDELETE and POST methods are now supported for /slurm/v0.0.39/node/{node_id}\npaths.\n\nopenapi/v0.0.37\nDeprecation notice\nThe v0.0.37 plugin has now been marked as deprecated.\n\nopenapi/v0.0.36\nRemoval notice\nThe v0.0.36 plugin has now been removed.\n\nopenapi/dbv0.0.39\nNew plugin\nThe dbv0.0.39 plugin forked from the dbv0.0.38 plugin.\n\nNew Schema\nPlugin has been converted to new data_parser/v0.0.39 plugin for handling\nparsing and dumping of all data structures and automatic data structure schema\ngeneration. There have been significant changes to all schemas and care needs\nto be take to update data sent and expected from requests while porting to new\nplugin. Developers are advised to review the generated OpenAPI output directly\nwhile updating.\n\nopenapi/dbv0.0.37\nDeprecation notice\nThe dbv0.0.37 plugin has now been marked as deprecated.\n\nopenapi/dbv0.0.36\nRemoval notice\nThe dbv0.0.36 plugin has now been removed.\n\nSlurm 22.05.8\nopenapi/v0.0.38\nFix invalid type for nice\n\n\nField modified\n\n\t\t\t.components.schemas.\"v0.0.38_job_properties\".properties.nice\n\t\n\n\nopenapi/dbv0.0.38\nPopulate POST /tres/ body\n\n\nnew path\n.paths.\"/tres/\".post.requestBody\n\n\nnew path\n.components.schemas.\"dbv0.0.38_tres_update\"\n\n\nnew field\n.components.schemas.\"dbv0.0.38_update_qos\".type\n\n\nAdd missing type field\n\n\nnew field\n.components.schemas.\"dbv0.0.38_update_account\".type\n\n\nnew field\n.components.schemas.\"dbv0.0.38_update_users\".type\n\n\nAdd missing requestBody field\n\n\nnew path\n.paths.\"/associations/\".post.requestBody\n\n\nnew path\n.paths.\"/wckeys/\".post.requestBody\n\n\nAdd missing requestBody field\n\n\nnew field\n.paths.\"/config\".post.requestBody\n\n\nnew field\n.components.schemas.\"dbv0.0.38_set_config\"\n\n\nCorrect type of field accounts->accounting in wckeys\n\n\nnew schema\n.components.schemas.\"dbv0.0.38_accounting\"\n\n\nnew field\n.components.schemas.\"dbv0.0.38_wckey\".properties.accounting\n\n\nremoved field\n.components.schemas.\"dbv0.0.38_wckey\".properties.accounts\n\n\nUse \"QOS\" in dbv0.0.38_qos_info\n\n\nprevious path\n.components.schemas.\"dbv0.0.38_qos_info\".properties.qos\n\n\nnew path\n.components.schemas.\"dbv0.0.38_qos_info\".properties.QOS\n\n\nAdd oversubscribe option to job submission properties\n\n\nField added\n\n\t\t\t.components.schemas.\"v0.0.38_job_properties\".properties.oversubscribe\n\t\n\n\nSlurm 22.05.7\nopenapi/v0.0.38\nAdd missing fields\n\n\nField added\n\n\t\t\t.components.schemas.\"v0.0.38_partition\".properties.maximum_memory_per_cpu\n\t\n\n\nField added\n\n\t\t\t.components.schemas.\"v0.0.38_partition\".properties.default_memory_per_node\n\t\n\n\nSlurm 22.05.6openapi/v0.0.38Fix misnamed reference to prior version\n\nField modified\n\n\t\t\t.components.schemas.\"dbv0.0.38_tres_info\".properties.tres\n\t\n\nSlurm 22.05.4openapi/v0.0.38Add missing field to job properties\n\nAdd field prefer to jobs\n\n\t\t\t.components.schemas.\"v0.0.38_job_properties\".properties.prefer\n\t\n\nSlurm 22.05.0openapi/dbv0.0.38add plugin\n\nnew dbv0.0.38 openapi plugin\n\n\nclone of existing dbv0.0.37 openapi plugin\nall paths renamed from dbv0.0.37 to dbv0.0.38\n\n\n\nAdd POST method for /associations\n\n\nnew path\n.paths.\"/associations/\".post\n\n\nnew path\n.components.schemas.\"dbv0.0.38_response_associations\"\n\n\nCorrect placement of step TRES\n\n\nprevious path\n.components.schemas.\"dbv0.0.38_job_step\".properties.step.properties.tres\n\n\nnew path\n.components.schemas.\"dbv0.0.38_job_step\".properties.tres\n\n\nAdd association fields\n\n\nnew fields\n\n\n.components.schemas.\"dbv0.0.38_association\".properties.is_default\n.components.schemas.\"dbv0.0.38_association\".properties.max.tres.group.minutes\n.components.schemas.\"dbv0.0.38_association\".properties.max.tres.group.active\n.components.schemas.\"dbv0.0.38_association\".properties.max.jobs.active\n.components.schemas.\"dbv0.0.38_association\".properties.max.jobs.accruing\n.components.schemas.\"dbv0.0.38_association\".properties.max.jobs.total\n.components.schemas.\"dbv0.0.38_association\".properties.max.tres.minutes.per.job\n\n\n\n\nAdd error response contents\n\n\nnew fields\n\n\n.paths.\"/account/{account_name}\"[].responses.default[]\n.paths.\"/accounts/\"[].responses.default[]\n.paths.\"/association/\"[].responses.default[]\n.paths.\"/associations/\"[].responses.default[]\n.paths.\"/cluster/{cluster_name}\"[].responses.default[]\n.paths.\"/clusters/\"[].responses.default[]\n.paths.\"/config/\"[].responses.default[]\n.paths.\"/diag/\"[].responses.default[]\n.paths.\"/job/{job_id}\"[].responses.default[]\n.paths.\"/jobs/\"[].responses.default[]\n.paths.\"/qos/{qos_name}/\"[].responses.default[]\n.paths.\"/qos/\"[].responses.default[]\n.paths.\"/tres/\"[].responses.default[]\n.paths.\"/users/\"[].responses.default[]\n.paths.\"/user/{user_name}\"[].responses.default[]\n.paths.\"/wckeys/\"[].responses.default[]\n.paths.\"/wckey/{wckey}\"[].responses.default[]\n.components.schemas.\"dbv0.0.38_meta\"\n\n\n\n\nMove incorrectly named field\n\n\nold path\n.components.schemas.\"dbv0.0.38_qos\".properties.limits.max.jobs.per.account\n\n\nnew path\n.components.schemas.\"dbv0.0.38_qos\".properties.limits.max.jobs.active_jobs.per.account\n\n\nMove incorrectly named field\n\n\nold path\n.components.schemas.\"dbv0.0.38_qos\".properties.limits.properties.max.properties.jobs.properties.per.properties.user\n\n\nnew path\n.components.schemas.\"dbv0.0.38_qos\".properties.limits.properties.max.properties.jobs.properties.active_jobs.properties.per.properties.user\n\n\nAdd QOS fields\n\n\nnew fields\n\n\n.components.schemas.\"dbv0.0.38_qos\".properties.limits.properties.factor\n.components.schemas.\"dbv0.0.38_qos\".properties.limits.properties.max.properties.accruing.properties.per.properties.account\n.components.schemas.\"dbv0.0.38_qos\".properties.limits.properties.max.properties.accruing.properties.per.properties.user\n\n\n\n\nAdd diag fields\n\n\nnew fields\n\n\n.components.schemas.\"v0.0.38_diag\".bf_table_size\n.components.schemas.\"v0.0.38_diag\".bf_table_size_mean\n\n\n\n\nSplit up token and user\n\n\nchange array values\n\n\n.security\n\n\n\n\nAdd meta entry\n\n\nfield added\n\n\n.components.schemas.\"dbv0.0.38_diag\".properties.meta\n.components.schemas.\"dbv0.0.38_account_info\".properties.meta\n.components.schemas.\"dbv0.0.38_response_account_delete\".properties.meta\n.components.schemas.\"dbv0.0.38_response_wckey_add\".properties.meta\n.components.schemas.\"dbv0.0.38_wckey_info\".properties.meta\n.components.schemas.\"dbv0.0.38_response_wckey_delete\".properties.meta\n.components.schemas.\"dbv0.0.38_response_cluster_add\".properties.meta\n.components.schemas.\"dbv0.0.38_response_cluster_delete\".properties.meta\n.components.schemas.\"dbv0.0.38_response_user_update\".properties.meta\n.components.schemas.\"dbv0.0.38_user_info\".properties.meta\n.components.schemas.\"dbv0.0.38_response_user_delete\".properties.meta\n.components.schemas.\"dbv0.0.38_response_association_delete\".properties.meta\n.components.schemas.\"dbv0.0.38_associations_info\".properties.meta\n.components.schemas.\"dbv0.0.38_qos_info\".properties.meta\n.components.schemas.\"dbv0.0.38_response_qos_delete\".properties.meta\n.components.schemas.\"dbv0.0.38_response_associations\".properties.meta\n.components.schemas.\"dbv0.0.38_response_tres\".properties.meta\n.components.schemas.\"dbv0.0.38_tres_info\".properties.meta\n.components.schemas.\"dbv0.0.38_job_info\".properties.meta\n.components.schemas.\"dbv0.0.38_config_info\".properties.meta\n.components.schemas.\"dbv0.0.38_account_response\".properties.meta\n.components.schemas.\"dbv0.0.38_config_response\".properties.meta\n.components.schemas.\"dbv0.0.38_errors\"\n\n\n\n\nAdd missing response field\n\n\nnew fields\n\n\n.components.schemas.\"dbv0.0.38_response_association_delete\".properties.removed_associations\n.components.schemas.\"dbv0.0.38_error\".properties.error_number\n.components.schemas.\"dbv0.0.38_error\".properties.source\n.components.schemas.\"dbv0.0.38_error\".properties.description\n.components.schemas.\"/clusters/\".properties.post.properties.requestBody\n.components.schemas.\"dbv0.0.38_clusters_properties\"\n\n\n\n\nSwitch field from object to array\n\n\nmodified field\n\n\n.components.schemas.\"dbv0.0.38_user\".properties.associations\n\n\n\n\nAdd missing field\n\n\nnew field\n\n\n.components.schemas.\"dbv0.0.38_qos\".properties.limits.properties.max.properties.tres.properties.minutes.properties.per.properties.qos\n.components.schemas.\"dbv0.0.38_qos\".properties.name\n\n\n\n\nCorrect field type to reference\n\n\nmodified field\n\n\n.components.schemas.\"dbv0.0.38_config_info\".properties.tres\n.components.schemas.\"dbv0.0.38\".properties.het.properties.job_id\n.components.schemas.\"dbv0.0.38\".properties.het.properties.job_offset\n.components.schemas.\"dbv0.0.38_job_step\".properties.task\n\n\n\n\nAdd requestBody field and associated schema\n\n\nnew fields\n\n\n.components.schemas.\"dbv0.0.38_update_users\"\n.paths.\"/users/\".post.requestBody'\n.components.schemas.\"dbv0.0.38_update_accounts\"\n.paths.\"/accounts/\".post.requestBody'\n\n\n\n\nCorrect parameter styles to \"form\"\n\n\nmodified fields\n\n\n.paths.\"/job/{job_id}\"[].get.parameters[]|select(.name=\"job_id)\"\n\n\n\n\nChange operationId naming schema to include url path\n\n\nmodified fields\n\n\n.paths[][].operationId\n\n\n\n\nFix issue where association's QOS list consisted of IDs instead of names\n\n\nmodified fields\n\n\n.components.schemas.\"dbv0.0.38_association\".properties.qos\n\n\n\n\nAdd POST method for /qos\n\n\nnew method\n.paths.\"/qos/\".post\n\n\nnew path\n.components.schemas.\"dbv0.0.38_response_qos\"\n\n\nnew path\n.components.schemas.\"dbv0.0.38_update_qos\"\n\n\nMove response fields in dbv0.0.37_diag under \"statistics\"\n\n\nnew parent field\n\n\n.components.schemas.\"dbv0.0.38_diag\".properties.statistics\n\n\n\n\nsubordinated fields\n\n\n.components.schemas.\"dbv0.0.38_diag\".properties.time_start\n.components.schemas.\"dbv0.0.38_diag\".properties.rollups\n.components.schemas.\"dbv0.0.38_diag\".properties.RPCs\n.components.schemas.\"dbv0.0.38_diag\".properties.users\n\n\n\n\nAllow strings for JobIds instead of only numerical JobIds.\n\n\nChange job_id parameter to string\n\n\n.paths.\"/job/{job_id}\".get.parameters[].schema\n\n\n\n\nAdd with_deleted input parameter to GET /user, /users\n\n\nNew parameter\n\n\n.paths.\"/user/{user_name}\".get.parameters[] | select(.name==\"with_deleted\")\n.paths.\"/users/\".get.parameters[] | select(.name==\"with_deleted\")\n\n\n\n\nAdd deleted flag to /user, /users output\n\n\nNew property\n\n\n.components.schemas.\"dbv0.0.38_user\".properties.flags\n\n\n\n\nAdd with_deleted input parameter to GET /qos\n\n\nNew parameter\n\n\n.paths.\"/qos/\".get.parameters[] | select(.name==\"with_deleted\")\n.paths.\"/qos/{qos_name}\".get.parameters[] | select(.name==\"with_deleted\")\n\n\n\n\nAdd with_deleted input parameter to GET /account, /accounts\n\n\nNew parameter\n\n\n.paths.\"/account/{account_name}\".get.parameters[] | select(.name==\"with_deleted\")\n.paths.\"/accounts/\".get.parameters[] | select(.name==\"with_deleted\")\n\n\n\n\nAdd container field to job description\n\n\nnew fields\n\n\n.components.schemas.\"dbv0.0.38_job\".properties.container\n\n\n\n\nEnforce limit to only DELETE or GET a single association instead of using\nrequired parameters.\n\n\nchanged value\n\n\n.paths.\"/association/\".get.parameters[].required\n.paths.\"/association/\".delete.parameters[].required\n\n\n\n\nAdd filter parameters to GET /associations\n\n\nnew method\n\n\n.paths.\"/associations/\".get.parameters\n\n\n\n\nopenapi/dbv0.0.36\nDeprecation notice\nThe dbv0.0.36 plugin has now been marked as deprecated.\n\nopenapi/v0.0.38\nadd plugin\n\n\nnew v0.0.38 openapi plugin\n\n\nclone of existing v0.0.37 openapi plugin\nall paths renamed from v0.0.37 to v0.0.38\n\n\n\n\nAllow strings for JobIds instead of only numerical JobIds.\n\n\nChange job_id parameter to string\n\n\n.paths.\"/job/{job_id}\".get.parameters[].schema\n.paths.\"/job/{job_id}\".post.parameters[].schema\n.paths.\"/job/{job_id}\".delete.parameters[].schema\n\n\n\n\nCorrect multiple type mistakes\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.array_job_id\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.array_task_id\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.array_max_tasks\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.association_id\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.billable_tres\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.deadline\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.delay_boot\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.derived_exit_code\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.group_id\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.job_id\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.last_sched_evaluation\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.max_cpus\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.max_nodes\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.nice\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.tasks_per_core\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.tasks_per_socket\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.tasks_per_board\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.cpus\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.node_count\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.tasks\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.het_job_id\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.het_job_offset\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.memory_per_node\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.memory_per_cpu\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.minimum_cpus_per_node\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.minimum_tmp_disk_per_node\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.priority\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.restart_cnt\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.sockets_per_board\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.sockets_per_node\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.time_limit\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.time_minimum\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.threads_per_core\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.user_id\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.allocated_nodes\n\n\nmodified entry\n.components.schemas.\"v0.0.38_job_response_properties\".properties.cpus\n\n\nFix errant space after JOB_CPUS_SET flag.\n\n\nFix response.\n\n\n.components.schemas.\"v0.0.37_job_response_properties\".properties.flags\n\n\n\n\nAdd new /licenses endpoint\n\n\nadd\n\n\n.components.schemas.\"v0.0.38_license\"\n.components.schemas.\"v0.0.38_licenses\"\n.paths.\"/slurm/v0.0.38/licenses\"\n\n\n\n\nAdd meta entry\n\n\nfield added\n\n\n.components.schemas.\"v0.0.38_diag\".properties.meta\n.components.schemas.\"v0.0.38_pings\".properties.meta\n.components.schemas.\"v0.0.38_partitions_response\".properties.meta\n.components.schemas.\"v0.0.38_reservations_response\".properties.meta\n.components.schemas.\"v0.0.38_job_submission_response\".properties.meta\n.components.schemas.\"v0.0.38_jobs_response\".properties.meta\n.components.schemas.\"v0.0.38_nodes_response\".properties.meta\n.components.schemas.\"v0.0.38_errors\"\n\n\n\n\nAdd error response contents\n\n\nnew fields\n\n\n.paths.\"/diag/\"[].responses.default[]\n.paths.\"/ping/\"[].responses.default[]\n.paths.\"/jobs/\"[].responses.default[]\n.paths.\"/job/{job_id}\"[].responses.default[]\n.paths.\"/job/submit\"[].responses.default[]\n.paths.\"/nodes/\"[].responses.default[]\n.paths.\"/node/{node_name}\"[].responses.default[]\n.paths.\"/partitions/\"[].responses.default[]\n.paths.\"/partition/{partition_name}\"[].responses.default[]\n.paths.\"/reservations/\"[].responses.default[]\n.paths.\"/reservation/{reservation_name}\"[].responses.default[]\n\n\n\n\nRename errno to error_number\n\n\nrename\n\n\n.components.schemas.\"dbv0.0.38_error\".properties.errnum\n.components.schemas.\"dbv0.0.38_error\".properties.error_number\n\n\n\n\nCorrect parameter styles to \"form\"\n\n\nmodified fields\n\n\n.paths.\"/jobs/\"[].parameters[]|select(.name=\"update_type)\"\n.paths.\"/job/{job_id}\"[].get.parameters[]|select(.name=\"job_id)\"\n.paths.\"/job/{job_id}\"[].post.parameters[]|select(.name=\"job_id)\"\n.paths.\"/nodes/\"[].get.parameters[]|select(.name=\"update_time)\"\n.paths.\"/node/{node_name}\"[].get.parameters[]|select(.name=\"node_name)\"\n.paths.\"/partitions/\"[].get.parameters[]|select(.name=\"update_time)\"\n.paths.\"/partition/{partition_name}\"[].get.parameters[]|select(.name=\"partition_name)\"\n.paths.\"/partition/{partition_name}\"[].get.parameters[]|select(.name=\"update_time)\"\n.paths.\"/reservations/\"[].get.parameters[]|select(.name=\"reservation_name)\"\n.paths.\"/reservations/\"[].get.parameters[]|select(.name=\"update_time)\"\n\n\n\n\nChange operationId naming schema to include url path\n\n\nmodified fields\n\n\n.paths[][].operationId\n\n\n\n\nResponse changed to move \"cores\" into \"sockets\" to differentiate which\n\tcores and sockets are allocated. Changed from named dictionary of node\n\tnames to array containing objects with nodename set.\n\n\nUpdated response.\n\n\n.components.schemas.\"v0.0.38_job_resources\".properties.allocated_nodes\n.components.schemas.\"v0.0.38_node_allocation\"\n\n\n\n\nNew fields add to diag endpoint\n\n\nnew fields\n\n\n.components.schemas.\"v0.0.38_diag_rpcm\".rpcs_by_message_type\n.components.schemas.\"v0.0.38_diag_rpcm\".rpcs_by_user\n.components.schemas.\"v0.0.38_diag_rpcm\"\n.components.schemas.\"v0.0.38_diag_rpcu\"\n\n\n\n\nAdd container field to job description\n\n\nnew fields\n\n\n.components.schemas.\"v0.0.38_job_response_properties\".properties.container\n.components.schemas.\"v0.0.38_job_properties\".properties.container\n\n\n\n\nAdd method to delete associations using filters\n\n\nnew method\n\n\n.paths.\"/associations/\".delete\n\n\n\n\nRename response schema entry\n\n\nprevious path\n.components.schemas.\"dbv0.0.38_response_association_delete\"\n\n\nnew path\n.components.schemas.\"dbv0.0.38_response_associations_delete\"\n\n\nopenapi/v0.0.36\nDeprecation notice\nThe v0.0.36 plugin has now been marked as deprecated.\n\nopenapi/v0.0.35\nRemoval notice\nThe v0.0.35 plugin has now been removed.\n\nSlurm 21.08.8\nopenapi/dbv0.0.37\nMove response fields in dbv0.0.37_diag under \"statistics\"\n\n\nnew parent field\n\n\n.components.schemas.\"dbv0.0.37_diag\".properties.statistics\n\n\n\n\nsubordinated fields\n\n\n.components.schemas.\"dbv0.0.37_diag\".properties.time_start\n.components.schemas.\"dbv0.0.37_diag\".properties.rollups\n.components.schemas.\"dbv0.0.37_diag\".properties.RPCs\n.components.schemas.\"dbv0.0.37_diag\".properties.users\n\n\n\n\nSlurm 21.08.7\nopenapi/v0.0.37\nFix misspelling: change account_gather_freqency to account_gather_frequency\n(only the spec was wrong; the underlying code already worked with the correct spelling).\n\n\nprevious path\n.components.schemas.\"v0.0.37_job_properties\".properties.account_gather_freqency\n\n\nnew path\n.components.schemas.\"v0.0.37_job_properties\".properties.account_gather_frequency\n\n\nFix misspelling: change cluster_constraints to cluster_constraint (only the\nspec was wrong; the underlying code already worked with the correct spelling).\n\n\n\nprevious path\n.components.schemas.\"v0.0.37_job_properties\".properties.cluster_constraints\n\n\nnew path\n.components.schemas.\"v0.0.37_job_properties\".properties.cluster_constraint\n\n\nSlurm 21.08.6\nopenapi/dbv0.0.37\nAdd association fields\n\n\nnew fields\n\n\n.components.schemas.\"dbv0.0.37_association\".properties.is_default\n.components.schemas.\"dbv0.0.37_association\".properties.max.tres.group.minutes\n.components.schemas.\"dbv0.0.37_association\".properties.max.tres.group.active\n.components.schemas.\"dbv0.0.37_association\".properties.max.jobs.active\n.components.schemas.\"dbv0.0.37_association\".properties.max.jobs.accruing\n.components.schemas.\"dbv0.0.37_association\".properties.max.jobs.total\n.components.schemas.\"dbv0.0.37_association\".properties.max.tres.minutes.per.job\n\n\n\n\nMove incorrectly named field\n\n\nold path\n.components.schemas.\"dbv0.0.37_qos\".properties.limits.max.jobs.per.account\n\n\nnew path\n.components.schemas.\"dbv0.0.37_qos\".properties.limits.max.jobs.active_jobs.per.account\n\n\nMove incorrectly named field\n\n\nold path\n.components.schemas.\"dbv0.0.37_qos\".properties.limits.properties.max.properties.jobs.properties.per.properties.user\n\n\nnew path\n.components.schemas.\"dbv0.0.37_qos\".properties.limits.properties.max.properties.jobs.properties.active_jobs.properties.per.properties.user\n\n\nAdd QOS fields\n\n\nnew fields\n\n\n.components.schemas.\"dbv0.0.37_qos\".properties.limits.properties.factor\n.components.schemas.\"dbv0.0.37_qos\".properties.limits.properties.max.properties.accruing.properties.per.properties.account\n.components.schemas.\"dbv0.0.37_qos\".properties.limits.properties.max.properties.accruing.properties.per.properties.user\n\n\n\n\nSlurm 21.08.3\nopenapi/v0.0.37\nAllow strings for JobIds instead of only numerical JobIds.\n\n\nChange job_id parameter to string\n\n\n.paths.\"/job/{job_id}\".get.parameters[].schema\n.paths.\"/job/{job_id}\".post.parameters[].schema\n.paths.\"/job/{job_id}\".delete.parameters[].schema\n\n\n\n\nopenapi/dbv0.0.37\nCorrect placement of step TRES\n\n\nprevious path\n.components.schemas.\"dbv0.0.37_job_step\".properties.step.properties.tres\n\n\nnew path\n.components.schemas.\"dbv0.0.37_job_step\".properties.tres\n\n\nopenapi/dbv0.0.36\nCorrect placement of step TRES\n\n\nprevious path\n.components.schemas.\"dbv0.0.36_job_step\".properties.step.properties.tres\n\n\nnew path\n.components.schemas.\"dbv0.0.36_job_step\".properties.tres\n\n\nSlurm 21.08.0\nAll of the OpenAPI plugins have moved from \"src/slurmrestd/plugins/openapi/\"\nto \"src/plugins/openapi/\".\nopenapi/v0.0.35\nDeprecation notice\nThe v0.0.35 plugin has now been deprecated.\n\nopenapi/dbv0.0.37\nadd plugin\n\n\nnew dbv0.0.37 openapi plugin\n\n\nclone of existing dbv0.0.36 openapi plugin\nall paths renamed from v0.0.36 to v0.0.37\n\n\n\n\nrename previous -> reason\n\n\nprevious path\n.components.schemas.\"dbv0.0.37_job\".properties.state.previous\n\n\nnew path\n.components.schemas.\"dbv0.0.37_job\".properties.state.reason\n\n\nopenapi/v0.0.37\nadd plugin\n\n\nnew v0.0.37 openapi plugin\n\n\nclone of existing v0.0.36 openapi plugin\nall paths renamed from v0.0.36 to v0.0.37\n\n\n\n\nrename standard_in -> standard_input\n\n\nprevious path\n.components.schemas.\"v0.0.37_job_response_properties\".properties.standard_in\n\n\nnew path\n.components.schemas.\"v0.0.37_job_response_properties\".properties.standard_input\n\n\nrename standard_out -> standard_output\n\n\nprevious path\n.components.schemas.\"v0.0.37_job_response_properties\".properties.standard_out\n\n\nnew path\n.components.schemas.\"v0.0.37_job_response_properties\".properties.standard_output\n\n\nAdd update_time field to Jobs query to allow clients to only get jobs list based on change timestamp.\n\n\nnew path\n.paths.\"/jobs/\".get.parameters[0]\n\n\nadd api to fetch reservation(s) info\n\n\nadded path\n.paths.\"/reservations/\"\n\n\nadded path\n.paths.\"/reservation/{reservation_name}\"\n\n\nadded path\n.components.schemas.\"v0.0.37_reservation\"\n\n\nMark job environment as required\n\n\nnew path\n.components.schemas.\"v0.0.37_job_properties\".required\n\n\nCorrect preemption_mode type to list of strings\n\n\nmodify path\n.components.schemas.\"v0.0.37_partition\".properties.preemption_mode\n\n\nSet UNIX timestamps to int64 instead of string\n\n\nmodify path\n.components.schemas.\"v0.0.37_job_response_properties\".properties.accrue_time\n\n\nmodify path\n.components.schemas.\"v0.0.37_job_response_properties\".properties.eligible_time\n\n\nmodify path\n.components.schemas.\"v0.0.37_job_response_properties\".properties.end_time\n\n\nmodify path\n.components.schemas.\"v0.0.37_job_response_properties\".properties.preempt_time\n\n\nmodify path\n.components.schemas.\"v0.0.37_job_response_properties\".properties.pre_sus_time\n\n\nmodify path\n.components.schemas.\"v0.0.37_job_response_properties\".properties.resize_time\n\n\nAdd new fields to node properties\n\n\nadd path\n.components.schemas.\"v0.0.37_node\".properties.tres_used\n\n\nadd path\n.components.schemas.\"v0.0.37_node\".properties.tres_weighted\n\n\nadd path\n.components.schemas.\"v0.0.37_node\".properties.alloc_cpus\n\n\nadd path\n.components.schemas.\"v0.0.37_node\".properties.idle_cpus\n\n\nadd path\n.components.schemas.\"v0.0.37_node\".properties.alloc_memory\n\n\nadd path\n.components.schemas.\"v0.0.37_node\".properties.partitions\n\n\nreplace nodes_online with state in /partitions endpoint\n\n\nremoved path\n.components.schemas.\"v0.0.37_partitions_response\".nodes_online\n\n\nadded path\n.components.schemas.\"v0.0.37_partitions_response\".state\n\n\nAdd POST method for /associations\n\n\nnew path\n.paths.\"/associations/\".post\n\n\nnew path\n.components.schemas.\"dbv0.0.37_response_associations\"\n\n\nSlurm 20.11.5\nopenapi/dbv0.0.36\nMark job environment as required\n\n\nnew path\n.components.schemas.\"v0.0.36_job_properties\".required\n\n\nAdd state flags\n\n\nnew path\n.components.schemas.\"v0.0.37_node\".properties.state_flags\n\n\nnew path\n.components.schemas.\"v0.0.37_node\".properties.next_state_after_reboot_flags\n\n\nCorrect description for previous state\n\n\npath\n.components.schemas.\"dbv0.0.36_job\".properties.state.properties.previous\n\n\nSlurm 20.11.3\nopenapi/dbv0.0.36\nCorrect structure of dbv0.0.36_tres_list\n\n\npath\n.components.schemas.\"dbv0.0.36_tres_list\"\n\n\nAccept Job Query CSV parameters as a list (JSON array)\n\n\npath\n.paths.\"/jobs/\".get.parameters\n\n\nSlurm 20.11.1\nopenapi/v0.0.36\nCorrect name for partition field\n\n\nprevious path\n.components.schemas.\"v0.0.36_partition\".properties.\"min nodes per job\"\n\n\nnew path\n.components.schemas.\"v0.0.36_partition\".properties.\"min_nodes_per_job\"\n\n\nAdd node comment field\n\n\nnew path\n.components.schemas.\"v0.0.36_node\".properties.comment\n\n\nSlurm 20.11.0\nopenapi/dbv0.0.36\nInitial Implementation of database queries.\nopenapi/v0.0.36\nadd plugin\n\n\nnew v0.0.36 openapi plugin\n\n\nclone of existing v0.0.35 openapi plugin.\nall paths renamed from v0.0.35 to v0.0.36.\n\n\n\n\nAdd error schema\n\n\npath\n.components.schemas.\"v0.0.36_error\"\n\n\nreturn array of nodes instead of dictionary\n\n\npath\n.components.schemas.\"v0.0.36_nodes_response\"\n\n\nreturn array of partitions instead of dictionary\n\n\npath\n.components.schemas.\"v0.0.36_partitions_response\"\n\n\nreturn array of pings instead of dictionary\n\n\npath\n.components.schemas.\"v0.0.36_ping\"\n\n\nSimplify possible signals for canceling jobs\n\n\npath\n.paths.\"/job/{job_id}\".delete.parameters[1]\n\n\nSimplify exclusive for jobs submissions\n\n\npath\n.components.schemas.\"v0.0.36_job_properties\".properties.exclusive\n\n\nSimplify nodes for jobs submissions\n\n\npath\n.components.schemas.\"v0.0.36_job_properties\".properties.nodes\n\n\nChange server URL\n\n\nprevious\n.servers[0].url=/\n\n\nnew\n.servers[0].url=/slurm/v0.0.36/\n\n\nAdd operationId tag\n\n\nparent path\n.paths\n\n\nprepend every schema with v0.0.36_\n\n\nparent path\n.components.schemas\n\n\nadd tags openapi and slurm\n\n\nnew path\n.tags\n\n\nadd support contact\n\n\nnew path\n.info.contact\n\n\npopulate response from partitions query\n\n\nnew path\n.components.schemas.partitions_response\n\n\nrename \"node_info\" to \"nodes_response\"\n\n\nremoved path\n.components.schemas.node_info\n\n\nnew path\n.components.schemas.nodes_response\n\n\nadd jobs query response properties\n\n\nnew path\n.components.schemas.diag\n\n\ndefine response to diag\n\n\nnew path\n.components.schemas.diag\n\n\nadd job query response properties\n\n\nnew path\n.components.schemas.job_response_properties\n\n\nremove \"requested_node_by_index\"\n\n\nremoved path\n.components.schemas.job_response_properties.properties.requested_node_by_index\n\n\nrename \"pn_min_tmp_disk\" to \"minimum_tmp_disk_per_node\"\n\n\nremoved path\n.components.schemas.job_response_properties.properties.pn_min_tmp_disk\n\n\nnew path\n.components.schemas.job_response_properties.properties.minimum_tmp_disk_per_node\n\n\nrenamed \"nodes\" to \"node_count\"\n\n\nremoved path\n.components.schemas.job_response_properties.properties.nodes\n\n\nnew path\n.components.schemas.job_response_properties.properties.node_count\n\n\nadd get job responses schema\n\n\nnew path\n.components.schemas.job_submission_response\n\n\nuse job_submission for job_submit instead of job_properties\n\n\nchanged field\n.paths.\"/job/submit\".requestBody.content.\"application/json\"\n\n\nchanged field\n.paths.\"/job/submit\".requestBody.content.\"application/x-yaml\"\n\n\nnew path\n.components.schemas.job_submission\n\n\nSet type for \"job_properties\" schema\n\n\nnew path\n.components.schemas.v0.0.36_job_properties.properties.type\n\n\nadd security bearer\n\n\nnew path\n.security\n\n\nSlurm 20.02.0\nv0.0.35\nInitial Implementation of Slurm REST API.\n\nLast modified 25 August 2023\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Generated openapi.json will only populate \"deprecated\" fields if true.",
                "content": "Reduced number of \"$ref\" fields. Where there was only 1 reference for the\n\tschema, the \"$ref\" schema will be directly populated in place. Content\n\tof the schemas will be relatively unchanged.openapi/dbv0.0.39 (src/plugins/openapi/dbv0.0.39/openapi.json)Deprecation noticeopenapi/dbv0.0.38 (src/plugins/openapi/dbv0.0.38/openapi.json)Removal noticeSlurm 23.11.7data_parser/v0.0.40Add missing schema for job states query\n\nmodified field\n.paths.\"/slurm/v0.0.40/jobs/state\".get.responses.\"200\".content\n\n\nmodified field\n.paths.\"/slurm/v0.0.40/jobs/state\".get.responses.\"default\".content\n\n\nnew schema\n.components.schemas.\"v0.0.40_openapi_job_state_resp\"\n\nSlurm 23.11.6data_parser/v0.0.40Add new endpoint\n\nnew path\n.paths.\"/slurm/v0.0.40/jobs\".delete\n\n\nnew path\n.components.schemas.\"v0.0.40_openapi_kill_jobs_resp\"\n\n\nnew path\n.components.schemas.\"v0.0.40_kill_jobs_msg\"\n\n\nnew path\n.components.schemas.\"v0.0.40_kill_jobs_resp_msg\"\n\n\nModified field\n.components.schemas.\"v0.0.40_kill_jobs_msg\".properties.flags.items.enum\n\nSlurm 23.11.5data_parser/v0.0.40Add field\n\npath\n.components.schemas.\"v0.0.40_job_desc_msg\".properties.cores_per_socket\n\nSlurm 23.11.2data_parser/v0.0.40Added field\n\nbefore\n\n\n.components.schemas.\"v0.0.40_job\".properties.hold\n.components.schemas.\"v0.0.40_job\".properties.priority.description\n.components.schemas.\"v0.0.40_job_desc_msg\".properties.hold\n.components.schemas.\"v0.0.40_job_desc_msg\".properties.priority.description\n.components.schemas.\"v0.0.40_job_info\".properties.hold\n.components.schemas.\"v0.0.40_job_info\".properties.priority.description\n\n\n\nAdd GRES_ONE_TASK_PER_SHARING and GRES_MULT_TASKS_PER_SHARING flags to /job_info, /job_desc_msg output\n\nNew property\n\n\n.components.schemas.\"v0.0.40_job_info\".properties.flags\n.components.schemas.\"v0.0.40_job_desc_msg\".properties.flags\n\n\n\nSlurm 23.11.1data_parser/v0.0.40Renamed field\n\nbefore\n\n\n.components.schemas.v0.0.39_job_mem_per_node\n\n\n\n\nafter\n\n\n.components.schemas.v0.0.39_mem_per_node\n\n\n\nRenamed field\n\nbefore\n\n\n.components.schemas.v0.0.39_job_mem_per_cpu\n\n\n\n\nafter\n\n\n.components.schemas.v0.0.39_mem_per_cpu\n\n\n\nopenapi/slurmctldAdd field\n\nadd\n\n\n.components.schemas.\"v0.0.40_openapi_meta\".properties.slurm.properties.cluster\n\n\n\n\nadd\n\n\n.components.schemas.\"v0.0.40_partition_info\".properties.defaults.properties.partition_memory_per_cpu\n\n\n\n\nadd\n\n\n.components.schemas.\"v0.0.40_partition_info\".properties.defaults.properties.partition_memory_per_node\n\n\n\n\nadd\n\n\n.components.schemas.\"v0.0.40_partition_info\".properties.maximums.properties.partition_memory_per_cpu\n\n\n\n\nadd\n\n\n.components.schemas.\"v0.0.40_partition_info\".properties.maximums.properties.oversubscribe\n\n\n\n\nadd\n\n\n.components.schemas.\"v0.0.40_partition_info\".properties.maximums.properties.partition_memory_per_node\n\n\n\nopenapi/slurmdbdAdd field\n\nadd\n\n\n.components.schemas.\"v0.0.40_openapi_meta\".properties.slurm.properties.cluster\n\n\n\nSlurm 23.11.0openapi/slurmctldNew pluginSwap job exit codes to verbose output\n\nField modified\n\n\t\t.components.schemas.\"v0.0.40_job_info\".properties.exit_code\n\t\n\n\nField modified\n\n\t\t.components.schemas.\"v0.0.40_job_info\".properties.derived_exit_code\n\t\n\nRemove the \"required/memory\" field. It would dump very large integers if\n    the job required per cpu memory but dump correct amounts for per node\n    memory.\n\nField removed\n\n\t\t.component.schemas.\"v0.0.40_job\".properties.required.properties.memory\n\t\n\nAdd timestamps for last change to data or generation times.\n\nFields added\n\n\t\t.components.schemas.\"v0.0.40_openapi_job_info_resp\".properties.last_backfill\n\t\t.components.schemas.\"v0.0.40_openapi_job_info_resp\".properties.last_update\n\t\t.components.schemas.\"v0.0.40_openapi_nodes_resp\".properties.last_update\n\t\t.components.schemas.\"v0.0.40_openapi_partition_resp\".properties.last_update\n\t\t.components.schemas.\"v0.0.40_openapi_reservation_resp\".properties.last_update\n\t\t.components.schemas.\"v0.0.40_openapi_licenses_resp\".properties.last_update\n\t\n\nopenapi/v0.0.38 (src/plugins/openapi/v0.0.38/openapi.json)Deprecation noticeopenapi/v0.0.37 (src/plugins/openapi/v0.0.37/openapi.json)Removal noticeopenapi/slurmdbdNew pluginSwap job exit codes to process exit codes\n\nField modified\n\n\t\t.components.schemas.\"v0.0.40_job\".properties.exit_code\n\t\n\n\nField modified\n\n\t\t.components.schemas.\"v0.0.40_job\".properties.derived_exit_code\n\t\n\n\nField modified\n\n\t\t.components.schemas.\"v0.0.40_step\".properties.exit_code\n\t\n\nSwitch StepID field to be string to match CLI format\n\nField modified\n\n\t\t.components.schemas.\"v0.0.40_step\".properties.step\n\t\n\nAdd fields to assocations\n\nAdded\n\n\t\t.components.schemas.\"v0.0.40_assoc\".properties.accounting\n\t\n\n\nAdded\n\n\t\t.components.schemas.\"v0.0.40_assoc\".properties.id\n\t\n\n\nAdded\n\n\t\t.components.schemas.\"v0.0.40_assoc\".properties.comment\n\t\n\nRemoved field from assocations\n\nRemoved\n\n\t\t.components.schemas.\"v0.0.40_assoc\".properties.usage\n\t\n\nAdd new /accounts_association endpoint\n\nadd\n\n\n.components.schemas.\"v0.0.40_openapi_accounts_add_cond_resp_str\"\n.components.schemas.\"v0.0.40_openapi_accounts_add_cond_resp\"\n.paths.\"/slurmdb/v0.0.40/accounts_association\"\n\n\n\nAdd new /users_association endpoint\n\nadd\n\n\n.components.schemas.\"v0.0.40_openapi_users_add_cond_resp_str\"\n.components.schemas.\"v0.0.40_openapi_users_add_cond_resp\"\n.paths.\"/slurmdb/v0.0.40/users_association\"\n\n\n\nopenapi/dbv0.0.38 (src/plugins/openapi/dbv0.0.38/openapi.json)Deprecation noticeopenapi/dbv0.0.37 (src/plugins/openapi/dbv0.0.37/openapi.json)Removal noticeSlurm 23.02.6openapi/v0.0.39Correct path for responses types\n\nUpdated paths\n\n\t\t.paths.\"/slurm/v0.0.39/licenses\".get.responses.default\n\t\t.paths.\"/slurm/v0.0.39/job/{job_id}\".get.responses.default\n\t\n\nopenapi/dbv0.0.39Switch integer to have NO_VAL tagging to allow for complex values.\n\nField modified\n\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.per.properties.count\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.active\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.accruing\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.total\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.grace_time\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.active_jobs.properties.accruing\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.active_jobs.properties.count\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.wall_clock.properties.per.properties.qos\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.wall_clock.properties.per.properties.job\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.jobs.properties.active_jobs.properties.account\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.jobs.properties.active_jobs.properties.user\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.jobs.properties.per.properties.account\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.accruing.properties.per.properties.account\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.accruing.properties.per.properties.user\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.min.properties.priority_threshold\n\t\n\nSlurm 23.02.5openapi/v0.0.39Add missing fields\n\npath\n\n\t\t.components.schemas.\"v0.0.39_job_desc_msg\".properties.hold\n\t\t.components.schemas.\"v0.0.39_job_desc_msg\".properties.priority\n\t\t.components.schemas.\"v0.0.39_job_desc_msg\".properties.required\n\t\t.components.schemas.\"v0.0.39_cron_entry\".properties.line.properties.start\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.nodes.properties.allowed_allocation\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.nodes.properties.configured\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.accounts.properties.allowed\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.qos.properties.allowed\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.qos.properties.deny\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.tres.properties.billing_weights\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.cpus.properties.task_binding\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.default.properties.memory_per_cpu\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.default.properties.time\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.maximums.properties.cpus_per_node\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.maximums.properties.cpus_per_socket\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.maximums.properties.memory_per_cpu\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.maximums.properties.nodes\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.maximums.properties.shares\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.maximums.properties.time\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.priority.properties.job_factor\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.timeouts.properties.resume\n\t\t.components.schemas.\"v0.0.39_job_info\".properties.hold\n\t\n\nAdd required/memory_per_cpu and required/memory_per_node to v0.0.39_job to\n\tdifferientate between jobs that require memory per node or per CPU.\n\tJobs that required per cpu memory will dump very large integers due to\n\tinternal bit packing required/memory but dump correct amounts for per\n\tnode memory.\n\nField added\n\n\t\t\t.components.schemas.\"v0.0.39_job\".properties.required.properties.mem_per_cpu\n\t\t\n\n\nField added\n\n\t\t\t.components.schemas.\"v0.0.39_job\".properties.required.properties.mem_per_node\n\t\t\n\nopenapi/dbv0.0.39Add missing fields\n\npath\n\n\t\t.components.schemas.\"v0.0.39_stats_rpc\".properties.time.properties.average\n\t\t.components.schemas.\"v0.0.39_stats_user\".properties.time.properties.average\n\t\t.components.schemas.\"v0.0.39_user\".properties.default.properties.account\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.tres\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.active\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.accruing\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.total\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.per.properties.count\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.per.properties.accruing\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.per.properties.submitted\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.grace_time\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.jobs\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.factor\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.min.properties.priority_threshold\n\t\t.components.schemas.\"v0.0.39_qos\".properties.preempt.properties.list\n\t\t.components.schemas.\"v0.0.39_qos\".properties.preempt.properties.mode\n\t\t.components.schemas.\"v0.0.39_job\".properties.hold\n\t\t.components.schemas.\"v0.0.39_job\".properties.comment.properties.administrator\n\t\t.components.schemas.\"v0.0.39_job\".properties.comment.properties.job\n\t\t.components.schemas.\"v0.0.39_job\".properties.array.properties.job_id\n\t\t.components.schemas.\"v0.0.39_job\".properties.array.properties.limits\n\t\t.components.schemas.\"v0.0.39_job\".properties.array.properties.task_id\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.elapsed\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.eligible\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.end\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.start\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.submission\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.suspended\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.system\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.limit\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.total\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.user.properties.seconds\n\t\t.components.schemas.\"v0.0.39_job\".properties.het.properties.job_id\n\t\t.components.schemas.\"v0.0.39_job\".properties.required.properties.CPUs\n\t\t.components.schemas.\"v0.0.39_job\".properties.reservation.properties.id\n\t\t.components.schemas.\"v0.0.39_job\".properties.state.properties.current\n\t\t.components.schemas.\"v0.0.39_job\".properties.tres.properties.allocated\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.elapsed\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.end\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.start\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.suspended\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.system\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.total\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.user.properties.seconds\n\t\t.components.schemas.\"v0.0.39_step\".properties.nodes.properties.count\n\t\t.components.schemas.\"v0.0.39_step\".properties.nodes.properties.range\n\t\t.components.schemas.\"v0.0.39_step\".properties.CPU.properties.requested_frequency\n\t\t.components.schemas.\"v0.0.39_step\".properties.statistics.properties.CPU\n\t\t.components.schemas.\"v0.0.39_step\".properties.step.properties.id\n\t\t.components.schemas.\"v0.0.39_step\".properties.tres.properties.requested\n\t\t.components.schemas.\"v0.0.39_step\".properties.tres.properties.consumed\n\t\t.components.schemas.\"v0.0.39_cluster_rec\".properties.controller.properties.host\t\n\nSwitch integer to have NO_VAL tagging to allow for complex values.\n\nField modified\n\n\t\t.components.schemas.\"v0.0.39_acct_gather_energy\".properties.current_watts\n\t\n\n\nField modified\n\n\t\t.components.schemas.\"v0.0.39_qos\".properties.priority\n\t\n\nSlurm 23.02.4openapi/v0.0.39Tag job description environment field as required\n\nField add\n\n\t\t.components.schemas.\"v0.0.39_job_desc_msg\".required\n\t\n\nAdd status schema to default\n\nField added\n\n\t\t.paths.\"/licenses/\".get.responses.default\n\t\n\n\nField added\n\n\t\t.paths.\"/job/{job_id}\".get.responses.default\n\t\n\nTag derived_exit_code and exit_code as UINT32_NO_VAL to avoid 4294967295 on still running jobs.\n\nField changed\n\n\t\t.components.schemas.\"v0.0.39_job_info\".properties.derived_exit_code\n\t\n\n\nField changed\n\n\t\t.components.schemas.\"v0.0.39_job_info\".properties.exit_code\n\t\n\n\nSlurm 23.02.3openapi/v0.0.39Correct invalid reference in status schema\n\nField modified\n\n\t\t.components.schemas.status.properties.errors\n\t\n\nRevert removal of Job description \"oversubscribe\" field\n\nNew field\n\n\t\t       .components.schemas.\"v0.0.39_job_desc_msg\".properties.oversubscribe\n       \n\n\nNew field\n\n\t\t       .components.schemas.\"v0.0.39_job_info\".properties.oversubscribe\n       \n\nSlurm 23.02.2openapi/v0.0.39Revert format change for job updates\n\nModify field\n\n\t\t\t.paths.\"/job/{job_id}\".post.requestBody.content\n\t\n\nRevert removal of Job description \"exclusive\" field\n\nNew field\n\n                       .components.schemas.\"v0.0.39_job_desc_msg\".properties.exclusive\n       \n\n\nNew field\n\n                       .components.schemas.\"v0.0.39_job_info\".properties.exclusive\n       \n\nopenapi/dbv0.0.39Revert removal of Job description \"exclusive\" field\n\nNew field\n\n                       .components.schemas.\"v0.0.39_job_desc_msg\".properties.exclusive\n       \n\n\nNew field\n\n                       .components.schemas.\"v0.0.39_job_info\".properties.exclusive\n       \n\nSlurm 23.02.0openapi/v0.0.39New pluginNew SchemaNew methods for node queriesopenapi/v0.0.37Deprecation noticeopenapi/v0.0.36Removal noticeopenapi/dbv0.0.39New pluginNew Schemaopenapi/dbv0.0.37Deprecation noticeopenapi/dbv0.0.36Removal noticeSlurm 22.05.8openapi/v0.0.38Fix invalid type for nice\n\nField modified\n\n\t\t\t.components.schemas.\"v0.0.38_job_properties\".properties.nice\n\t\n\nopenapi/dbv0.0.38Populate POST /tres/ body\n\n\nnew path\n.paths.\"/tres/\".post.requestBody\n\n\nnew path\n.components.schemas.\"dbv0.0.38_tres_update\"\n\n\nnew field\n.components.schemas.\"dbv0.0.38_update_qos\".type\n\n\nAdd missing type field\n\n\nnew field\n.components.schemas.\"dbv0.0.38_update_account\".type\n\n\nnew field\n.components.schemas.\"dbv0.0.38_update_users\".type\n\n\nAdd missing requestBody field\n\n\nnew path\n.paths.\"/associations/\".post.requestBody\n\n\nnew path\n.paths.\"/wckeys/\".post.requestBody\n\n\nAdd missing requestBody field\n\n\nnew field\n.paths.\"/config\".post.requestBody\n\n\nnew field\n.components.schemas.\"dbv0.0.38_set_config\"\n\n\nCorrect type of field accounts->accounting in wckeys\n\n\nnew schema\n.components.schemas.\"dbv0.0.38_accounting\"\n\n\nnew field\n.components.schemas.\"dbv0.0.38_wckey\".properties.accounting\n\n\nremoved field\n.components.schemas.\"dbv0.0.38_wckey\".properties.accounts\n\n\nUse \"QOS\" in dbv0.0.38_qos_info\n\n\nprevious path\n.components.schemas.\"dbv0.0.38_qos_info\".properties.qos\n\n\nnew path\n.components.schemas.\"dbv0.0.38_qos_info\".properties.QOS\n\n\nAdd oversubscribe option to job submission properties\n\n\nField added\n\n\t\t\t.components.schemas.\"v0.0.38_job_properties\".properties.oversubscribe\n\t\n\n\nSlurm 22.05.7\nopenapi/v0.0.38\nAdd missing fields\n\n\nField added\n\n\t\t\t.components.schemas.\"v0.0.38_partition\".properties.maximum_memory_per_cpu\n\t\n\n\nField added\n\n\t\t\t.components.schemas.\"v0.0.38_partition\".properties.default_memory_per_node\n\t\n\n\n"
            },
            {
                "title": "Slurm 22.05.7",
                "content": "openapi/v0.0.38Add missing fields\n\nField added\n\n\t\t\t.components.schemas.\"v0.0.38_partition\".properties.maximum_memory_per_cpu\n\t\n\n\nField added\n\n\t\t\t.components.schemas.\"v0.0.38_partition\".properties.default_memory_per_node\n\t\n\n"
            },
            {
                "title": "Slurm 21.08.8",
                "content": "openapi/dbv0.0.37Move response fields in dbv0.0.37_diag under \"statistics\"\n\nnew parent field\n\n\n.components.schemas.\"dbv0.0.37_diag\".properties.statistics\n\n\n\n\nsubordinated fields\n\n\n.components.schemas.\"dbv0.0.37_diag\".properties.time_start\n.components.schemas.\"dbv0.0.37_diag\".properties.rollups\n.components.schemas.\"dbv0.0.37_diag\".properties.RPCs\n.components.schemas.\"dbv0.0.37_diag\".properties.users\n\n\n\nSlurm 21.08.7openapi/v0.0.37Fix misspelling: change account_gather_freqency to account_gather_frequency\n(only the spec was wrong; the underlying code already worked with the correct spelling).\n\nprevious path\n.components.schemas.\"v0.0.37_job_properties\".properties.account_gather_freqency\n\n\nnew path\n.components.schemas.\"v0.0.37_job_properties\".properties.account_gather_frequency\n\nFix misspelling: change cluster_constraints to cluster_constraint (only the\nspec was wrong; the underlying code already worked with the correct spelling).\n\n\nprevious path\n.components.schemas.\"v0.0.37_job_properties\".properties.cluster_constraints\n\n\nnew path\n.components.schemas.\"v0.0.37_job_properties\".properties.cluster_constraint\n\nSlurm 21.08.6openapi/dbv0.0.37Add association fields\n\nnew fields\n\n\n.components.schemas.\"dbv0.0.37_association\".properties.is_default\n.components.schemas.\"dbv0.0.37_association\".properties.max.tres.group.minutes\n.components.schemas.\"dbv0.0.37_association\".properties.max.tres.group.active\n.components.schemas.\"dbv0.0.37_association\".properties.max.jobs.active\n.components.schemas.\"dbv0.0.37_association\".properties.max.jobs.accruing\n.components.schemas.\"dbv0.0.37_association\".properties.max.jobs.total\n.components.schemas.\"dbv0.0.37_association\".properties.max.tres.minutes.per.job\n\n\n\nMove incorrectly named field\n\nold path\n.components.schemas.\"dbv0.0.37_qos\".properties.limits.max.jobs.per.account\n\n\nnew path\n.components.schemas.\"dbv0.0.37_qos\".properties.limits.max.jobs.active_jobs.per.account\n\nMove incorrectly named field\n\nold path\n.components.schemas.\"dbv0.0.37_qos\".properties.limits.properties.max.properties.jobs.properties.per.properties.user\n\n\nnew path\n.components.schemas.\"dbv0.0.37_qos\".properties.limits.properties.max.properties.jobs.properties.active_jobs.properties.per.properties.user\n\nAdd QOS fields\n\nnew fields\n\n\n.components.schemas.\"dbv0.0.37_qos\".properties.limits.properties.factor\n.components.schemas.\"dbv0.0.37_qos\".properties.limits.properties.max.properties.accruing.properties.per.properties.account\n.components.schemas.\"dbv0.0.37_qos\".properties.limits.properties.max.properties.accruing.properties.per.properties.user\n\n\n\nSlurm 21.08.3openapi/v0.0.37Allow strings for JobIds instead of only numerical JobIds.\n\nChange job_id parameter to string\n\n\n.paths.\"/job/{job_id}\".get.parameters[].schema\n.paths.\"/job/{job_id}\".post.parameters[].schema\n.paths.\"/job/{job_id}\".delete.parameters[].schema\n\n\n\nopenapi/dbv0.0.37Correct placement of step TRES\n\nprevious path\n.components.schemas.\"dbv0.0.37_job_step\".properties.step.properties.tres\n\n\nnew path\n.components.schemas.\"dbv0.0.37_job_step\".properties.tres\n\nopenapi/dbv0.0.36Correct placement of step TRES\n\nprevious path\n.components.schemas.\"dbv0.0.36_job_step\".properties.step.properties.tres\n\n\nnew path\n.components.schemas.\"dbv0.0.36_job_step\".properties.tres\n\nSlurm 21.08.0All of the OpenAPI plugins have moved from \"src/slurmrestd/plugins/openapi/\"\nto \"src/plugins/openapi/\".openapi/v0.0.35Deprecation noticeopenapi/dbv0.0.37add plugin\n\nnew dbv0.0.37 openapi plugin\n\n\nclone of existing dbv0.0.36 openapi plugin\nall paths renamed from v0.0.36 to v0.0.37\n\n\n\nrename previous -> reason\n\nprevious path\n.components.schemas.\"dbv0.0.37_job\".properties.state.previous\n\n\nnew path\n.components.schemas.\"dbv0.0.37_job\".properties.state.reason\n\nopenapi/v0.0.37add plugin\n\nnew v0.0.37 openapi plugin\n\n\nclone of existing v0.0.36 openapi plugin\nall paths renamed from v0.0.36 to v0.0.37\n\n\n\nrename standard_in -> standard_input\n\nprevious path\n.components.schemas.\"v0.0.37_job_response_properties\".properties.standard_in\n\n\nnew path\n.components.schemas.\"v0.0.37_job_response_properties\".properties.standard_input\n\nrename standard_out -> standard_output\n\nprevious path\n.components.schemas.\"v0.0.37_job_response_properties\".properties.standard_out\n\n\nnew path\n.components.schemas.\"v0.0.37_job_response_properties\".properties.standard_output\n\nAdd update_time field to Jobs query to allow clients to only get jobs list based on change timestamp.\n\nnew path\n.paths.\"/jobs/\".get.parameters[0]\n\nadd api to fetch reservation(s) info\n\nadded path\n.paths.\"/reservations/\"\n\n\nadded path\n.paths.\"/reservation/{reservation_name}\"\n\n\nadded path\n.components.schemas.\"v0.0.37_reservation\"\n\nMark job environment as required\n\nnew path\n.components.schemas.\"v0.0.37_job_properties\".required\n\nCorrect preemption_mode type to list of strings\n\nmodify path\n.components.schemas.\"v0.0.37_partition\".properties.preemption_mode\n\nSet UNIX timestamps to int64 instead of string\n\nmodify path\n.components.schemas.\"v0.0.37_job_response_properties\".properties.accrue_time\n\n\nmodify path\n.components.schemas.\"v0.0.37_job_response_properties\".properties.eligible_time\n\n\nmodify path\n.components.schemas.\"v0.0.37_job_response_properties\".properties.end_time\n\n\nmodify path\n.components.schemas.\"v0.0.37_job_response_properties\".properties.preempt_time\n\n\nmodify path\n.components.schemas.\"v0.0.37_job_response_properties\".properties.pre_sus_time\n\n\nmodify path\n.components.schemas.\"v0.0.37_job_response_properties\".properties.resize_time\n\nAdd new fields to node properties\n\nadd path\n.components.schemas.\"v0.0.37_node\".properties.tres_used\n\n\nadd path\n.components.schemas.\"v0.0.37_node\".properties.tres_weighted\n\n\nadd path\n.components.schemas.\"v0.0.37_node\".properties.alloc_cpus\n\n\nadd path\n.components.schemas.\"v0.0.37_node\".properties.idle_cpus\n\n\nadd path\n.components.schemas.\"v0.0.37_node\".properties.alloc_memory\n\n\nadd path\n.components.schemas.\"v0.0.37_node\".properties.partitions\n\nreplace nodes_online with state in /partitions endpoint\n\n\nremoved path\n.components.schemas.\"v0.0.37_partitions_response\".nodes_online\n\n\nadded path\n.components.schemas.\"v0.0.37_partitions_response\".state\n\n\nAdd POST method for /associations\n\n\nnew path\n.paths.\"/associations/\".post\n\n\nnew path\n.components.schemas.\"dbv0.0.37_response_associations\"\n\n\nSlurm 20.11.5\nopenapi/dbv0.0.36\nMark job environment as required\n\n\nnew path\n.components.schemas.\"v0.0.36_job_properties\".required\n\n\nAdd state flags\n\n\nnew path\n.components.schemas.\"v0.0.37_node\".properties.state_flags\n\n\nnew path\n.components.schemas.\"v0.0.37_node\".properties.next_state_after_reboot_flags\n\n\nCorrect description for previous state\n\n\npath\n.components.schemas.\"dbv0.0.36_job\".properties.state.properties.previous\n\n\nSlurm 20.11.3\nopenapi/dbv0.0.36\nCorrect structure of dbv0.0.36_tres_list\n\n\npath\n.components.schemas.\"dbv0.0.36_tres_list\"\n\n\nAccept Job Query CSV parameters as a list (JSON array)\n\n\npath\n.paths.\"/jobs/\".get.parameters\n\n\nSlurm 20.11.1\nopenapi/v0.0.36\nCorrect name for partition field\n\n\nprevious path\n.components.schemas.\"v0.0.36_partition\".properties.\"min nodes per job\"\n\n\nnew path\n.components.schemas.\"v0.0.36_partition\".properties.\"min_nodes_per_job\"\n\n\nAdd node comment field\n\n\nnew path\n.components.schemas.\"v0.0.36_node\".properties.comment\n\n\nSlurm 20.11.0\nopenapi/dbv0.0.36\nInitial Implementation of database queries.\nopenapi/v0.0.36\nadd plugin\n\n\nnew v0.0.36 openapi plugin\n\n\nclone of existing v0.0.35 openapi plugin.\nall paths renamed from v0.0.35 to v0.0.36.\n\n\n\n\nAdd error schema\n\n\npath\n.components.schemas.\"v0.0.36_error\"\n\n\nreturn array of nodes instead of dictionary\n\n\npath\n.components.schemas.\"v0.0.36_nodes_response\"\n\n\nreturn array of partitions instead of dictionary\n\n\npath\n.components.schemas.\"v0.0.36_partitions_response\"\n\n\nreturn array of pings instead of dictionary\n\n\npath\n.components.schemas.\"v0.0.36_ping\"\n\n\nSimplify possible signals for canceling jobs\n\n\npath\n.paths.\"/job/{job_id}\".delete.parameters[1]\n\n\nSimplify exclusive for jobs submissions\n\n\npath\n.components.schemas.\"v0.0.36_job_properties\".properties.exclusive\n\n\nSimplify nodes for jobs submissions\n\n\npath\n.components.schemas.\"v0.0.36_job_properties\".properties.nodes\n\n\nChange server URL\n\n\nprevious\n.servers[0].url=/\n\n\nnew\n.servers[0].url=/slurm/v0.0.36/\n\n\nAdd operationId tag\n\n\nparent path\n.paths\n\n\nprepend every schema with v0.0.36_\n\n\nparent path\n.components.schemas\n\n\nadd tags openapi and slurm\n\n\nnew path\n.tags\n\n\nadd support contact\n\n\nnew path\n.info.contact\n\n\npopulate response from partitions query\n\n\nnew path\n.components.schemas.partitions_response\n\n\nrename \"node_info\" to \"nodes_response\"\n\n\nremoved path\n.components.schemas.node_info\n\n\nnew path\n.components.schemas.nodes_response\n\n\nadd jobs query response properties\n\n\nnew path\n.components.schemas.diag\n\n\ndefine response to diag\n\n\nnew path\n.components.schemas.diag\n\n\nadd job query response properties\n\n\nnew path\n.components.schemas.job_response_properties\n\n\nremove \"requested_node_by_index\"\n\n\nremoved path\n.components.schemas.job_response_properties.properties.requested_node_by_index\n\n\nrename \"pn_min_tmp_disk\" to \"minimum_tmp_disk_per_node\"\n\n\nremoved path\n.components.schemas.job_response_properties.properties.pn_min_tmp_disk\n\n\nnew path\n.components.schemas.job_response_properties.properties.minimum_tmp_disk_per_node\n\n\nrenamed \"nodes\" to \"node_count\"\n\n\nremoved path\n.components.schemas.job_response_properties.properties.nodes\n\n\nnew path\n.components.schemas.job_response_properties.properties.node_count\n\n\nadd get job responses schema\n\n\nnew path\n.components.schemas.job_submission_response\n\n\nuse job_submission for job_submit instead of job_properties\n\n\nchanged field\n.paths.\"/job/submit\".requestBody.content.\"application/json\"\n\n\nchanged field\n.paths.\"/job/submit\".requestBody.content.\"application/x-yaml\"\n\n\nnew path\n.components.schemas.job_submission\n\n\nSet type for \"job_properties\" schema\n\n\nnew path\n.components.schemas.v0.0.36_job_properties.properties.type\n\n\nadd security bearer\n\n\nnew path\n.security\n\n\nSlurm 20.02.0\nv0.0.35\nInitial Implementation of Slurm REST API.\n\nLast modified 25 August 2023\n"
            },
            {
                "title": "Slurm 20.11.5",
                "content": "openapi/dbv0.0.36Mark job environment as required\n\nnew path\n.components.schemas.\"v0.0.36_job_properties\".required\n\nAdd state flags\n\nnew path\n.components.schemas.\"v0.0.37_node\".properties.state_flags\n\n\nnew path\n.components.schemas.\"v0.0.37_node\".properties.next_state_after_reboot_flags\n\nCorrect description for previous state\n\npath\n.components.schemas.\"dbv0.0.36_job\".properties.state.properties.previous\n\nSlurm 20.11.3openapi/dbv0.0.36Correct structure of dbv0.0.36_tres_list\n\npath\n.components.schemas.\"dbv0.0.36_tres_list\"\n\nAccept Job Query CSV parameters as a list (JSON array)\n\npath\n.paths.\"/jobs/\".get.parameters\n\nSlurm 20.11.1openapi/v0.0.36Correct name for partition field\n\nprevious path\n.components.schemas.\"v0.0.36_partition\".properties.\"min nodes per job\"\n\n\nnew path\n.components.schemas.\"v0.0.36_partition\".properties.\"min_nodes_per_job\"\n\nAdd node comment field\n\nnew path\n.components.schemas.\"v0.0.36_node\".properties.comment\n\nSlurm 20.11.0openapi/dbv0.0.36Initial Implementation of database queries.openapi/v0.0.36add plugin\n\nnew v0.0.36 openapi plugin\n\n\nclone of existing v0.0.35 openapi plugin.\nall paths renamed from v0.0.35 to v0.0.36.\n\n\n\nAdd error schema\n\npath\n.components.schemas.\"v0.0.36_error\"\n\nreturn array of nodes instead of dictionary\n\npath\n.components.schemas.\"v0.0.36_nodes_response\"\n\nreturn array of partitions instead of dictionary\n\npath\n.components.schemas.\"v0.0.36_partitions_response\"\n\nreturn array of pings instead of dictionary\n\npath\n.components.schemas.\"v0.0.36_ping\"\n\nSimplify possible signals for canceling jobs\n\npath\n.paths.\"/job/{job_id}\".delete.parameters[1]\n\nSimplify exclusive for jobs submissions\n\npath\n.components.schemas.\"v0.0.36_job_properties\".properties.exclusive\n\nSimplify nodes for jobs submissions\n\npath\n.components.schemas.\"v0.0.36_job_properties\".properties.nodes\n\nChange server URL\n\nprevious\n.servers[0].url=/\n\n\nnew\n.servers[0].url=/slurm/v0.0.36/\n\nAdd operationId tag\n\nparent path\n.paths\n\nprepend every schema with v0.0.36_\n\nparent path\n.components.schemas\n\nadd tags openapi and slurm\n\nnew path\n.tags\n\nadd support contact\n\nnew path\n.info.contact\n\npopulate response from partitions query\n\nnew path\n.components.schemas.partitions_response\n\nrename \"node_info\" to \"nodes_response\"\n\nremoved path\n.components.schemas.node_info\n\n\nnew path\n.components.schemas.nodes_response\n\nadd jobs query response properties\n\nnew path\n.components.schemas.diag\n\ndefine response to diag\n\nnew path\n.components.schemas.diag\n\nadd job query response properties\n\nnew path\n.components.schemas.job_response_properties\n\nremove \"requested_node_by_index\"\n\nremoved path\n.components.schemas.job_response_properties.properties.requested_node_by_index\n\nrename \"pn_min_tmp_disk\" to \"minimum_tmp_disk_per_node\"\n\nremoved path\n.components.schemas.job_response_properties.properties.pn_min_tmp_disk\n\n\nnew path\n.components.schemas.job_response_properties.properties.minimum_tmp_disk_per_node\n\nrenamed \"nodes\" to \"node_count\"\n\nremoved path\n.components.schemas.job_response_properties.properties.nodes\n\n\nnew path\n.components.schemas.job_response_properties.properties.node_count\n\nadd get job responses schema\n\nnew path\n.components.schemas.job_submission_response\n\nuse job_submission for job_submit instead of job_properties\n\nchanged field\n.paths.\"/job/submit\".requestBody.content.\"application/json\"\n\n\nchanged field\n.paths.\"/job/submit\".requestBody.content.\"application/x-yaml\"\n\n\nnew path\n.components.schemas.job_submission\n\nSet type for \"job_properties\" schema\n\nnew path\n.components.schemas.v0.0.36_job_properties.properties.type\n\nadd security bearer\n\nnew path\n.security\n\nSlurm 20.02.0v0.0.35Initial Implementation of Slurm REST API.Last modified 25 August 2023"
            },
            {
                "title": "openapi/v0.0.39 (src/plugins/openapi/v0.0.39/openapi.json)",
                "content": "Deprecation noticeopenapi/v0.0.38 (src/plugins/openapi/v0.0.38/openapi.json)Removal noticeopenapi/slurmdbdGenerated openapi.json will only populate \"deprecated\" fields if true.Reduced number of \"$ref\" fields. Where there was only 1 reference for the\n\tschema, the \"$ref\" schema will be directly populated in place. Content\n\tof the schemas will be relatively unchanged.openapi/dbv0.0.39 (src/plugins/openapi/dbv0.0.39/openapi.json)Deprecation noticeopenapi/dbv0.0.38 (src/plugins/openapi/dbv0.0.38/openapi.json)Removal noticeSlurm 23.11.7data_parser/v0.0.40Add missing schema for job states query\n\nmodified field\n.paths.\"/slurm/v0.0.40/jobs/state\".get.responses.\"200\".content\n\n\nmodified field\n.paths.\"/slurm/v0.0.40/jobs/state\".get.responses.\"default\".content\n\n\nnew schema\n.components.schemas.\"v0.0.40_openapi_job_state_resp\"\n\nSlurm 23.11.6data_parser/v0.0.40Add new endpoint\n\nnew path\n.paths.\"/slurm/v0.0.40/jobs\".delete\n\n\nnew path\n.components.schemas.\"v0.0.40_openapi_kill_jobs_resp\"\n\n\nnew path\n.components.schemas.\"v0.0.40_kill_jobs_msg\"\n\n\nnew path\n.components.schemas.\"v0.0.40_kill_jobs_resp_msg\"\n\n\nModified field\n.components.schemas.\"v0.0.40_kill_jobs_msg\".properties.flags.items.enum\n\nSlurm 23.11.5data_parser/v0.0.40Add field\n\npath\n.components.schemas.\"v0.0.40_job_desc_msg\".properties.cores_per_socket\n\nSlurm 23.11.2data_parser/v0.0.40Added field\n\nbefore\n\n\n.components.schemas.\"v0.0.40_job\".properties.hold\n.components.schemas.\"v0.0.40_job\".properties.priority.description\n.components.schemas.\"v0.0.40_job_desc_msg\".properties.hold\n.components.schemas.\"v0.0.40_job_desc_msg\".properties.priority.description\n.components.schemas.\"v0.0.40_job_info\".properties.hold\n.components.schemas.\"v0.0.40_job_info\".properties.priority.description\n\n\n\nAdd GRES_ONE_TASK_PER_SHARING and GRES_MULT_TASKS_PER_SHARING flags to /job_info, /job_desc_msg output\n\nNew property\n\n\n.components.schemas.\"v0.0.40_job_info\".properties.flags\n.components.schemas.\"v0.0.40_job_desc_msg\".properties.flags\n\n\n\nSlurm 23.11.1data_parser/v0.0.40Renamed field\n\nbefore\n\n\n.components.schemas.v0.0.39_job_mem_per_node\n\n\n\n\nafter\n\n\n.components.schemas.v0.0.39_mem_per_node\n\n\n\nRenamed field\n\nbefore\n\n\n.components.schemas.v0.0.39_job_mem_per_cpu\n\n\n\n\nafter\n\n\n.components.schemas.v0.0.39_mem_per_cpu\n\n\n\nopenapi/slurmctldAdd field\n\nadd\n\n\n.components.schemas.\"v0.0.40_openapi_meta\".properties.slurm.properties.cluster\n\n\n\n\nadd\n\n\n.components.schemas.\"v0.0.40_partition_info\".properties.defaults.properties.partition_memory_per_cpu\n\n\n\n\nadd\n\n\n.components.schemas.\"v0.0.40_partition_info\".properties.defaults.properties.partition_memory_per_node\n\n\n\n\nadd\n\n\n.components.schemas.\"v0.0.40_partition_info\".properties.maximums.properties.partition_memory_per_cpu\n\n\n\n\nadd\n\n\n.components.schemas.\"v0.0.40_partition_info\".properties.maximums.properties.oversubscribe\n\n\n\n\nadd\n\n\n.components.schemas.\"v0.0.40_partition_info\".properties.maximums.properties.partition_memory_per_node\n\n\n\nopenapi/slurmdbdAdd field\n\nadd\n\n\n.components.schemas.\"v0.0.40_openapi_meta\".properties.slurm.properties.cluster\n\n\n\nSlurm 23.11.0openapi/slurmctldNew pluginSwap job exit codes to verbose output\n\nField modified\n\n\t\t.components.schemas.\"v0.0.40_job_info\".properties.exit_code\n\t\n\n\nField modified\n\n\t\t.components.schemas.\"v0.0.40_job_info\".properties.derived_exit_code\n\t\n\nRemove the \"required/memory\" field. It would dump very large integers if\n    the job required per cpu memory but dump correct amounts for per node\n    memory.\n\nField removed\n\n\t\t.component.schemas.\"v0.0.40_job\".properties.required.properties.memory\n\t\n\nAdd timestamps for last change to data or generation times.\n\nFields added\n\n\t\t.components.schemas.\"v0.0.40_openapi_job_info_resp\".properties.last_backfill\n\t\t.components.schemas.\"v0.0.40_openapi_job_info_resp\".properties.last_update\n\t\t.components.schemas.\"v0.0.40_openapi_nodes_resp\".properties.last_update\n\t\t.components.schemas.\"v0.0.40_openapi_partition_resp\".properties.last_update\n\t\t.components.schemas.\"v0.0.40_openapi_reservation_resp\".properties.last_update\n\t\t.components.schemas.\"v0.0.40_openapi_licenses_resp\".properties.last_update\n\t\n\nopenapi/v0.0.38 (src/plugins/openapi/v0.0.38/openapi.json)Deprecation noticeopenapi/v0.0.37 (src/plugins/openapi/v0.0.37/openapi.json)Removal noticeopenapi/slurmdbdNew pluginSwap job exit codes to process exit codes\n\nField modified\n\n\t\t.components.schemas.\"v0.0.40_job\".properties.exit_code\n\t\n\n\nField modified\n\n\t\t.components.schemas.\"v0.0.40_job\".properties.derived_exit_code\n\t\n\n\nField modified\n\n\t\t.components.schemas.\"v0.0.40_step\".properties.exit_code\n\t\n\nSwitch StepID field to be string to match CLI format\n\nField modified\n\n\t\t.components.schemas.\"v0.0.40_step\".properties.step\n\t\n\nAdd fields to assocations\n\nAdded\n\n\t\t.components.schemas.\"v0.0.40_assoc\".properties.accounting\n\t\n\n\nAdded\n\n\t\t.components.schemas.\"v0.0.40_assoc\".properties.id\n\t\n\n\nAdded\n\n\t\t.components.schemas.\"v0.0.40_assoc\".properties.comment\n\t\n\nRemoved field from assocations\n\nRemoved\n\n\t\t.components.schemas.\"v0.0.40_assoc\".properties.usage\n\t\n\nAdd new /accounts_association endpoint\n\nadd\n\n\n.components.schemas.\"v0.0.40_openapi_accounts_add_cond_resp_str\"\n.components.schemas.\"v0.0.40_openapi_accounts_add_cond_resp\"\n.paths.\"/slurmdb/v0.0.40/accounts_association\"\n\n\n\nAdd new /users_association endpoint\n\nadd\n\n\n.components.schemas.\"v0.0.40_openapi_users_add_cond_resp_str\"\n.components.schemas.\"v0.0.40_openapi_users_add_cond_resp\"\n.paths.\"/slurmdb/v0.0.40/users_association\"\n\n\n\nopenapi/dbv0.0.38 (src/plugins/openapi/dbv0.0.38/openapi.json)Deprecation noticeopenapi/dbv0.0.37 (src/plugins/openapi/dbv0.0.37/openapi.json)Removal noticeSlurm 23.02.6openapi/v0.0.39Correct path for responses types\n\nUpdated paths\n\n\t\t.paths.\"/slurm/v0.0.39/licenses\".get.responses.default\n\t\t.paths.\"/slurm/v0.0.39/job/{job_id}\".get.responses.default\n\t\n\nopenapi/dbv0.0.39Switch integer to have NO_VAL tagging to allow for complex values.\n\nField modified\n\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.per.properties.count\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.active\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.accruing\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.total\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.grace_time\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.active_jobs.properties.accruing\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.active_jobs.properties.count\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.wall_clock.properties.per.properties.qos\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.wall_clock.properties.per.properties.job\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.jobs.properties.active_jobs.properties.account\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.jobs.properties.active_jobs.properties.user\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.jobs.properties.per.properties.account\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.accruing.properties.per.properties.account\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max.properties.accruing.properties.per.properties.user\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.min.properties.priority_threshold\n\t\n\nSlurm 23.02.5openapi/v0.0.39Add missing fields\n\npath\n\n\t\t.components.schemas.\"v0.0.39_job_desc_msg\".properties.hold\n\t\t.components.schemas.\"v0.0.39_job_desc_msg\".properties.priority\n\t\t.components.schemas.\"v0.0.39_job_desc_msg\".properties.required\n\t\t.components.schemas.\"v0.0.39_cron_entry\".properties.line.properties.start\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.nodes.properties.allowed_allocation\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.nodes.properties.configured\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.accounts.properties.allowed\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.qos.properties.allowed\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.qos.properties.deny\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.tres.properties.billing_weights\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.cpus.properties.task_binding\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.default.properties.memory_per_cpu\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.default.properties.time\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.maximums.properties.cpus_per_node\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.maximums.properties.cpus_per_socket\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.maximums.properties.memory_per_cpu\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.maximums.properties.nodes\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.maximums.properties.shares\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.maximums.properties.time\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.priority.properties.job_factor\n\t\t.components.schemas.\"v0.0.39_partition_info\".properties.timeouts.properties.resume\n\t\t.components.schemas.\"v0.0.39_job_info\".properties.hold\n\t\n\nAdd required/memory_per_cpu and required/memory_per_node to v0.0.39_job to\n\tdifferientate between jobs that require memory per node or per CPU.\n\tJobs that required per cpu memory will dump very large integers due to\n\tinternal bit packing required/memory but dump correct amounts for per\n\tnode memory.\n\nField added\n\n\t\t\t.components.schemas.\"v0.0.39_job\".properties.required.properties.mem_per_cpu\n\t\t\n\n\nField added\n\n\t\t\t.components.schemas.\"v0.0.39_job\".properties.required.properties.mem_per_node\n\t\t\n\nopenapi/dbv0.0.39Add missing fields\n\npath\n\n\t\t.components.schemas.\"v0.0.39_stats_rpc\".properties.time.properties.average\n\t\t.components.schemas.\"v0.0.39_stats_user\".properties.time.properties.average\n\t\t.components.schemas.\"v0.0.39_user\".properties.default.properties.account\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.tres\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.active\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.accruing\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.total\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.per.properties.count\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.per.properties.accruing\n\t\t.components.schemas.\"v0.0.39_assoc\".properties.max.properties.jobs.properties.per.properties.submitted\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.grace_time\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.max\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.jobs\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.factor\n\t\t.components.schemas.\"v0.0.39_qos\".properties.limits.properties.min.properties.priority_threshold\n\t\t.components.schemas.\"v0.0.39_qos\".properties.preempt.properties.list\n\t\t.components.schemas.\"v0.0.39_qos\".properties.preempt.properties.mode\n\t\t.components.schemas.\"v0.0.39_job\".properties.hold\n\t\t.components.schemas.\"v0.0.39_job\".properties.comment.properties.administrator\n\t\t.components.schemas.\"v0.0.39_job\".properties.comment.properties.job\n\t\t.components.schemas.\"v0.0.39_job\".properties.array.properties.job_id\n\t\t.components.schemas.\"v0.0.39_job\".properties.array.properties.limits\n\t\t.components.schemas.\"v0.0.39_job\".properties.array.properties.task_id\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.elapsed\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.eligible\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.end\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.start\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.submission\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.suspended\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.system\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.limit\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.total\n\t\t.components.schemas.\"v0.0.39_job\".properties.time.properties.user.properties.seconds\n\t\t.components.schemas.\"v0.0.39_job\".properties.het.properties.job_id\n\t\t.components.schemas.\"v0.0.39_job\".properties.required.properties.CPUs\n\t\t.components.schemas.\"v0.0.39_job\".properties.reservation.properties.id\n\t\t.components.schemas.\"v0.0.39_job\".properties.state.properties.current\n\t\t.components.schemas.\"v0.0.39_job\".properties.tres.properties.allocated\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.elapsed\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.end\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.start\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.suspended\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.system\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.total\n\t\t.components.schemas.\"v0.0.39_step\".properties.time.properties.user.properties.seconds\n\t\t.components.schemas.\"v0.0.39_step\".properties.nodes.properties.count\n\t\t.components.schemas.\"v0.0.39_step\".properties.nodes.properties.range\n\t\t.components.schemas.\"v0.0.39_step\".properties.CPU.properties.requested_frequency\n\t\t.components.schemas.\"v0.0.39_step\".properties.statistics.properties.CPU\n\t\t.components.schemas.\"v0.0.39_step\".properties.step.properties.id\n\t\t.components.schemas.\"v0.0.39_step\".properties.tres.properties.requested\n\t\t.components.schemas.\"v0.0.39_step\".properties.tres.properties.consumed\n\t\t.components.schemas.\"v0.0.39_cluster_rec\".properties.controller.properties.host\t\n\nSwitch integer to have NO_VAL tagging to allow for complex values.\n\nField modified\n\n\t\t.components.schemas.\"v0.0.39_acct_gather_energy\".properties.current_watts\n\t\n\n\nField modified\n\n\t\t.components.schemas.\"v0.0.39_qos\".properties.priority\n\t\n\nSlurm 23.02.4openapi/v0.0.39Tag job description environment field as required\n\nField add\n\n\t\t.components.schemas.\"v0.0.39_job_desc_msg\".required\n\t\n\nAdd status schema to default\n\nField added\n\n\t\t.paths.\"/licenses/\".get.responses.default\n\t\n\n\nField added\n\n\t\t.paths.\"/job/{job_id}\".get.responses.default\n\t\n\nTag derived_exit_code and exit_code as UINT32_NO_VAL to avoid 4294967295 on still running jobs.\n\nField changed\n\n\t\t.components.schemas.\"v0.0.39_job_info\".properties.derived_exit_code\n\t\n\n\nField changed\n\n\t\t.components.schemas.\"v0.0.39_job_info\".properties.exit_code\n\t\n\n\nSlurm 23.02.3openapi/v0.0.39Correct invalid reference in status schema\n\nField modified\n\n\t\t.components.schemas.status.properties.errors\n\t\n\nRevert removal of Job description \"oversubscribe\" field\n\nNew field\n\n\t\t       .components.schemas.\"v0.0.39_job_desc_msg\".properties.oversubscribe\n       \n\n\nNew field\n\n\t\t       .components.schemas.\"v0.0.39_job_info\".properties.oversubscribe\n       \n\nSlurm 23.02.2openapi/v0.0.39Revert format change for job updates\n\nModify field\n\n\t\t\t.paths.\"/job/{job_id}\".post.requestBody.content\n\t\n\nRevert removal of Job description \"exclusive\" field\n\nNew field\n\n                       .components.schemas.\"v0.0.39_job_desc_msg\".properties.exclusive\n       \n\n\nNew field\n\n                       .components.schemas.\"v0.0.39_job_info\".properties.exclusive\n       \n\nopenapi/dbv0.0.39Revert removal of Job description \"exclusive\" field\n\nNew field\n\n                       .components.schemas.\"v0.0.39_job_desc_msg\".properties.exclusive\n       \n\n\nNew field\n\n                       .components.schemas.\"v0.0.39_job_info\".properties.exclusive\n       \n\nSlurm 23.02.0openapi/v0.0.39New pluginNew SchemaNew methods for node queriesopenapi/v0.0.37Deprecation noticeopenapi/v0.0.36Removal noticeopenapi/dbv0.0.39New pluginNew Schemaopenapi/dbv0.0.37Deprecation noticeopenapi/dbv0.0.36Removal noticeSlurm 22.05.8openapi/v0.0.38Fix invalid type for nice\n\nField modified\n\n\t\t\t.components.schemas.\"v0.0.38_job_properties\".properties.nice\n\t\n\nopenapi/dbv0.0.38Populate POST /tres/ body\n\n\nnew path\n.paths.\"/tres/\".post.requestBody\n\n\nnew path\n.components.schemas.\"dbv0.0.38_tres_update\"\n\n\nnew field\n.components.schemas.\"dbv0.0.38_update_qos\".type\n\n\nAdd missing type field\n\n\nnew field\n.components.schemas.\"dbv0.0.38_update_account\".type\n\n\nnew field\n.components.schemas.\"dbv0.0.38_update_users\".type\n\n\nAdd missing requestBody field\n\n\nnew path\n.paths.\"/associations/\".post.requestBody\n\n\nnew path\n.paths.\"/wckeys/\".post.requestBody\n\n\nAdd missing requestBody field\n\n\nnew field\n.paths.\"/config\".post.requestBody\n\n\nnew field\n.components.schemas.\"dbv0.0.38_set_config\"\n\n\nCorrect type of field accounts->accounting in wckeys\n\n\nnew schema\n.components.schemas.\"dbv0.0.38_accounting\"\n\n\nnew field\n.components.schemas.\"dbv0.0.38_wckey\".properties.accounting\n\n\nremoved field\n.components.schemas.\"dbv0.0.38_wckey\".properties.accounts\n\n\nUse \"QOS\" in dbv0.0.38_qos_info\n\n\nprevious path\n.components.schemas.\"dbv0.0.38_qos_info\".properties.qos\n\n\nnew path\n.components.schemas.\"dbv0.0.38_qos_info\".properties.QOS\n\n\nAdd oversubscribe option to job submission properties\n\n\nField added\n\n\t\t\t.components.schemas.\"v0.0.38_job_properties\".properties.oversubscribe\n\t\n\n\nSlurm 22.05.7\nopenapi/v0.0.38\nAdd missing fields\n\n\nField added\n\n\t\t\t.components.schemas.\"v0.0.38_partition\".properties.maximum_memory_per_cpu\n\t\n\n\nField added\n\n\t\t\t.components.schemas.\"v0.0.38_partition\".properties.default_memory_per_node\n\t\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/cli_filter_plugins.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "cli_filter Plugin API",
                "content": "OverviewThis document describes Slurm cli_filter plugins and the API that\ndefines them. It is intended as a resource to programmers wishing to write\ntheir own Slurm cli_filter plugins. The purpose of the cli_filter plugins is to provide programmatic hooks\nduring the execution of the salloc, sbatch, and srun\ncommand line interface (CLI) programs. Three hooks are defined:\n\ncli_filter_p_setup_defaults \u2014\nCalled before any option processing is done,\nper job component, allowing a plugin to replace default option\nvalues.\ncli_filter_p_pre_submit \u2014\nCalled after option processing per job\ncomponent but before any communication\nis made with the slurm controller. This location\nis ideal for policy enforcement because the plugin can read all the options\nsupplied by the user (as well as the environment) - thus invalid job requests\ncan be stopped before they ever reach the slurmctld.\ncli_filter_p_post_submit \u2014\nCalled after the jobid (and, in the case of\nsrun, after the stepid) is generated, and typically before or in\nparallel with job execution.  In combination with data collected in the\ncli_filter_p_pre_submit() hook, this is an ideal location for logging\nactivity.\n\ncli_filter plugins vary from the job_submit\nplugin as it is entirely executed client-side, whereas job_submit is\nprocessed server-side (within the slurm controller). The benefit of the\ncli_filter is that it has access to all command line options in a simple and\nconsistent interface as well as being safer to run disruptive operations\n(e.g., quota checks or other long running operations you might want to use\nfor integrating policy decisions), which can be problematic if run from the\ncontroller. The disadvantage of the cli_filter is that it must not be relied\nupon for security purposes as an enterprising user can circumvent it simply\nby providing an alternate slurm.conf with the CliFilterPlugins option\ndisabled. If you plan to use the cli_filter for managing policies, you should\nalso configure a job_submit plugin to reinforce those policies.Slurm cli_filter plugins must conform to the\nSlurm Plugin API with the following specifications:const char\nplugin_name[]=\"full\u00a0text\u00a0name\"\n\nA free-formatted ASCII text string that identifies the plugin.\n\nconst char\nplugin_type[]=\"major/minor\"\n\nThe major type must be \"cli_filter.\"\nThe minor type can be any suitable name for the type of job submission package.\nWe include samples in the Slurm distribution for\n\nnone \u2014 An empty plugin with no actions taken, a useful starting\ntemplate for a new plugin.\n\nconst uint32_t plugin_version\nIf specified, identifies the version of Slurm used to build this plugin and\nany attempt to load the plugin from a different version of Slurm will result\nin an error.\nIf not specified, then the plugin may be loaded by Slurm commands and\ndaemons from any version, however this may result in difficult to diagnose\nfailures due to changes in the arguments to plugin functions or changes\nin other Slurm functions used by the plugin.\nSlurm can be configured to use multiple cli_filter plugins if desired,\nhowever the lua plugin will only execute one lua script named \"cli_filter.lua\"\nlocated in the default script directory (typically the subdirectory \"etc\" of\nthe installation directory).\nAPI Functions\nAll of the following functions are required. Functions which are not\nimplemented must be stubbed.\nint init(void)\nDescription:\n  Called when the plugin is loaded, before any other functions are\n  called. Put global initialization here.\nReturns: \nSLURM_SUCCESS on success, or\nSLURM_ERROR on failure.\nvoid fini(void)\nDescription:\n  Called when the plugin is removed. Clear any allocated storage here.\nReturns: None.\nNote: These init and fini functions are not the same as those\ndescribed in the dlopen (3) system library.\nThe C run-time system co-opts those symbols for its own initialization.\nThe system _init() is called before the Slurm\ninit(), and the Slurm\nfini() is called before the system's\n_fini().\nint cli_filter_p_setup_defaults(slurm_opt_t *options, bool early)\nDescription:\nThis function is called by the salloc, sbatch, or srun command line interface\n(CLI) programs shortly before processing any options from the environment,\ncommand line, or script (#SBATCH). The hook may be run multiple times per job\ncomponent, once for an early pass (if implemented by the CLI), and again for\nthe main pass.\nNote that this call is skipped for any srun command run within an existing job\nallocation to prevent settings from overriding the set of options that have been\npopulated for the job based on the job environment.\nThe options and early arguments are meant to be passed to slurm_option_set()\nwhich will set the option if it is in the appropriate pass. Failures to set\nan option may be a symptom of trying to set the option on the wrong pass. Given\nthat you should not return SLURM_ERROR simply because of a failure to set an option.\nArguments: \noptions\n(input) slurm option data structure; meant to be passed to the slurm_option_* API\nwithin src/common/slurm_opt.h.\nearly\n(input) boolean indicating if this is the early pass or not; meant to be passed to\nthe slurm_option_* API within src/common/slurm_opt.h.\nReturns: \nSLURM_SUCCESS on success, or\nSLURM_ERROR on failure, will terminate execution\nof the CLI.\n\nint cli_filter_p_pre_submit(slurm_opt_t *options, int offset)\nDescription:\nThis function is called by the CLI after option processing but before any\ncommunication with the slurmctld is made.  This is after all\ncli_filter_p_setup_defaults()\nhooks are executed (for the current job component), environment variables\nprocessed, command line options and #SBATCH directives interpreted.\ncli_filter_p_pre_submit() is called before any parts of\nthe data structure are rewritten, so it is safe to\nboth read and write or unset any options from the plugin that you desire.\nNote that cli_filter_p_post_submit() cannot safely read (or write) the options,\nso you should save any state for logging in cli_filter_p_post_submit() during\ncli_filter_p_pre_submit(). This function is called once per job component.\nArguments: \noptions\n(input/output) the job allocation request specifications.\noffset\n(input) integer value for the current hetjob offset; should be used as a key when\nstoring data for communication between cli_filter_p_pre_submit() and\ncli_filter_p_post_submit().\nReturns: \nSLURM_SUCCESS on success, or\nSLURM_ERROR on failure, will terminate execution\nof the CLI.\n\nvoid cli_filter_p_post_submit(int offset, uint32_t jobid, uint32_t stepid)\nDescription:\nThis function is called by the CLI after a jobid (and, if srun, a stepid) has\nbeen assigned by the controller.  It is no longer safe to read or write to the\noptions data structure, so it has been removed from this function.  You should\nsave any state you need in cli_filter_p_pre_submit() using het_job_offset as a\nkey, since the function is called separately for every job component, and access\nit here.\nArguments: \noffset\n(input) integer value for the current hetjob offset; should be used as a key\nwhen storing data for communication between cli_filter_p_pre_submit() and\ncli_filter_p_post_submit().\njobid\n(input) job id of the job\nstepid\n(input) step id of the job if appropriate, NO_VAL otherwise\nLUA Interface\nSetting CliFilterPlugins=cli_filter/lua in slurm.conf will allow\nyou to implement the API functions mentioned using Lua language. The file must\nbe named \"cli_filter.lua\" and, similar to the job_submit plugin, it must be\nlocated in the default configuration directory (typically the subdirectory\n\"etc\" of the installation).\nAn example is provided within the source code\n\nhere.\nNOTE: Although available options are defined in the struct\nslurm_opt_t within\nsrc/common/slurm_opt.h, some options might be renamed. The provided\nexample shows a way of displaying the configured options by using\nslurm.json_cli_options(options).\nUser Defaults\nSetting CliFilterPlugins=cli_filter/user_defaults in slurm.conf will\nallow users to define their own defaults for jobs submitted from the machine(s)\nwith the configured file. The plugin looks for the definition file in\n$HOME/.slurm/defaults. It will read each line as a\ncomponent=value pair, where component is any of the\njob submission options available to salloc, sbatch, or srun and\nvalue is a default value defined by the user. The following\nexample would configure each job to have a default name, time limit, amount\nof memory, and error and output files:\n\njob-name=default_name\ntime=10:00\nmem=256\nerror = slurm-%j.errfile\noutput = slurm-%j.logfile\n\nYou can also specify different default settings for jobs based on the\ncommand being used to submit the job and/or the cluster being submitted to.\nThe syntax for this would be:\n<command>:<cluster>:<component>\n<command> could be one of:\n\nsalloc: Jobs submitted with the salloc command.\nsbatch: Jobs submitted with the sbatch command.\nsrun: Jobs submitted with the srun command.\n*: Jobs submitted with any submission command.\n\n<cluster> could be any defined cluster on your system,\nor * to have it match any cluster.\n<component> is any of the job submission options\navailable to salloc, sbatch, or srun.\nThe following example would assign different default partitions based on\nthe command used to submit the job. It would also assign different partitions\nfor jobs submitted with salloc, depending on the cluster being used:\n\nsalloc:cluster1:partition = interactive\nsalloc:cluster2:partition = member\nsbatch:*:partition = high\nsrun:*:partition = short\n\nLast modified 19 February 2024\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "API Functions",
                "content": "All of the following functions are required. Functions which are not\nimplemented must be stubbed.int init(void)\nDescription:\n  Called when the plugin is loaded, before any other functions are\n  called. Put global initialization here.\nReturns: \nSLURM_SUCCESS on success, or\nSLURM_ERROR on failure.\nvoid fini(void)\nDescription:\n  Called when the plugin is removed. Clear any allocated storage here.\nReturns: None.\nNote: These init and fini functions are not the same as those\ndescribed in the dlopen (3) system library.\nThe C run-time system co-opts those symbols for its own initialization.\nThe system _init() is called before the Slurm\ninit(), and the Slurm\nfini() is called before the system's\n_fini().\nint cli_filter_p_setup_defaults(slurm_opt_t *options, bool early)\nDescription:\nThis function is called by the salloc, sbatch, or srun command line interface\n(CLI) programs shortly before processing any options from the environment,\ncommand line, or script (#SBATCH). The hook may be run multiple times per job\ncomponent, once for an early pass (if implemented by the CLI), and again for\nthe main pass.\nNote that this call is skipped for any srun command run within an existing job\nallocation to prevent settings from overriding the set of options that have been\npopulated for the job based on the job environment.\nThe options and early arguments are meant to be passed to slurm_option_set()\nwhich will set the option if it is in the appropriate pass. Failures to set\nan option may be a symptom of trying to set the option on the wrong pass. Given\nthat you should not return SLURM_ERROR simply because of a failure to set an option.\nArguments: \noptions\n(input) slurm option data structure; meant to be passed to the slurm_option_* API\nwithin src/common/slurm_opt.h.\nearly\n(input) boolean indicating if this is the early pass or not; meant to be passed to\nthe slurm_option_* API within src/common/slurm_opt.h.\nReturns: \nSLURM_SUCCESS on success, or\nSLURM_ERROR on failure, will terminate execution\nof the CLI.\n\nint cli_filter_p_pre_submit(slurm_opt_t *options, int offset)\nDescription:\nThis function is called by the CLI after option processing but before any\ncommunication with the slurmctld is made.  This is after all\ncli_filter_p_setup_defaults()\nhooks are executed (for the current job component), environment variables\nprocessed, command line options and #SBATCH directives interpreted.\ncli_filter_p_pre_submit() is called before any parts of\nthe data structure are rewritten, so it is safe to\nboth read and write or unset any options from the plugin that you desire.\nNote that cli_filter_p_post_submit() cannot safely read (or write) the options,\nso you should save any state for logging in cli_filter_p_post_submit() during\ncli_filter_p_pre_submit(). This function is called once per job component.\nArguments: \noptions\n(input/output) the job allocation request specifications.\noffset\n(input) integer value for the current hetjob offset; should be used as a key when\nstoring data for communication between cli_filter_p_pre_submit() and\ncli_filter_p_post_submit().\nReturns: \nSLURM_SUCCESS on success, or\nSLURM_ERROR on failure, will terminate execution\nof the CLI.\n\nvoid cli_filter_p_post_submit(int offset, uint32_t jobid, uint32_t stepid)\nDescription:\nThis function is called by the CLI after a jobid (and, if srun, a stepid) has\nbeen assigned by the controller.  It is no longer safe to read or write to the\noptions data structure, so it has been removed from this function.  You should\nsave any state you need in cli_filter_p_pre_submit() using het_job_offset as a\nkey, since the function is called separately for every job component, and access\nit here.\nArguments: \noffset\n(input) integer value for the current hetjob offset; should be used as a key\nwhen storing data for communication between cli_filter_p_pre_submit() and\ncli_filter_p_post_submit().\njobid\n(input) job id of the job\nstepid\n(input) step id of the job if appropriate, NO_VAL otherwise\nLUA Interface\nSetting CliFilterPlugins=cli_filter/lua in slurm.conf will allow\nyou to implement the API functions mentioned using Lua language. The file must\nbe named \"cli_filter.lua\" and, similar to the job_submit plugin, it must be\nlocated in the default configuration directory (typically the subdirectory\n\"etc\" of the installation).\nAn example is provided within the source code\n\nhere.\nNOTE: Although available options are defined in the struct\nslurm_opt_t within\nsrc/common/slurm_opt.h, some options might be renamed. The provided\nexample shows a way of displaying the configured options by using\nslurm.json_cli_options(options).\nUser Defaults\nSetting CliFilterPlugins=cli_filter/user_defaults in slurm.conf will\nallow users to define their own defaults for jobs submitted from the machine(s)\nwith the configured file. The plugin looks for the definition file in\n$HOME/.slurm/defaults. It will read each line as a\ncomponent=value pair, where component is any of the\njob submission options available to salloc, sbatch, or srun and\nvalue is a default value defined by the user. The following\nexample would configure each job to have a default name, time limit, amount\nof memory, and error and output files:\n\njob-name=default_name\ntime=10:00\nmem=256\nerror = slurm-%j.errfile\noutput = slurm-%j.logfile\n\nYou can also specify different default settings for jobs based on the\ncommand being used to submit the job and/or the cluster being submitted to.\nThe syntax for this would be:\n<command>:<cluster>:<component>\n<command> could be one of:\n\nsalloc: Jobs submitted with the salloc command.\nsbatch: Jobs submitted with the sbatch command.\nsrun: Jobs submitted with the srun command.\n*: Jobs submitted with any submission command.\n\n<cluster> could be any defined cluster on your system,\nor * to have it match any cluster.\n<component> is any of the job submission options\navailable to salloc, sbatch, or srun.\nThe following example would assign different default partitions based on\nthe command used to submit the job. It would also assign different partitions\nfor jobs submitted with salloc, depending on the cluster being used:\n\nsalloc:cluster1:partition = interactive\nsalloc:cluster2:partition = member\nsbatch:*:partition = high\nsrun:*:partition = short\n\nLast modified 19 February 2024\n"
            },
            {
                "title": "LUA Interface",
                "content": "Setting CliFilterPlugins=cli_filter/lua in slurm.conf will allow\nyou to implement the API functions mentioned using Lua language. The file must\nbe named \"cli_filter.lua\" and, similar to the job_submit plugin, it must be\nlocated in the default configuration directory (typically the subdirectory\n\"etc\" of the installation).\nAn example is provided within the source code\n\nhere.NOTE: Although available options are defined in the struct\nslurm_opt_t within\nsrc/common/slurm_opt.h, some options might be renamed. The provided\nexample shows a way of displaying the configured options by using\nslurm.json_cli_options(options).User DefaultsSetting CliFilterPlugins=cli_filter/user_defaults in slurm.conf will\nallow users to define their own defaults for jobs submitted from the machine(s)\nwith the configured file. The plugin looks for the definition file in\n$HOME/.slurm/defaults. It will read each line as a\ncomponent=value pair, where component is any of the\njob submission options available to salloc, sbatch, or srun and\nvalue is a default value defined by the user. The following\nexample would configure each job to have a default name, time limit, amount\nof memory, and error and output files:\n\njob-name=default_name\ntime=10:00\nmem=256\nerror = slurm-%j.errfile\noutput = slurm-%j.logfile\nYou can also specify different default settings for jobs based on the\ncommand being used to submit the job and/or the cluster being submitted to.\nThe syntax for this would be:\n<command>:<cluster>:<component><command> could be one of:\n\nsalloc: Jobs submitted with the salloc command.\nsbatch: Jobs submitted with the sbatch command.\nsrun: Jobs submitted with the srun command.\n*: Jobs submitted with any submission command.\n<cluster> could be any defined cluster on your system,\nor * to have it match any cluster.<component> is any of the job submission options\navailable to salloc, sbatch, or srun.The following example would assign different default partitions based on\nthe command used to submit the job. It would also assign different partitions\nfor jobs submitted with salloc, depending on the cluster being used:\n\nsalloc:cluster1:partition = interactive\nsalloc:cluster2:partition = member\nsbatch:*:partition = high\nsrun:*:partition = short\nLast modified 19 February 2024"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/sched_config.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Scheduling Configuration Guide",
                "content": "OverviewSlurm is designed to perform a quick and simple scheduling attempt at\nevents such as job submission or completion and configuration changes.\nDuring these event-triggered scheduling events, default_queue_depth\n(default is 100) number of jobs will be considered.At less frequent intervals, defined by sched_interval, the main\nscheduling loop will run, considering all jobs while still honoring the\npartition_job_depth limit.In both cases, jobs are evaluated in a strict priority order and once any\njob or job array task in a partition is left pending, no other jobs in that\npartition will be scheduled to avoid taking resources from the higher-priority\npending job.A more comprehensive scheduling attempt is typically done by the backfill\nscheduling plugin, which considers job run time and resources required to\ndetermine if lower-priority jobs would actually take resources needed by\nhigher-priority jobs. This allows the backfill scheduler to assign more specific\nreasons to pending jobs, or to start jobs\nthat were previously pending.Scheduling Configuration\n\nThe SchedulerType configuration parameter specifies the scheduler\nplugin to use.\nOptions are sched/backfill, which performs backfill scheduling, and\nsched/builtin, which attempts to schedule jobs in a strict priority order within\neach partition/queue.There is also a SchedulerParameters configuration parameter which\ncan specify a wide range of parameters as described below.\nThis first set of parameters applies to all scheduling configurations.\nSee the slurm.conf(5) man page for more details.\n\ndefault_queue_depth=# - Specifies the number of jobs to consider for\nscheduling on each event that may result in a job being scheduled.\nDefault value is 100 jobs. Since this happens frequently, a relatively\nsmall number is generally best.\ndefer - Do not attempt to schedule jobs individually at submit time.\nCan be useful for high-throughput computing.\nmax_switch_wait=# - Specifies the maximum time a job can wait for\ndesired number of leaf switches. Default value is 300 seconds.\npartition_job_depth=# - Specifies how many jobs are tested in any\nsingle partition, default value is 0 (no limit).\nsched_interval=# - Specifies how frequently, in seconds, the main\nscheduling loop will execute and test all pending jobs, with the\npartition_job_depth limit in place. The default value is 60 seconds.\nBackfill Scheduling\n\nThe backfill scheduling plugin is loaded by default.\nWithout backfill scheduling, each partition is scheduled strictly in priority\norder, which typically results in significantly lower system utilization and\nresponsiveness than otherwise possible.\nBackfill scheduling will start lower priority jobs if doing so does not delay\nthe expected start time of any higher priority jobs.\nSince the expected start time of pending jobs depends upon the expected\ncompletion time of running jobs, reasonably accurate time limits are important\nfor backfill scheduling to work well.Slurm's backfill scheduler takes into consideration every running job.\nIt then considers pending jobs in priority order, determining when and where\neach will start, taking into consideration the possibility of\njob preemption,\ngang scheduling,\ngeneric resource (GRES) requirements,\nmemory requirements, etc.\nIf the job under consideration can start immediately without impacting the\nexpected start time of any higher priority job, then it does so.\nOtherwise the resources required by the job will be reserved during the job's\nexpected execution time.\nThe backfill plugin will set the expected start time for pending jobs setting\nthese reserved nodes into a 'Planned' state. A job's\nexpected start time can be seen using the squeue --start command.\nFor performance reasons, the backfill scheduler reserves whole nodes for jobs,\neven if jobs don't require whole nodes.\nThe scheduling logic builds a sorted list of job-partition pairs. Jobs\nsubmitted to multiple partitions will have as many entries in the list as\nrequested partitions. By default, the backfill scheduler may evaluate all the\njob-partition pairs for a single job, potentially reserving resources for each\npair, but only starting the job in the reservation offering the earliest start\ntime.Having a single job reserving resources for multiple partitions could impede\nother jobs (or hetjob components) from reserving resources already reserved for\nthe partitions that don't offer the earliest start time.\nA single job that requests multiple partitions can also prevent itself\nfrom starting earlier in a lower priority partition if the partitions overlap\nnodes and a backfill reservation in the higher priority partition blocks nodes\nthat are also in the lower priority partition.Backfill scheduling is difficult without reasonable time limit estimates\nfor jobs, but some configuration parameters that can help.\nDefaultTime - Default job time limit (specify value by partition)\nMaxTime - Maximum job time limit (specify value by partition)\nOverTimeLimit - Amount by which a job can exceed its time limit\nbefore it is killed. A system-wide configuration parameter.\nBackfill scheduling is a time consuming operation.\nLocks are released briefly every two seconds so that other options can be\nprocessed, for example to process new job submission requests.\nBackfill scheduling can optionally continue execution after the lock release\nand ignore newly submitted jobs (SchedulerParameters=bf_continue).\nDoing so will permit consideration of more jobs, but may result in the delayed\nscheduling of newly submitted jobs.\nA partial list of SchedulerParameters configuration parameters related to\nbackfill scheduling follows.\nFor more details and a complete list of the backfill related SchedulerParameters\nsee the slurm.conf(5) man page.\n\nbf_continue - If set, then continue backfill scheduling after\nperiodically releasing locks for other operations.\nbf_interval=# - Interval between backfill scheduling attempts.\nDefault value is 30 seconds.\nbf_max_job_part=# - Maximum number of jobs to initiate per partition\nin each backfill cycle. Default value is 0 (no limit).\nbf_max_job_start=# - Maximum number of jobs to initiate\nin each backfill cycle. Default value is 0 (no limit).\nbf_max_job_test=# - Maximum number of jobs consider for backfill\nscheduling in each backfill cycle. Default value is 100 jobs.\nbf_max_job_user=# - Maximum number of jobs to initiate per user\nin each backfill cycle. Default value is 0 (no limit).\nbf_max_time=# - Maximum time in seconds the backfill scheduler can\nspend (including time spent sleeping when locks are released) before\ndiscontinuing. The default value is the value of bf_interval, which\ndefaults to 30 seconds.\nbf_one_resv_per_job - Disallow adding more than one backfill\nreservation per job. This option makes it so that a job submitted to multiple\npartitions will stop reserving resources once the first job-partition pair\nhas booked a backfill reservation. Subsequent pairs from the same job will\nonly be tested to start now. This allows for other jobs to be able to book the\nother pairs resources at the cost of not guaranteeing that the multi-partition\njob will start in the partition offering the earliest start time (unless it\ncan start immediately). This option is disabled by default.\nbf_resolution=# - Time resolution of backfill scheduling.\nDefault value is 60 seconds.\nLarger values are appropriate if job time limits are imprecise and/or\nsmall delays in starting pending jobs in order to achieve higher system\nutilization is desired.\nbf_window=# - How long, in minutes, into the future to look when\ndetermining when and where jobs can start.\nHigher values result in more overhead and less responsiveness.\nA value at least as long as the highest allowed time limit is generally\nadvisable to prevent job starvation.\nIn order to limit the amount of data managed by the backfill scheduler,\nif the value of bf_window is increased, then it is generally advisable\nto also increase bf_resolution.\nThe default value is 1440 minutes (one day).\nbf_yield_interval=# - \nThe backfill scheduler will periodically relinquish locks in order for other\npending operations to take place. This specifies the times when the locks are\nrelinquished in microseconds. The default value is 2,000,000  microseconds\n(2 seconds). Smaller values may be helpful for high throughput computing when\nused in conjunction with the bf_continue option.\nbf_yield_sleep=# - \nThe backfill scheduler will periodically relinquish locks in order for other\npending operations to take place. This specifies the length of time for which\nthe locks are relinquished in microseconds. The default value is 500,000\nmicroseconds (0.5 seconds).  \nLast modified 04 June 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://www.schedmd.com/slurm-support/our-services/",
        "sections": [
            {
                "title": "Can SchedMD provide remote training options for Slurm?",
                "content": "\n\n\nYes, we offer remote training options in addition to our onsite training program.\n\n\n"
            },
            {
                "title": "Does SchedMD offer ongoing support after the completion of training?",
                "content": "\n\n\nYes, we provide ongoing support to ensure that you continue to maximize the benefits of Slurm.\n\n\n"
            },
            {
                "title": "How can SchedMD assist with optimizing job scheduling in Slurm?",
                "content": "\n\n\nOur experts can provide guidance and best practices for efficient job scheduling to improve system performance.\n\n\n"
            },
            {
                "title": "Does SchedMD offer customized Slurm workshops tailored to specific needs?",
                "content": "\n\n\nYes, we can tailor workshops to address specific requirements, providing targeted training for your organization.\n\n\n"
            },
            {
                "title": "Can SchedMD help integrate Slurm with other HPC system components?",
                "content": "\n\n\nAbsolutely. We have extensive experience in integrating Slurm with other components of HPC environments.\n\n\n"
            },
            {
                "title": "What is the process for scheduling a consultation with SchedMD?",
                "content": "\n\n\nTo schedule a consultation with SchedMD for Slurm services, contact us.\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/mcs.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Multi-Category Security (MCS) Guide",
                "content": "OverviewThe MCS Plugin is meant to extend the current Slurm functionality related to\njob node exclusivity and job/node information display privacy.\n\nSlurm's OverSubscribe option controls the ability of a partition to\nexecute more than one job at a time on each resource, no matter \"what type\"\nof job. Slurm job submission clients can also use the --exclusive and\n--oversubscribe parameters to request how the job can be\nshared. The ExclusiveUser\nslurm.conf parameter and the --exclusive=user client parameter value\nmodifies the exclusivity functionality. With this parameter enabled, the\n\"type of job\" now matters when considering exclusivity, so jobs can share\nresources based on job users, meaning that only jobs whose user is the\nsame can share resources. This indeed adds a new dimension to how Slurm manages\nexclusivity. With the introduction of the MCS Plugin, Slurm can now be\nconfigured to associate an MCS_label to jobs and optionally ensure that\nnodes can only be shared among jobs having the same label.\nThis adds even more degrees of freedom to how Slurm manages exclusivity,\nproviding end users with much more flexibility in this area.\nSlurm also has the PrivateData slurm.conf\nparameter, which is used to control what type of information is hidden from\nregular users. Similar to the exclusivity property, the MCS Plugin also\nextends the privacy by filtering job and/or node information\nbased on the user's access to their MCS_label. This means that privacy\nis now less restrictive, and information is not just hidden or not to regular\nusers, but now it is filtered depending on these configurable/requestable\nlabels in coordination with the PrivateData option.\n\nLimitations\n\nUse of MCS restricts the operation of Preemption.\nSpecifically, any job that has an MCS label and requests node exclusivity\naccording to that label will be prevented from preempting or being preempted by\nany job that doesn't match that label. If\nMCSParameters=enforced,select is set, these restrictions will\napply to all jobs.\nConfiguration\nTwo parameters are currently available to configure MCS: MCSPlugin\nand MCSParameters.\n\n\nMCSPlugin Specifies which plugin should be used. Plugins are\nmutually exclusive, and the type of label to be associated depends\non the loaded plugin.\n\nmcs/none is the default and disables MCS labels and functionality.\n\nmcs/account MCS labels can only take a value equal to the job's\n--account. NOTE: this option requires enabling of accounting.\n\nmcs/group MCS labels can only take a value equal to the job's user\ngroup.\n\nmcs/user MCS labels can only take a value equal to the username of\nthe job's --uid.\n\n\n\nMCS_labels on jobs can be displayed through 'squeue' with the format option\nmcslabel or through 'scontrol show job'. Nodes also acquire a MCS_label,\nwhich is inherited from the allocated job's MCS_label. The node's label can be\ndisplayed with 'scontrol show nodes'. The 'sview' command can also be used to\nsee these MCS_labels.\nUsers may either request a particular category label for a job (through the\n--mcs-label option), or use the default value generated by the specific\nMCS plugin implementation. So labels can be configured to be enforced or set\non demand, and the specific MCS Plugin is responsible for checking the validity\nof these labels. When enforced, the MCS Plugin implementation will always\nassociate a MCS label to a submitted job, either the default value or the one\nrequested by the user (if it's considered correct).\nThe selection (exclusivity) of nodes can be filtered on MCS labels either\non demand (ondemand) or always (select) or never (noselect). Users can force\nthe filter with the --exclusive=mcs option (except if noselect mode is\nenabled).\n\nLabel enforcement, node selection filtering policy, private data based on\nlabels and a list of user groups allowed to be mapped to MCS labels can be\nconfigured through MCSParameters option.\n\nMCSParameters Specifies the options to pass to the specific MCS\nPlugin implementation. Options should satisfy the following expression:\n\n\"[ondemand|enforced][,noselect|select|ondemandselect][,privatedata]:[mcs_plugin_parameters]\".\nThe defaults are \"ondemand,ondemandselect\" and no privatedata.\n\n\n\nondemand|enforced set MCS label on jobs either on demand (using\n--mcs-label option) or always\nnoselect|select|ondemandselect select nodes with filter on MCS\nlabel: never, always or on demand (using --exclusive=mcs option)\nprivatedata accordingly with PrivateData specific option:\n\n\nIf configured with privatedata and PrivateData=jobs, job\ninformation is filtered based on their MCS labels.\n\n\nIf configured with privatedata and PrivateData=nodes, node\ninformation is filtered based on their MCS labels.\n\n\nmcs_plugin_parameters Only mcs/group is currently supporting this\noption. It can be used to specify the list of user groups (separated by the\n'|' character) that are allowed to be mapped to MCS labels by the\nmcs/group plugin.\n\n\n\n\nWhen enforcing privatedata with mcs/account, users will need to have an\nassociation with just the relevant Account to see other jobs in that Account.\nIf a Partition is included in the association, it will be over-constrained\nand will not allow them to view other jobs in the Account.\n\nDifferent requests and configurations lead to different combinations of\nuse-cases. The following table is intended to help understand the end user\nthe expected behavior (related to exclusivity) for a subset of these use-cases:\n\n\n\n\n\n\t\t        Node filtering:\n\t\t\n\n\t\t\tLabel enforcement: ondemand\n\t\t\t(MCS_label set only if requested.)\n\t\t\n\n\t\t\tLabel enforcement: enforced\n\t\t\t(MCS_label is mandatory.)\n\t\t\n\n\n\nnoselect\n\n\n\t\t        No filter on nodes even if --exclusive=mcs requested.\n\t\t\n\n\t\t\tNo filter on nodes even if --exclusive=mcs requested.\n\t\t\n\n\n\nselect\n\n\n\t\t\tFilter on nodes only if job MCS_label is set.\n\t\t\n\n\t\t\tAlways filter on nodes.\n\t\t\n\n\n\nondemandselect\n\n\n\t\t\tFilter on nodes only if --exclusive=mcs.\n\t\t\n\n\t\t\tFilter on nodes only if --exclusive=mcs.\n\t\t\n\n\nSome examples\nslurm.conf:\n\nMCSPlugin=mcs/account\nMCSParameters=enforced,select,privatedata\n\n\nor\n\n\n\nMCSPlugin=mcs/group\nMCSParameters=ondemand,noselect:groupA|groupB|groupC\n\n\nor\n\n\n\nMCSPlugin=mcs/user\nMCSParameters=enforced,select,privatedata\n\nView MCS parameters\n\n\nConfigured MCS parameters can be viewed using the scontrol command.\n\nscontrol show config |grep MCS\nMCSPlugin          = mcs/group\nMCSParameters      = ondemand,noselect:groupA|groupB|groupC\n\nPut an mcs_label on a job using salloc, sbatch or srun\n\n\n\nsrun -n10 -t 1000 --mcs-label=groupB ./job &\n\nPut an mcs_label on a job with exclusivity\n\n\n\nsrun -n10 -t 1000 --mcs-label=groupB --exclusive=mcs ./job &\n\n\nPut a different mcs_label on a job with the mcs/account plugin\n\n\n\nsrun -n10 -t 1000 -A another_account_than_default ./job &\n\nView MCS parameters in usage\n\n\nThe squeue command can also display the MCS with mcslabel\nformat option\n\nsqueue -O jobid,username,mcslabel\nJOBID               USER                MCSLABEL\n2                   foo                 groupA\n3\t            bar                 groupB\n\nThe scontrol command will now display the MCS_label\n\nscontrol show nodes\nNodeName=node0001 Arch=x86_64 CoresPerSocket=4\n   CPUAlloc=0 CPUTot=8 CPULoad=0.01 Features=(null)\n   Gres=(null)\n   NodeAddr=noder0001 NodeHostName=node0001 Version=15.08\n   OS=Linux RealMemory=23780 AllocMem=0 Sockets=2 Boards=1\n   State=IDLE ThreadsPerCore=1 TmpDisk=0 Weight=40 Owner=N/A MCS_label=groupA\n   BootTime=2015-08-05T10:14:41 SlurmdStartTime=2015-09-07T13:42:28\n   CurrentWatts=0 AveWatts=0\n\nLast modified 21 May 2024\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Configuration",
                "content": "Two parameters are currently available to configure MCS: MCSPlugin\nand MCSParameters.\n\n\nMCSPlugin Specifies which plugin should be used. Plugins are\nmutually exclusive, and the type of label to be associated depends\non the loaded plugin.\n\nmcs/none is the default and disables MCS labels and functionality.\n\nmcs/account MCS labels can only take a value equal to the job's\n--account. NOTE: this option requires enabling of accounting.\n\nmcs/group MCS labels can only take a value equal to the job's user\ngroup.\n\nmcs/user MCS labels can only take a value equal to the username of\nthe job's --uid.\n\n\n\nMCS_labels on jobs can be displayed through 'squeue' with the format option\nmcslabel or through 'scontrol show job'. Nodes also acquire a MCS_label,\nwhich is inherited from the allocated job's MCS_label. The node's label can be\ndisplayed with 'scontrol show nodes'. The 'sview' command can also be used to\nsee these MCS_labels.\nUsers may either request a particular category label for a job (through the\n--mcs-label option), or use the default value generated by the specific\nMCS plugin implementation. So labels can be configured to be enforced or set\non demand, and the specific MCS Plugin is responsible for checking the validity\nof these labels. When enforced, the MCS Plugin implementation will always\nassociate a MCS label to a submitted job, either the default value or the one\nrequested by the user (if it's considered correct).\nThe selection (exclusivity) of nodes can be filtered on MCS labels either\non demand (ondemand) or always (select) or never (noselect). Users can force\nthe filter with the --exclusive=mcs option (except if noselect mode is\nenabled).\n\nLabel enforcement, node selection filtering policy, private data based on\nlabels and a list of user groups allowed to be mapped to MCS labels can be\nconfigured through MCSParameters option.\n\nMCSParameters Specifies the options to pass to the specific MCS\nPlugin implementation. Options should satisfy the following expression:\n\n\"[ondemand|enforced][,noselect|select|ondemandselect][,privatedata]:[mcs_plugin_parameters]\".\nThe defaults are \"ondemand,ondemandselect\" and no privatedata.\n\n\n\nondemand|enforced set MCS label on jobs either on demand (using\n--mcs-label option) or always\nnoselect|select|ondemandselect select nodes with filter on MCS\nlabel: never, always or on demand (using --exclusive=mcs option)\nprivatedata accordingly with PrivateData specific option:\n\n\nIf configured with privatedata and PrivateData=jobs, job\ninformation is filtered based on their MCS labels.\n\n\nIf configured with privatedata and PrivateData=nodes, node\ninformation is filtered based on their MCS labels.\n\n\nmcs_plugin_parameters Only mcs/group is currently supporting this\noption. It can be used to specify the list of user groups (separated by the\n'|' character) that are allowed to be mapped to MCS labels by the\nmcs/group plugin.\n\n\n\n\nWhen enforcing privatedata with mcs/account, users will need to have an\nassociation with just the relevant Account to see other jobs in that Account.\nIf a Partition is included in the association, it will be over-constrained\nand will not allow them to view other jobs in the Account.\n\nDifferent requests and configurations lead to different combinations of\nuse-cases. The following table is intended to help understand the end user\nthe expected behavior (related to exclusivity) for a subset of these use-cases:\n\n\n\n\n\n\t\t        Node filtering:\n\t\t\n\n\t\t\tLabel enforcement: ondemand\n\t\t\t(MCS_label set only if requested.)\n\t\t\n\n\t\t\tLabel enforcement: enforced\n\t\t\t(MCS_label is mandatory.)\n\t\t\n\n\n\nnoselect\n\n\n\t\t        No filter on nodes even if --exclusive=mcs requested.\n\t\t\n\n\t\t\tNo filter on nodes even if --exclusive=mcs requested.\n\t\t\n\n\n\nselect\n\n\n\t\t\tFilter on nodes only if job MCS_label is set.\n\t\t\n\n\t\t\tAlways filter on nodes.\n\t\t\n\n\n\nondemandselect\n\n\n\t\t\tFilter on nodes only if --exclusive=mcs.\n\t\t\n\n\t\t\tFilter on nodes only if --exclusive=mcs.\n\t\t\n\n\nSome examples\nslurm.conf:\n\nMCSPlugin=mcs/account\nMCSParameters=enforced,select,privatedata\n\n\nor\n\n\n\nMCSPlugin=mcs/group\nMCSParameters=ondemand,noselect:groupA|groupB|groupC\n\n\nor\n\n\n\nMCSPlugin=mcs/user\nMCSParameters=enforced,select,privatedata\n\nView MCS parameters\n\n\nConfigured MCS parameters can be viewed using the scontrol command.\n\nscontrol show config |grep MCS\nMCSPlugin          = mcs/group\nMCSParameters      = ondemand,noselect:groupA|groupB|groupC\n\nPut an mcs_label on a job using salloc, sbatch or srun\n\n\n\nsrun -n10 -t 1000 --mcs-label=groupB ./job &\n\nPut an mcs_label on a job with exclusivity\n\n\n\nsrun -n10 -t 1000 --mcs-label=groupB --exclusive=mcs ./job &\n\n\nPut a different mcs_label on a job with the mcs/account plugin\n\n\n\nsrun -n10 -t 1000 -A another_account_than_default ./job &\n\nView MCS parameters in usage\n\n\nThe squeue command can also display the MCS with mcslabel\nformat option\n\nsqueue -O jobid,username,mcslabel\nJOBID               USER                MCSLABEL\n2                   foo                 groupA\n3\t            bar                 groupB\n\nThe scontrol command will now display the MCS_label\n\nscontrol show nodes\nNodeName=node0001 Arch=x86_64 CoresPerSocket=4\n   CPUAlloc=0 CPUTot=8 CPULoad=0.01 Features=(null)\n   Gres=(null)\n   NodeAddr=noder0001 NodeHostName=node0001 Version=15.08\n   OS=Linux RealMemory=23780 AllocMem=0 Sockets=2 Boards=1\n   State=IDLE ThreadsPerCore=1 TmpDisk=0 Weight=40 Owner=N/A MCS_label=groupA\n   BootTime=2015-08-05T10:14:41 SlurmdStartTime=2015-09-07T13:42:28\n   CurrentWatts=0 AveWatts=0\n\nLast modified 21 May 2024\n"
            },
            {
                "title": "Some examples",
                "content": "slurm.conf:\n\nMCSPlugin=mcs/account\nMCSParameters=enforced,select,privatedata\n\n\nor\n\n\n\nMCSPlugin=mcs/group\nMCSParameters=ondemand,noselect:groupA|groupB|groupC\n\n\nor\n\n\n\nMCSPlugin=mcs/user\nMCSParameters=enforced,select,privatedata\n\nView MCS parameters\n\n\nConfigured MCS parameters can be viewed using the scontrol command.\n\nscontrol show config |grep MCS\nMCSPlugin          = mcs/group\nMCSParameters      = ondemand,noselect:groupA|groupB|groupC\n\nPut an mcs_label on a job using salloc, sbatch or srun\n\n\n\nsrun -n10 -t 1000 --mcs-label=groupB ./job &\n\nPut an mcs_label on a job with exclusivity\n\n\n\nsrun -n10 -t 1000 --mcs-label=groupB --exclusive=mcs ./job &\n\n\nPut a different mcs_label on a job with the mcs/account plugin\n\n\n\nsrun -n10 -t 1000 -A another_account_than_default ./job &\n\nView MCS parameters in usage\n\n\nThe squeue command can also display the MCS with mcslabel\nformat option\n\nsqueue -O jobid,username,mcslabel\nJOBID               USER                MCSLABEL\n2                   foo                 groupA\n3\t            bar                 groupB\n\nThe scontrol command will now display the MCS_label\n\nscontrol show nodes\nNodeName=node0001 Arch=x86_64 CoresPerSocket=4\n   CPUAlloc=0 CPUTot=8 CPULoad=0.01 Features=(null)\n   Gres=(null)\n   NodeAddr=noder0001 NodeHostName=node0001 Version=15.08\n   OS=Linux RealMemory=23780 AllocMem=0 Sockets=2 Boards=1\n   State=IDLE ThreadsPerCore=1 TmpDisk=0 Weight=40 Owner=N/A MCS_label=groupA\n   BootTime=2015-08-05T10:14:41 SlurmdStartTime=2015-09-07T13:42:28\n   CurrentWatts=0 AveWatts=0\n\nLast modified 21 May 2024\n"
            },
            {
                "title": "View MCS parameters\n\n",
                "content": "Configured MCS parameters can be viewed using the scontrol command.\n\nscontrol show config |grep MCS\nMCSPlugin          = mcs/group\nMCSParameters      = ondemand,noselect:groupA|groupB|groupC\n\nPut an mcs_label on a job using salloc, sbatch or srun\n\n\n\nsrun -n10 -t 1000 --mcs-label=groupB ./job &\n\nPut an mcs_label on a job with exclusivity\n\n\n\nsrun -n10 -t 1000 --mcs-label=groupB --exclusive=mcs ./job &\n\n\nPut a different mcs_label on a job with the mcs/account plugin\n\n\n\nsrun -n10 -t 1000 -A another_account_than_default ./job &\n\nView MCS parameters in usage\n\n\nThe squeue command can also display the MCS with mcslabel\nformat option\n\nsqueue -O jobid,username,mcslabel\nJOBID               USER                MCSLABEL\n2                   foo                 groupA\n3\t            bar                 groupB\n\nThe scontrol command will now display the MCS_label\n\nscontrol show nodes\nNodeName=node0001 Arch=x86_64 CoresPerSocket=4\n   CPUAlloc=0 CPUTot=8 CPULoad=0.01 Features=(null)\n   Gres=(null)\n   NodeAddr=noder0001 NodeHostName=node0001 Version=15.08\n   OS=Linux RealMemory=23780 AllocMem=0 Sockets=2 Boards=1\n   State=IDLE ThreadsPerCore=1 TmpDisk=0 Weight=40 Owner=N/A MCS_label=groupA\n   BootTime=2015-08-05T10:14:41 SlurmdStartTime=2015-09-07T13:42:28\n   CurrentWatts=0 AveWatts=0\n\nLast modified 21 May 2024\n"
            },
            {
                "title": "View MCS parameters in usage\n\n",
                "content": "The squeue command can also display the MCS with mcslabel\nformat option\n\nsqueue -O jobid,username,mcslabel\nJOBID               USER                MCSLABEL\n2                   foo                 groupA\n3\t            bar                 groupB\n\nThe scontrol command will now display the MCS_label\n\nscontrol show nodes\nNodeName=node0001 Arch=x86_64 CoresPerSocket=4\n   CPUAlloc=0 CPUTot=8 CPULoad=0.01 Features=(null)\n   Gres=(null)\n   NodeAddr=noder0001 NodeHostName=node0001 Version=15.08\n   OS=Linux RealMemory=23780 AllocMem=0 Sockets=2 Boards=1\n   State=IDLE ThreadsPerCore=1 TmpDisk=0 Weight=40 Owner=N/A MCS_label=groupA\n   BootTime=2015-08-05T10:14:41 SlurmdStartTime=2015-09-07T13:42:28\n   CurrentWatts=0 AveWatts=0\n\nLast modified 21 May 2024\n"
            },
            {
                "title": "Limitations\n",
                "content": "Use of MCS restricts the operation of Preemption.\nSpecifically, any job that has an MCS label and requests node exclusivity\naccording to that label will be prevented from preempting or being preempted by\nany job that doesn't match that label. If\nMCSParameters=enforced,select is set, these restrictions will\napply to all jobs.ConfigurationTwo parameters are currently available to configure MCS: MCSPlugin\nand MCSParameters.\n\n\nMCSPlugin Specifies which plugin should be used. Plugins are\nmutually exclusive, and the type of label to be associated depends\non the loaded plugin.\n\nmcs/none is the default and disables MCS labels and functionality.\n\nmcs/account MCS labels can only take a value equal to the job's\n--account. NOTE: this option requires enabling of accounting.\n\nmcs/group MCS labels can only take a value equal to the job's user\ngroup.\n\nmcs/user MCS labels can only take a value equal to the username of\nthe job's --uid.\n\n\n\nMCS_labels on jobs can be displayed through 'squeue' with the format option\nmcslabel or through 'scontrol show job'. Nodes also acquire a MCS_label,\nwhich is inherited from the allocated job's MCS_label. The node's label can be\ndisplayed with 'scontrol show nodes'. The 'sview' command can also be used to\nsee these MCS_labels.\nUsers may either request a particular category label for a job (through the\n--mcs-label option), or use the default value generated by the specific\nMCS plugin implementation. So labels can be configured to be enforced or set\non demand, and the specific MCS Plugin is responsible for checking the validity\nof these labels. When enforced, the MCS Plugin implementation will always\nassociate a MCS label to a submitted job, either the default value or the one\nrequested by the user (if it's considered correct).\nThe selection (exclusivity) of nodes can be filtered on MCS labels either\non demand (ondemand) or always (select) or never (noselect). Users can force\nthe filter with the --exclusive=mcs option (except if noselect mode is\nenabled).\n\nLabel enforcement, node selection filtering policy, private data based on\nlabels and a list of user groups allowed to be mapped to MCS labels can be\nconfigured through MCSParameters option.\n\nMCSParameters Specifies the options to pass to the specific MCS\nPlugin implementation. Options should satisfy the following expression:\n\n\"[ondemand|enforced][,noselect|select|ondemandselect][,privatedata]:[mcs_plugin_parameters]\".\nThe defaults are \"ondemand,ondemandselect\" and no privatedata.\n\n\n\nondemand|enforced set MCS label on jobs either on demand (using\n--mcs-label option) or always\nnoselect|select|ondemandselect select nodes with filter on MCS\nlabel: never, always or on demand (using --exclusive=mcs option)\nprivatedata accordingly with PrivateData specific option:\n\n\nIf configured with privatedata and PrivateData=jobs, job\ninformation is filtered based on their MCS labels.\n\n\nIf configured with privatedata and PrivateData=nodes, node\ninformation is filtered based on their MCS labels.\n\n\nmcs_plugin_parameters Only mcs/group is currently supporting this\noption. It can be used to specify the list of user groups (separated by the\n'|' character) that are allowed to be mapped to MCS labels by the\nmcs/group plugin.\n\n\n\n\nWhen enforcing privatedata with mcs/account, users will need to have an\nassociation with just the relevant Account to see other jobs in that Account.\nIf a Partition is included in the association, it will be over-constrained\nand will not allow them to view other jobs in the Account.\n\nDifferent requests and configurations lead to different combinations of\nuse-cases. The following table is intended to help understand the end user\nthe expected behavior (related to exclusivity) for a subset of these use-cases:\n\n\n\n\n\n\t\t        Node filtering:\n\t\t\n\n\t\t\tLabel enforcement: ondemand\n\t\t\t(MCS_label set only if requested.)\n\t\t\n\n\t\t\tLabel enforcement: enforced\n\t\t\t(MCS_label is mandatory.)\n\t\t\n\n\n\nnoselect\n\n\n\t\t        No filter on nodes even if --exclusive=mcs requested.\n\t\t\n\n\t\t\tNo filter on nodes even if --exclusive=mcs requested.\n\t\t\n\n\n\nselect\n\n\n\t\t\tFilter on nodes only if job MCS_label is set.\n\t\t\n\n\t\t\tAlways filter on nodes.\n\t\t\n\n\n\nondemandselect\n\n\n\t\t\tFilter on nodes only if --exclusive=mcs.\n\t\t\n\n\t\t\tFilter on nodes only if --exclusive=mcs.\n\t\t\n\n\nSome examples\nslurm.conf:\n\nMCSPlugin=mcs/account\nMCSParameters=enforced,select,privatedata\n\n\nor\n\n\n\nMCSPlugin=mcs/group\nMCSParameters=ondemand,noselect:groupA|groupB|groupC\n\n\nor\n\n\n\nMCSPlugin=mcs/user\nMCSParameters=enforced,select,privatedata\n\nView MCS parameters\n\n\nConfigured MCS parameters can be viewed using the scontrol command.\n\nscontrol show config |grep MCS\nMCSPlugin          = mcs/group\nMCSParameters      = ondemand,noselect:groupA|groupB|groupC\n\nPut an mcs_label on a job using salloc, sbatch or srun\n\n\n\nsrun -n10 -t 1000 --mcs-label=groupB ./job &\n\nPut an mcs_label on a job with exclusivity\n\n\n\nsrun -n10 -t 1000 --mcs-label=groupB --exclusive=mcs ./job &\n\n\nPut a different mcs_label on a job with the mcs/account plugin\n\n\n\nsrun -n10 -t 1000 -A another_account_than_default ./job &\n\nView MCS parameters in usage\n\n\nThe squeue command can also display the MCS with mcslabel\nformat option\n\nsqueue -O jobid,username,mcslabel\nJOBID               USER                MCSLABEL\n2                   foo                 groupA\n3\t            bar                 groupB\n\nThe scontrol command will now display the MCS_label\n\nscontrol show nodes\nNodeName=node0001 Arch=x86_64 CoresPerSocket=4\n   CPUAlloc=0 CPUTot=8 CPULoad=0.01 Features=(null)\n   Gres=(null)\n   NodeAddr=noder0001 NodeHostName=node0001 Version=15.08\n   OS=Linux RealMemory=23780 AllocMem=0 Sockets=2 Boards=1\n   State=IDLE ThreadsPerCore=1 TmpDisk=0 Weight=40 Owner=N/A MCS_label=groupA\n   BootTime=2015-08-05T10:14:41 SlurmdStartTime=2015-09-07T13:42:28\n   CurrentWatts=0 AveWatts=0\n\nLast modified 21 May 2024\n"
            },
            {
                "title": "Put an mcs_label on a job using salloc, sbatch or srun\n\n",
                "content": "\nsrun -n10 -t 1000 --mcs-label=groupB ./job &\nPut an mcs_label on a job with exclusivity\n\n\nsrun -n10 -t 1000 --mcs-label=groupB --exclusive=mcs ./job &\n\nPut a different mcs_label on a job with the mcs/account plugin\n\n\nsrun -n10 -t 1000 -A another_account_than_default ./job &\nView MCS parameters in usage\n\nThe squeue command can also display the MCS with mcslabel\nformat option\n\nsqueue -O jobid,username,mcslabel\nJOBID               USER                MCSLABEL\n2                   foo                 groupA\n3\t            bar                 groupB\n\nThe scontrol command will now display the MCS_label\n\nscontrol show nodes\nNodeName=node0001 Arch=x86_64 CoresPerSocket=4\n   CPUAlloc=0 CPUTot=8 CPULoad=0.01 Features=(null)\n   Gres=(null)\n   NodeAddr=noder0001 NodeHostName=node0001 Version=15.08\n   OS=Linux RealMemory=23780 AllocMem=0 Sockets=2 Boards=1\n   State=IDLE ThreadsPerCore=1 TmpDisk=0 Weight=40 Owner=N/A MCS_label=groupA\n   BootTime=2015-08-05T10:14:41 SlurmdStartTime=2015-09-07T13:42:28\n   CurrentWatts=0 AveWatts=0\n\nLast modified 21 May 2024\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/prep_plugins.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "PrEp Plugin API",
                "content": "OverviewThis document describes the Slurm PrEp \u2014 short for \"Pr\"olog and\n\"Ep\"ilog \u2014 plugins API. It is intended as a resource to programmers\nwishing to write their own Slurm prep plugins.The purpose of the prep plugin APIs to provide a native C interface to the\nsame hooks traditionally used by the Prolog, Epilog,\nPrologSlurmctld, and EpilogSlurmctld scripts. Those interfaces\nare now implemented through the prep/script plugin, and that plugin\nserves as a good example of how to approach development of additional\nplugins.Slurm PrEp plugins must conform to the Slurm Plugin API with the following\nspecifications:\nconst char plugin_name[]=\"full\u00a0text\u00a0name\"\n\nA free-formatted ASCII text string that identifies the plugin.\n\nconst char plugin_type[]=\"major/minor\"\n\nThe major type must be \"prep\".\nThe minor type can be any suitable name for the type of prep plugin.\nconst uint32_t plugin_version\nIf specified, identifies the version of Slurm used to build this plugin and\nany attempt to load the plugin from a different version of Slurm will result\nin an error.\nIf not specified, then the plugin may be loaded by Slurm commands and\ndaemons from any version, however this may result in difficult to diagnose\nfailures due to changes in the arguments to plugin functions or changes\nin other Slurm functions used by the plugin.\nSlurm can be configured to use multiple prep plugins if desired through the\nPrEpPlugins configuration option. Additional plugins should be comma-separated.\nNote that, by default, the prep/script plugin is loaded if that option\nis not set, but will not be loaded if an explicit setting has been made. Thus,\nif you do set that option, and intend to still use the Prolog,\nEpilog, PrologSlurmctld, and/or EpilogSlurmctld options\nyou will need to ensure both your additional plugin and prep/script are\nset.Special care must be used when developing against the\nprep_p_prolog_slurmctld() or\nprep_p_epilog_slurmctld() interfaces. These\nfunctions are called while the slurmctld holds a number of internal locks,\nand need to return quickly otherwise slurmctld responsiveness and system\nthroughput will be impacted. For simple logging, this is not required, and\nthe \"async\" option can be left to false. But, especially for anything\ncommunicating with an external API or spawning additional processes, it is\nhighly recommended to first make a local copy of any job record details\nrequired, and then spawn a separate processing thread \u2014 which, by default,\nwill not have inherited any slurmctld locks \u2014 to continue processing.\nYou must set the async return value to true, and call the corresponding\nprolog_slurmctld_callback() or\nepilog_slurmctld_callback() function before\nthe thread exits. These callbacks are provided as function pointers when the\nslurmctld starts through\nprep_p_register_callbacks() call, and these\nfunction pointers should be cached locally in your plugin.API FunctionsAll of the following functions are required. Functions which are not\nimplemented must be stubbed.int init(void)Description:\n  Called when the plugin is loaded, before any other functions are\n  called. Put global initialization here.Returns: \nSLURM_SUCCESS on success, or\nSLURM_ERROR on failure.void fini(void)Description:\n  Called when the plugin is removed. Clear any allocated storage here.Returns: None.Note: These init and fini functions are not the same as those\ndescribed in the dlopen (3) system library.\nThe C run-time system co-opts those symbols for its own initialization.\nThe system _init() is called before the Slurm\ninit(), and the Slurm\nfini() is called before the system's\n_fini().\nvoid prep_p_register_callbacks(prep_callbacks_t *callbacks)\nDescription:\nThis function is called by the slurmctld to pass function pointer addresses\nused with asynchronous operation with the prep_p_prolog_slurmctld() and\nprep_p_epilog_slurmctld() interfaces. These pointers must be saved if\nasynchronous operation is used, otherwise this function can be an empty stub.\nArguments: \ncallbacks\n(input) contains function pointers for use with asynchronous operation within the slurmctld\nReturns: None.\nint prep_p_prolog(job_env_t *job_env, slurm_cred_t *cred)\nDescription:\nCalled within the slurmd as root before the first step of a job starts on the\ncompute node.\nArguments: \njob_env\n(input) details from the step launch request\ncred\n(input) launch credential with additional verifiable launch details signed by\nthe slurmctld\nReturns: \nSLURM_SUCCESS on success, or\nSLURM_ERROR on failure, will cause job failure.\n\n\nint prep_p_epilog(job_env_t *job_env, slurm_cred_t *cred)\n\nDescription:\nCalled within the slurmd as root after all job steps have completed.\nArguments: \njob_env\n(input) details from the step launch request\ncred\n(input) launch credential with additional verifiable launch details signed by\nthe slurmctld\nReturns: \nSLURM_SUCCESS on success, or\nSLURM_ERROR on failure, will cause job failure.\n\n\nint prep_p_prolog_slurmctld(job_record_t *job_ptr, bool *async)\n\nDescription:\nCalled within the slurmctld before a job launches.\nArguments: \njob_ptr\n(input) raw job record\nasync\n(output) set to true if this interface has spawned a separate processing thread\nthat must complete before the job starts execution\nReturns: \nSLURM_SUCCESS on success, or\nSLURM_ERROR on failure, will cause job failure.\n\n\nint prep_p_epilog_slurmctld(job_record_t *job_ptr, bool *async)\n\nDescription:\nCalled within the slurmctld as a job is terminating.\nArguments: \njob_ptr\n(input) raw job record\nasync\n(output) set to true if this interface has spawned a separate processing thread\nthat must complete before the job is marked complete\nReturns: \nSLURM_SUCCESS on success, or\nSLURM_ERROR on failure, will cause job failure.\n\nLast modified 18 April 2023\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/nss_slurm.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "nss_slurm",
                "content": "nss_slurm is an optional NSS plugin that can permit passwd, group and\ncloud node host\nresolution for a job on the compute node to be serviced through the local\nslurmstepd process, rather than through some alternate network-based service\nsuch as LDAP, DNS, SSSD, or NSLCD.When enabled on the cluster, for each job, the job's user will have their\nfull struct passwd info \u2014 username, uid, primary gid, gecos info,\nhome directory, and shell \u2014 securely sent as part of each step launch,\nand cached within the slurmstepd process. This info will then be provided to\nany process launched by that step through the\ngetpwuid()/getpwnam()/getpwent() system calls.For group information \u2014 from the\ngetgrgid()/getgrnam()/getgrent() system calls \u2014,\nan abbreviated view of struct group will be provided. Within a given\nprocess, the response will include only those groups that the user belongs to,\nbut with only the user themselves listed as a member. The full list of group\nmembers is not provided.For host information \u2014 from the\ngethostbyname()/gethostbyname system calls \u2014,\nan abbreviated view of struct hostent will be provided. Within a given\nprocess, the response will include only the cloud hosts that belong to\nallocation.All of this information is populated by slurmctld as it is seen on the\nhost running slurmctld.Installation\n\nSource:In your Slurm build directory, navigate to contribs/nss_slurm/\nand run:make && make installThis will install libnss_slurm.so.2 alongside your other Slurm library files\nin your install path.Depending on your Linux distribution, you will likely need to symlink this\nto the directory which includes your other NSS plugins to enable it.\nOn Debian/Ubuntu, /lib/x86_64-linux-gnu is\nrecommended, and for RHEL-based distributions\n/usr/lib64 is recommended. If in doubt,\na command such as\nfind /lib /usr/ -name 'libnss*' should help.\n\n\nSetup\nThe slurmctld must be configured to look up and send the appropriate passwd\nand group details as part of the launch credential. This is handled by setting\nLaunchParameters=enable_nss_slurm in slurm.conf and restarting\nslurmctld.\nOnce enabled, the scontrol getent command\ncan be used on a compute node to print all passwd and group info associated\nwith job steps on that node. As an example:\n\ntim@node0001:~$ scontrol getent node0001\nJobId=1268.Extern:\nUser:\ntim:x:1000:1000:Tim Wickberg:/home/tim:/bin/bash\nGroups:\ntim:x:1000:tim\nprojecta:x:1001:tim\n\nJobId=1268.0:\nUser:\ntim:x:1000:1000:Tim Wickberg:/home/tim:/bin/bash\nGroups:\ntim:x:1000:tim\nprojecta:x:1001:tim\n\nNSS Slurm Configuration\n\n\nnss_slurm has an optional configuration file \u2014\n/etc/nss_slurm.conf. This configuration file is only needed if:\n\nThe node's hostname does not match the NodeName, in which case you must\nexplicitly set the NodeName option.\nThe SlurmdSpoolDir does not match Slurm's default location of\n/var/spool/slurmd, in which case it must be provided as well.\n\nNodeName and SlurmdSpoolDir are the only configuration options supported\nat this time.\nInitial Testing\n\n\nBefore enabling NSS Slurm directly on the node, you should use the\n-s slurm option to getent within a newly launched job step\nto verify that the rest of the setup has been completed successfully. The\n-s option to getent allows it to query a specific database \u2014\neven if it has not been enabled by default through the system's\nnsswitch.conf. Note that nss_slurm only responds to requests from\nprocesses within the job step itself \u2014 you must launch the getent\ncommand within a job step to see any data returned.\nAs an example of a successful query:\n\ntim@blackhole:~$ srun getent -s slurm passwd\ntim:x:1000:1000:Tim Wickberg:/home/tim:/bin/bash\ntim@blackhole:~$ srun getent -s slurm group\ntim:x:1000:tim\nprojecta:x:1001:tim\n\nNSS Configuration\n\n\nEnabling nss_slurm is as simple as adding slurm to the passwd and\ngroup database in /etc/nsswitch.conf. It is recommended that\nslurm is listed first, as the order (from left to right) determines\nthe sequence in which the NSS databases will be queried, and this ensures Slurm\nhandles the request if able before submitting the query to other sources.\nTo enable cloud node name resolution slurm needs to be added to the\nto hosts database in /etc/nsswitch.conf.\nIt is recommended that slurm is listed last.\nOnce enabled, test it by launching getent queries such as:\n\ntim@blackhole:~$ srun getent passwd tim\ntim:x:1000:1000:Tim Wickberg:/home/tim:/bin/bash\ntim@blackhole:~$ srun getent group projecta\nprojecta:x:1001:tim\n\nLimitations\n\n\nnss_slurm will only return results for processes within a given job step.\nIt will not return any results for processes outside of these steps, such as\nsystem monitoring, node health checks, prolog or epilog scripts, and related\nnode system processes.\nnss_slurm is not meant as a full replacement for network directory services\nsuch as LDAP, but as a way to remove load from those systems to improve the\nperformance of large-scale job launches. It accomplishes this by removing\nthe \"thundering-herd\" issue should all tasks of a large job make simultaneous\nlookup requests \u2014 generally for info related to the user themselves,\nwhich is the only information nss_slurm will be able to provide \u2014 and\noverwhelm the underlying directory services.\nnss_slurm is only able to communicate with a single slurmd. If running\nwith --enable-multiple-slurmd, you can specify which slurmd is used with NodeName\nand SlurmdSpoolDir parameters in the nss_slurm.conf file.\nSince the information is gathered from the slurmctld node, there can be\nunexpected consequences if the information differs between the controller and\nthe worker nodes. One possible scenario is if a user's shell is /sbin/nologin\non the slurmctld machine but /bin/bash on the slurmd node. An interactive\nsalloc may fail to launch since it will try to spawn the default shell,\nwhich according to the slurmctld is /sbin/nologin.\nWhen using proctrack/pgid, nss_slurm will rely on the pgid of the process\nto determine if it can respond to that request. The login shell spawned with\nsrun --pty must be run in its own session,\nand therefore its own pgid, so nss_slurm will not respond to requests in an\ninteractive session.\nLast modified 30 Aug 2023\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Setup",
                "content": "The slurmctld must be configured to look up and send the appropriate passwd\nand group details as part of the launch credential. This is handled by setting\nLaunchParameters=enable_nss_slurm in slurm.conf and restarting\nslurmctld.Once enabled, the scontrol getent command\ncan be used on a compute node to print all passwd and group info associated\nwith job steps on that node. As an example:\ntim@node0001:~$ scontrol getent node0001\nJobId=1268.Extern:\nUser:\ntim:x:1000:1000:Tim Wickberg:/home/tim:/bin/bash\nGroups:\ntim:x:1000:tim\nprojecta:x:1001:tim\n\nJobId=1268.0:\nUser:\ntim:x:1000:1000:Tim Wickberg:/home/tim:/bin/bash\nGroups:\ntim:x:1000:tim\nprojecta:x:1001:tim\nNSS Slurm Configuration\n\nnss_slurm has an optional configuration file \u2014\n/etc/nss_slurm.conf. This configuration file is only needed if:\n\nThe node's hostname does not match the NodeName, in which case you must\nexplicitly set the NodeName option.\nThe SlurmdSpoolDir does not match Slurm's default location of\n/var/spool/slurmd, in which case it must be provided as well.\n\nNodeName and SlurmdSpoolDir are the only configuration options supported\nat this time.\nInitial Testing\n\n\nBefore enabling NSS Slurm directly on the node, you should use the\n-s slurm option to getent within a newly launched job step\nto verify that the rest of the setup has been completed successfully. The\n-s option to getent allows it to query a specific database \u2014\neven if it has not been enabled by default through the system's\nnsswitch.conf. Note that nss_slurm only responds to requests from\nprocesses within the job step itself \u2014 you must launch the getent\ncommand within a job step to see any data returned.\nAs an example of a successful query:\n\ntim@blackhole:~$ srun getent -s slurm passwd\ntim:x:1000:1000:Tim Wickberg:/home/tim:/bin/bash\ntim@blackhole:~$ srun getent -s slurm group\ntim:x:1000:tim\nprojecta:x:1001:tim\n\nNSS Configuration\n\n\nEnabling nss_slurm is as simple as adding slurm to the passwd and\ngroup database in /etc/nsswitch.conf. It is recommended that\nslurm is listed first, as the order (from left to right) determines\nthe sequence in which the NSS databases will be queried, and this ensures Slurm\nhandles the request if able before submitting the query to other sources.\nTo enable cloud node name resolution slurm needs to be added to the\nto hosts database in /etc/nsswitch.conf.\nIt is recommended that slurm is listed last.\nOnce enabled, test it by launching getent queries such as:\n\ntim@blackhole:~$ srun getent passwd tim\ntim:x:1000:1000:Tim Wickberg:/home/tim:/bin/bash\ntim@blackhole:~$ srun getent group projecta\nprojecta:x:1001:tim\n\nLimitations\n\n\nnss_slurm will only return results for processes within a given job step.\nIt will not return any results for processes outside of these steps, such as\nsystem monitoring, node health checks, prolog or epilog scripts, and related\nnode system processes.\nnss_slurm is not meant as a full replacement for network directory services\nsuch as LDAP, but as a way to remove load from those systems to improve the\nperformance of large-scale job launches. It accomplishes this by removing\nthe \"thundering-herd\" issue should all tasks of a large job make simultaneous\nlookup requests \u2014 generally for info related to the user themselves,\nwhich is the only information nss_slurm will be able to provide \u2014 and\noverwhelm the underlying directory services.\nnss_slurm is only able to communicate with a single slurmd. If running\nwith --enable-multiple-slurmd, you can specify which slurmd is used with NodeName\nand SlurmdSpoolDir parameters in the nss_slurm.conf file.\nSince the information is gathered from the slurmctld node, there can be\nunexpected consequences if the information differs between the controller and\nthe worker nodes. One possible scenario is if a user's shell is /sbin/nologin\non the slurmctld machine but /bin/bash on the slurmd node. An interactive\nsalloc may fail to launch since it will try to spawn the default shell,\nwhich according to the slurmctld is /sbin/nologin.\nWhen using proctrack/pgid, nss_slurm will rely on the pgid of the process\nto determine if it can respond to that request. The login shell spawned with\nsrun --pty must be run in its own session,\nand therefore its own pgid, so nss_slurm will not respond to requests in an\ninteractive session.\nLast modified 30 Aug 2023\n"
            },
            {
                "title": "Initial Testing\n\n",
                "content": "Before enabling NSS Slurm directly on the node, you should use the\n-s slurm option to getent within a newly launched job step\nto verify that the rest of the setup has been completed successfully. The\n-s option to getent allows it to query a specific database \u2014\neven if it has not been enabled by default through the system's\nnsswitch.conf. Note that nss_slurm only responds to requests from\nprocesses within the job step itself \u2014 you must launch the getent\ncommand within a job step to see any data returned.As an example of a successful query:\ntim@blackhole:~$ srun getent -s slurm passwd\ntim:x:1000:1000:Tim Wickberg:/home/tim:/bin/bash\ntim@blackhole:~$ srun getent -s slurm group\ntim:x:1000:tim\nprojecta:x:1001:tim\nNSS Configuration\n\nEnabling nss_slurm is as simple as adding slurm to the passwd and\ngroup database in /etc/nsswitch.conf. It is recommended that\nslurm is listed first, as the order (from left to right) determines\nthe sequence in which the NSS databases will be queried, and this ensures Slurm\nhandles the request if able before submitting the query to other sources.To enable cloud node name resolution slurm needs to be added to the\nto hosts database in /etc/nsswitch.conf.\nIt is recommended that slurm is listed last.Once enabled, test it by launching getent queries such as:\ntim@blackhole:~$ srun getent passwd tim\ntim:x:1000:1000:Tim Wickberg:/home/tim:/bin/bash\ntim@blackhole:~$ srun getent group projecta\nprojecta:x:1001:tim\nLimitations\n\nnss_slurm will only return results for processes within a given job step.\nIt will not return any results for processes outside of these steps, such as\nsystem monitoring, node health checks, prolog or epilog scripts, and related\nnode system processes.nss_slurm is not meant as a full replacement for network directory services\nsuch as LDAP, but as a way to remove load from those systems to improve the\nperformance of large-scale job launches. It accomplishes this by removing\nthe \"thundering-herd\" issue should all tasks of a large job make simultaneous\nlookup requests \u2014 generally for info related to the user themselves,\nwhich is the only information nss_slurm will be able to provide \u2014 and\noverwhelm the underlying directory services.nss_slurm is only able to communicate with a single slurmd. If running\nwith --enable-multiple-slurmd, you can specify which slurmd is used with NodeName\nand SlurmdSpoolDir parameters in the nss_slurm.conf file.Since the information is gathered from the slurmctld node, there can be\nunexpected consequences if the information differs between the controller and\nthe worker nodes. One possible scenario is if a user's shell is /sbin/nologin\non the slurmctld machine but /bin/bash on the slurmd node. An interactive\nsalloc may fail to launch since it will try to spawn the default shell,\nwhich according to the slurmctld is /sbin/nologin.When using proctrack/pgid, nss_slurm will rely on the pgid of the process\nto determine if it can respond to that request. The login shell spawned with\nsrun --pty must be run in its own session,\nand therefore its own pgid, so nss_slurm will not respond to requests in an\ninteractive session.Last modified 30 Aug 2023"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/plugins.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Slurm Plugin API",
                "content": "OverviewA Slurm plugin is a dynamically linked code object which is loaded explicitly\nat run time by the Slurm libraries. A plugin provides a customized implementation\nof a well-defined API connected to tasks such as authentication, interconnect\nfabric, and task scheduling.Identification\nA Slurm plugin identifies itself by a short character string formatted similarly\nto a MIME type: <major>/<minor>. The major type identifies\nwhich API the plugin implements. The minor type uniquely distinguishes a plugin\nfrom other plugins that implement that same API, by such means as the intended\nplatform or the internal algorithm. For example, a plugin to interface to the\nMaui scheduler would give its type as \"sched/maui.\" It would implement\nthe Slurm Scheduler API.Versioning\n\nSlurm plugin version numbers comprise a major, minor and micro revision number.\nIf the major and/or minor revision number changes, this indicates major changes\nto the Slurm functionality including changes to APIs, command options, and\nplugins.\nThese plugin changes may include new functions and/or function arguments.\nIf only the micro revision number changes, this is indicative of bug fixes\nand possibly minor enhancements which should not adversely impact users.\nIn all cases, rebuilding and installing all Slurm plugins is recommended\nat upgrade time.\nNot all compute nodes in a cluster need be updated at the same time, but\nall Slurm APIs, commands, plugins, etc. on a compute node should represent\nthe same version of Slurm.Data Objects\nA plugin must define and export the following symbols:\nchar plugin_type[]\nA unique, short, formatted string to identify the plugin's purpose as\ndescribed above. A \"null\" plugin (i.e., one that implements the desired\nAPI as stubs) should have a minor type of \"none.\"\nchar plugin_name[]\nA free-form string that identifies the plugin in human-readable terms,\nsuch as \"Kerberos authentication.\" Slurm will use this string to identify\nthe plugin to end users.\nconst uint32_t plugin_version\nIdentifies the version of Slurm used to build this plugin and\nany attempt to load the plugin from a different version of Slurm will result\nin an error.\nThe micro version is not considered for SPANK plugins.\nAPI Functions in All Plugins\n\nint init (void);Description: If present, this function is called\njust after the plugin is loaded. This allows the plugin to perform any global\ninitialization prior to any actual API calls.Arguments: None.Returns: SLURM_SUCCESS if the plugin's initialization\nwas successful. Any other return value indicates to Slurm that the plugin should\nbe unloaded and not used.void fini (void);Description: If present, this function is called\njust before the plugin is unloaded. This allows the plugin to do any finalization\nafter the last plugin-specific API call is made.Arguments: None.Returns: None.Note: These init and fini functions are not the same as those\ndescribed in the dlopen (3) system library.\nThe C run-time system co-opts those symbols for its own initialization.\nThe system _init() is called before the Slurm\ninit(), and the Slurm\nfini() is called before the system's\n_fini().The functions need not appear. The plugin may provide either\ninit() or fini() or both.Thread Safety\n\nSlurm is a multithreaded application. The Slurm plugin library may exercise\nthe plugin functions in a re-entrant fashion. It is the responsibility of the\nplugin author to provide the necessarily mutual exclusion and synchronization\nin order to avoid the pitfalls of re-entrant code.Run-time Support\n\nThe standard system libraries are available to the plugin. The Slurm libraries\nare also available and plugin authors are encouraged to make use of them rather\nthan develop their own substitutes. Plugins should use the Slurm log to print\nerror messages.The plugin author is responsible for specifying any specific non-standard libraries\nneeded for correct operation. Plugins will not load if their dependent libraries\nare not available, so it is the installer's job to make sure the specified libraries\nare available.Performance\n\nAll plugin functions are expected to execute very quickly. If any function\nentails delays (e.g. transactions with other systems), it should be written to\nutilize a thread for that functionality. This thread may be created by the\ninit() function and deleted by the\nfini() functions. See plugins/sched/backfill\nfor an example of how to do this.Data Structure Consistency\n\n\n  In certain situations Slurm iterates over different data structures elements\n  using counters. For example, with environment variable arrays.\n  In order to avoid buffer overflows and other undesired situations, when a\n  plugin modifies certain elements it must also update these counters accordingly.\n  Other situations may require other types of changes.\n\n  The following advice indicates which structures have arrays with associated\n  counters that must be maintained when modifying data, plus other possible\n  important information to take in consideration when manipulating these\n  structures.\n  This list is not fully exhaustive due to constant modifications in code,\n  but it is a first start point and basic guideline for most common situations.\n  Complete structure information can be seen in the slurm/slurm.h.in\n  file.\nslurm_job_info_t (job_info_t) Data Structure\n\n\n  uint32_t env_size;\n  char **environment;\n\n  uint32_t spank_job_env_size;\n  char **spank_job_env;\n\n  uint32_t gres_detail_cnt;\n  char **gres_detail_str;\n\n  These pairs of array pointers and element counters must kept updated in order\n  to avoid subsequent buffer overflows, so if you update the array you must\n  also update the related counter.\n\n  char *nodes;\n  int32_t *node_inx;\n\n  int32_t *req_node_inx;\n  char *req_nodes;\n\nnode_inx and req_node_inx represents a list of index pairs for\n  ranges of nodes defined in the nodes and req_nodes fields\n  respectively. In each case, both array variables must match the count.\n\n  uint32_t het_job_id;\n  char *het_job_id_set;\n\n  The het_job_id field should be the first element of the\n  het_job_id_set array.\njob_step_info_t Data Structure\n\n\n  char *nodes;\n  int32_t *node_inx;\n\nnode_inx represents a list of index pairs for range of nodes defined in\n  nodes. Both variables must match the node count.\npriority_factors_object_t Data Structure\n\n\n  uint32_t tres_cnt;\n  char **tres_names;\n  double *tres_weights;\n\n  This value must match the configured TRES on the system, otherwise\n  iteration over the tres_names or tres_weights arrays can cause\n  buffer overflows.\njob_step_pids_t Data Structure\n\n\n  uint32_t pid_cnt;\n  uint32_t *pid;\n\n  Array pid represents the list of Process IDs for the job step, and\n  pid_cnt is the counter that must match the size of the array.\nslurm_step_layout_t Data Structure\n\n\n  uint32_t node_cnt;\n  char *node_list;\n\n  The node_list array size must match node_cnt.\n\n  uint16_t *tasks;\n  uint32_t node_cnt;\n  uint32_t task_cnt;\n\n  In the tasks array, each element is the number of tasks assigned\n  to the corresponding node, to its size must match node_cnt. Moreover\n  task_cnt represents the sum of tasks registered in tasks.\n\n  uint32_t **tids;\n\ntids is an array of length node_cnt of task ID arrays. Each\n  subarray is designated by the corresponding value in the tasks array,\n  so tasks, tids and task_cnt must be set to match this\n  layout.\nslurm_step_launch_params_t Data Structure\n\n\n  uint32_t envc;\n  char **env;\n\n  When modifying the environment variables in the env array, you must\n  also modify the envc counter accordingly to prevent buffer overflows\n  in subsequent loops over that array.\n\n  uint32_t het_job_nnodes;\n  uint32_t het_job_ntasks;\n\n  uint16_t *het_job_task_cnts;\n  uint32_t **het_job_tids;\n  uint32_t *het_job_node_list;\n\n  This het_job_* related variables must match the current heterogeneous\n  job configuration.\n  \n  For example, if for whatever reason you are reducing the number of tasks for\n  a node in a heterogeneous job, you should at least remove that task ID from\n  het_job_tids, decrement het_job_ntasks and\n  het_job_task_cnts, and possibly decrement the number of nodes of the\n  heterogeneous job in het_job_nnodes and het_job_node_list.\n\n  char **spank_job_env;\n  uint32_t spank_job_env_size;\n\n  When modifying the spank_job_env structure, the\n  spank_job_env_size field must be updated to prevent buffer overflows\n  in subsequent loops over that array.\nnode_info_t Data Structure\n\n\n  char *features;\n  char *features_act;\n\n  In a system containing Intel KNL processors the features_act field is\n  set by the plugin to match the currently running modes on the node. On other\n  systems the features_act is not usually used.\n  If you program such a plugin you must ensure that features_act contains\n  a subset of features.\n\nchar *reason;\ntime_t reason_time;\nuint32_t reason_uid;\n\n  If reason is modified then reason_time and reason_uid\n  should be updated.\nreserve_info_t Data Structure\n\n\n  int32_t *node_inx;\n  uint32_t node_cnt;\n\nnode_inx represents a list of index pairs for range of nodes associated\n  with the reservation and its count must equal node_cnt.\npartition_info_t Data Structure\n\n\n  No special advice.\nslurm_step_layout_req_t Data Structure\n\n\n  No special advice.\nslurm_step_ctx_params_t\n\n\n  No special advice.\nLast modified 25 August 2022"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/high_throughput.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "High Throughput Computing Administration Guide",
                "content": "This document contains Slurm administrator information specifically\nfor high throughput computing, namely the execution of many short jobs.\nGetting optimal performance for high throughput computing does require\nsome tuning and this document should help you off to a good start.\nA working knowledge of Slurm should be considered a prerequisite\nfor this material.Performance Results\n\nSlurm has also been validated to execute 500 simple batch jobs per second\non a sustained basis with short bursts of activity at a much higher level.\nActual performance depends upon the jobs to be executed plus the hardware and\nconfiguration used.System configuration\n\nSeveral system configuration parameters may require modification to support a large number\nof open files and TCP connections with large bursts of messages. Changes can\nbe made using the /etc/rc.d/rc.local or /etc/sysctl.conf \nscript to preserve changes after reboot. In either case, you can write values\ndirectly into these files\n(e.g. \"echo 32832 > /proc/sys/fs/file-max\").\n/proc/sys/fs/file-max:\nThe maximum number of concurrently open files.\nWe recommend a limit of at least 32,832.\n/proc/sys/net/ipv4/tcp_max_syn_backlog:\nThe maximum number of SYN requests to keep in memory that we have yet to get\nthe third packet in a 3-way handshake from.\nThe default value is 1024 for systems with more than 128Mb of memory, and 128\nfor low memory machines. If server suffers of overload, try to increase this\nnumber.\n/proc/sys/net/ipv4/tcp_syncookies:\nUsed to send out syncookies to hosts when the kernels syn backlog queue\nfor a specific socket is overflowed.\nThe default value is 0, which disables this functionality.\nSet the value to 1.\n/proc/sys/net/ipv4/tcp_synack_retries:\nHow many times to retransmit the SYN,ACK reply to an SYN request.\nIn other words, this tells the system how many times to try to establish a\npassive TCP connection that was started by another host.\nThis variable takes an integer value, but should under no circumstances be\nlarger than 255.\nEach retransmission will take approximately 30 to 40 seconds.\nThe default value of 5, which results in a timeout of passive TCP connections\nof approximately 180 seconds and is generally satisfactory.\n/proc/sys/net/core/somaxconn:\nLimit of socket listen() backlog, known in userspace as SOMAXCONN. Defaults to\n128. The value should be raised substantially to support bursts of request.\nFor example, to support a burst of 1024 requests, set somaxconn to 1024.\n/proc/sys/net/ipv4/ip_local_port_range:\nIdentify the ephemeral ports available, which are used for many Slurm\ncommunications. The value may be raised to support a high volume of\ncommunications.\nFor example, write the value \"32768 65535\" into the ip_local_port_range file\nin order to make that range of ports available.\nThe transmit queue length (txqueuelen) may also need to be modified\nusing the ifconfig command. A value of 4096 has been found to work well for one\nsite with a very large cluster\n(e.g. \"ifconfig  txqueuelen 4096\").Munge configuration\n\nBy default the Munge daemon runs with two threads, but a higher thread count\ncan improve its throughput. We suggest starting the Munge daemon with ten\nthreads for high throughput support (e.g. \"munged --num-threads 10\").User limits\n\nThe ulimit values in effect for the slurmctld daemon should\nbe set quite high for memory size, open file count and stack size.Slurm Configuration\n\nSeveral Slurm configuration parameters should be adjusted to\nreflect the needs of high throughput computing. The changes described below\nwill not be possible in all environments, but these are the configuration\noptions that you may want to consider for higher throughput.\nAccountingStorageType:\nDisabling storing accounting records by not setting this plugin.\nTurning accounting off provides minimal improvement in performance.\nIf using the SlurmDBD increased speedup can be achieved by setting the\nCommitDelay option in the slurmdbd.conf\nJobAcctGatherType:\nDisabling the collection of job accounting information will improve job\nthroughput. Disable collection of accounting by using the\njobacct_gather/none plugin.\nJobCompType:\nDisabling recording of job completion information will improve job\nthroughput. Disable recording of job completion information by using the\njobcomp/none plugin.\nJobSubmitPlugins:\nUse of a lua job submit plugin is not recommended. slurmctld runs this\nscript while holding internal locks, and only a single copy of this script\ncan run at a time. This blocks most concurrency in slurmctld. Therefore, we\ndo not recommend using it in a high throughput environment.\nMaxJobCount:\nControls how many jobs may be in the slurmctld daemon records at any\npoint in time (pending, running, suspended or completed[temporarily]).\nThe default value is 10,000.\nMessageTimeout:\nControls how long to wait for a response to messages.\nThe default value is 10 seconds.\nWhile the slurmctld daemon is highly threaded, its responsiveness\nis load dependent. This value might need to be increased somewhat.\nMinJobAge:\nControls how soon the record of a completed job can be purged from the\nslurmctld memory and thus not visible using the squeue command.\nThe record of jobs run will be preserved in accounting records and logs.\nThe default value is 300 seconds. The value should be reduced to a few\nseconds if possible. Use of accounting records for older jobs can increase\nthe job throughput rate compared with retaining old jobs in the memory of\nthe slurmctld daemon.\nPriorityType:\nThe priority/basic is considerably faster than other options, but\nschedules jobs only on a First In First Out (FIFO) basis.\nPrologSlurmctld/EpilogSlurmctld:\nNeither of these is recommended for a high throughput environment. When they\nare enabled a separate slurmctld thread has to be created for every job start\n(or task for a job array).\nCurrent architecture requires acquisition of a job write lock in every thread,\nwhich is a costly operation that severely limits scheduler throughput.\nSchedulerParameters:\nMany scheduling parameters are available.\n\nSetting option batch_sched_delay will control how long the\nscheduling of batch jobs can be delayed. This effects only batch jobs.\nFor example, if many jobs are submitted each second, the overhead of\ntrying to schedule each one will adversely impact the rate at which jobs\ncan be submitted. The default value is 3 seconds.\nSetting option defer will avoid attempting to schedule each job\nindividually at job submit time, but defer it until a later time when\nscheduling multiple jobs simultaneously may be possible.\nThis option may improve system responsiveness when large numbers of jobs\n(many hundreds) are submitted at the same time, but it will delay the\ninitiation time of individual jobs.\nSetting the defer_batch option is similar to the defer\noption, as explained above. The difference is that defer_batch will\nallow interactive jobs to be started immediately, but jobs submitted with\nsbatch will be deferred to allow multiple jobs to accumulate and be scheduled\nat once.\nsched_min_interval is yet another configuration parameter to control\nhow frequently the scheduling logic runs. It can still be triggered on each\njob submit, job termination, or other state change which could permit a new\njob to be started. However that triggering does not cause the scheduling logic\nto be started immediately, but only within the configured sched_interval.\nFor example, if sched_min_interval=2000000 (microseconds) and 100 jobs are submitted\nwithin a 2 second time window, then the scheduling logic will be executed one time\nrather than 100 times if sched_min_interval was set to 0 (no delay).\nBesides controlling how frequently the scheduling logic is executed, the\ndefault_queue_depth configuration parameter controls how many jobs are\nconsidered to be started in each scheduler iteration. The default value of\ndefault_queue_depth is 100 (jobs), which should be fine in most cases.\nThe sched/backfill plugin has relatively high overhead if used with\nlarge numbers of job. Configuring bf_max_job_test to a modest size (say 100\njobs or less) and bf_interval to 30 seconds or more will limit the\noverhead of backfill scheduling (NOTE: the default values are fine for\nboth of these parameters). Other backfill options available for tuning backfill\nscheduling include bf_max_job_user, bf_resolution and\nbf_window. See the slurm.conf man page for details.\nA set of scheduling parameters currently used for running hundreds of jobs\nper second on a sustained basis on one cluster follows. Note that every\nenvironment is different and this set of parameters will not work well\nin every case, but it may serve as a good starting point.\n\nbatch_sched_delay=20\nbf_continue\nbf_interval=300\nbf_min_age_reserve=10800\nbf_resolution=600\nbf_yield_interval=1000000\npartition_job_depth=500\nsched_max_job_start=200\nsched_min_interval=2000000\n\n\nSchedulerType:\nIf most jobs are short lived then use of the sched/builtin plugin is\nrecommended. This manages a queue of jobs on a First-In-First-Out (FIFO) basis\nand eliminates logic used to sort the queue by priority.\nSlurmctldDebug:\nMore detailed logging will decrease system throughput. Set to error or\ninfo for regular operations with high throughput workload.\nSlurmctldPort:\nIt is desirable to configure the slurmctld daemon to accept incoming\nmessages on more than one port in order to avoid having incoming messages\ndiscarded by the operating system due to exceeding the SOMAXCONN limit\ndescribed above. Using between two and ten ports is suggested when large\nnumbers of simultaneous requests are to be supported.\nSlurmdDebug:\nMore detailed logging will decrease system throughput. Set to error or\ninfo for regular operations with high throughput workload.\nSlurmdLogFile:\nWriting to local storage is recommended.\nThe ability to do RPC rate limiting on a per-user basis is a new feature\nwith 23.02. It acts as a virtual bucket of tokens that users consume with\nRemote Procedure Calls. This allows users to submit a large number of requests\nin a short period of time, but not a sustained high rate of requests that\nwould add stress to the scheduler. You can define the maximum number of tokens\nwith rl_bucket_size, the rate at which new tokens are added with\nrl_refill_rate, the frequency with which tokens are refilled with\nrl_refill_period and the number of entities to track with\nrl_table_size. It is enabled with rl_enable.\nOther: Configure logging, accounting and other overhead to a minimum\nappropriate for your environment.\nSlurmDBD Configuration\n\nTurning accounting off provides a minimal improvement in performance.\n  If using SlurmDBD increased speedup can be achieved by setting the CommitDelay\n  option in the slurmdbd.conf to introduce a\n  delay between the time slurmdbd receives a connection from slurmctld and\n  when it commits the information to the database. This allows multiple\n  requests to be accumulated and reduces the number of commit requests\n  to the database.You might also consider setting the 'Purge*' options in your\n  slurmdbd.conf to clear out old Data.  A Typical configuration would\n  look like this...\nPurgeEventAfter=12months\nPurgeJobAfter=12months\nPurgeResvAfter=2months\nPurgeStepAfter=2months\nPurgeSuspendAfter=1month\nPurgeTXNAfter=12months\nPurgeUsageAfter=12months\nLast modified 6 December 2023"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/kubernetes.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Kubernetes Guide",
                "content": "Contents\nOverview\nPresentations\nOverviewKubernetes is being adopted into HPC\nclusters to orchestrate deployments (e.g. software, infrastructure) and run\ncertain workloads (e.g. AI/ML inference). There is ongoing interest in\nintegrating Kubernetes and Slurm to achieve a unified cluster, optimized\nresource utilization, and workflows that leverage each system.The ways in which Slurm and Kubernetes are designed to handle certain types\nof workloads may change over time. Additionally, how they interact with each\nother may change, allowing for new possibilities. This is still an evolving\narea.Presentations\n\nNote that older presentations may contain outdated information.Presentations from 2023\n\nSlurm and/or/vs\n    Kubernetes, Tim Wickberg, SchedMD (SC23, November 2023)\n  \n\nNever use Slurm HA again: Solve all\n    your problems with Kubernetes, Chris Samuel and Doug Jacobsen, NERSC\n    (SLUG23, November 2023)\n  \nLast modified 14 February 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/rosetta.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Rosetta Stone of Workload Managers",
                "content": "PBS/Torque, Slurm, LSF, SGE and LoadLevelerThis table lists the most common command, environment variables, and\njob specification options used by the major workload management systems:\nPBS/Torque, Slurm, LSF, SGE and LoadLeveler.\nEach of these workload managers has unique features, but the most commonly\nused functionality is available in all of these environments as listed\nin the table.\nThis should be considered a work in progress and contributions to improve\nthe document are welcome.Click to get full size fileLast modified 17 January 2013"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/multi_cluster.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Multi-Cluster Operation",
                "content": "A cluster is comprised of all the nodes managed by a single slurmctld\ndaemon.  Slurm offers the ability to target commands to other\nclusters instead of, or in addition to, the local cluster on which the\ncommand is invoked.  When this behavior is enabled, users can submit\njobs to one or many clusters and receive status from those remote\nclusters.For example:\njuser@dawn> squeue -M dawn,dusk\nCLUSTER: dawn\nJOBID PARTITION   NAME   USER  ST   TIME NODES BP_LIST(REASON)\n76897    pdebug  myJob  juser   R   4:10   128 dawn001[8-15]\n76898    pdebug  myJob  juser   R   4:10   128 dawn001[16-23]\n16899    pdebug  myJob  juser   R   4:10   128 dawn001[24-31]\n\nCLUSTER: dusk\nJOBID PARTITION   NAME   USER  ST   TIME NODES BP_LIST(REASON)\n11950    pdebug   aJob  juser   R   4:20   128 dusk000[0-15]\n11949    pdebug   aJob  juser   R   5:01   128 dusk000[48-63]\n11946    pdebug   aJob  juser   R   6:35   128 dusk000[32-47]\n11945    pdebug   aJob  juser   R   6:36   128 dusk000[16-31]\nMost of the Slurm client commands offer the \"-M, --clusters=\"\noption which provides the ability to communicate to and from a comma\nseparated list of clusters.When sbatch, salloc or srun is invoked with a cluster\nlist, Slurm will immediately submit the job to the cluster that offers the\nearliest start time subject its queue of pending and running jobs.  Slurm will\nmake no subsequent effort to migrate the job to a different cluster (from the\nlist) whose resources become available when running jobs finish before their\nscheduled end times.NOTE: In order for salloc or srun to work with the \"-M,\n--clusters\" option in a multi-cluster environment, the compute nodes must be\naccessible to and from the submission host.Multi-Cluster Configuration\n\nThe multi-cluster functionality requires the use of the SlurmDBD.\nThe AccountingStorageType in the slurm.conf file must be set to the\naccounting_storage/slurmdbd plugin and the MUNGE or authentication\nkeys must be installed to allow each cluster to communicate with the\nSlurmDBD.  Note that MUNGE can be configured to use different keys for\ncommunications within a cluster and across clusters if desired.\nSee accounting for details.Once configured, Slurm commands specifying the \"-M, --clusters=\"\noption will become active for all of the clusters listed by the\n\"sacctmgr show clusters\" command.\nSee also the Slurm Federated Scheduling Guide.\nLast modified 9 June 2021"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/rest_quickstart.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "REST API Quick Start Guide",
                "content": "Slurm provides a REST API through the slurmrestd\ndaemon, using JSON Web Tokens for authentication.\nThis page provides a brief tutorial for setting up these components.\nSee also:\n\nREST API Reference\nREST API Implementation Details\nslurmrestd man page\n\nContents\nPrerequisites\nQuick Start\n\nRunning with systemd\nCustomizing slurmrestd.service\n\n\nBasic Usage\n\nToken management\n\n\nCommon Issues\n\nUnable to bind socket\nConnection refused\nProtocol authentication error (HTTP 500)\n      \nUnable to find requested URL (HTTP 404)\nRejecting thread config token (HTTP 401)\n      \nUnexpected URL character (HTTP 400)\n      \nOther Slurm commands not working\n\n\nPrerequisites\nThe following development libraries are required at compile time\nin order for slurmrestd to be compiled (minimum versions are on the related\nsoftware page linked below):\nHTTP Parser\n\nLibYAML (optional)\n\t\nJSON-C\nJWT Authentication\n    (optional, used in this guide)\nAdditionally, it is highly recommended that you have\nSlurmDBD set up for\naccounting. Without slurmdbd, slurmrestd may fail\nwhen loading some plugins (use\n-s to specify\nwhich to load) or when attempting JWT authentication.Quick Start\nThis may be done on a dedicated REST API machine or\nyour existing 'slurmctld' machine, depending on demand.\nInstall components for slurmrestd\n\nDEB: slurm-smd slurm-smd-slurmrestd\nRPM: slurm slurm-slurmrestd (requires\n--with slurmrestd at build time)\n\nSet up JSON Web Tokens for authentication\nEnsure /etc/slurm/slurm.conf is present and correct for your\ncluster (see Quick Start Admin Guide and\nslurm.conf man page)\nRun slurmrestd (see below for systemd\ninstructions) on your preferred [HOST]:PORT combination\n(':6820' is the default for production)\nexport SLURM_JWT=daemon\nexport SLURMRESTD_DEBUG=debug\nslurmrestd <host>:<port>\n\nAdjust SLURMRESTD_DEBUG to the desired level of output (as described on the\nman page)\nRunning with systemd\nSlurm ships with a slurmrestd service unit for systemd,\nhowever, it might require some additional setup to run properly.\nThis section assumes you have either installed slurmrestd using DEB/RPM packages\nor built it manually such that the files are in the same places.\nNote the versions associated with certain steps in the instructions\nbelow; these steps should be ignored on other versions.\nCreate a local service account to run slurmrestd. To prevent privilege\n  escalation, the user account should be:\n  \nNot root or SlurmUser\nNot used by real users or for any other functions\nNot granted any special permissions\n\nsudo useradd -M -r -s /usr/sbin/nologin -U slurmrestd\nConfigure slurmrestd service to use this user and associated group.\nThis can be accomplished in either of two ways:\n\nEdit /etc/default/slurmrestd\nor /etc/sysconfig/slurmrestd\nAdd -u slurmrestd and -g slurmrestd\nto SLURMRESTD_OPTIONS\nRun systemctl edit slurmrestd to edit overrides for the\nservice.\nAdd content like this to the prescribed location in the overrides file:\n\n[Service]\nUser=slurmrestd\nGroup=slurmrestd\n\n\n\n(Slurm 24.05 and newer) Optional: Customize the socket for\nslurmrestd. By default it will listen only on TCP port 6820. You may change\nthis behavior by changing SLURMRESTD_LISTEN\n(see Customizing slurmrestd.service).\n(Slurm 23.11 and earlier) Configure the socket for slurmrestd. This\nmay be accomplished by creating/changing permissions on the parent directory\nand/or changing the path to the socket in the service file.\n\nPermissions: The user running the service must have write+execute\npermissions on the directory that will contain the UNIX socket\nChanging socket: On Slurm 23.11, the way to change or disable the\nsocket is to modify the 'ExecStart' line of the service\n\nRun systemctl edit slurmrestd\nAdd the following contents to the [Service] section:\n\nExecStart=\nExecStart=/usr/sbin/slurmrestd $SLURMRESTD_OPTIONS\nEnvironment=SLURMRESTD_LISTEN=:6820\n\nAdjust the assignment of SLURMRESTD_LISTEN to contain the socket(s) you want\nthe daemon to listen on.\nAfter a future upgrade to Slurm 24.05+, the 'ExecStart' overrides will be\nunnecessary but will not conflict with the newer version.\n\n\n\n\nCustomizing slurmrestd.service\nThe Slurm 24.05 release changes the operation of\nthe default service file and may break existing overrides. If you have\noverridden ExecStart= to contain any TCP/UNIX sockets directly, it\nwill cause the service to fail if it duplicates any sockets contained in\nSLURMRESTD_LISTEN. These overrides will need to be changed after upgrading.The default slurmrestd.service file has two intended ways of\ncustomizing its operation:\nEnvironment files:\nThe service will read environment variables from two files:\n/etc/default/slurmrestd and /etc/sysconfig/slurmrestd.\nYou may set any environment variables recognized by\nslurmrestd,\nbut the following are particularly relevant:\n\nSLURMRESTD_OPTIONS: CLI options to add to the slurmrestd command\n(see slurmrestd)\nSLURMRESTD_LISTEN: Comma-delimited list of host:port pairs or\nunix:$SOCKET_PATH sockets to listen on\nNOTE: If this duplicates what is already set in the\n'ExecStart' line in the service file, it will fail. Starting in Slurm 24.05,\nthe default service file contains no sockets in 'ExecStart' and fully relies on\nthis variable to contain the desired sockets.\n\n\nService editing: Systemd has a built in way to edit services\nby running systemctl edit slurmrestd.\n\nThis will create an override file in '/etc/systemd/' containing directives\nthat will add to or replace directives in the default unit in '/lib/systemd/'.\n\nThe override file must have the appropriate section declaration(s)\nfor the directives you use (e.g., [Service]).\nEnvironment variables may be set with Environment=NAME=value\n(refer to systemd documentation for more details)\nChanges may be reverted by running systemctl revert slurmrestd\n\n\n\nBasic Usage\n\nFind the latest supported API version\nslurmrestd -d list\nGet an authentication token for JWT\nunset SLURM_JWT; export $(scontrol token)\n\nThis ensures an old token doesn't prevent a new one from being issued\nBy default, tokens will expire after 1800 seconds (30 minutes).\nAdd lifespan=SECONDS to the 'scontrol' command to change this.\n\n\nRun a basic curl command to hit the API when listening on a TCP host:port\ncurl -s -o \"/tmp/curl.log\" -k -vvvv \\\n-H X-SLURM-USER-TOKEN:$SLURM_JWT \\\n-X GET 'http://<server>:<port>/slurm/v0.0.<api-version>/diag'\n\n\nReplace the server, port, and api-version\nwith the appropriate values.\nExamine the output to ensure the response was 200 OK,\nand examine /tmp/curl.log for a valid JSON response.\nTry other endpoints described in the API Reference\n. Change GET to the correct method for the endpoint.\n\n\nAlternate command to use the UNIX socket instead\n\ncurl -s -o \"/tmp/curl.log\" -k -vvvv \\\n-H X-SLURM-USER-TOKEN:$SLURM_JWT \\\n--unix-socket /path/to/slurmrestd.socket \\\n'http://<server>/slurm/v0.0.<api-version>/diag'\n\n\nReplace the path, server, and api-version\nwith the appropriate values.\nExamine the output to ensure the response was 200 OK,\nand examine /tmp/curl.log for a valid JSON response.\n\n\nToken management\nThis guide provides a simple overview using scontrol to\nobtain tokens. This is a basic introductory approach that in many cases\nshould be disabled in favor of more sophisticated token management.\nRefer to the JWT page for more details.Common Issues\nIn general, look out for these things:\nValidity of authentication token in SLURM_JWT\nHostname and port number\nAPI version and endpoint\nLog output of slurmrestd\nUnable to bind socket\nThis may be due to a permissions issue while attempting to set up the socket.\nCheck the log output from slurmrestd for the path to the socket.\nEnsure that the user running the slurmrestd service has permissions to the\nparent directory of the configured socket path, or change/remove the socket path\nas described above.If it says \"Address already in use\", check the command being run\nand the contents of \"SLURMRESTD_LISTEN\" for duplicates of the same TCP or UNIX\nsocket.Connection refused\nVerify that slurmrestd is running and listening on the port you are\nattempting to connect to.Protocol authentication error (HTTP 500)\nOne common authentication problem is an expired token. Request a new one:\nunset SLURM_JWT; export $(scontrol token)This solution also applies to an HTTP 401 error caused by no authentication\ntoken being sent at all. This may appear in the slurmrestd logs as\n\"Authentication does not apply to request.\"Otherwise, consult the logs on the slurmctld and slurmdbd.Unable to find requested URL (HTTP 404)\nCheck the API Reference page to ensure you're\nusing a valid URL and the correct method for it. Pay attention to the path as\nthere are different endpoints for slurm and slurmdbd.Rejecting thread config token (HTTP 401)\nCheck that slurmrestd has loaded the auth/jwt plugin.\nYou should see a debug message like this:\nslurmrestd: debug:  auth/jwt: init: JWT authentication plugin loaded\nIf it didn't load jwt, run this in the terminal you're using for slurmrestd:\nexport SLURM_JWT=daemon\nUnexpected URL character (HTTP 400)\nCheck the request URL and slurmrestd logs for characters that may be causing\nthe URL to be parsed incorrectly. Use the appropriate URL encoding sequence in\nplace of the problematic character (e.g., / = %2F).\n\n... -X GET \"localhost:8080/slurmdb/v0.0.40/jobs?submit_time=02/28/24\"\n### 400 BAD REQUEST\n... -X GET \"localhost:8080/slurmdb/v0.0.40/jobs?submit_time=02%2F28%2F24\"\n### 200 OK\n\nOther slurm commands not working\nIf SLURM_JWT is set, other slurm commands will attempt to use JWT\nauthentication, causing failures. This can be fixed by clearing the variable:\nunset SLURM_JWTLast modified 17 July 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/rest_clients.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "REST API Client Writing Guide",
                "content": "Contents\nOpenAPI Specification (OAS)\nOpenAPI Standard Compliance\n\n\t\tOpenAPI Specification (OAS) Documentation\nClient design\nOpenAPI Specification (OAS) changes\nOpenAPI Specification (OAS)\n\nSlurmrestd is compliant with\n\t\n\t\tOpenAPI 3.0.2\n\t.\n\tThe generated OAS can be viewed at the following URLs:\n\n/openapi.json\n/openapi.yaml\n/openapi/v3\nThe generated OAS can be generated directly via calling:\n\nGenerate OAS with only a compiled slurmrestd:\nenv SLURM_CONF=/dev/null slurmrestd --generate-openapi-spec -s slurmctld,slurmdbd -d v0.0.40\n\nGenerate OAS with fully configured Slurm install:\nslurmrestd --generate-openapi-spec -s slurmctld,slurmdbd -d v0.0.40\n\n\nThe OAS is designed to be the primary documentation for the request methods\nand responses including their contents. There are several third party tools\nthat automatically generate documentation against the OAS output listed by\nopenapi.tools.\nAn example of how to generate the docs is \nhere.The generated OpenAPI specification changes depending on the configuration of\nslurmrestd at run time. slurmrestd is a\nframework, and the actual content is provided by plugins, which are optional at\nruntime. However, the specific plugin versions (as noted by the v0.0.XX in the\npaths) will be stable across Slurm versions, if the relevant plugin is still\npresent. Plugin life cycles are described\nhere.OpenAPI Standard Compliance\n\nSlurm attempts to strictly comply with the relevant\n\nOpenAPI standards.\nFor reasons of compatibility, Slurm's OAS is tested against publicly available\nOpenAPI client generators, but Slurm does not directly support any of them as\nthey are outside the control of SchedMD and may change at any time. The goal\nis to comply with the standards, supporting as many clients as possible,\nwithout favoring any one client. Sites are always welcome to write their own\nclients that are OpenAPI compliant. As a rule, SchedMD will debug the HTTP\nsent to and received by slurmrestd but will not directly debug any client\nsource code.\n\nopenapi/v0.0.37:\n\t\t\n\n\t\t\t\t\tv4.x of OpenAPI-generator\n\n\n\t\t\t\t\tv5.x of OpenAPI-generator\n\n\n\nopenapi/v0.0.38:\n\t\t\n\n\t\t\t\t\tv4.x of OpenAPI-generator\n\n\n\t\t\t\t\tv5.x of OpenAPI-generator\n\n\n\nopenapi/v0.0.39:\n\t\t\n\n\t\t\t\t\tv6.x of OpenAPI-generator\n\n\n\nopenapi/dbv0.0.37:\n\t\t\n\n\t\t\t\t\tv4.x of OpenAPI-generator\n\n\n\t\t\t\t\tv5.x of OpenAPI-generator\n\n\n\nopenapi/dbv0.0.38:\n\t\t\n\n\t\t\t\t\tv4.x of OpenAPI-generator\n\n\n\t\t\t\t\tv5.x of OpenAPI-generator\n\n\n\nopenapi/dbv0.0.39:\n\t\t\n\n\t\t\t\t\tv6.x of OpenAPI-generator\n\n\n\nOpenAPI Specification (OAS) Documentation\n\nSlurm includes example generated documentation,\nprovided with each release. Slurm's documentation only includes the latest\nplugins to encourage sites to develop against the latest plugins, as they\nwill have the longest lifespan and, by extension, the new clients will continue\nto work for longer. Plugin life cycles are described\nhere. This documentation is\ngenerated via the following steps using\n\n\tOpenAPI-generator HTML output:\nGenerate OAS:\n\nenv SLURM_CONF=/dev/null slurmrestd --generate-openapi-spec -s slurmctld,slurmdbd -d v0.0.40 > openapi.json\n\n\nGenerate documentation:\n\nopenapi-generator-cli generate -i openapi.json -g html -o rest_api_docs\n\n\nPoint browser to rest_api_docs/index.html\nSwagger provides a web editor\nto view and interact with the generated OAS. It makes generating clients and\ndocumentation via\nSwagger Codegen\nrelatively simple.\nClient Design\n\nClients should always be generated against or designed to work with specific\nversions of the tagged paths from slurmrestd.\nClient developers are strongly encouraged to not include functionality for\nversions of methods not intended for use.\nClient developers need to plan how to gracefully handle changes between\ndifferent Slurm versions if they plan to support multiple versions.\nSlurm's method of versioning is done explicitly to allow old code to continue\nto work with newer Slurm releases while that older version is still\nsupported. For example, v0.0.38 methods were added in Slurm-22.05 but can be\nused until Slurm-24.05. While this works, these methods will not get any new\nfeatures or functionality, but generally only security fixes. Slurm will get\nseveral new features every release, and those changes are then reflected by the\nchanges in the new plugin version. A client wishing to use the new features will\nhave to move to the newer version as new features will not be backported.Using an OpenAPI schema generated for just one version is\nadvised. Many of the OpenAPI client generators have a way to strip out the\nversion tag from the struct names (i.e. V0039AccountFlagsDELETED\n-> AccountFlagsDELETED). This could allow for a set of\nunversioned base code could be created and then adjusted for material changes\nin the outputted code with newer Slurm versions. Having a strongly typed\nlanguage can help with this considerably. Generally, only parts of the schema\nchange between different versions for specific endpoints, although looking at\na diff of them can be intimidating even if using something like\njson-diff. Another option is\nhaving wrappers to account for version differences in the same fashion as many\nc libraries account for differences between Windows and Linux.The generated OpenAPI schema can change, depending on which plugins are\npresent, but the versioned paths and their schemas will not (with limited\nexceptions). As such, generating a schema limited to only v0.0.40\nand placing it in your repo should result in a schema that can be used in\nSlurm-23.11 to Slurm-25.05. In general, regenerating the client code and OpenAPI\nschema will be counter-productive, as even the OpenAPI generators themselves can\ngenerate different results for the same OpenAPI specification between their\nversions. The same driver code would likely not even compile even though\nnothing about the server has changed. An example of this\nspecific type of issue can be found\n\nhere.Developers may want to consider having a somewhat static set of compiled\nclient code in your client's code repository. That code will then only need to\nbe updated for revisions inside of the tagged versions, which are generally\nquite rare. That will remove the need for end users to run the code generators\nand limit the chances of any change disrupting your workflow.\nIt will also allow you to plan for upgrades at a convenient time rather than\nhaving to ensure compatibility of multiple permutations at all times.Developers should be aware that older versions of the versioned plugins are\nremoved from Slurm in a documented cadence as given\nhere.\nClients will need to be upgraded once the relevant plugins versions are removed\nto continue to communicate with slurmrestd.If slurmrestd compiles, then all of it will compile. Run time args to\nslurmrestd and slurm.conf will, however, change the output of OAS. For instance,\nif slurmdbd accounting is not configured then the /slurmdb/ paths\nwill automatically not be included as there is an invalid prerequisite\nfor them. A client that queries them will get a 404 error. Slurmrestd can and\nshould be told to load the minimal number of plugins too (via -d and -s)\nwhich will also change which paths are present and thus included in the OAS. To\nslurmrestd, the OAS is just a form of documentation and doesn't have any\nbearing to how it functions. A client could be generated with many paths that\nthe current running slurmrestd does not have loaded. That client will just get\n404 errors for those queries and should try to avoid them via internal logic.\nThe client only needs to have a matching OAS for the paths/endpoints that client\nwill actually query. Since all endpoints in slurmrestd are versioned, there is\nan automatic guarantee they will work (if present) as though the client is\nquerying the original slurmrestd it used to generate the original OAS it was\ncompiled against. If a path of the same version does not behave the same, then\nthat is a bug, and we kindly ask that a\nticket be opened so we can fix it.There are very limited situations where slurmrestd will generate an OAS with\nthe same endpoint having different functionalities.\n\nIf the specification is somehow fundamentally broken so that it violates the\nOpenAPI standard. Slurm has test units to try catch this but those tests are\nnot perfect.\nA new field or path has been added. This should never break a client as\nclients should ignore unknown fields in JSON/YAML.\nOpenAPI Specification (OAS) changes\n\nChanges to the OAS are always listed with every release in the\nOpenAPI Release Notes.A simple trick to see the differences between versions is to query both and\nthen mask the newer one, to avoid having diff list out every version tag that\nchanged:\n\nenv SLURM_CONF=/dev/null slurmrestd -d v0.0.41 -s slurmdbd,slurmctld --generate-openapi-spec > /tmp/v41.json\nenv SLURM_CONF=/dev/null slurmrestd -d v0.0.40 -s slurmdbd,slurmctld --generate-openapi-spec > /tmp/v40.json\ncat /tmp/v41.json | sed -e 's#v0.0.41#v0.0.40#g' > /tmp/v41_masked.json\nvimdiff /tmp/v40.json /tmp/v41_masked.json\njsondiff /tmp/v40.json /tmp/v41_masked.json\nSometimes this trick still produces too much change in the diff output to be\nuseful. In those cases, selecting a specific (sub)schema can be helpful:\n\njq '.components.schemas.\"v0.0.40_job\"' /tmp/v40.json > /tmp/v40_job.json\njq '.components.schemas.\"v0.0.40_openapi_job_info_resp\".properties.jobs.items' /tmp/v41_masked.json > /tmp/v41_masked_job.json\nvimdiff /tmp/v40_job.json /tmp/v41_masked_job.json\nThe generated OpenAPI schemas are very detailed and get more detailed\nevery release as we add more enums, better expose possible values and\nincrease documentation in comments. Even minor changes to tree structure can\nresult in a large number of changes in the generated schema, which can be\nconfusing while looking at diffs. The example above shows the inlining of\nv0.0.40_job into v0.0.40_openapi_job_info_resp.\nDepending on the generated client, that change may not change the\nresultant client code at all.Last modified 24 September 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://www.schedmd.com/download-slurm/",
        "sections": []
    },
    {
        "url": "https://slurm.schedmd.com/job_submit_plugins.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Job Submit Plugin API",
                "content": "Overview This document describes Slurm job submit plugins and the API that\ndefines them. It is intended as a resource to programmers wishing to write\ntheir own Slurm job submit plugins. This is version 100 of the API.Slurm job submit plugins must conform to the\nSlurm Plugin API with the following specifications:const char\nplugin_name[]=\"full\u00a0text\u00a0name\"\n\nA free-formatted ASCII text string that identifies the plugin.\n\nconst char\nplugin_type[]=\"major/minor\"\n\nThe major type must be \"job_submit.\"\nThe minor type can be any suitable name for the type of job submission package.\nWe include samples in the Slurm distribution for\n\nall_partitions \u2014 Set default partition to all partitions on\nthe cluster.\ndefaults \u2014 Set default values for job submission or modify\nrequests.\nlogging \u2014 Log select job submission and modification\nparameters.\nlua \u2014 Interface to Lua scripts\nimplementing these functions (actually a slight variation of them). Sample Lua\nscripts can be found with the Slurm distribution in the directory\ncontribs/lua. The Lua script must be named \"job_submit.lua\" and must\nbe located in the default configuration directory (typically the subdirectory\n\"etc\" of the installation directory). Slurmctld will fatal on startup if the\nconfigured lua script is invalid. Slurm will try to load the script for each\njob submission. If the script is broken or removed while slurmctld is running,\nSlurm will fallback to the previous working version of the script.\nWarning: slurmctld runs this script while holding internal locks, and\nonly a single copy of this script can run at a time. This blocks most\nconcurrency in slurmctld. Therefore, this script should run to completion as\nquickly as possible.\npartition \u2014 Sets a job's default partition based upon job\nsubmission parameters and available partitions.\npbs \u2014 Translate PBS job submission options to Slurm equivalent\n(if possible).\nrequire_timelimit \u2014 Force job submissions to specify a\ntimelimit.\n\nconst uint32_t plugin_version\nIf specified, identifies the version of Slurm used to build this plugin and\nany attempt to load the plugin from a different version of Slurm will result\nin an error.\nIf not specified, then the plugin may be loaded by Slurm commands and\ndaemons from any version, however this may result in difficult to diagnose\nfailures due to changes in the arguments to plugin functions or changes\nin other Slurm functions used by the plugin.\nSlurm can be configured to use multiple job_submit plugins if desired,\nhowever the lua plugin will only execute one lua script named \"job_submit.lua\"\nlocated in the default script directory (typically the subdirectory \"etc\" of\nthe installation directory).\nAPI Functions\nAll of the following functions are required. Functions which are not\nimplemented must be stubbed.\n\n int init (void)\nDescription:\n  Called when the plugin is loaded, before any other functions are\n  called. Put global initialization here.\nReturns: \nSLURM_SUCCESS on success, or\nSLURM_ERROR on failure.\n void fini (void)\nDescription:\n  Called when the plugin is removed. Clear any allocated storage here.\nReturns: None.\nNote: These init and fini functions are not the same as those\ndescribed in the dlopen (3) system library.\nThe C run-time system co-opts those symbols for its own initialization.\nThe system _init() is called before the Slurm\ninit(), and the Slurm\nfini() is called before the system's\n_fini().\n\nint job_submit(struct job_descriptor *job_desc, uint32_t submit_uid, char **error_msg)\nDescription:\nThis function is called by the slurmctld daemon with the job submission\nparameters supplied by the user regardless of the command used (e.g.\nsalloc, sbatch, slurmrestd). Only explicitly\ndefined values will be represented. For values not defined at submit time\nslurm.NO_VAL/16/64 or\nnil will be set. It can be used to log and/or\nmodify the job parameters supplied by the user as desired. Note that this\nfunction has access to the slurmctld's global data structures, for example\nto examine the available partitions, reservations, etc.\nArguments: \njob_desc\n(input/output) the job allocation request specifications, before job defaults\nare set.\nsubmit_uid\n(input) user ID initiating the request.\nerror_msg\n(output) If the argument is not null, then a plugin generated error message\ncan be stored here. The error message is expected to have allocated memory\nwhich Slurm will release using the xfree function. The error message is always\npropagated to the caller, no matter the return code.\nReturns: \nSLURM_SUCCESS on success, or\nSLURM_ERROR on failure.\n\n\nint job_modify(struct job_descriptor *job_desc, job_record_t *job_ptr, uint32_t modify_uid)\nDescription:\nThis function is called by the slurmctld daemon with job modification parameters\nsupplied by the user regardless of the command used (e.g. scontrol, sview,\nslurmrestd). It can be used to log and/or\nmodify the job parameters supplied by the user as desired. Note that this\nfunction has access to the slurmctld's global data structures, for example to\nexamine the available partitions, reservations, etc.\nArguments: \njob_desc\n(input/output) the job allocation request specifications, before job defaults\nare set.\njob_ptr\n(input/output) slurmctld daemon's current data structure for the job to\nbe modified.\nmodify_uid\n(input) user ID initiating the request.\nReturns: \nSLURM_SUCCESS on success, or\nSLURM_ERROR on failure.\n\nLua Functions\nThe Lua functions differ slightly from those implemented in C for\nbetter ease of use. Sample Lua scripts can be found with the Slurm distribution\nin the directory contribs/lua. The default installation location of\nthe Lua scripts is the same location as the Slurm configuration file,\nslurm.conf.\nReading and writing of job environment variables using Lua is possible\nby referencing the environment variables as a data structure containing\nnamed elements.\nNOTE: Only sbatch sends the environment to slurmctld. salloc and srun\ndo not send the environment to slurmctld, so job_desc.environment is not\navailable in the job_submit plugin for these jobs.\nFor example:\n\n...\n\t-- job_desc.environment is only available for batch jobs.\n\tif (job_desc.script) then\n\t\tif (job_desc.environment ~= nil) then\n\t\t\tif (job_desc.environment[\"FOO\"] ~= nil) then\n\t\t\t\tslurm.log_user(\"Found env FOO=%s\",\n\t\t\t\t\t       job_desc.environment[\"FOO\"])\n\t\t\tend\n\t\tend\n\tend\n...\n\nNOTE: To get/set the environment for all types of jobs, an alternate\napproach is to use CliFilterPlugins.\n\nint slurm_job_submit(job_desc_msg_t *job_desc, List part_list, uint32_t\nsubmit_uid)\nDescription:\nThis function is called by the slurmctld daemon with the job submission\nparameters supplied by the user regardless of the command used (e.g.\nsalloc, sbatch, slurmrestd). Only explicitly\ndefined values will be represented. For values not defined at submit time\nslurm.NO_VAL/16/64 or\nnil will be set. It can be used to log and/or\nmodify the job parameters supplied by the user as desired. Note that this\nfunction has access to the slurmctld's global data structures, for example\nto examine the available partitions, reservations, etc.\nArguments: \njob_desc\n(input/output) the job allocation request specifications.\npart_list\n(input) List of pointer to partitions which this user is authorized to use.\nsubmit_uid\n(input) user ID initiating the request.\nReturns: \nslurm.SUCCESS \u2014\nJob submission accepted by plugin.\nslurm.FAILURE \u2014\nJob submission rejected due to error (Deprecated in 19.05).\nslurm.ERROR \u2014\nJob submission rejected due to error.\nslurm.ESLURM_* \u2014\nJob submission rejected due to error as defined by\nslurm/slurm_errno.h and src/common/slurm_errno.c.\nNOTE: As job_desc contains only\nuser-specified values, undefined values can be recognized (before defaults\nare set) by either checking for nil or for\nthe corresponding slurm.NO_VAL/16/64. This\nallows sites to apply policies, such as requiring users to define the number\nof nodes, as in the example below:\n\n...\n\t-- Number of nodes must be defined at submit time\n\tif (job_desc.max_nodes == slurm.NO_VAL) then\n\t\tslurm.log_user(\"No max_nodes specified, please specify a number of nodes\")\n\t\treturn slurm.ERROR\n\tend\n...\n\n\nint slurm_job_modify(job_desc_msg_t *job_desc, job_record_t *job_ptr,\nList part_list, int modify_uid)\nDescription:\nThis function is called by the slurmctld daemon with job modification parameters\nsupplied by the user regardless of the command used (e.g. scontrol, sview,\nslurmrestd). It can be used to log and/or\nmodify the job parameters supplied by the user as desired. Note that this\nfunction has access to the slurmctld's global data structures, for example to\nexamine the available partitions, reservations, etc.\nArguments: \njob_desc\n(input/output) the job allocation request specifications.\njob_ptr\n(input/output) slurmctld daemon's current data structure for the job to\nbe modified.\npart_list\n(input) List of pointer to partitions which this user is authorized to use.\nmodify_uid\n(input) user ID initiating the request.\nReturns: \nReturns from job_modify() are the same as the returns from job_submit().\nLua Job Attributes\nThe available job attributes change occasionally with different versions of\nSlurm. To find the job attributes that are available for the version of Slurm\nyou're using, go to the  SchedMD\ngithub page, and navigate to\nsrc/plugins/job_submit/lua/job_submit_lua.c.\n_job_rec_field() contains the list of attributes available for the\njob_record (e.g. current record in Slurm). _get_job_req_field() contains\nthe list of attributes available for the job_descriptor (e.g. submission or\nmodification request).\n\nBuilding\nGenerally using a LUA interface for a job submit plugin is best:\nIt is simple to write and maintain with minimal dependencies upon the Slurm\ndata structures.\nHowever using C does provide a mechanism to get more information than available\nusing LUA including full access to all of the data structures and functions\nin the slurmctld daemon.\nThe simplest way to build a C program would be to just replace one of the\njob submit plugins included in the Slurm distribution with your own code\n(i.e. use a patch with your own code).\nThen just build and install Slurm with that new code.\nBuilding a new plugin outside of the Slurm distribution is possible, but\nfar more complex.\nIt also requires access to a multitude of Slurm header files as shown in the\nprocedure below.\n\nYou will need to at least partly build Slurm first. The \"configure\" command\nmust be executed in order to build the \"config.h\" file in the build directory.\nCreate a local directory somewhere for your files to build with.\nAlso create subdirectories named \".libs\" and \".deps\".\nCopy a \".deps/job_submit_*Plo\" file from another job_submit plugin's \".deps\"\ndirectory (made as part of the build process) into your local \".deps\" subdirectory.\nRename the file as appropriate to reflect your plugins name (e.g. rename\n\"job_submit_partition.Plo\" to be something like \"job_submit_mine.Plo\").\nCompile and link your plugin. Those options might differ depending\nupon your build environment. Check the options used for building the\nother job_submit plugins and modify the example below as required.\nInstall the plugin.\n\n\n# Example:\n# The Slurm source is in ~/SLURM/slurm.git\n# The Slurm build directory is ~/SLURM/slurm.build\n# The plugin build is to take place in the directory\n#   \"~/SLURM/my_submit\"\n# The installation location is \"/usr/local\"\n\n# Build Slurm from ~/SLURM/slurm.build\n# (or at least run \"~/SLURM/slurm.git/configure\")\n\n# Set up your plugin files\ncd ~/SLURM\nmkdir my_submit\ncd my_submit\nmkdir .libs\nmkdir .deps\n# Create your plugin code\nvi job_submit_mine.c\n\n# Copy up a dependency file\ncp ~/SLURM/slurm.build/src/plugins/job_submit/partition/.deps/job_submit_partition.Plo \\\n   .deps/job_submit_mine.Plo\n\n# Compile\ngcc -DHAVE_CONFIG_H -I~/SLURM/slurm.build -I~/slurm.git \\\n   -g -O2 -pthread -fno-gcse -Werror -Wall -g -O0       \\\n   -fno-strict-aliasing -MT job_submit_mine.lo          \\\n   -MD -MP -MF .deps/job_submit_mine.Tpo                \\\n   -c job_submit_mine.c -o .libs/job_submit_mine.o\n\n# Some clean up\nmv -f .deps/job_submit_mine.Tpo .deps/job_submit_mine.Plo\nrm -fr .libs/job_submit_mine.a .libs/job_submit_mine.la \\\n   .libs/job_submit_mine.lai job_submit_mine.so\n\n# Link\ngcc -shared -fPIC -DPIC .libs/job_submit_mine.o -O2         \\\n   -pthread -O0 -pthread -Wl,-soname -Wl,job_submit_mine.so \\\n   -o job_submit_mine.so\n\n# Install\ncp job_submit_mine.so file \\\n   /usr/local/lib/slurm/job_submit_mine.so\n\nLast modified 04 October 2023\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "API Functions",
                "content": "All of the following functions are required. Functions which are not\nimplemented must be stubbed.\n\n int init (void)\nDescription:\n  Called when the plugin is loaded, before any other functions are\n  called. Put global initialization here.\nReturns: \nSLURM_SUCCESS on success, or\nSLURM_ERROR on failure.\n void fini (void)\nDescription:\n  Called when the plugin is removed. Clear any allocated storage here.\nReturns: None.\nNote: These init and fini functions are not the same as those\ndescribed in the dlopen (3) system library.\nThe C run-time system co-opts those symbols for its own initialization.\nThe system _init() is called before the Slurm\ninit(), and the Slurm\nfini() is called before the system's\n_fini().\n\nint job_submit(struct job_descriptor *job_desc, uint32_t submit_uid, char **error_msg)\nDescription:\nThis function is called by the slurmctld daemon with the job submission\nparameters supplied by the user regardless of the command used (e.g.\nsalloc, sbatch, slurmrestd). Only explicitly\ndefined values will be represented. For values not defined at submit time\nslurm.NO_VAL/16/64 or\nnil will be set. It can be used to log and/or\nmodify the job parameters supplied by the user as desired. Note that this\nfunction has access to the slurmctld's global data structures, for example\nto examine the available partitions, reservations, etc.\nArguments: \njob_desc\n(input/output) the job allocation request specifications, before job defaults\nare set.\nsubmit_uid\n(input) user ID initiating the request.\nerror_msg\n(output) If the argument is not null, then a plugin generated error message\ncan be stored here. The error message is expected to have allocated memory\nwhich Slurm will release using the xfree function. The error message is always\npropagated to the caller, no matter the return code.\nReturns: \nSLURM_SUCCESS on success, or\nSLURM_ERROR on failure.\n\n\nint job_modify(struct job_descriptor *job_desc, job_record_t *job_ptr, uint32_t modify_uid)\nDescription:\nThis function is called by the slurmctld daemon with job modification parameters\nsupplied by the user regardless of the command used (e.g. scontrol, sview,\nslurmrestd). It can be used to log and/or\nmodify the job parameters supplied by the user as desired. Note that this\nfunction has access to the slurmctld's global data structures, for example to\nexamine the available partitions, reservations, etc.\nArguments: \njob_desc\n(input/output) the job allocation request specifications, before job defaults\nare set.\njob_ptr\n(input/output) slurmctld daemon's current data structure for the job to\nbe modified.\nmodify_uid\n(input) user ID initiating the request.\nReturns: \nSLURM_SUCCESS on success, or\nSLURM_ERROR on failure.\n\nLua Functions\nThe Lua functions differ slightly from those implemented in C for\nbetter ease of use. Sample Lua scripts can be found with the Slurm distribution\nin the directory contribs/lua. The default installation location of\nthe Lua scripts is the same location as the Slurm configuration file,\nslurm.conf.\nReading and writing of job environment variables using Lua is possible\nby referencing the environment variables as a data structure containing\nnamed elements.\nNOTE: Only sbatch sends the environment to slurmctld. salloc and srun\ndo not send the environment to slurmctld, so job_desc.environment is not\navailable in the job_submit plugin for these jobs.\nFor example:\n\n...\n\t-- job_desc.environment is only available for batch jobs.\n\tif (job_desc.script) then\n\t\tif (job_desc.environment ~= nil) then\n\t\t\tif (job_desc.environment[\"FOO\"] ~= nil) then\n\t\t\t\tslurm.log_user(\"Found env FOO=%s\",\n\t\t\t\t\t       job_desc.environment[\"FOO\"])\n\t\t\tend\n\t\tend\n\tend\n...\n\nNOTE: To get/set the environment for all types of jobs, an alternate\napproach is to use CliFilterPlugins.\n\nint slurm_job_submit(job_desc_msg_t *job_desc, List part_list, uint32_t\nsubmit_uid)\nDescription:\nThis function is called by the slurmctld daemon with the job submission\nparameters supplied by the user regardless of the command used (e.g.\nsalloc, sbatch, slurmrestd). Only explicitly\ndefined values will be represented. For values not defined at submit time\nslurm.NO_VAL/16/64 or\nnil will be set. It can be used to log and/or\nmodify the job parameters supplied by the user as desired. Note that this\nfunction has access to the slurmctld's global data structures, for example\nto examine the available partitions, reservations, etc.\nArguments: \njob_desc\n(input/output) the job allocation request specifications.\npart_list\n(input) List of pointer to partitions which this user is authorized to use.\nsubmit_uid\n(input) user ID initiating the request.\nReturns: \nslurm.SUCCESS \u2014\nJob submission accepted by plugin.\nslurm.FAILURE \u2014\nJob submission rejected due to error (Deprecated in 19.05).\nslurm.ERROR \u2014\nJob submission rejected due to error.\nslurm.ESLURM_* \u2014\nJob submission rejected due to error as defined by\nslurm/slurm_errno.h and src/common/slurm_errno.c.\nNOTE: As job_desc contains only\nuser-specified values, undefined values can be recognized (before defaults\nare set) by either checking for nil or for\nthe corresponding slurm.NO_VAL/16/64. This\nallows sites to apply policies, such as requiring users to define the number\nof nodes, as in the example below:\n\n...\n\t-- Number of nodes must be defined at submit time\n\tif (job_desc.max_nodes == slurm.NO_VAL) then\n\t\tslurm.log_user(\"No max_nodes specified, please specify a number of nodes\")\n\t\treturn slurm.ERROR\n\tend\n...\n\n\nint slurm_job_modify(job_desc_msg_t *job_desc, job_record_t *job_ptr,\nList part_list, int modify_uid)\nDescription:\nThis function is called by the slurmctld daemon with job modification parameters\nsupplied by the user regardless of the command used (e.g. scontrol, sview,\nslurmrestd). It can be used to log and/or\nmodify the job parameters supplied by the user as desired. Note that this\nfunction has access to the slurmctld's global data structures, for example to\nexamine the available partitions, reservations, etc.\nArguments: \njob_desc\n(input/output) the job allocation request specifications.\njob_ptr\n(input/output) slurmctld daemon's current data structure for the job to\nbe modified.\npart_list\n(input) List of pointer to partitions which this user is authorized to use.\nmodify_uid\n(input) user ID initiating the request.\nReturns: \nReturns from job_modify() are the same as the returns from job_submit().\nLua Job Attributes\nThe available job attributes change occasionally with different versions of\nSlurm. To find the job attributes that are available for the version of Slurm\nyou're using, go to the  SchedMD\ngithub page, and navigate to\nsrc/plugins/job_submit/lua/job_submit_lua.c.\n_job_rec_field() contains the list of attributes available for the\njob_record (e.g. current record in Slurm). _get_job_req_field() contains\nthe list of attributes available for the job_descriptor (e.g. submission or\nmodification request).\n\nBuilding\nGenerally using a LUA interface for a job submit plugin is best:\nIt is simple to write and maintain with minimal dependencies upon the Slurm\ndata structures.\nHowever using C does provide a mechanism to get more information than available\nusing LUA including full access to all of the data structures and functions\nin the slurmctld daemon.\nThe simplest way to build a C program would be to just replace one of the\njob submit plugins included in the Slurm distribution with your own code\n(i.e. use a patch with your own code).\nThen just build and install Slurm with that new code.\nBuilding a new plugin outside of the Slurm distribution is possible, but\nfar more complex.\nIt also requires access to a multitude of Slurm header files as shown in the\nprocedure below.\n\nYou will need to at least partly build Slurm first. The \"configure\" command\nmust be executed in order to build the \"config.h\" file in the build directory.\nCreate a local directory somewhere for your files to build with.\nAlso create subdirectories named \".libs\" and \".deps\".\nCopy a \".deps/job_submit_*Plo\" file from another job_submit plugin's \".deps\"\ndirectory (made as part of the build process) into your local \".deps\" subdirectory.\nRename the file as appropriate to reflect your plugins name (e.g. rename\n\"job_submit_partition.Plo\" to be something like \"job_submit_mine.Plo\").\nCompile and link your plugin. Those options might differ depending\nupon your build environment. Check the options used for building the\nother job_submit plugins and modify the example below as required.\nInstall the plugin.\n\n\n# Example:\n# The Slurm source is in ~/SLURM/slurm.git\n# The Slurm build directory is ~/SLURM/slurm.build\n# The plugin build is to take place in the directory\n#   \"~/SLURM/my_submit\"\n# The installation location is \"/usr/local\"\n\n# Build Slurm from ~/SLURM/slurm.build\n# (or at least run \"~/SLURM/slurm.git/configure\")\n\n# Set up your plugin files\ncd ~/SLURM\nmkdir my_submit\ncd my_submit\nmkdir .libs\nmkdir .deps\n# Create your plugin code\nvi job_submit_mine.c\n\n# Copy up a dependency file\ncp ~/SLURM/slurm.build/src/plugins/job_submit/partition/.deps/job_submit_partition.Plo \\\n   .deps/job_submit_mine.Plo\n\n# Compile\ngcc -DHAVE_CONFIG_H -I~/SLURM/slurm.build -I~/slurm.git \\\n   -g -O2 -pthread -fno-gcse -Werror -Wall -g -O0       \\\n   -fno-strict-aliasing -MT job_submit_mine.lo          \\\n   -MD -MP -MF .deps/job_submit_mine.Tpo                \\\n   -c job_submit_mine.c -o .libs/job_submit_mine.o\n\n# Some clean up\nmv -f .deps/job_submit_mine.Tpo .deps/job_submit_mine.Plo\nrm -fr .libs/job_submit_mine.a .libs/job_submit_mine.la \\\n   .libs/job_submit_mine.lai job_submit_mine.so\n\n# Link\ngcc -shared -fPIC -DPIC .libs/job_submit_mine.o -O2         \\\n   -pthread -O0 -pthread -Wl,-soname -Wl,job_submit_mine.so \\\n   -o job_submit_mine.so\n\n# Install\ncp job_submit_mine.so file \\\n   /usr/local/lib/slurm/job_submit_mine.so\n\nLast modified 04 October 2023\n"
            },
            {
                "title": "Lua Functions",
                "content": "The Lua functions differ slightly from those implemented in C for\nbetter ease of use. Sample Lua scripts can be found with the Slurm distribution\nin the directory contribs/lua. The default installation location of\nthe Lua scripts is the same location as the Slurm configuration file,\nslurm.conf.\nReading and writing of job environment variables using Lua is possible\nby referencing the environment variables as a data structure containing\nnamed elements.NOTE: Only sbatch sends the environment to slurmctld. salloc and srun\ndo not send the environment to slurmctld, so job_desc.environment is not\navailable in the job_submit plugin for these jobs.For example:\n...\n\t-- job_desc.environment is only available for batch jobs.\n\tif (job_desc.script) then\n\t\tif (job_desc.environment ~= nil) then\n\t\t\tif (job_desc.environment[\"FOO\"] ~= nil) then\n\t\t\t\tslurm.log_user(\"Found env FOO=%s\",\n\t\t\t\t\t       job_desc.environment[\"FOO\"])\n\t\t\tend\n\t\tend\n\tend\n...\nNOTE: To get/set the environment for all types of jobs, an alternate\napproach is to use CliFilterPlugins.\nint slurm_job_submit(job_desc_msg_t *job_desc, List part_list, uint32_t\nsubmit_uid)\nDescription:\nThis function is called by the slurmctld daemon with the job submission\nparameters supplied by the user regardless of the command used (e.g.\nsalloc, sbatch, slurmrestd). Only explicitly\ndefined values will be represented. For values not defined at submit time\nslurm.NO_VAL/16/64 or\nnil will be set. It can be used to log and/or\nmodify the job parameters supplied by the user as desired. Note that this\nfunction has access to the slurmctld's global data structures, for example\nto examine the available partitions, reservations, etc.\nArguments: \njob_desc\n(input/output) the job allocation request specifications.\npart_list\n(input) List of pointer to partitions which this user is authorized to use.\nsubmit_uid\n(input) user ID initiating the request.\nReturns: \nslurm.SUCCESS \u2014\nJob submission accepted by plugin.\nslurm.FAILURE \u2014\nJob submission rejected due to error (Deprecated in 19.05).\nslurm.ERROR \u2014\nJob submission rejected due to error.\nslurm.ESLURM_* \u2014\nJob submission rejected due to error as defined by\nslurm/slurm_errno.h and src/common/slurm_errno.c.\nNOTE: As job_desc contains only\nuser-specified values, undefined values can be recognized (before defaults\nare set) by either checking for nil or for\nthe corresponding slurm.NO_VAL/16/64. This\nallows sites to apply policies, such as requiring users to define the number\nof nodes, as in the example below:\n\n...\n\t-- Number of nodes must be defined at submit time\n\tif (job_desc.max_nodes == slurm.NO_VAL) then\n\t\tslurm.log_user(\"No max_nodes specified, please specify a number of nodes\")\n\t\treturn slurm.ERROR\n\tend\n...\n\n\nint slurm_job_modify(job_desc_msg_t *job_desc, job_record_t *job_ptr,\nList part_list, int modify_uid)\nDescription:\nThis function is called by the slurmctld daemon with job modification parameters\nsupplied by the user regardless of the command used (e.g. scontrol, sview,\nslurmrestd). It can be used to log and/or\nmodify the job parameters supplied by the user as desired. Note that this\nfunction has access to the slurmctld's global data structures, for example to\nexamine the available partitions, reservations, etc.\nArguments: \njob_desc\n(input/output) the job allocation request specifications.\njob_ptr\n(input/output) slurmctld daemon's current data structure for the job to\nbe modified.\npart_list\n(input) List of pointer to partitions which this user is authorized to use.\nmodify_uid\n(input) user ID initiating the request.\nReturns: \nReturns from job_modify() are the same as the returns from job_submit().\nLua Job Attributes\nThe available job attributes change occasionally with different versions of\nSlurm. To find the job attributes that are available for the version of Slurm\nyou're using, go to the  SchedMD\ngithub page, and navigate to\nsrc/plugins/job_submit/lua/job_submit_lua.c.\n_job_rec_field() contains the list of attributes available for the\njob_record (e.g. current record in Slurm). _get_job_req_field() contains\nthe list of attributes available for the job_descriptor (e.g. submission or\nmodification request).\n\nBuilding\nGenerally using a LUA interface for a job submit plugin is best:\nIt is simple to write and maintain with minimal dependencies upon the Slurm\ndata structures.\nHowever using C does provide a mechanism to get more information than available\nusing LUA including full access to all of the data structures and functions\nin the slurmctld daemon.\nThe simplest way to build a C program would be to just replace one of the\njob submit plugins included in the Slurm distribution with your own code\n(i.e. use a patch with your own code).\nThen just build and install Slurm with that new code.\nBuilding a new plugin outside of the Slurm distribution is possible, but\nfar more complex.\nIt also requires access to a multitude of Slurm header files as shown in the\nprocedure below.\n\nYou will need to at least partly build Slurm first. The \"configure\" command\nmust be executed in order to build the \"config.h\" file in the build directory.\nCreate a local directory somewhere for your files to build with.\nAlso create subdirectories named \".libs\" and \".deps\".\nCopy a \".deps/job_submit_*Plo\" file from another job_submit plugin's \".deps\"\ndirectory (made as part of the build process) into your local \".deps\" subdirectory.\nRename the file as appropriate to reflect your plugins name (e.g. rename\n\"job_submit_partition.Plo\" to be something like \"job_submit_mine.Plo\").\nCompile and link your plugin. Those options might differ depending\nupon your build environment. Check the options used for building the\nother job_submit plugins and modify the example below as required.\nInstall the plugin.\n\n\n# Example:\n# The Slurm source is in ~/SLURM/slurm.git\n# The Slurm build directory is ~/SLURM/slurm.build\n# The plugin build is to take place in the directory\n#   \"~/SLURM/my_submit\"\n# The installation location is \"/usr/local\"\n\n# Build Slurm from ~/SLURM/slurm.build\n# (or at least run \"~/SLURM/slurm.git/configure\")\n\n# Set up your plugin files\ncd ~/SLURM\nmkdir my_submit\ncd my_submit\nmkdir .libs\nmkdir .deps\n# Create your plugin code\nvi job_submit_mine.c\n\n# Copy up a dependency file\ncp ~/SLURM/slurm.build/src/plugins/job_submit/partition/.deps/job_submit_partition.Plo \\\n   .deps/job_submit_mine.Plo\n\n# Compile\ngcc -DHAVE_CONFIG_H -I~/SLURM/slurm.build -I~/slurm.git \\\n   -g -O2 -pthread -fno-gcse -Werror -Wall -g -O0       \\\n   -fno-strict-aliasing -MT job_submit_mine.lo          \\\n   -MD -MP -MF .deps/job_submit_mine.Tpo                \\\n   -c job_submit_mine.c -o .libs/job_submit_mine.o\n\n# Some clean up\nmv -f .deps/job_submit_mine.Tpo .deps/job_submit_mine.Plo\nrm -fr .libs/job_submit_mine.a .libs/job_submit_mine.la \\\n   .libs/job_submit_mine.lai job_submit_mine.so\n\n# Link\ngcc -shared -fPIC -DPIC .libs/job_submit_mine.o -O2         \\\n   -pthread -O0 -pthread -Wl,-soname -Wl,job_submit_mine.so \\\n   -o job_submit_mine.so\n\n# Install\ncp job_submit_mine.so file \\\n   /usr/local/lib/slurm/job_submit_mine.so\n\nLast modified 04 October 2023\n"
            },
            {
                "title": "Lua Job Attributes",
                "content": "The available job attributes change occasionally with different versions of\nSlurm. To find the job attributes that are available for the version of Slurm\nyou're using, go to the  SchedMD\ngithub page, and navigate to\nsrc/plugins/job_submit/lua/job_submit_lua.c.\n_job_rec_field() contains the list of attributes available for the\njob_record (e.g. current record in Slurm). _get_job_req_field() contains\nthe list of attributes available for the job_descriptor (e.g. submission or\nmodification request).\nBuildingGenerally using a LUA interface for a job submit plugin is best:\nIt is simple to write and maintain with minimal dependencies upon the Slurm\ndata structures.\nHowever using C does provide a mechanism to get more information than available\nusing LUA including full access to all of the data structures and functions\nin the slurmctld daemon.\nThe simplest way to build a C program would be to just replace one of the\njob submit plugins included in the Slurm distribution with your own code\n(i.e. use a patch with your own code).\nThen just build and install Slurm with that new code.\nBuilding a new plugin outside of the Slurm distribution is possible, but\nfar more complex.\nIt also requires access to a multitude of Slurm header files as shown in the\nprocedure below.\nYou will need to at least partly build Slurm first. The \"configure\" command\nmust be executed in order to build the \"config.h\" file in the build directory.\nCreate a local directory somewhere for your files to build with.\nAlso create subdirectories named \".libs\" and \".deps\".\nCopy a \".deps/job_submit_*Plo\" file from another job_submit plugin's \".deps\"\ndirectory (made as part of the build process) into your local \".deps\" subdirectory.\nRename the file as appropriate to reflect your plugins name (e.g. rename\n\"job_submit_partition.Plo\" to be something like \"job_submit_mine.Plo\").\nCompile and link your plugin. Those options might differ depending\nupon your build environment. Check the options used for building the\nother job_submit plugins and modify the example below as required.\nInstall the plugin.\n\n# Example:\n# The Slurm source is in ~/SLURM/slurm.git\n# The Slurm build directory is ~/SLURM/slurm.build\n# The plugin build is to take place in the directory\n#   \"~/SLURM/my_submit\"\n# The installation location is \"/usr/local\"\n\n# Build Slurm from ~/SLURM/slurm.build\n# (or at least run \"~/SLURM/slurm.git/configure\")\n\n# Set up your plugin files\ncd ~/SLURM\nmkdir my_submit\ncd my_submit\nmkdir .libs\nmkdir .deps\n# Create your plugin code\nvi job_submit_mine.c\n\n# Copy up a dependency file\ncp ~/SLURM/slurm.build/src/plugins/job_submit/partition/.deps/job_submit_partition.Plo \\\n   .deps/job_submit_mine.Plo\n\n# Compile\ngcc -DHAVE_CONFIG_H -I~/SLURM/slurm.build -I~/slurm.git \\\n   -g -O2 -pthread -fno-gcse -Werror -Wall -g -O0       \\\n   -fno-strict-aliasing -MT job_submit_mine.lo          \\\n   -MD -MP -MF .deps/job_submit_mine.Tpo                \\\n   -c job_submit_mine.c -o .libs/job_submit_mine.o\n\n# Some clean up\nmv -f .deps/job_submit_mine.Tpo .deps/job_submit_mine.Plo\nrm -fr .libs/job_submit_mine.a .libs/job_submit_mine.la \\\n   .libs/job_submit_mine.lai job_submit_mine.so\n\n# Link\ngcc -shared -fPIC -DPIC .libs/job_submit_mine.o -O2         \\\n   -pthread -O0 -pthread -Wl,-soname -Wl,job_submit_mine.so \\\n   -o job_submit_mine.so\n\n# Install\ncp job_submit_mine.so file \\\n   /usr/local/lib/slurm/job_submit_mine.so\nLast modified 04 October 2023"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/intel_knl.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Intel Knights Landing (KNL) User and Administrator Guide",
                "content": "OverviewThis document describes the unique features of Slurm on the computers with\nthe Intel Knights Landing processor.\nYou should be familiar with the Slurm's mode of operation on Linux clusters\nbefore studying the relatively few differences in Intel KNL system operation\ndescribed in this document.User Tools\n\nThe desired NUMA and MCDRAM modes for a KNL processor should be specified\nusing the -C or --constraint option of Slurm's job submission commands: salloc,\nsbatch, and srun. Currently available NUMA and MCDRAM modes are shown in the\ntable below. Each node's available and current NUMA and MCDRAM modes are\nvisible in the \"available features\" and \"active features\" fields respectively,\nwhich may be seen using the scontrol, sinfo, or sview commands.\nNote that a node may need to be rebooted to get the desired NUMA and MCDRAM\nmodes and nodes may only be rebooted when they contain no running jobs\n(i.e. sufficient resources may be available to run a pending job, but until\nthe node is idle and can be rebooted, the pending job may not be allocated\nresources). Also note that the job will be charged for resources from the time\nof resource allocation, which may include time to reboot a node into the\ndesired NUMA and MCDRAM configuration.Slurm supports a very rich set of options for the node constraint options\n(exclusive OR, node counts for each constraint, etc.).\nSee the man pages for the salloc, sbatch and srun commands for more information\nabout the constraint syntax.\nJobs may specify their desired NUMA and/or MCDRAM configuration. If no\nNUMA and/or MCDRAM configuration is specified, then a node with any possible\nvalue for that configuration will be used.\n\nType\nName\nDescription\n\nMCDRAMcacheAll of MCDRAM to be used as cache\nMCDRAMequalMCDRAM to be used partly as cache and partly combined with primary memory\nMCDRAMflatMCDRAM to be combined with primary memory into a \"flat\" memory space\nNUMAa2aAll to all\nNUMAhemiHemisphere\nNUMAsnc2Sub-NUMA cluster 2\nNUMAsnc4Sub-NUMA cluster 4 (NOTE)\nNUMAquadQuadrant (NOTE)\nJobs requiring some or all of the KNL high bandwidth memory (HBM) should\nexplicitly request that memory using Slurm's Generic RESource (GRES) options.\nThe HBM will always be known by Slurm GRES name of \"hbm\".\nExamples below demonstrate use of HBM.Sorting of the free cache pages at step startup using Intel's zonesort\nmodule can be configured as the default for all steps using the\n\"LaunchParameters=mem_sort\" option in the slurm.conf file.\nIndividual job steps can enable or disable sorting using the \"--mem-bind=sort\"\nor \"--mem-bind=nosort\" command line options for srun.\nSorting will be performed only for the NUMA nodes allocated to the job step.NOTE: Slurm provides limited support\nfor restricting use of HBM. At some point in the future, the amount of HBM\nrequested by the job will be compared with the total amount of HBM and number of\nmemory-containing NUMA nodes available on the KNL processor. The job will then\nbe bound to specific NUMA nodes in order to limit the total amount of HBM\navailable to the job, and thus reserve the remaining HBM for other jobs running\non that KNL processor.NOTE: Slurm can only\nsupport homogeneous nodes (e.g. the same number of cores per NUMA node).\nKNL processors with 68 cores (a subset of KNL models) will not have\nhomogeneous NUMA nodes in snc4 mode, but each NUMA node will have\neither 16 or 18 cores. This will result in Slurm using the lower core count,\nfinding a total of 256 threads rather than 272 threads and setting the node\nto a DOWN state.AccountingIf a node requires rebooting for a job's required configuration, the job\nwill be charged for the resource allocation from the time of allocation through\nthe lifetime of the job, including the time consumed for booting the nodes.\nThe job's time limit will be calculated from the time that all nodes are ready\nfor use.\nFor example, a job with a 10 minute time limit may be allocated resources at\n10:00:00.\nIf the nodes require rebooting, they might not be available for use until\n10:20:00, 20 minutes after allocation, and the job will begin execution at\nthat time.\nThe job must complete no later than 10:30:00 in order to satisfy its time limit\n(10 minutes after execution actually begins).\nHowever, the job will be charged for 30 minutes of resource use, which includes\nthe boot time.Sample Use Cases\n\n\n$ sbatch -C flat,a2a -N2 --gres=hbm:8g --exclusive my.script\n$ srun --constraint=hemi,cache -n36 a.out\n$ srun --constraint=flat --gres=hbm:2g -n36 a.out\n\n$ sinfo -o \"%30N %20b %f\"\nNODELIST       ACTIVE_FEATURES  AVAIL_FEATURES\nnid000[10-11]\nnid000[12-35]  flat,a2a         flat,a2a,snc2,hemi\nnid000[36-43]  cache,a2a        flat,equal,cache,a2a,hemi\nNetwork Topology\n\nSlurm will optimize performance using those resources available without\nrebooting. If node rebooting is required, then it will optimize layout with\nrespect to network bandwidth using both nodes currently in the desired\nconfiguration and those which can be made available after rebooting.\nThis can result in more nodes being rebooted than strictly needed, but will\nimprove application performance.Users can specify they want all resources allocated on a specific count of\nleaf switches (Dragonfly group) using Slurm's --switches option.\nThey can also specify how much additional time they are willing to wait for\nsuch a configuration. If the desired configuration can not be made available\nwithin the specified time interval, the job will be allocated nodes optimized\nwith respect to network bandwidth to the extent possible. On a Dragonfly\nnetwork, this means allocating resources over either single group or\ndistributed evenly over as many groups as possible. For example:\nsrun --switches=1@10:00 N16 a.out\nNote that system administrators can disable use of the --switches\noption or limit the amount of time the job can be deferred using the\nSchedulerParameters max-switch-wait option.Booting Problems\n\nIf node boots fail, those nodes are drained and the job is requeued so that\nit can be allocated a different set of nodes. The nodes originally allocated\nto the job will remain available to the job, so likely a small number of\nadditional nodes will be required.System Administration\n\nFour important components are required to use Slurm on an Intel KNL system.\nSlurm needs a mechanism to determine the node's current topology (e.g.\nhow many NUMA exist and which cores are associated with each NUMA). Slurm\nrelies upon \nPortable Hardware Locality (HWLOC) for this functionality. Please install\nHWLOC before building Slurm.\nThe node features plugin manages the available and active features\ninformation available for each KNL node.\nA configuration file is used to define various timeouts, default\nconfiguration, etc. The configuration file name and contents will depend upon\nthe node features plugins used. See the knl.conf\nman page for more information.\nA mechanism is required to boot nodes in the desired configuration. This\nmechanism must be integrated with existing Slurm infrastructure for\nrebooting nodes on user request (--reboot).\nIn addition, there is a DebugFlags option of \"NodeFeatures\" which will\ngenerate detailed information about KNL operations.The KNL-specific available and active features are configured differently\nbased upon the plugin configured.\nFor the knl_generic plugin, KNL-specific features should be defined\nin the \"slurm.conf\" configuration file. When the slurmd daemon starts on each\ncompute node, it will update the available and active features as needed.\nFeatures which are not KNL-specific (e.g. rack number, \"knl\", etc.) will be\ncopied from the node's \"Features\" configuration in \"slurm.conf\" to both the\navailable and active feature fields and not modified by the NodeFeatures\nplugin.NOTE: For Dell KNL systems you must also include the\nSystemType=Dell option for successful operation and will likely need to\nincrease the SyscfgTimeout to allow enough time for the command to\nsuccessfully complete.  Experience at one site has shown that a 10 second\ntimeout may be necessary, configured as SyscfgTimeout=10000.Slurm does not support the concept of multiple NUMA nodes\nwithin a single socket. If a KNL node is booted with multiple NUMA, then each\nNUMA node will appear in Slurm as a separate socket.\nIn the slurm.conf configuration file, set node socket and\ncore counts to values which are appropriate for some NUMA mode to be used on the\nnode. When the node boots and the slurmd daemon on the node starts, it will\nreport to the slurmctld daemon the node's actual socket (NUMA) and core counts,\nwhich will update Slurm data structures for the node to the values which are\ncurrently configured.\nNote that Slurm currently does not support the concept of\ndiffering numbers of cores in each socket (or NUMA node). We are currently\nworking to address these issues.Mode of Operation\n\n\nThe node's configured \"Features\" are copied to the available and active\nfeature fields.\nThe node features plugin determines the node's current MCDRAM and NUMA\nvalues as well as those which are available and adds those values to the node's\nactive and available feature fields respectively. Note that these values may\nnot be available until the node has booted and the slurmd daemon on the\ncompute node sends that information to the slurmctld daemon.\nJobs will be allocated nodes already in the requested MCDRAM and NUMA mode\nif possible. If insufficient resources are available with the requested\nconfiguration then other nodes will be selected and booted into the desired\nconfiguration once no other jobs are active on the node. Until a node is idle,\nits configuration can not be changed. Note that node reboot time is roughly\non the order of 20 minutes.\nGeneric Cluster Configuration\n\nAll other clusters should have NodeFeaturesPlugins configured to \"knl_generic\".\nThis plugin performs all operations directly on the compute nodes using Intel's\nsyscfg program to get and modify the node's MCDRAM and NUMA mode and\nuses the Linux reboot program to reboot the compute node in order for\nmodifications in MCDRAM and/or NUMA mode to take effect.\nMake sure that RebootProgram is defined in the slurm.conf file.\nThis plugin currently does not permit the specification of ResumeProgram,\nSuspendProgram, SuspendTime, etc. in slurm.conf, however that limitation may\nbe removed in the future (the ResumeProgram currently has no means of changing\nthe node's MCDRAM and/or NUMA mode prior to reboot).NOTE: The syscfg program reports the MCDRAM and NUMA mode to be used\nwhen the node is next booted. If the syscfg program is used to modify the MCDRAM\nor NUMA mode of a node, but it is not rebooted, then Slurm will be making\nscheduling decisions based upon incorrect state information. If you want to\nchange node state information outside of Slurm then use the following procedure:\n\nDrain the nodes of interest\nChange their MCDRAM and/or NUMA mode\nReboot the nodes, then\nRestore them to service in Slurm\n\nSample knl_generic.conf File\n# Sample knl_generic.conf\nSyscfgPath=/usr/bin/syscfg\nDefaultNUMA=a2a         # NUMA=all2all\nAllowNUMA=a2a,snc2,hemi\nDefaultMCDRAM=cache     # MCDRAM=cache\nSample slurm.conf File\n# Sample slurm.conf\nNodeFeaturesPlugins=knl_generic\nDebugFlags=NodeFeatures\nGresTypes=hbm\nRebootProgram=/sbin/reboot\n...\nNodename=default Sockets=1 CoresPerSocket=68 ThreadsPerCore=4 RealMemory=128000 Feature=knl\nNodeName=nid[00000-00127] State=UNKNOWN\nLast modified 13 March 2024"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/cons_tres.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Consumable Resources in Slurm",
                "content": "Slurm, using the default node allocation plug-in, allocates nodes to jobs in\nexclusive mode.  This means that even when all the resources within a node are\nnot utilized by a given job, another job will not have access to these resources.\nNodes possess resources such as processors, memory, swap, local\ndisk, etc. and jobs consume these resources. The exclusive use default policy\nin Slurm can result in inefficient utilization of the cluster and of its nodes\nresources.\nSlurm's cons_tres plugin is available to\nmanage resources on a much more fine-grained basis as described below.\nUsing the Consumable Trackable Resource Plugin: select/cons_tres\n\nThe Consumable Trackable Resources (cons_tres) plugin has been built\nto work with several resources. It can track a Board, Socket, Core, CPU, Memory\nas well as any combination of the logical processors with Memory:\nCPU (CR_CPU): CPU as a consumable resource.\n\nNo notion of sockets, cores, or threads.\nOn a multi-core system CPUs will be cores.\nOn a multi-core/hyperthread system CPUs will be threads.\nOn a single-core system CPUs are CPUs.\n\nBoard (CR_Board): Baseboard as a consumable resource.\nSocket (CR_Socket): Socket as a consumable resource.\nCore (CR_Core): Core as a consumable resource.\nMemory (CR_Memory) Memory only as a\n  consumable resource. NOTE: CR_Memory assumes OverSubscribe=Yes\nSocket and Memory (CR_Socket_Memory): Socket\n  and Memory as consumable resources.\nCore and Memory (CR_Core_Memory): Core and\n  Memory as consumable resources.\nCPU and Memory (CR_CPU_Memory) CPU and Memory\n  as consumable resources.\nAll CR_* parameters assume OverSubscribe=No or\nOverSubscribe=Force EXCEPT for CR_MEMORY which assumes\nOverSubscribe=Yes.The cons_tres plugin also provides functionality specifically\nrelated to GPUs.Additional parameters available for the cons_tres plugin:\nDefCpuPerGPU: Default number of CPUs allocated per GPU.\nDefMemPerGPU: Default amount of memory allocated per GPU.\nAdditional job submit options available for the cons_tres plugin:\n--cpus-per-gpu=: Number of CPUs for every GPU.\n--gpus=: Count of GPUs for entire job allocation.\n--gpu-bind=: Bind task to specific GPU(s).\n--gpu-freq=: Request specific GPU/memory frequencies.\n--gpus-per-node=: Number of GPUs per node.\n--gpus-per-socket=: Number of GPUs per socket.\n--gpus-per-task=: Number of GPUs per task.\n--mem-per-gpu=: Amount of memory for each GPU.\nsrun's -B extension for sockets, cores, and threads is\nignored within the node allocation mechanism when CR_CPU or\nCR_CPU_MEMORY is selected. It is used to compute the total\nnumber of tasks when -n is not specified.In the cases where Memory is a consumable resource, the RealMemory\nparameter must be set in the slurm.conf to define a node's amount of real\nmemory.The job submission commands (salloc, sbatch and srun) support the options\n--mem=MB and --mem-per-cpu=MB, permitting users to specify\nthe maximum amount of real memory required per node or per allocated CPU.\nThis option is required in the environments where Memory is a consumable\nresource. It is important to specify enough memory since Slurm will not allow\nthe application to use more than the requested amount of real memory. The\ndefault value for --mem is inherited from DefMemPerNode. See\nsrun(1) for more details.Using --overcommit or -O is allowed. When the process to\nlogical processor pinning is enabled by using an appropriate TaskPlugin\nconfiguration parameter, the extra processes will time share the allocated\nresources.The Consumable Trackable Resource plugin is enabled via the SelectType\nparameter in the slurm.conf.\n# Excerpt from sample slurm.conf file\nSelectType=select/cons_tres\nGeneral CommentsSlurm's default select/linear plugin is using a best fit algorithm\nbased on number of consecutive nodes.The select/cons_tres plugin is enabled or disabled cluster-wide.In the case where select/linear is enabled, the normal Slurm\nbehaviors are not disrupted. The major change users see when using the\nselect/cons_tres plugin is that jobs can be\nco-scheduled on nodes when resources permit it. Generic resources (such as GPUs)\ncan also be tracked individually with this plugin.\nThe rest of Slurm, such as srun and its options (except srun -s ...), etc. are not\naffected by this plugin. Slurm is, from the user's point of view, working the\nsame way as when using the default node selection scheme.The --exclusive srun option allows users to request nodes in\nexclusive mode even when consumable resources is enabled. See\nsrun(1) for details. srun's -s or --oversubscribe is incompatible with the consumable\nresource environment and will therefore not be honored. Since this\nenvironment's nodes are shared by default, --exclusive allows users to\nobtain dedicated nodes.The --oversubscribe and --exclusive options are mutually\nexclusive when used at job submission. If both options are set when submitting\na job, the job submission command used will fatal.Examples of CR_Memory, CR_Socket_Memory, and CR_CPU_Memory\ntype consumable resources\n\n\n# sinfo -lNe\nNODELIST     NODES PARTITION  STATE  CPUS  S:C:T MEMORY\nhydra[12-16]     5 allNodes*  ...       4  2:2:1   2007\nUsing select/cons_tres plug-in with CR_Memory\nExample:\n# srun -N 5 -n 20 --mem=1000 sleep 100 &  <-- running\n# srun -N 5 -n 20 --mem=10 sleep 100 &    <-- running\n# srun -N 5 -n 10 --mem=1000 sleep 100 &  <-- queued and waiting for resources\n\n# squeue\nJOBID PARTITION   NAME   USER ST  TIME  NODES NODELIST(REASON)\n 1820  allNodes  sleep sballe PD  0:00      5 (Resources)\n 1818  allNodes  sleep sballe  R  0:17      5 hydra[12-16]\n 1819  allNodes  sleep sballe  R  0:11      5 hydra[12-16]\nUsing select/cons_tres plug-in with CR_Socket_Memory (2 sockets/node)\nExample 1:\n# srun -N 5 -n 5 --mem=1000 sleep 100 &        <-- running\n# srun -n 1 -w hydra12 --mem=2000 sleep 100 &  <-- queued and waiting for resources\n\n# squeue\nJOBID PARTITION   NAME   USER ST  TIME  NODES NODELIST(REASON)\n 1890  allNodes  sleep sballe PD  0:00      1 (Resources)\n 1889  allNodes  sleep sballe  R  0:08      5 hydra[12-16]\n\nExample 2:\n# srun -N 5 -n 10 --mem=10 sleep 100 & <-- running\n# srun -n 1 --mem=10 sleep 100 & <-- queued and waiting for resourcessqueue\n\n# squeue\nJOBID PARTITION   NAME   USER ST  TIME  NODES NODELIST(REASON)\n 1831  allNodes  sleep sballe PD  0:00      1 (Resources)\n 1830  allNodes  sleep sballe  R  0:07      5 hydra[12-16]\nUsing select/cons_tres plug-in with CR_CPU_Memory (4 CPUs/node)\nExample 1:\n# srun -N 5 -n 5 --mem=1000 sleep 100 &  <-- running\n# srun -N 5 -n 5 --mem=10 sleep 100 &    <-- running\n# srun -N 5 -n 5 --mem=1000 sleep 100 &  <-- queued and waiting for resources\n\n# squeue\nJOBID PARTITION   NAME   USER ST  TIME  NODES NODELIST(REASON)\n 1835  allNodes  sleep sballe PD  0:00      5 (Resources)\n 1833  allNodes  sleep sballe  R  0:10      5 hydra[12-16]\n 1834  allNodes  sleep sballe  R  0:07      5 hydra[12-16]\n\nExample 2:\n# srun -N 5 -n 20 --mem=10 sleep 100 & <-- running\n# srun -n 1 --mem=10 sleep 100 &       <-- queued and waiting for resources\n\n# squeue\nJOBID PARTITION   NAME   USER ST  TIME  NODES NODELIST(REASON)\n 1837  allNodes  sleep sballe PD  0:00      1 (Resources)\n 1836  allNodes  sleep sballe  R  0:11      5 hydra[12-16]\n\nExample of Node Allocations Using Consumable Resource Plugin\n\nThe following example illustrates the different ways four jobs\nare allocated across a cluster using (1) Slurm's default allocation method\n(exclusive mode) and (2) a processor as consumable resource\napproach.It is important to understand that the example listed below is a\ncontrived example and is only given here to illustrate the use of CPUs as\nconsumable resources. Job 2 and Job 3 call for the node count to equal\nthe processor count. This would typically be done because\nthat one task per node requires all of the memory, disk space, etc. The\nbottleneck would not be processor count.Trying to execute more than one job per node will almost certainly severely\nimpact a parallel job's performance.\nThe biggest beneficiary of CPUs as consumable resources will be serial jobs or\njobs with modest parallelism, which can effectively share resources. On many\nsystems with larger processor count, jobs typically run one fewer task than\nthere are processors to minimize interference by the kernel and daemons.The example cluster is composed of 4 nodes (10 CPUs in total):\nlinux01 (with 2 processors), \nlinux02 (with 2 processors), \nlinux03 (with 2 processors), and\nlinux04 (with 4 processors). \nThe four jobs are the following:\n[2] srun -n 4 -N 4 sleep 120 &\n[3] srun -n 3 -N 3 sleep 120 &\n[4] srun -n 1 sleep 120 &\n[5] srun -n 3 sleep 120 &\nThe user launches them in the same order as listed above.Using Slurm's Default Node Allocation (Non-shared Mode)\n\nThe four jobs have been launched and 3 of the jobs are now\npending, waiting to get resources allocated to them. Only Job 2 is running\nsince it uses one CPU on all 4 nodes. This means that linux01 to linux03 each\nhave one idle CPU and linux04 has 3 idle CPUs.\n# squeue\nJOBID PARTITION   NAME  USER  ST  TIME  NODES NODELIST(REASON)\n    3       lsf  sleep  root  PD  0:00      3 (Resources)\n    4       lsf  sleep  root  PD  0:00      1 (Resources)\n    5       lsf  sleep  root  PD  0:00      1 (Resources)\n    2       lsf  sleep  root   R  0:14      4 linux[01-04]\nOnce Job 2 is finished, Job 3 is scheduled and runs on\nlinux01, linux02, and linux03. Job 3 is only using one CPU on each of the 3\nnodes. Job 4 can be allocated onto the remaining idle node (linux04) so Job 3\nand Job 4 can run concurrently on the cluster.Job 5 has to wait for idle nodes to be able to run.\n# squeue\nJOBID PARTITION   NAME  USER  ST  TIME  NODES NODELIST(REASON)\n    5       lsf  sleep  root  PD  0:00      1 (Resources)\n    3       lsf  sleep  root   R  0:11      3 linux[01-03]\n    4       lsf  sleep  root   R  0:11      1 linux04\nOnce Job 3 finishes, Job 5 is allocated resources and can run.The advantage of the exclusive mode scheduling policy is\nthat the a job gets all the resources of the assigned nodes for optimal\nparallel performance. The drawback is\nthat jobs can tie up large amount of resources that it does not use and which\ncannot be shared with other jobs.Using a Processor Consumable Resource Approach\n\nWe will run through the same scenario again using the cons_tres\nplugin and CPUs as the consumable resource. The output of squeue shows that we\nhave 3 out of the 4 jobs allocated and running. This is a 2 running job\nincrease over the default Slurm approach. Job 2 is running on nodes linux01\nto linux04. Job 2's allocation is the same as for Slurm's default allocation\nwhich is that it uses one CPU on each of the 4 nodes. Once Job 2 is scheduled\nand running, nodes linux01, linux02 and linux03 still have one idle CPU each\nand node linux04 has 3 idle CPUs. The main difference between this approach and\nthe exclusive mode approach described above is that idle CPUs within a node\nare now allowed to be assigned to other jobs.It is important to note that\nassigned doesn't mean oversubscription. The consumable resource approach\ntracks how much of each available resource (in our case CPUs) must be dedicated\nto a given job. This allows us to prevent per node oversubscription of\nresources (CPUs).Once Job 2 is running, Job 3 is\nscheduled onto node linux01, linux02, and Linux03 (using one CPU on each of the\nnodes) and Job 4 is scheduled onto one of the remaining idle CPUs on Linux04.Job 2, Job 3, and Job 4 are now running concurrently on the cluster.\n# squeue\nJOBID PARTITION   NAME  USER  ST  TIME  NODES NODELIST(REASON)\n    5       lsf  sleep  root  PD  0:00      1 (Resources)\n    2       lsf  sleep  root   R  0:13      4 linux[01-04]\n    3       lsf  sleep  root   R  0:09      3 linux[01-03]\n    4       lsf  sleep  root   R  0:05      1 linux04\n\n# sinfo -lNe\nNODELIST     NODES PARTITION       STATE CPUS MEMORY TMP_DISK WEIGHT FEATURES REASON\nlinux[01-03]     3      lsf*   allocated    2   2981        1      1   (null) none\nlinux04          1      lsf*   allocated    4   3813        1      1   (null) none\nOnce Job 2 finishes, Job 5, which was pending, is allocated available resources and is then\nrunning as illustrated below:\n# squeue\nJOBID PARTITION   NAME  USER  ST  TIME  NODES NODELIST(REASON)\n   3       lsf   sleep  root   R  1:58      3 linux[01-03]\n   4       lsf   sleep  root   R  1:54      1 linux04\n   5       lsf   sleep  root   R  0:02      3 linux[01-03]\n# sinfo -lNe\nNODELIST     NODES PARTITION       STATE CPUS MEMORY TMP_DISK WEIGHT FEATURES REASON\nlinux[01-03]     3      lsf*   allocated    2   2981        1      1   (null) none\nlinux04          1      lsf*        idle    4   3813        1      1   (null) none\nJob 3, Job 4, and Job 5 are now running concurrently on the cluster.\n# squeue\nJOBID PARTITION   NAME  USER  ST  TIME  NODES NODELIST(REASON)\n    5       lsf  sleep  root   R  1:52      3 linux[01-03]\nJob 3 and Job 4 have finished and Job 5 is still running on nodes linux[01-03].The advantage of the consumable resource scheduling policy\nis that the job throughput can increase dramatically. The overall job\nthroughput and productivity of the cluster increases, thereby reducing the\namount of time users have to wait for their job to complete as well as\nincreasing the overall efficiency of the use of the cluster. The drawback is\nthat users do not have entire nodes dedicated to their jobs by default.We have added the --exclusive option to srun (see\nsrun(1) for more details),\nwhich allows users to specify that they would like\ntheir nodes to be allocated in exclusive mode.\nThis is to accommodate users who might have mpi/threaded/openMP\nprograms that will take advantage of all the CPUs on a node but only need\none mpi process per node.Last modified 18 May 2023"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/rest.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "REST API Reference",
                "content": "Slurm provides a\n\t\n\t\tREST API\n\t daemon named slurmrestd. This daemon is designed to allow clients to\n\tcommunicate with Slurm via a REST API (in addition to the command line\n\tinterface (CLI) or C API).\nSee also:\n\nREST API Quick Start Guide\n\nCommon Issues\n\nREST API Implementation Details\nOpenAPI Plugin Release Notes\nREST API Client Guide\n\nStatelessSlurmrestd is stateless as it does not cache or save any state between\nrequests. Each request is handled in a thread and then all of that state is\ndiscarded. Any request to slurmrestd is completely synchronous with the\nSlurm controller (slurmctld or slurmdbd) and is only considered complete once\nthe HTTP response code has been sent to the client. Slurmrestd will hold a\nclient connection open while processing a request. Slurm database commands are\ncommitted at the end of every request, on the success of all API calls in the\nrequest.Sites are strongly encouraged to setup a caching proxy between slurmrestd\nand clients to avoid having clients repeatedly call queries, causing usage to\nbe higher than needed (and causing lock contention) on the controller.Run modesSlurmrestd currently supports two run modes: inet service mode and listening\nmode.Inet Service ModeThe Slurmrestd daemon acts as an\n\n\tInet Service\n treating STDIN and STDOUT as the client. This mode allows clients to use\ninetd, xinetd, or systemd socket activated services and avoid the need to run a\ndaemon on the host at all times. This mode creates an instance for each client\nand does not support reusing the same instance for different clients.Listening ModeThe Slurmrestd daemon acts as a full UNIX service and continuously listens\nfor new TCP connections. Each connection and request are independently\nauthenticated.Configurationdoc/man/man1/slurmrestd.8REST API Quick Start GuidePluginsAs of Slurm 20.11, the REST API uses plugins for authentication and\ngenerating content. As of Slurm-21.08, the OpenAPI plugins are available\noutside of slurmrestd daemon and other slurm commands may provide or accept the\nlatest version of the OpenAPI formatted output. This functionality is provided\non a per command basis. These plugins can be optionally listed or selected via\ncommand line arguments as described in the \nslurmrestd documentation.Plugin life cycle\n\nPlugins provided upstream with Slurm in any release are considered supported\nby that release. In general, only the latest versioned plugins will receive\nminor bug fixes but all will receive security fixes. Due to the nature of the\nplugins, one plugin can be supported across multiple Slurm releases to ensure\n(limited) backward client compatibility. SchedMD is currently only explicitly\ndeprecating plugins per the\n\nOpenAPI standard method of setting \"Deprecated: True\"\nin the method path. Any plugin that is not marked as deprecated will\ncontinue to exist in the next Slurm Major release pending any critical issues\nwhich will be announced separately. Any plugin marked for deprecation will be\nremoved on the next Slurm major release. In general, the last three plugins\nhave been retained in each Slurm major release with the oldest getting marked\nas deprecated.\nSites are always encouraged to use the latest stable plugin version\navailable for code development. There is no guarantee of compatibility\nbetween different versions of the same plugin with clients. Clients should\nalways make sure to validate their code when migrating to newer versions of\nplugins. Plugin versions will always be included in the path for every method\nprovided by a given plugin to ensure no two plugins will provide the same\npath.As the plugins utilize the Slurm API internally, plugins that existed in\nprevious versions of Slurm should continue to be able to communicate with the\ntwo previous versions of Slurm, similar to other components of Slurm. Newer\nplugins may have required RPC changes which will exclude them from working with\nprevious Slurm versions. For instance, the openapi/dbv0.0.36 plugin will not be\nable to communicate with any slurmdbd servers prior to the slurm-20.11\nrelease.As with all other plugins in Slurm, sites are more than welcome to write\ntheir own plugins and are suggested to submit them as code contributions via\nbugzilla, tagged as a contribution.\nThe plugin API provided may change between releases which could cause site\nspecific plugins to break.SecurityThe Slurm REST API is written to provide the necessary functionality for\nclients to control Slurm using REST commands. It is not designed to be\ndirectly internet facing. Only unencrypted and uncompressed HTTP communications\nare supported. Slurmrestd also has no protection against man in the middle or\nreplay attacks. Slurmrestd should only be placed in a trusted network that will\ncommunicate with a trusted client.Any site wishing to expose Slurm REST API to the internet or outside of the\ncluster should at the very least use a proxy to wrap all communications with\nTLS v1.3 (or later). You should also add monitoring to reject any client who\nrepeatedly attempts invalid logins at either the network perimeter firewall or\nat the TLS proxy. Any client filtering that can be done via a proxy is\nsuggested to avoid common internet crawlers from talking to slurmrestd and\nwasting system resource or even causing higher latency for valid clients.\nSites are recommended to use shorter lived JWT tokens for clients and renew\noften, possibly via non-Slurm JWT generator to avoid having to enforce JWT\nlifespan limits. It is also suggested that sites use an authenticating proxy\nto handle all client authentication against the sites preferred Single Sign\nOn (SSO) provider instead of Slurm scontrol generated tokens. This will\nprevent any unauthenticated client from connecting to slurmrestd.The Slurm REST API is an HTTP server and all general possible precautions\nfor security of any web server should be applied. As these precautions are site\nspecific, it is highly recommended that you work with your site's security\ngroup to ensure all policies are enforced at the proxy before connecting to\nslurmrestd.Slurm tries not to give potential attackers any hints when there are\nauthentication failures. This results in the client getting this rather terse\nmessage: Authentication failure. When this happens, take a look at\nthe logs for the relevant Slurm daemon (i.e. slurmdbd, slurmctld\nor slurmd) for information about the actual issue.JSON Web Token (JWT) Authentication\n\nslurmrestd supports using JWT to authenticate users.\nJWT can be used to authenticate user over REST protocol.\n\nUser Name Header: X-SLURM-USER-NAME\nJWT Header: X-SLURM-USER-TOKEN\n\nSlurmUser or root can provide alternative user names to act as a proxy for the\ngiven user. While using JWT authentication, slurmrestd should be run as a\nunique, unprivileged user and group. Slurmrestd should be provided an\ninvalid SLURM_JWT environment variable at startup to activate JWT authentication.\nThis will allow users to provide their own JWT tokens while authenticating to\nthe proxy and ensuring against any possible accidental authorizations.When using JWT, it is important that AuthAltTypes=auth/jwt be\nconfigured in your slurm.conf for slurmrestd.Local Authentication\n\nslurmrestd supports using UNIX domain sockets to have the kernel\nauthenticate local users. By default, slurmrestd will not start as root or\nSlurmUser or if the user's primary group belongs to root or SlurmUser.\nSlurmrestd must be located in the Munge security domain in order to function\nand communicate with Slurm in local authentication mode.\nAuthenticating Proxy\n\nThere is a wide array of authentication systems that a site could choose\nfrom, if using JWT authentication doesn't meet your\nrequirements. An authenticating proxy is setup with a JWT token assigned to\nthe SlurmUser that can then be used to proxy for any user on the cluster.\nThis ability is only allowed for SlurmUser and the root users, all other\ntokens will only work with their locally assigned users.If using a third-party authenticating proxy, it is expected that it will\nprovide the correct HTTP headers (X-SLURM-USER-NAME and\nX-SLURM-USER-TOKEN) to slurmrestd along with the user's request.Slurm places no requirements on the authenticating proxy beyond its being\nHTTP 1.1 compliant and that it provides the correct HTTP headers to allow\nclient authentication. Slurm will explicitly trust the HTTP headers provided\nand has no way to verify them (beyond the proxy's trusted token\nX-SLURM-USER-TOKEN). Any authenticating proxy will need to follow\nyour site's security policies and ensure that the proxied requests come from\nthe correct user. These requirements are standard to any authenticated\nproxy and are not Slurm specific.A working trivial example can be found in an \ninternal tool used for testing and training. It uses\nPHP and\nNGINX to provide the authentication logic.\nLast modified 16 March 2023"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/elasticsearch.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Elasticsearch Guide",
                "content": "Slurm provides multiple Job Completion Plugins.\nThese plugins are an orthogonal way to provide historical job\naccounting data for finished jobs.In most installations, Slurm is already configured with an\nAccountingStorageType\nplugin \u2014 usually slurmdbd. In these situations, the information\ncaptured by a completion plugin is intentionally redundant.The jobcomp/elasticsearch plugin can be used together with a web\nlayer on top of the Elasticsearch server \u2014 such as\nKibana \u2014 to\nvisualize your finished jobs and the state of your cluster. Some of these\nvisualization tools also let you easily create different types of dashboards,\ndiagrams, tables, histograms and/or apply customized filters when searching.\nPrerequisitesThe plugin requires additional libraries for compilation:\nlibcurl development files\nJSON-C\nConfigurationThe Elasticsearch instance should be running and reachable from the multiple\nSlurmctldHost configured.\nRefer to the Elasticsearch\nOfficial Documentation for further details on setup and configuration.\n\nThere are three slurm.conf options related to\nthis plugin:\n\n\nJobCompType\nis used to select the job completion plugin type to activate. It should be set\nto jobcomp/elasticsearch.\nJobCompType=jobcomp/elasticsearch\n\n\nJobCompLoc should be set to\nthe Elasticsearch server URL endpoint (including the port number and the target\nindex).\nJobCompLoc=<host>:<port>/<target>/_doc\nNOTE: Since Elasticsearch 8.0 the APIs that accept types are removed,\nthereby moving to a typeless mode. The Slurm elasticsearch plugin in versions\nprior to 20.11 removed any trailing slashes from this option URL and appended\na hardcoded /slurm/jobcomp suffix representing the /index/type\nrespectively.\nStarting from Slurm 20.11 the URL is fully configurable and handed as-is without\nmodification to the libcurl library functions. In addition, this also allows\nusers to index data from different clusters to the same server but to different\nindices.\nNOTE: The Elasticsearch official documentation provides detailed\ninformation around these concepts, the type to typeless deprecation transition\nas well as reindex API references on how to copy data from one index to another\nif needed.\n\n\n\nDebugFlags could include\nthe Elasticsearch flag for extra debugging purposes.\nDebugFlags=Elasticsearch\nIt is a good idea to turn this on initially until you have verified that\nfinished jobs are properly indexed. Note that you do not need to manually\ncreate the Elasticsearch index, since the plugin will automatically\ndo so when trying to index the first job document.\n\n\nVisualization\n\n\nOnce jobs are being indexed, it is a good idea to use a web visualization\nlayer to analyze the data.\nKibana is a\nrecommended open-source data visualization plugin for Elasticsearch.\nOnce installed, an Elasticsearch index name or pattern has to be\nconfigured to instruct Kibana to retrieve the data. Once data is loaded it is\npossible to create tables where each row is a finished job, ordered by\nany column you choose \u2014 the @end_time timestamp is suggested \u2014 and\nany dashboards, graphs, or other analysis of interest.\n\nTesting and Debugging\n\n\nFor debugging purposes, you can use the curl command or any similar\ntool to perform REST requests against Elasticsearch directly. Some of the\nfollowing examples using the curl tool may be useful.\nQuery information assuming a slurm index name, including the\ndocument count (which should be one per job indexed):\n\n$ curl -XGET http://localhost:9200/_cat/indices/slurm?v\nhealth status index uuid                   pri rep docs.count docs.deleted store.size pri.store.size\nyellow open   slurm 103CW7GqQICiMQiSQv6M_g   5   1          9            0    142.8kb        142.8kb\n\nQuery all indexed jobs in the slurm index:\n\n$ curl -XGET 'http://localhost:9200/slurm/_search?pretty=true&q=*:*' | less\n\nDelete the slurm index (caution!):\n\n$ curl -XDELETE http://localhost:9200/slurm\n{\"acknowledged\":true}\n\nQuery information about _cat options. More can be found in the\nofficial documentation.\n\n$ curl -XGET http://localhost:9200/_cat\n\nFailure management\n\n\nWhen the primary slurmctld is shut down, information about all completed but\nnot yet indexed jobs held within the Elasticsearch plugin saved to a\nfile named elasticsearch_state, which is located in the\nStateSaveLocation. This\npermits the plugin to restore the information when the slurmctld is restarted,\nand will be sent to the Elasticsearch database when the connection is\nrestored.\nAcknowledgments\nThe Elasticsearch plugin was created as part of Alejandro Sanchez's\nMaster's Thesis.\nLast modified 6 August 2021\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Visualization\n\n",
                "content": "Once jobs are being indexed, it is a good idea to use a web visualization\nlayer to analyze the data.\nKibana is a\nrecommended open-source data visualization plugin for Elasticsearch.\nOnce installed, an Elasticsearch index name or pattern has to be\nconfigured to instruct Kibana to retrieve the data. Once data is loaded it is\npossible to create tables where each row is a finished job, ordered by\nany column you choose \u2014 the @end_time timestamp is suggested \u2014 and\nany dashboards, graphs, or other analysis of interest.\n\nTesting and Debugging\n\n\nFor debugging purposes, you can use the curl command or any similar\ntool to perform REST requests against Elasticsearch directly. Some of the\nfollowing examples using the curl tool may be useful.\nQuery information assuming a slurm index name, including the\ndocument count (which should be one per job indexed):\n\n$ curl -XGET http://localhost:9200/_cat/indices/slurm?v\nhealth status index uuid                   pri rep docs.count docs.deleted store.size pri.store.size\nyellow open   slurm 103CW7GqQICiMQiSQv6M_g   5   1          9            0    142.8kb        142.8kb\n\nQuery all indexed jobs in the slurm index:\n\n$ curl -XGET 'http://localhost:9200/slurm/_search?pretty=true&q=*:*' | less\n\nDelete the slurm index (caution!):\n\n$ curl -XDELETE http://localhost:9200/slurm\n{\"acknowledged\":true}\n\nQuery information about _cat options. More can be found in the\nofficial documentation.\n\n$ curl -XGET http://localhost:9200/_cat\n\nFailure management\n\n\nWhen the primary slurmctld is shut down, information about all completed but\nnot yet indexed jobs held within the Elasticsearch plugin saved to a\nfile named elasticsearch_state, which is located in the\nStateSaveLocation. This\npermits the plugin to restore the information when the slurmctld is restarted,\nand will be sent to the Elasticsearch database when the connection is\nrestored.AcknowledgmentsThe Elasticsearch plugin was created as part of Alejandro Sanchez's\nMaster's Thesis.Last modified 6 August 2021"
            },
            {
                "title": "Testing and Debugging\n\n",
                "content": "For debugging purposes, you can use the curl command or any similar\ntool to perform REST requests against Elasticsearch directly. Some of the\nfollowing examples using the curl tool may be useful.Query information assuming a slurm index name, including the\ndocument count (which should be one per job indexed):\n$ curl -XGET http://localhost:9200/_cat/indices/slurm?v\nhealth status index uuid                   pri rep docs.count docs.deleted store.size pri.store.size\nyellow open   slurm 103CW7GqQICiMQiSQv6M_g   5   1          9            0    142.8kb        142.8kb\nQuery all indexed jobs in the slurm index:\n$ curl -XGET 'http://localhost:9200/slurm/_search?pretty=true&q=*:*' | less\nDelete the slurm index (caution!):\n$ curl -XDELETE http://localhost:9200/slurm\n{\"acknowledged\":true}\nQuery information about _cat options. More can be found in the\nofficial documentation.\n$ curl -XGET http://localhost:9200/_cat\nFailure management\n\nelasticsearch_stateStateSaveLocation"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/job_reason_codes.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Job Reason Codes",
                "content": "These reason codes can be used to identify why a pending job has not yet been\nstarted by the scheduler. There may be multiple reasons why a job cannot start,\nin which case only the reason that was encountered by the attempted scheduling\nmethod will be displayed. Refer to the \nScheduling Configuration Guide for more details.Common Reasons\n\n\nAssocGrp* \u2014 The job's association has reached an aggregate\nlimit.\nAssocMax* \u2014 A portion of the job request exceeds a maximum\nlimit (e.g., PerJob, PerNode) for the\nrequested association.\nBeginTime \u2014 The job's earliest \nstart time has not yet been reached.\nDependency \u2014 This job has a\ndependency on another job that has not\nbeen satisfied.\nMax*PerAccount \u2014 A portion of the job request exceeds the\nper-Account limit on the job's\nQOS.\nPriority \u2014 One of more higher\npriority jobs exist for the partition\nassociated with the job or for the advanced\nreservation.\nQOSGrp* \u2014 The job's QOS has reached an\naggregate limit.\nQOSMax* \u2014 A portion of the job request exceeds a\nmaximum limit (e.g., PerJob, PerNode) for\nthe requested QOS.\nResources \u2014 The resources requested by the job are not\navailable (e.g., already used by other jobs).\nAll Reasons\n\n\nAccountingPolicy \u2014 Fallback reason when others not matched.\n\nAccountNotAllowed \u2014 Job is in an account not allowed in a\npartition.\nAssocGrpBB \u2014 The job's association has reached its aggregate\nBurst Buffer limit.\nAssocGrpBBMinutes \u2014 The job's association has reached the\nmaximum number of minutes allowed in aggregate for Burst Buffers by past,\npresent and future jobs.\nAssocGrpBBRunMinutes \u2014 The job's association has reached the\nmaximum number of minutes allowed in aggregate for Burst Buffers by\ncurrently running jobs.\nAssocGrpBilling \u2014 The job's association has reached its\naggregate Billing limit.\nAssocGrpBillingMinutes \u2014 The job's association has reached\nthe maximum number of minutes allowed in aggregate for the Billing value of\na resource by past, present and future jobs.\nAssocGrpBillingRunMinutes \u2014 The job's association has reached\nthe maximum number of minutes allowed in aggregate for the Billing value of a\nresource by currently running jobs.\nAssocGrpCpuLimit \u2014 The job's association has reached its\naggregate CPU limit.\nAssocGrpCPUMinutesLimit \u2014 The job's association has reached\nthe maximum number of minutes allowed in aggregate for CPUs by past, present\nand future jobs.\nAssocGrpCPURunMinutesLimit \u2014 The job's association has reached\nthe maximum number of minutes allowed in aggregate for CPUs by currently\nrunning jobs.\nAssocGrpEnergy \u2014 The job's association has reached its\naggregate Energy limit.\nAssocGrpEnergyMinutes \u2014 The job's association has reached the\nmaximum number of minutes allowed in aggregate for Energy by past, present\nand future jobs.\nAssocGrpEnergyRunMinutes \u2014 The job's association has reached\nthe maximum number of minutes allowed in aggregate for Energy by currently\nrunning jobs.\nAssocGrpGRES \u2014 The job's association has reached its aggregate\nGRES limit.\nAssocGrpGRESMinutes \u2014 The job's association has reached the\nmaximum number of minutes allowed in aggregate for a GRES by past, present\nand future jobs.\nAssocGrpGRESRunMinutes \u2014 The job's association has reached the\nmaximum number of minutes allowed in aggregate for a GRES by currently\nrunning jobs.\nAssocGrpJobsLimit \u2014 The job's association has reached the\nmaximum number of allowed jobs in aggregate.\nAssocGrpLicense \u2014 The job's association has reached its\naggregate license limit.\nAssocGrpLicenseMinutes \u2014 The job's association has reached the\nmaximum number of minutes allowed in aggregate for Licenses by past, present\nand future jobs.\nAssocGrpLicenseRunMinutes \u2014 The job's association has reached\nthe maximum number of minutes allowed in aggregate for Licenses by currently\nrunning jobs.\nAssocGrpMemLimit \u2014 The job's association has reached its\naggregate Memory limit.\nAssocGrpMemMinutes \u2014 The job's association has reached the\nmaximum number of minutes allowed in aggregate for Memory by past, present\nand future jobs.\nAssocGrpMemRunMinutes \u2014 The job's association has reached the\nmaximum number of minutes allowed in aggregate for Memory by currently\nrunning jobs.\nAssocGrpNodeLimit \u2014 The job's association has reached its\naggregate Node limit.\nAssocGrpNodeMinutes \u2014 The job's association has reached the\nmaximum number of minutes allowed in aggregate for Nodes by past, present and\nfuture jobs.\nAssocGrpNodeRunMinutes \u2014 The job's association has reached the\nmaximum number of minutes allowed in aggregate for Nodes by currently running\njobs.\nAssocGrpSubmitJobsLimit \u2014 The job's association has reached the\nmaximum number of jobs that can be running or pending in aggregate at a given\ntime.\nAssocGrpUnknown \u2014 The job's association has reached its\naggregate limit for an unknown generic resource.\nAssocGrpUnknownMinutes \u2014 The job's association has reached\nthe maximum number of minutes allowed in aggregate for an unknown generic\nresource by past, present and future jobs.\nAssocGrpUnknownRunMinutes \u2014 The job's association has reached\nthe maximum number of minutes allowed in aggregate for an unknown generic\nresource by currently running jobs.\nAssocGrpWallLimit \u2014 The job's association has reached its\naggregate limit for the amount of walltime requested by running jobs.\nAssocMaxBBMinutesPerJob \u2014 The Burst Buffer request exceeds\nthe maximum number of minutes each job is allowed to use for the requested\nassociation.\nAssocMaxBBPerJob \u2014 The Burst Buffer request exceeds the\nmaximum each job is allowed to use for the requested association.\nAssocMaxBBPerNode \u2014 The Burst Buffer request exceeds the\nmaximum number each node in a job allocation is allowed to use for the\nrequested association.\nAssocMaxBillingMinutesPerJob \u2014 The request exceeds the\nmaximum number of minutes each job is allowed to use, with Billing taken into\naccount, for the requested association.\nAssocMaxBillingPerJob \u2014 The resource request exceeds the\nmaximum Billing limit each job is allowed to use for the requested\nassociation.\nAssocMaxBillingPerNode \u2014 The request exceeds the maximum\nBilling limit each node in a job allocation is allowed to use for the\nrequested association.\nAssocMaxCpuMinutesPerJobLimit \u2014 The CPU request exceeds the\nmaximum number of minutes each job is allowed to use for the requested\nassociation.\nAssocMaxCpuPerJobLimit \u2014 The CPU request exceeds the maximum\neach job is allowed to use for the requested association.\nAssocMaxCpuPerNode \u2014 The request exceeds the maximum number\nof CPUs each node in a job allocation is allowed to use for the requested\nassociation.\nAssocMaxEnergyMinutesPerJob \u2014 The Energy request exceeds the\nmaximum number of minutes each job is allowed to use for the requested\nassociation.\nAssocMaxEnergyPerJob \u2014 The Energy request exceeds the maximum\neach job is allowed to use for the requested association.\nAssocMaxEnergyPerNode \u2014 The request exceeds the maximum\namount of Energy each node in a job allocation is allowed to use for the\nrequested association.\nAssocMaxGRESMinutesPerJob \u2014 The GRES request exceeds the\nmaximum number of minutes each job is allowed to use for the requested\nassociation.\nAssocMaxGRESPerJob \u2014 The GRES request exceeds the maximum\neach job is allowed to use for the requested association.\nAssocMaxGRESPerNode \u2014 The request exceeds the maximum number\nof a GRES each node in a job allocation is allowed to use for the requested\nassociation.\nAssocMaxJobsLimit \u2014 The limit on the number of jobs each\nuser is allowed to run at a given time has been met for the requested\nassociation.\nAssocMaxLicenseMinutesPerJob \u2014 The License request exceeds\nthe maximum number of minutes each job is allowed to use for the requested\nassociation.\nAssocMaxLicensePerJob \u2014 The License request exceeds the\nmaximum each job is allowed to use for the requested association.\nAssocMaxMemMinutesPerJob \u2014 The Memory request exceeds the\nmaximum number of minutes each job is allowed to use for the requested\nassociation.\nAssocMaxMemPerJob \u2014 The Memory request exceeds the maximum\neach job is allowed to use for the requested association.\nAssocMaxMemPerNode \u2014 The request exceeds the maximum amount\nof Memory each node in a job allocation is allowed to use for the requested\nassociation.\nAssocMaxNodeMinutesPerJob \u2014 The number of nodes requested\nexceeds the maximum number of minutes each job is allowed to use for the\nrequested association.\nAssocMaxNodePerJobLimit \u2014 The number of nodes requested\nexceeds the maximum each job is allowed to use for the requested\nassociation.\nAssocMaxSubmitJobLimit \u2014 The limit on the number of jobs each\nuser is allowed to have running or pending at a given time has been met for\nthe requested association.\nAssocMaxUnknownMinutesPerJob \u2014 The request of an unknown\ntrackable resource exceeds the maximum number of minutes each job is allowed\nto use for the requested association.\nAssocMaxUnknownPerJob \u2014 The request of an unknown trackable\nresource exceeds the maximum each job is allowed to use for the requested\nassociation.\nAssocMaxUnknownPerNode \u2014 The request exceeds the maximum\nnumber of an unknown trackable resource each node in a job allocation is\nallowed to use for the requested association.\nAssocMaxWallDurationPerJobLimit \u2014 The limit on the amount of\nwall time a job can request has been exceeded for the requested association.\n\nAssociationJobLimit \u2014 The job's association has reached\nits maximum job count.\nAssociationResourceLimit \u2014 The job's association has reached\nsome resource limit.\nAssociationTimeLimit \u2014 The job's association has reached its\ntime limit.\nBadConstraints \u2014 The job's constraints can not be satisfied.\n\nBeginTime \u2014 The job's earliest start time has not yet been\nreached.\nBurstBufferOperation \u2014 Burst Buffer operation for the job\nfailed.\nBurstBufferResources \u2014 There are insufficient resources\nin a Burst Buffer resource pool.\nBurstBufferStageIn \u2014 The Burst Buffer plugin is in the\nprocess of staging the environment for the job.\nCleaning \u2014 The job is being requeued and still cleaning up\nfrom its previous execution.\nDeadLine \u2014 This job has violated the configured Deadline.\nDependency \u2014 This job has a dependency on another job that\nhas not been satisfied.\nDependencyNeverSatisfied \u2014 This job has a dependency on\nanother job that will never be satisfied.\nFedJobLock \u2014 The job is waiting for the clusters in the\nfederation to sync up and issue a lock.\nFrontEndDown \u2014 No front end node is available to execute this\njob.\nInactiveLimit \u2014 The job reached the system InactiveLimit.\nInvalidAccount \u2014 The job's account is invalid.\nInvalidQOS \u2014 The job's QOS is invalid.\nJobArrayTaskLimit \u2014 The job array's limit on the number of\nsimultaneously running tasks has been reached.\nJobHeldAdmin \u2014 The job is held by a system administrator.\nJobHeldUser \u2014 The job is held by the user.\nJobHoldMaxRequeue \u2014 Job has been requeued enough times to\nreach the MAX_BATCH_REQUEUE limit.\nJobLaunchFailure \u2014 The job could not be launched. This may\nbe due to a file system problem, invalid program name, etc.\nLicenses \u2014 The job is waiting for a license.\nMaxBBPerAccount \u2014 The job's Burst Buffer request exceeds the\nper-Account limit on the job's QOS.\nMaxBillingPerAccount \u2014 The job's Billing request exceeds the\nper-Account limit on the job's QOS.\nMaxCpuPerAccount \u2014 The job's CPU request exceeds the\nper-Account limit on the job's QOS.\nMaxEnergyPerAccount \u2014 The job's Energy request exceeds the\nper-Account limit on the job's QOS.\nMaxGRESPerAccount \u2014 The job's GRES request exceeds the\nper-Account limit on the job's QOS.\nMaxJobsPerAccount \u2014 This job exceeds the per-Account limit\non the number of jobs for the job's QOS.\nMaxLicensePerAccount \u2014 The job's License request exceeds the\nper-Account limit on the job's QOS.\nMaxMemoryPerAccount \u2014 The job's Memory request exceeds the\nper-Account limit on the job's QOS.\nMaxMemPerLimit \u2014 The job violates the limit on the maximum\namount of Memory per-CPU or per-Node.\nMaxNodePerAccount \u2014 The number of nodes requested by the job\nexceeds the per-Account limit on the number of nodes for the job's QOS.\nMaxSubmitJobsPerAccount \u2014 This job exceeds the per-Account\nlimit on the number of jobs in a pending or running state for the job's QOS.\n\nMaxUnknownPerAccount \u2014 The jobs request of an unknown GRES\nexceeds the per-Account limit on the job's QOS.\nNodeDown \u2014 A node required by the job is down.\nNonZeroExitCode \u2014 The job terminated with a non-zero exit\ncode.\nNone \u2014 The job hasn't had a reason assigned to it yet.\nOutOfMemory \u2014 The job failed with an Out Of Memory error.\n\nPartitionConfig \u2014 Fallback reason when the job violates\nsome limit on the partition.\nPartitionDown \u2014 The partition required by this job is in\na DOWN state.\nPartitionInactive \u2014 The partition required by this job is\nin an Inactive state and not able to start jobs.\nPartitionNodeLimit \u2014 The number of nodes required by this\njob is outside of its partition's current limits. Can also indicate that\nrequired nodes are DOWN or DRAINED.\nPartitionTimeLimit \u2014 The job's time limit exceeds its\npartition's current time limit.\nPriority \u2014 One of more higher priority jobs exist for the\npartition associated with the job or for the advanced reservation.\nProlog \u2014 The job's Prolog program is still running.\n\nQOSGrpBB \u2014 The job's QOS has reached its aggregate\nBurst Buffer limit.\nQOSGrpBBMinutes \u2014 The job's QOS has reached the\nmaximum number of minutes allowed in aggregate for Burst Buffers by past,\npresent and future jobs.\nQOSGrpBBRunMinutes \u2014 The job's QOS has reached the\nmaximum number of minutes allowed in aggregate for Burst Buffers by\ncurrently running jobs.\nQOSGrpBilling \u2014 The job's QOS has reached its aggregate\nBilling limit.\nQOSGrpBillingMinutes \u2014 The job's QOS has reached\nthe maximum number of minutes allowed in aggregate for the Billing value of\na resource by past, present and future jobs.\nQOSGrpBillingRunMinutes \u2014 The job's QOS has reached the\nmaximum number of minutes allowed in aggregate for the Billing value of a\nresource by currently running jobs.\nQOSGrpCpuLimit \u2014 The job's QOS has reached its aggregate\nCPU limit.\nQOSGrpCPUMinutesLimit \u2014 The job's QOS has reached the\nmaximum number of minutes allowed in aggregate for CPUs by past, present\nand future jobs.\nQOSGrpCPURunMinutesLimit \u2014 The job's QOS has reached\nthe maximum number of minutes allowed in aggregate for CPUs by currently\nrunning jobs.\nQOSGrpEnergy \u2014 The job's QOS has reached its aggregate\nEnergy limit.\nQOSGrpEnergyMinutes \u2014 The job's QOS has reached the\nmaximum number of minutes allowed in aggregate for Energy by past, present\nand future jobs.\nQOSGrpEnergyRunMinutes \u2014 The job's QOS has reached the\nmaximum number of minutes allowed in aggregate for Energy by currently\nrunning jobs.\nQOSGrpGRES \u2014 The job's QOS has reached its aggregate GRES\nlimit.\nQOSGrpGRESMinutes \u2014 The job's QOS has reached the maximum\nnumber of minutes allowed in aggregate for a GRES by past, present and\nfuture jobs.\nQOSGrpGRESRunMinutes \u2014 The job's QOS has reached the\nmaximum number of minutes allowed in aggregate for a GRES by currently\nrunning jobs.\nQOSGrpJobsLimit \u2014 The job's QOS has reached the maximum\nnumber of allowed jobs in aggregate.\nQOSGrpLicense \u2014 The job's QOS has reached its aggregate\nlicense limit.\nQOSGrpLicenseMinutes \u2014 The job's QOS has reached the\nmaximum number of minutes allowed in aggregate for Licenses by past, present\nand future jobs.\nQOSGrpLicenseRunMinutes \u2014 The job's QOS has reached the\nmaximum number of minutes allowed in aggregate for Licenses by currently\nrunning jobs.\nQOSGrpMemLimit \u2014 The job's QOS has reached its aggregate\nMemory limit.\nQOSGrpMemoryMinutes \u2014 The job's QOS has reached the\nmaximum number of minutes allowed in aggregate for Memory by past, present\nand future jobs.\nQOSGrpMemoryRunMinutes \u2014 The job's QOS has reached the\nmaximum number of minutes allowed in aggregate for Memory by currently\nrunning jobs.\nQOSGrpNodeLimit \u2014 The job's QOS has reached its\naggregate Node limit.\nQOSGrpNodeMinutes \u2014 The job's QOS has reached the maximum\nnumber of minutes allowed in aggregate for Nodes by past, present and\nfuture jobs.\nQOSGrpNodeRunMinutes \u2014 The job's QOS has reached the maximum\nnumber of minutes allowed in aggregate for Nodes by currently running jobs.\nQOSGrpSubmitJobsLimit \u2014 The job's QOS has reached the maximum\nnumber of jobs that can be running or pending in aggregate at a given time.\nQOSGrpUnknown \u2014 The job's QOS has reached its aggregate limit\nfor an unknown generic resource.\nQOSGrpUnknownMinutes \u2014 The job's QOS has reached the maximum\nnumber of minutes allowed in aggregate for an unknown generic resource by\npast, present and future jobs.\nQOSGrpUnknownRunMinutes \u2014 The job's QOS has reached the\nmaximum number of minutes allowed in aggregate for an unknown generic\nresource by currently running jobs.\nQOSGrpWallLimit \u2014 The job's QOS has reached its aggregate\nlimit for the amount of walltime requested by running jobs.\nQOSJobLimit \u2014 The job's QOS has reached its maximum job\ncount.\nQOSMaxBBMinutesPerJob \u2014 The Burst Buffer request exceeds\nthe maximum number of minutes each job is allowed to use for the requested\nQOS.\nQOSMaxBBPerJob \u2014 The Burst Buffer request exceeds the\nmaximum each job is allowed to use for the requested QOS.\nQOSMaxBBPerNode \u2014 The Burst Buffer request exceeds the\nmaximum number each node in a job allocation is allowed to use for the\nrequested QOS.\nQOSMaxBBPerUser \u2014 The Burst Buffer request exceeds the\nmaximum number each user is allowed to use for the requested QOS.\nQOSMaxBillingMinutesPerJob \u2014 The request exceeds the\nmaximum number of minutes each job is allowed to use, with Billing taken into\naccount, for the requested QOS.\nQOSMaxBillingPerJob \u2014 The resource request exceeds the\nmaximum Billing limit each job is allowed to use for the requested\nQOS.\nQOSMaxBillingPerNode \u2014 The request exceeds the maximum\nBilling limit each node in a job allocation is allowed to use for the\nrequested QOS.\nQOSMaxBillingPerUser \u2014 The request exceeds the maximum\nBilling limit each user is allowed to use for the requested QOS.\nQOSMaxCpuMinutesPerJobLimit \u2014 The CPU request exceeds the\nmaximum number of minutes each job is allowed to use for the requested\nQOS.\nQOSMaxCpuPerJobLimit \u2014 The CPU request exceeds the maximum\neach job is allowed to use for the requested QOS.\nQOSMaxCpuPerNode \u2014 The request exceeds the maximum number\nof CPUs each node in a job allocation is allowed to use for the requested\nQOS.\nQOSMaxCpuPerUserLimit \u2014 The CPU request exceeds the maximum\neach user is allowed to use for the requested QOS.\nQOSMaxEnergyMinutesPerJob \u2014 The Energy request exceeds the\nmaximum number of minutes each job is allowed to use for the requested\nQOS.\nQOSMaxEnergyPerJob \u2014 The Energy request exceeds the maximum\neach job is allowed to use for the requested QOS.\nQOSMaxEnergyPerNode \u2014 The request exceeds the maximum\namount of Energy each node in a job allocation is allowed to use for the\nrequested QOS.\nQOSMaxEnergyPerUser \u2014 The request exceeds the maximum\namount of Energy each user is allowed to use for the requested QOS.\nQOSMaxGRESMinutesPerJob \u2014 The GRES request exceeds the\nmaximum number of minutes each job is allowed to use for the requested\nQOS.\nQOSMaxGRESPerJob \u2014 The GRES request exceeds the maximum\neach job is allowed to use for the requested QOS.\nQOSMaxGRESPerNode \u2014 The request exceeds the maximum number\nof a GRES each node in a job allocation is allowed to use for the requested\nQOS.\nQOSMaxGRESPerUser \u2014 The request exceeds the maximum number\nof a GRES each user is allowed to use for the requested QOS.\nQOSMaxJobsPerUserLimit \u2014 The limit on the number of jobs a\nuser is allowed to run at a given time has been met for the requested\nQOS.\nQOSMaxLicenseMinutesPerJob \u2014 The License request exceeds\nthe maximum number of minutes each job is allowed to use for the requested\nQOS.\nQOSMaxLicensePerJob \u2014 The License request exceeds the\nmaximum each job is allowed to use for the requested QOS.\nQOSMaxLicensePerUser \u2014 The License request exceeds the\nmaximum each user is allowed to use for the requested QOS.\nQOSMaxMemoryMinutesPerJob \u2014 The Memory request exceeds the\nmaximum number of minutes each job is allowed to use for the requested\nQOS.\nQOSMaxMemoryPerJob \u2014 The Memory request exceeds the maximum\neach job is allowed to use for the requested QOS.\nQOSMaxMemoryPerNode \u2014 The request exceeds the maximum amount\nof Memory each node in a job allocation is allowed to use for the requested\nQOS.\nQOSMaxMemoryPerUser \u2014 The request exceeds the maximum amount\nof Memory each user is allowed to use for the requested QOS.\nQOSMaxNodeMinutesPerJob \u2014 The number of nodes requested\nexceeds the maximum number of minutes each job is allowed to use for the\nrequested QOS.\nQOSMaxNodePerJobLimit \u2014 The number of nodes requested\nexceeds the maximum each job is allowed to use for the requested\nQOS.\nQOSMaxNodePerUserLimit \u2014 The number of nodes requested\nexceeds the maximum each user is allowed to use for the requested\nQOS.\nQOSMaxSubmitJobPerUserLimit \u2014 The limit on the number of\njobs each user is allowed to have running or pending at a given time has\nbeen met for the requested QOS.\nQOSMaxUnknownMinutesPerJob \u2014 The request of an unknown\ntrackable resource exceeds the maximum number of minutes each job is allowed\nto use for the requested QOS.\nQOSMaxUnknownPerJob \u2014 The request of an unknown trackable\nresource exceeds the maximum each job is allowed to use for the requested\nQOS.\nQOSMaxUnknownPerNode \u2014 The request exceeds the maximum\nnumber of an unknown trackable resource each node in a job allocation is\nallowed to use for the requested QOS.\nQOSMaxUnknownPerUser \u2014 The request exceeds the maximum\nnumber of an unknown trackable resource each user is allowed to use for\nthe requested QOS.\nQOSMaxWallDurationPerJobLimit \u2014 The limit on the amount of\nwall time a job can request has been exceeded for the requested QOS.\nQOSMinBB \u2014 The Burst Buffer request does not meet the\nminimum each job is required to request for the requested QOS.\nQOSMinBilling \u2014 The resource request does not meet the\nminimum Billing limit each job is allowed to use for the requested\nQOS.\nQOSMinCpuNotSatisfied \u2014 The CPU request does not meet the\nminimum each job is allowed to use for the requested QOS.\nQOSMinEnergy \u2014 The Energy request does not meet the\nminimum each job is allowed to use for the requested QOS.\nQOSMinGRES \u2014 The GRES request does not meet the\nminimum each job is allowed to use for the requested QOS.\nQOSMinLicense \u2014 The License request does not meet the\nminimum each job is allowed to use for the requested QOS.\nQOSMinMemory \u2014 The Memory request does not meet the\nminimum each job is allowed to use for the requested QOS.\nQOSMinNode \u2014 The number of nodes requested does not meet the\nminimum each job is allowed to use for the requested QOS.\nQOSMinUnknown \u2014 The request of an unknown trackable resource\ndoes not meet the minimum each job is allowed to use for the requested QOS.\nQOSNotAllowed \u2014 The job requests a QOS is not allowed by\nthe requested association or partition.\nQOSResourceLimit \u2014 The job's QOS has reached some resource\nlimit.\nQOSTimeLimit \u2014 The job's QOS has reached its time limit.\nQOSUsageThreshold \u2014 Required QOS threshold has been\nbreached.\nReqNodeNotAvail \u2014 Some node specifically required by the job\nis not currently available. The node may currently be in use, reserved for\nanother job, in an advanced reservation, DOWN, DRAINED, or not responding.\nNodes which are DOWN, DRAINED, or not responding will be identified as part\nof the job's \"reason\" field as \"UnavailableNodes\". Such nodes will typically\nrequire the intervention of a system administrator to make available.\nReservation \u2014 The job is waiting its advanced reservation to\nbecome available.\nReservationDeleted \u2014 The job requested a reservation that is\nno longer on the system.\nResources \u2014 The resources requested by the job are not\navailable (e.g., already used by other jobs).\nSchedDefer \u2014 The job requests an immediate allocation but\nSchedulerParameters=defer is configured in the slurm.conf.\nSystemFailure \u2014 Failure of the Slurm system, a file system,\nthe network, etc.\nTimeLimit \u2014 The job exhausted its time limit.\nLast modified 08 October 2024\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/fair_tree.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Fair Tree Fairshare Algorithm",
                "content": "Contents\nIntroduction\nOverview for End Users\nAlgorithm\nLevel Fairshare Calculation\nTies\nsshare\nConfiguration\nImportant notes\nIntroduction\nFair Tree prioritizes users such that if accounts A and B are siblings and A has\na higher fairshare factor than B, all children of A will have higher fairshare\nfactors than all children of B.Some of the benefits include:\n\n\t\tAll users from a higher priority account receive a higher fair\n\t\tshare factor than all users from a lower priority account.\n\t\n\n\t\tUsers are sorted and ranked to prevent errors due to precision\n\t\tloss. Ties are allowed.\n\t\n\n\t\tAccount coordinators cannot accidentally harm the priority of\n\t\ttheir users relative to users in other accounts.\n\t\n\n\t\tUsers are extremely unlikely to have exactly the same fairshare\n\t\tfactor as another user due to loss of precision in calculations.\n\t\n\n\t\tNew jobs are immediately assigned a priority.\n\t\nOverview for End Users\n\nThis section is intended for non-admin users who just want to know how their\nfairshare factor is determined.  Run sshare -l (lowercase \"L\") to\nview the following columns: FairShare, Level FS.  Note that\nLevel FS values are infinity if the association has no usage.If an account has a higher Level FS value than any other sibling user or\nsibling account, all children of that account will have a higher FairShare value\nthan the children of the other account. This is true at every level of the\nassociation tree.The FairShare value is obtained by using the Fair Tree\nalgorithm to rank all users in the order that they\nshould be prioritized (descending). The FairShare value is the user's rank\ndivided by the total number of user associations. The highest ranked user\nreceives a 1.0 fairshare value.If you (UserA) have a lower FairShare value than another user (UserB) and\nwant to know why, find the first common ancestor account. At the level\nbelow the common ancestor, compare the Level FS value of your ancestor to the\nLevel FS value of UserB's ancestor. Your ancestor has a lower Level FS value\nthan UserB's ancestor. For information on how Level FS value is\ncalculated, read the section about the Level FS\nequation.For example, assume the association tree contains UserA and UserB as\nfollows:\nroot => Acct1 => Acct12 => UserA\nroot => Acct1 => Acct16 => UserB\nAcct1 is the first common ancestor of UserA and UserB. Check the Level FS\nvalues of Acct12 and Acct16.  If UserB has a higher FairShare value than UserA,\nAcct16 has a higher Level FS value than Acct12.The sections below contain more information about the algorithm, including\nhow the final fairshare factor and the Level FS values are calculated.AlgorithmAn equation is used to calculate a Level Fairshare value for each\nassociation, only considering the shares and usage of itself and its siblings.\nA \nrooted plane tree (PDF download), also known as a rooted\nordered tree, is logically created then sorted by Level Fairshare with the\nhighest values on the left. The tree is then visited in a depth-first\ntraversal. Users are ranked in pre-order as they are found. The ranking is\nused to create the final fairshare factor for the user.The algorithm performs a single traversal of the tree since all the steps\ncan be combined. The basic idea is to set rank equal to the count of user\nassociations then start at root:Calculate Level Fairshare for the subtree's children\nSort children of the subtree\nVisit the children in descending order\nIf user, assign a final fairshare factor similar to\n\t(rank-- / user_assoc_count)\nIf account, descend to account\nLevel Fairshare Calculation\n\nThe Level Fairshare equation is described below. Under-served associations\nwill have a value greater than 1.0. Over-served associations will have a value\nbetween 0.0 and 1.0.\n\nLF = S / U\n\nLF\n is the association's Level Fairshare\n S\n also known as Shares Norm, S is the association's assigned shares\nnormalized to the shares assigned to itself and its siblings:\nS = Srawself / Srawself+siblings\n\n U\n also known as Effective Usage, U is the association's usage normalized to\nthe account's usage:\nU = Urawself / Urawself+siblings\n\nU and S are in the range 0.0 .. 1.0. LF is in the\nrange 0.0 .. infinity.TiesTies are handled as follows:\n\nSibling users with the same Level Fairshare receive the same rank\nA user with the same Level Fairshare as a sibling account will receive the\nsame rank as its highest ranked user\nSibling accounts with the same Level Fairshare have their children lists\nmerged before descending\n\nssharesshare was modified to show the Level Fairshare value as Level FS when\nthe -l (long) parameter is specified.  The field shows the value for each\nassociation, thus allowing users to see the results of the fairshare\ncalculation at each level.Note: Norm Usage is not used by Fair Tree but is still displayed.ConfigurationThe following slurm.conf parameters are used to\nconfigure the Fair Tree algorithm.  See slurm.conf(5) man page for more\ndetails.\nPriorityType\nSet this value to \"priority/multifactor\".\nPriorityCalcPeriod\nPriorityCalcPeriod is the frequency in minutes that job half-life decay\nand Fair Tree calculations are performed.\nImportant Notes\nAs the Fair Tree algorithm ranks all users, active or not, the\nadministrator must carefully consider how to apply other priority weights\nin the priority/multifactor plugin. The PriorityWeightFairshare can be\nusefully set to a much smaller value than usual, possibly as low as 1 or 2 times\nthe number of user associations.\n\nFair Tree requires the Slurm Accounting\nDatabase to provide usage information and the assigned shares values.\n\nscontrol reconfigure does not cause the Fair Tree algorithm to\nrun immediately, even if switching from a different algorithm. You may have to\nwait until the next iteration as defined by PriorityCalcPeriod.\n\nLast modified 26 June 2023"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            }
        ]
    },
    {
        "url": "https://slurm.schedmd.com/mc_support.html",
        "sections": [
            {
                "title": "\n\nSlurm Workload Manager\n\n",
                "content": "\n\nSchedMD\n\n"
            },
            {
                "title": "Support for Multi-core/Multi-thread Architectures",
                "content": "Contents\n Definitions\n Overview of srun flags\n Motivation behind high-level srun flags\n Extensions to sinfo/squeue/scontrol\n Configuration settings in slurm.conf\nDefinitions\nBaseBoard\nAlso called motherboard.\nLDom\nLocality domain or NUMA domain. May be equivalent to BaseBoard or Socket.\nSocket/Core/Thread\nFigure 1 illustrates the notion of Socket, Core and Thread as it is defined\nin Slurm's multi-core/multi-thread support documentation.\nCPU\nDepending upon system configuration, this can be either a core or a thread.\n\n\n\nFigure 1: Definitions of Socket, Core, & Thread\n\nAffinity\nThe state of being bound to a specific logical processor.\nAffinity Mask\nA bitmask where indices correspond to logical processors.\nThe least significant bit corresponds to the first\nlogical processor number on the system, while the most\nsignificant bit corresponds to the last logical processor\nnumber on the system.\nA '1' in a given position indicates a process can run\non the associated logical processor.\nFat Masks\nAffinity masks with more than 1 bit set\nallowing a process to run on more than one logical processor.\nOverview of srun flags\n\nMany flags have been defined to allow users to\nbetter take advantage of this architecture by\nexplicitly specifying the number of sockets, cores, and threads required\nby their application.  Table 1 summarizes these options.\n\n\n\n\nLow-level (explicit binding)\n\n\n --cpu-bind=... \nExplicit process affinity binding and control options\n\n\nHigh-level (automatic mask generation)\n\n\n --sockets-per-node=S\nNumber of sockets in a node to dedicate to a job (minimum)\n\n\n --cores-per-socket=C\n Number of cores in a socket to dedicate to a job (minimum)\n\n\n --threads-per-core=T\n Minimum number of threads in a core to dedicate to a job. In task\n\t layout, use the specified maximum number of threads per-core.\n\n\n -B S[:C[:T]]\n Combined shortcut option for --sockets-per-node, --cores-per_cpu, --threads-per_core\n\n\nTask Distribution Options\n\n\n -m / --distribution \n Distributions of: arbitrary | block | cyclic\n    \t\t| plane=x\n\t\t| [block|cyclic]:[block|cyclic|fcyclic]\n\n\nMemory as a consumable resource\n\n\n --mem=mem\n amount of real memory per node required by the job.\n\n\n --mem-per-cpu=mem\n amount of real memory per allocated CPU required by the job.\n\n\nTask invocation control\n\n\n --cpus-per-task=CPUs\n number of CPUs required per task\n\n --ntasks-per-node=ntasks\n number of tasks to invoke on each node\n\n --ntasks-per-socket=ntasks\n number of tasks to invoke on each socket\n\n --ntasks-per-core=ntasks\n number of tasks to invoke on each core\n\n --overcommit\n Permit more than one task per CPU\n\n\nApplication hints\n\n\n --hint=compute_bound\n use all cores in each socket\n\n --hint=memory_bound\n use only one core in each socket\n\n --hint=[no]multithread\n [don't] use extra threads with in-core multi-threading\n\n\nResources reserved for system use\n\n\n --core-spec=cores\n Count of cores to reserve for system use\n\n --thread-spec=threads\n Count of threads to reserve for system use\n\n\n\n\nTable 1: srun flags to support the multi-core/multi-threaded environment\n\nIt is important to note that many of these flags are only meaningful if the\nprocesses have some affinity to specific CPUs and (optionally) memory.\nInconsistent options generally result in errors.\nTask affinity is configured using the TaskPlugin parameter in the slurm.conf file.\nSeveral options exist for the TaskPlugin depending upon system architecture\nand available software, any of them except \"task/none\" will bind tasks to CPUs.\nSee the \"Task Launch\" section if generating slurm.conf via\nconfigurator.html.\nLow-level --cpu-bind=... - Explicit binding interface\n\n\nThe following srun flag provides a low-level core binding interface:\n\n--cpu-bind=        Bind tasks to CPUs\n    q[uiet]         quietly bind before task runs (default)\n    v[erbose]       verbosely report binding before task runs\n    no[ne]          don't bind tasks to CPUs (default)\n    rank            bind by task rank\n    map_cpu:<list>  specify a CPU ID binding for each task\n                    where <list> is\n                    <cpuid1>,<cpuid2>,...<cpuidN>\n    mask_cpu:<list> specify a CPU ID binding mask for each\n                    task where <list> is\n                    <mask1>,<mask2>,...<maskN>\n    rank_ldom       bind task by rank to CPUs in a NUMA\n                    locality domain\n    map_ldom:<list> specify a NUMA locality domain ID\n                    for each task where <list> is\n                    <ldom1>,<ldom2>,...<ldomN>\n    rank_ldom       bind task by rank to CPUs in a NUMA\n                    locality domain where <list> is\n                    <ldom1>,<ldom2>,...<ldomN>\n    mask_ldom:<list> specify a NUMA locality domain ID mask\n                    for each task where <list> is\n                    <ldom1>,<ldom2>,...<ldomN>\n    ldoms           auto-generated masks bind to NUMA locality\n                    domains\n    sockets         auto-generated masks bind to sockets\n    cores           auto-generated masks bind to cores\n    threads         auto-generated masks bind to threads\n    help            show this help message\n\n The affinity can be either set to either a specific logical processor\n(socket, core, threads) or at a coarser granularity than the lowest level\nof logical processor (core or thread).\nIn the later case the processes are allowed to utilize multiple processors\nwithin a specific socket or core.\n\nExamples:\n\n\n srun -n 8 -N 4 --cpu-bind=mask_cpu:0x1,0x4 a.out\n srun -n 8 -N 4 --cpu-bind=mask_cpu:0x3,0xD a.out\n\n\nSee also 'srun --cpu-bind=help' and 'man srun'\n\nHigh-level -B S[:C[:T]] - Automatic mask generation interface\n\n\nWe have updated the node\nselection infrastructure with a mechanism that allows selection of logical\nprocessors at a finer granularity. Users are able to request a specific number\nof nodes, sockets,\u00a0 cores, and threads:\n\n-B, --extra-node-info=S[:C[:T]]            Expands to:\n  --sockets-per-node=S   number of sockets per node to allocate\n  --cores-per-socket=C   number of cores per socket to allocate\n  --threads-per-core=T   number of threads per core to allocate\n                each field can be 'min' or wildcard '*'\n\n     Total cpus requested = (Nodes) x (S x C x T)\n\nExamples:\n\n\n\n srun -n 8 -N 4 -B 2:1 a.out\n srun -n 8 -N 4 -B 2   a.out\n\nnote: compare the above with the previous corresponding --cpu-bind=... examples\n\n srun -n 16 -N 4 a.out\n srun -n 16 -N 4 -B 2:2:1 a.out\n srun -n 16 -N 4 -B 2:2:1 a.out\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0or\n srun -n 16 -N 4 --sockets-per-node=2 --cores-per-socket=2 --threads-per-core=1 a.out\n srun -n 16 -N 2-4 -B '1:*:1' a.out\n srun -n 16 -N 4-2 -B '2:*:1' a.out\n srun -n 16 -N 4-4 -B '1:1' a.out\n\n\nNotes:\n\n Adding --cpu-bind=no to the command line will cause the processes\n      to not be bound the logical processors.\n  Adding --cpu-bind=verbose to the command line (or setting the\n      CPU_BIND environment variable to \"verbose\") will cause each task\n      to report the affinity mask in use\n  Binding is on by default when -B is used. The default binding on\n      multi-core/multi-threaded systems is equivalent to the level of\n      resource enumerated in the -B option.\n\nSee also 'srun --help' and 'man srun'\nTask distribution options: Extensions to -m / --distribution\n\n\nThe -m / --distribution option for distributing processes across nodes\nhas been extended to also describe the distribution within the lowest level\nof logical processors.\nAvailable distributions include:\n\narbitrary | block | cyclic | plane=x | [block|cyclic]:[block|cyclic|fcyclic]\n\nThe  plane distribution (plane=x)\nresults in a block:cyclic distribution of blocksize equal to x.\nIn the following we use \"lowest level of logical processors\"\nto describe sockets, cores or threads depending of the architecture.\nThe distribution divides\nthe cluster into planes (including a number of the lowest level of logical\nprocessors on each node) and then schedule first within each plane and then\nacross planes.\nFor the two dimensional distributions ([block|cyclic]:[block|cyclic|fcyclic]),\nthe second distribution (after \":\") allows users to specify a distribution\nmethod for processes within a node and applies to the lowest level of logical\nprocessors (sockets, core or thread depending on the architecture).\nWhen a task requires more than one CPU, the cyclic will allocate all\nof those CPUs as a group (i.e. within the same socket if possible) while \nfcyclic would distribute each of those CPU of the in a cyclic fashion\nacross sockets.\nThe binding is enabled automatically when high level flags are used as long\nas the task/affinity plug-in is enabled. To disable binding at the job level\nuse --cpu-bind=no.\nThe distribution flags can be combined with the other switches:\n\n\n\nsrun -n 16 -N 4 -B '2:*:1' -m block:cyclic --cpu-bind=socket a.out\nsrun -n 16 -N 4 -B '2:*:1' -m plane=2 --cpu-bind=core a.out\nsrun -n 16 -N 4 -B '2:*:1' -m plane=2 a.out\n\n\nThe default distribution on multi-core/multi-threaded systems is equivalent\nto -m block:cyclic with --cpu-bind=thread.\nSee also 'srun --help'\nMemory as a Consumable Resource\n\n\nThe --mem flag specifies the maximum amount of memory in MB\nneeded by the job per node.  This flag is used to support the memory\nas a consumable resource allocation strategy.\n\n--mem=MB      maximum amount of real memory per node\n              required by the job.\n\nThis flag allows the scheduler to co-allocate jobs on specific nodes\ngiven that their added memory requirement do not exceed the total amount\nof memory on the nodes.\nIn order to use memory as a consumable resource, the select/cons_tres\nplugin must be first enabled in slurm.conf:\n\nSelectType=select/cons_tres     # enable consumable resources\nSelectTypeParameters=CR_Memory  # memory as a consumable resource\n\n Using memory as a consumable resource is typically combined with\nthe CPU, Socket, or Core consumable resources using SelectTypeParameters\nvalues of: CR_CPU_Memory, CR_Socket_Memory or CR_Core_Memory\n\nSee the \"Resource Selection\" section if generating slurm.conf\nvia configurator.html.\n\nSee also 'srun --help' and 'man srun'\nTask invocation as a function of logical processors\n\n\nThe --ntasks-per-{node,socket,core}=ntasks flags\nallow the user to request that no more than ntasks\nbe invoked on each node, socket, or core.\nThis is similar to using --cpus-per-task=ncpus\nbut does not require knowledge of the actual number of cpus on\neach node.  In some cases, it is more convenient to be able to\nrequest that no more than a specific number of ntasks be invoked\non each node, socket, or core.  Examples of this include submitting\nan app where only one \"task/rank\" should be\nassigned to each node while allowing the job to utilize\nall of the parallelism present in the node, or submitting a single\nsetup/cleanup/monitoring job to each node of a pre-existing\nallocation as one step in a larger job script.\nThis can now be specified via the following flags:\n\n--ntasks-per-node=n    number of tasks to invoke on each node\n--ntasks-per-socket=n  number of tasks to invoke on each socket\n--ntasks-per-core=n    number of tasks to invoke on each core\n\nFor example, given a cluster with nodes containing two sockets,\neach containing two cores, the following commands illustrate the\nbehavior of these flags:\n\n% srun -n 4 hostname\nhydra12\nhydra12\nhydra12\nhydra12\n% srun -n 4 --ntasks-per-node=1 hostname\nhydra12\nhydra13\nhydra14\nhydra15\n% srun -n 4 --ntasks-per-node=2 hostname\nhydra12\nhydra12\nhydra13\nhydra13\n% srun -n 4 --ntasks-per-socket=1 hostname\nhydra12\nhydra12\nhydra13\nhydra13\n% srun -n 4 --ntasks-per-core=1 hostname\nhydra12\nhydra12\nhydra12\nhydra12\n\nSee also 'srun --help' and 'man srun'\nApplication hints\n\n\nDifferent applications will have various levels of resource\nrequirements. Some applications tend to be computationally intensive\nbut require little to no inter-process communication. Some applications\nwill be memory bound, saturating the memory bandwidth of a processor\nbefore exhausting the computational capabilities. Other applications\nwill be highly communication intensive causing processes to block\nawaiting messages from other processes. Applications with these\ndifferent properties tend to run well on a multi-core system given\nthe right mappings.\nFor computationally intensive applications, all cores in a multi-core\nsystem would normally be used. For memory bound applications, only\nusing a single core on each socket will result in the highest per\ncore memory bandwidth. For communication intensive applications,\nusing in-core multi-threading (e.g. hyperthreading, SMT, or TMT)\nmay also improve performance.\nThe following command line flags can be used to communicate these\ntypes of application hints to the Slurm multi-core support:\n\n--hint=             Bind tasks according to application hints\n    compute_bound   use all cores in each socket\n    memory_bound    use only one core in each socket\n    [no]multithread [don't] use extra threads with in-core multi-threading\n    help            show this help message\n\nFor example, given a cluster with nodes containing two sockets,\neach containing two cores, the following commands illustrate the\nbehavior of these flags.  In the verbose --cpu-bind output, tasks\nare described as 'hostname, task Global_ID Local_ID [PID]':\n\n% srun -n 4 --hint=compute_bound --cpu-bind=verbose sleep 1\ncpu-bind=MASK - hydra12, task  0  0 [15425]: mask 0x1 set\ncpu-bind=MASK - hydra12, task  1  1 [15426]: mask 0x4 set\ncpu-bind=MASK - hydra12, task  2  2 [15427]: mask 0x2 set\ncpu-bind=MASK - hydra12, task  3  3 [15428]: mask 0x8 set\n\n% srun -n 4 --hint=memory_bound --cpu-bind=verbose sleep 1\ncpu-bind=MASK - hydra12, task  0  0 [15550]: mask 0x1 set\ncpu-bind=MASK - hydra12, task  1  1 [15551]: mask 0x4 set\ncpu-bind=MASK - hydra13, task  2  0 [14974]: mask 0x1 set\ncpu-bind=MASK - hydra13, task  3  1 [14975]: mask 0x4 set\n\nSee also 'srun --hint=help' and 'man srun'\n\nMotivation behind high-level srun flags\n\n\nThe motivation behind allowing users to use higher level srun\nflags instead of --cpu-bind is that the later can be difficult to use. The\nproposed high-level flags are easier to use than --cpu-bind because:\n\nAffinity mask generation happens automatically when using the high-level flags. \nThe length and complexity of the --cpu-bind flag vs. the length\nof the combination of -B and --distribution flags make the high-level\nflags much easier to use.\n\nAlso as illustrated in the example below it is much simpler to specify\na different layout using the high-level flags since users do not have to\nrecalculate mask or CPU IDs. This approach is much simpler than\nrearranging the mask or map.\nGiven a 32-process job and a four node, dual-socket, dual-core\ncluster, we want to use a block distribution across the four nodes and then a\ncyclic distribution within the node across the physical processors. Below we\nshow how to obtain the wanted layout using 1) high-level flags and\n2) --cpubind\nHigh-Level flags\n\n\nUsing Slurm's high-level flag, users can obtain the above layout with\neither of the following submissions since --distribution=block:cyclic\nis the default distribution method.\n\n$ srun -n 32 -N 4 -B 4:2 --distribution=block:cyclic a.out\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0or\n\n$ srun -n 32 -N 4 -B 4:2 a.out\n\nThe cores are shown as c0 and c1 and the processors are shown\nas p0 through p3. The resulting task IDs are: \n\n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\n\nThe computation and assignment of the task IDs is transparent\nto the user. Users don't have to worry about the core numbering (Section\nPinning processes to cores) or any setting any CPU affinities. By default CPU affinity\nwill be set when using multi-core supporting flags. \nLow-level flag --cpu-bind\n\n\nUsing Slurm's --cpu-bind flag, users must compute the CPU IDs or\nmasks as well as make sure they understand the core numbering on their\nsystem. Another problem arises when core numbering is not the same on all\nnodes. The --cpu-bind option only allows users to specify a single\nmask for all the nodes. Using Slurm high-level flags remove this limitation\nsince Slurm will correctly generate the appropriate masks for each requested nodes.\n\nOn a four dual-socket dual-core node cluster with core block numbering\n\n\nThe cores are shown as c0 and c1 and the processors are shown\nas p0 through p3. The CPU IDs within a node in the block numbering are:\n(this information is available from the /proc/cpuinfo file on the system)\n\n\n\n\n \nc0c1\np0 0  1 \np2 4  5 \n\n\n\n\n\n\n \nc0c1\np1 2  3 \np3 6  7 \n\n\n\n\n\u00a0resulting in the following mapping for processor/cores\nand task IDs which users need to calculate: \n\nmapping for processors/cores\n\n\n\n\n\n \nc0c1\np0 0x01  0x02 \np2 0x10  0x20 \n\n\n\n\n\n\n \nc0c1\np1 0x04  0x08 \np3 0x40  0x80 \n\n\n\n\n\n\ntask IDs\n\n\n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\n\nThe above maps and task IDs can be translated into the\nfollowing command:\n\n$ srun -n 32 -N 4 --cpu-bind=mask_cpu:1,4,10,40,2,8,20,80 a.out\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0or\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,2,4,6,1,3,5,7 a.out\n\n\nSame cluster but with its core numbered cyclic instead of block\n\n\nOn a system with cyclically numbered cores, the correct mask\nargument to the srun command looks like: (this will\nachieve the same layout as the command above on a system with core block\nnumbering.)\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,1,2,3,4,5,6,7 a.out\n\nBlock map_cpu on a system with cyclic core numbering\n\n\nIf users do not check their system's core numbering before specifying\nthe map_cpu list and thereby do not realize that the system has cyclic core\nnumbering instead of block numbering then they will not get the expected\nlayout. For example, if they decide to re-use their command from above:\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,2,4,6,1,3,5,7 a.out\n\nthey get the following unintentional task ID layout:\n\n\n\n\n \nc0c1\np0 0  2 \np2 1  3 \n\n\n\n\n\n\n \nc0c1\np1 4  6 \np3 5  7 \n\n\n\n\nsince the processor IDs within a node in the cyclic numbering are:\n\n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\n\nThe important conclusion is that using the --cpu-bind flag is not\ntrivial and that it assumes that users are experts.\n\nExtensions to sinfo/squeue/scontrol\n\n\nSeveral extensions have also been made to the other Slurm utilities to\nmake working with multi-core/multi-threaded systems easier.\n\nsinfo\nThe long version (-l) of the sinfo node listing (-N) has been\nextended to display the sockets, cores, and threads present for each\nnode.  For example:\n\n\n% sinfo -N\nNODELIST     NODES PARTITION STATE\nhydra[12-15]     4    parts* idle\n\n% sinfo -lN\nThu Sep 14 17:47:13 2006\nNODELIST     NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT FEATURES REASON\nhydra[12-15]     4    parts*        idle   8+ 2+:4+:1+   2007    41447      1   (null) none\n\n% sinfo -lNe\nThu Sep 14 17:47:18 2006\nNODELIST     NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT FEATURES REASON\n\nhydra[12-14]     3    parts*        idle    8    2:4:1   2007    41447      1   (null) none\nhydra15          1    parts*        idle   64    8:4:2   2007    41447      1   (null) none\n\nFor user specified output formats (-o/--format) and sorting (-S/--sort),\nthe following identifiers are available:\n\n%X  Number of sockets per node\n%Y  Number of cores per socket\n%Z  Number of threads per core\n%z  Extended processor information: number of\n    sockets, core, threads (S:C:T) per node\n\nFor example:\n\n% sinfo -o '%9P %4c %8z %8X %8Y %8Z'\nPARTITION CPUS S:C:T    SOCKETS  CORES    THREADS\nparts*    4    2:2:1    2        2        1\n\nSee also 'sinfo --help' and 'man sinfo'\n\nsqueue\nFor user specified output formats (-o/--format) and sorting (-S/--sort),\nthe following identifiers are available:\n\n%m  Size of memory (in MB) requested by the job\n%H  Number of requested sockets per node\n%I  Number of requested cores per socket\n%J  Number of requested threads per core\n%z  Extended processor information: number of requested\n    sockets, cores, threads (S:C:T) per node\n\nBelow is an example squeue output after running 7 copies of:\n\n\n% srun -n 4 -B 2:2:1 --mem=1024 sleep 100 &\n\n\n\n% squeue -o '%.5i %.2t %.4M %.5D %7H %6I %7J %6z %R'\nJOBID ST TIME NODES SOCKETS CORES THREADS S:C:T NODELIST(REASON)\n   17 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   18 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   19 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   13  R 1:27     1 2       2     1       2:2:1 hydra12\n   14  R 1:26     1 2       2     1       2:2:1 hydra13\n   15  R 1:26     1 2       2     1       2:2:1 hydra14\n   16  R 1:26     1 2       2     1       2:2:1 hydra15\n\nThe squeue command can also display the memory size of jobs, for example:\n\n% sbatch --mem=123 tmp\nSubmitted batch job 24\n\n$ squeue -o \"%.5i %.2t %.4M %.5D %m\"\nJOBID ST TIME NODES MIN_MEMORY\n  24   R 0:05     1 123\n\nSee also 'squeue --help' and 'man squeue'\n\nscontrol\nThe following job settings can be adjusted using scontrol:\n\n\nRequested Allocation:\n  ReqSockets=<count>  Set the job's count of required sockets\n  ReqCores=<count>    Set the job's count of required cores\n  ReqThreads=<count>  Set the job's count of required threads\n\nFor example:\n\n# scontrol update JobID=17 ReqThreads=2\n# scontrol update JobID=18 ReqCores=4\n# scontrol update JobID=19 ReqSockets=8\n\n% squeue -o '%.5i %.2t %.4M %.5D %9c %7H %6I %8J'\nJOBID ST TIME NODES MIN_PROCS SOCKETS CORES THREADS\n   17 PD 0:00     1 1         4       2     1\n   18 PD 0:00     1 1         8       4     2\n   19 PD 0:00     1 1         4       2     1\n   13  R 1:35     1 0         0       0     0\n   14  R 1:34     1 0         0       0     0\n   15  R 1:34     1 0         0       0     0\n   16  R 1:34     1 0         0       0     0\n\nThe 'scontrol show job' command can be used to display\nthe number of allocated CPUs per node as well as the socket, cores,\nand threads specified in the request and constraints.\n\n\n% srun -N 2 -B 2:1 sleep 100 &\n% scontrol show job 20\nJobId=20 UserId=(30352) GroupId=users(1051)\n   Name=sleep\n   Priority=4294901749 Partition=parts BatchFlag=0\n   AllocNode:Sid=hydra16:3892 TimeLimit=UNLIMITED\n   JobState=RUNNING StartTime=09/25-17:17:30 EndTime=NONE\n   NodeList=hydra[12-14] NodeListIndices=0,2,-1\n   AllocCPUs=1,2,1\n   NumCPUs=4 ReqNodes=2 ReqS:C:T=2:1:*\n   OverSubscribe=0 Contiguous=0 CPUs/task=0\n   MinCPUs=0 MinMemory=0 MinTmpDisk=0 Features=(null)\n   Dependency=0 Account=(null) Reason=None Network=(null)\n   ReqNodeList=(null) ReqNodeListIndices=-1\n   ExcNodeList=(null) ExcNodeListIndices=-1\n   SubmitTime=09/25-17:17:30 SuspendTime=None PreSusTime=0\n\nSee also 'scontrol --help' and 'man scontrol'\n\nConfiguration settings in slurm.conf\n\n\nSeveral slurm.conf settings are available to control the multi-core\nfeatures described above.\n\nIn addition to the description below, also see the \"Task Launch\" and\n\"Resource Selection\" sections if generating slurm.conf\nvia configurator.html.\n\nAs previously mentioned, in order for the affinity to be set, the\ntask/affinity plugin must be first enabled in slurm.conf:\n\n\nTaskPlugin=task/affinity          # enable task affinity\n\nThis setting is part of the task launch specific parameters:\n\n# o Define task launch specific parameters\n#\n#    \"TaskProlog\" : Define a program to be executed as the user before each\n#                   task begins execution.\n#    \"TaskEpilog\" : Define a program to be executed as the user after each\n#                   task terminates.\n#    \"TaskPlugin\" : Define a task launch plugin. This may be used to\n#                   provide resource management within a node (e.g. pinning\n#                   tasks to specific processors). Permissible values are:\n#      \"task/affinity\" : CPU affinity support\n#      \"task/cgroup\"   : bind tasks to resources using Linux cgroup\n#      \"task/none\"     : no task launch actions, the default\n#\n# Example:\n#\n# TaskProlog=/usr/local/slurm/etc/task_prolog # default is none\n# TaskEpilog=/usr/local/slurm/etc/task_epilog # default is none\n# TaskPlugin=task/affinity                    # default is task/none\n\nDeclare the node hardware configuration in slurm.conf:\n\n\nNodeName=dualcore[01-16] CoresPerSocket=2 ThreadsPerCore=1\n\nFor a more complete description of the various node configuration options\nsee the slurm.conf man page.\n\nLast modified 19 May 2023\n"
            },
            {
                "title": "Navigation",
                "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
            },
            {
                "title": "Motivation behind high-level srun flags\n\n",
                "content": "The motivation behind allowing users to use higher level srun\nflags instead of --cpu-bind is that the later can be difficult to use. The\nproposed high-level flags are easier to use than --cpu-bind because:\nAffinity mask generation happens automatically when using the high-level flags. \nThe length and complexity of the --cpu-bind flag vs. the length\nof the combination of -B and --distribution flags make the high-level\nflags much easier to use.\nAlso as illustrated in the example below it is much simpler to specify\na different layout using the high-level flags since users do not have to\nrecalculate mask or CPU IDs. This approach is much simpler than\nrearranging the mask or map.Given a 32-process job and a four node, dual-socket, dual-core\ncluster, we want to use a block distribution across the four nodes and then a\ncyclic distribution within the node across the physical processors. Below we\nshow how to obtain the wanted layout using 1) high-level flags and\n2) --cpubindHigh-Level flags\n\nUsing Slurm's high-level flag, users can obtain the above layout with\neither of the following submissions since --distribution=block:cyclic\nis the default distribution method.\n$ srun -n 32 -N 4 -B 4:2 --distribution=block:cyclic a.out\n\n$ srun -n 32 -N 4 -B 4:2 a.out\nThe cores are shown as c0 and c1 and the processors are shown\nas p0 through p3. The resulting task IDs are: \n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\nThe computation and assignment of the task IDs is transparent\nto the user. Users don't have to worry about the core numbering (Section\nPinning processes to cores) or any setting any CPU affinities. By default CPU affinity\nwill be set when using multi-core supporting flags. Low-level flag --cpu-bind\n\nUsing Slurm's --cpu-bind flag, users must compute the CPU IDs or\nmasks as well as make sure they understand the core numbering on their\nsystem. Another problem arises when core numbering is not the same on all\nnodes. The --cpu-bind option only allows users to specify a single\nmask for all the nodes. Using Slurm high-level flags remove this limitation\nsince Slurm will correctly generate the appropriate masks for each requested nodes.\nOn a four dual-socket dual-core node cluster with core block numbering\n\nThe cores are shown as c0 and c1 and the processors are shown\nas p0 through p3. The CPU IDs within a node in the block numbering are:\n(this information is available from the /proc/cpuinfo file on the system)\n\n\n\n \nc0c1\np0 0  1 \np2 4  5 \n\n\n\n\n\n\n \nc0c1\np1 2  3 \np3 6  7 \n\n\n\n\u00a0resulting in the following mapping for processor/cores\nand task IDs which users need to calculate: \nmapping for processors/cores\n\n\n\n\n \nc0c1\np0 0x01  0x02 \np2 0x10  0x20 \n\n\n\n\n\n\n \nc0c1\np1 0x04  0x08 \np3 0x40  0x80 \n\n\n\n\n\ntask IDs\n\n\n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\n\nThe above maps and task IDs can be translated into the\nfollowing command:\n\n$ srun -n 32 -N 4 --cpu-bind=mask_cpu:1,4,10,40,2,8,20,80 a.out\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0or\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,2,4,6,1,3,5,7 a.out\n\n\nSame cluster but with its core numbered cyclic instead of block\n\n\nOn a system with cyclically numbered cores, the correct mask\nargument to the srun command looks like: (this will\nachieve the same layout as the command above on a system with core block\nnumbering.)\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,1,2,3,4,5,6,7 a.out\n\nBlock map_cpu on a system with cyclic core numbering\n\n\nIf users do not check their system's core numbering before specifying\nthe map_cpu list and thereby do not realize that the system has cyclic core\nnumbering instead of block numbering then they will not get the expected\nlayout. For example, if they decide to re-use their command from above:\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,2,4,6,1,3,5,7 a.out\n\nthey get the following unintentional task ID layout:\n\n\n\n\n \nc0c1\np0 0  2 \np2 1  3 \n\n\n\n\n\n\n \nc0c1\np1 4  6 \np3 5  7 \n\n\n\n\nsince the processor IDs within a node in the cyclic numbering are:\n\n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\n\nThe important conclusion is that using the --cpu-bind flag is not\ntrivial and that it assumes that users are experts.\n\nExtensions to sinfo/squeue/scontrol\n\n\nSeveral extensions have also been made to the other Slurm utilities to\nmake working with multi-core/multi-threaded systems easier.\n\nsinfo\nThe long version (-l) of the sinfo node listing (-N) has been\nextended to display the sockets, cores, and threads present for each\nnode.  For example:\n\n\n% sinfo -N\nNODELIST     NODES PARTITION STATE\nhydra[12-15]     4    parts* idle\n\n% sinfo -lN\nThu Sep 14 17:47:13 2006\nNODELIST     NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT FEATURES REASON\nhydra[12-15]     4    parts*        idle   8+ 2+:4+:1+   2007    41447      1   (null) none\n\n% sinfo -lNe\nThu Sep 14 17:47:18 2006\nNODELIST     NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT FEATURES REASON\n\nhydra[12-14]     3    parts*        idle    8    2:4:1   2007    41447      1   (null) none\nhydra15          1    parts*        idle   64    8:4:2   2007    41447      1   (null) none\n\nFor user specified output formats (-o/--format) and sorting (-S/--sort),\nthe following identifiers are available:\n\n%X  Number of sockets per node\n%Y  Number of cores per socket\n%Z  Number of threads per core\n%z  Extended processor information: number of\n    sockets, core, threads (S:C:T) per node\n\nFor example:\n\n% sinfo -o '%9P %4c %8z %8X %8Y %8Z'\nPARTITION CPUS S:C:T    SOCKETS  CORES    THREADS\nparts*    4    2:2:1    2        2        1\n\nSee also 'sinfo --help' and 'man sinfo'\n\nsqueue\nFor user specified output formats (-o/--format) and sorting (-S/--sort),\nthe following identifiers are available:\n\n%m  Size of memory (in MB) requested by the job\n%H  Number of requested sockets per node\n%I  Number of requested cores per socket\n%J  Number of requested threads per core\n%z  Extended processor information: number of requested\n    sockets, cores, threads (S:C:T) per node\n\nBelow is an example squeue output after running 7 copies of:\n\n\n% srun -n 4 -B 2:2:1 --mem=1024 sleep 100 &\n\n\n\n% squeue -o '%.5i %.2t %.4M %.5D %7H %6I %7J %6z %R'\nJOBID ST TIME NODES SOCKETS CORES THREADS S:C:T NODELIST(REASON)\n   17 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   18 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   19 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   13  R 1:27     1 2       2     1       2:2:1 hydra12\n   14  R 1:26     1 2       2     1       2:2:1 hydra13\n   15  R 1:26     1 2       2     1       2:2:1 hydra14\n   16  R 1:26     1 2       2     1       2:2:1 hydra15\n\nThe squeue command can also display the memory size of jobs, for example:\n\n% sbatch --mem=123 tmp\nSubmitted batch job 24\n\n$ squeue -o \"%.5i %.2t %.4M %.5D %m\"\nJOBID ST TIME NODES MIN_MEMORY\n  24   R 0:05     1 123\n\nSee also 'squeue --help' and 'man squeue'\n\nscontrol\nThe following job settings can be adjusted using scontrol:\n\n\nRequested Allocation:\n  ReqSockets=<count>  Set the job's count of required sockets\n  ReqCores=<count>    Set the job's count of required cores\n  ReqThreads=<count>  Set the job's count of required threads\n\nFor example:\n\n# scontrol update JobID=17 ReqThreads=2\n# scontrol update JobID=18 ReqCores=4\n# scontrol update JobID=19 ReqSockets=8\n\n% squeue -o '%.5i %.2t %.4M %.5D %9c %7H %6I %8J'\nJOBID ST TIME NODES MIN_PROCS SOCKETS CORES THREADS\n   17 PD 0:00     1 1         4       2     1\n   18 PD 0:00     1 1         8       4     2\n   19 PD 0:00     1 1         4       2     1\n   13  R 1:35     1 0         0       0     0\n   14  R 1:34     1 0         0       0     0\n   15  R 1:34     1 0         0       0     0\n   16  R 1:34     1 0         0       0     0\n\nThe 'scontrol show job' command can be used to display\nthe number of allocated CPUs per node as well as the socket, cores,\nand threads specified in the request and constraints.\n\n\n% srun -N 2 -B 2:1 sleep 100 &\n% scontrol show job 20\nJobId=20 UserId=(30352) GroupId=users(1051)\n   Name=sleep\n   Priority=4294901749 Partition=parts BatchFlag=0\n   AllocNode:Sid=hydra16:3892 TimeLimit=UNLIMITED\n   JobState=RUNNING StartTime=09/25-17:17:30 EndTime=NONE\n   NodeList=hydra[12-14] NodeListIndices=0,2,-1\n   AllocCPUs=1,2,1\n   NumCPUs=4 ReqNodes=2 ReqS:C:T=2:1:*\n   OverSubscribe=0 Contiguous=0 CPUs/task=0\n   MinCPUs=0 MinMemory=0 MinTmpDisk=0 Features=(null)\n   Dependency=0 Account=(null) Reason=None Network=(null)\n   ReqNodeList=(null) ReqNodeListIndices=-1\n   ExcNodeList=(null) ExcNodeListIndices=-1\n   SubmitTime=09/25-17:17:30 SuspendTime=None PreSusTime=0\n\nSee also 'scontrol --help' and 'man scontrol'\n\nConfiguration settings in slurm.conf\n\n\nSeveral slurm.conf settings are available to control the multi-core\nfeatures described above.\n\nIn addition to the description below, also see the \"Task Launch\" and\n\"Resource Selection\" sections if generating slurm.conf\nvia configurator.html.\n\nAs previously mentioned, in order for the affinity to be set, the\ntask/affinity plugin must be first enabled in slurm.conf:\n\n\nTaskPlugin=task/affinity          # enable task affinity\n\nThis setting is part of the task launch specific parameters:\n\n# o Define task launch specific parameters\n#\n#    \"TaskProlog\" : Define a program to be executed as the user before each\n#                   task begins execution.\n#    \"TaskEpilog\" : Define a program to be executed as the user after each\n#                   task terminates.\n#    \"TaskPlugin\" : Define a task launch plugin. This may be used to\n#                   provide resource management within a node (e.g. pinning\n#                   tasks to specific processors). Permissible values are:\n#      \"task/affinity\" : CPU affinity support\n#      \"task/cgroup\"   : bind tasks to resources using Linux cgroup\n#      \"task/none\"     : no task launch actions, the default\n#\n# Example:\n#\n# TaskProlog=/usr/local/slurm/etc/task_prolog # default is none\n# TaskEpilog=/usr/local/slurm/etc/task_epilog # default is none\n# TaskPlugin=task/affinity                    # default is task/none\n\nDeclare the node hardware configuration in slurm.conf:\n\n\nNodeName=dualcore[01-16] CoresPerSocket=2 ThreadsPerCore=1\n\nFor a more complete description of the various node configuration options\nsee the slurm.conf man page.\n\nLast modified 19 May 2023\n"
            },
            {
                "title": "Extensions to sinfo/squeue/scontrol\n\n",
                "content": "Several extensions have also been made to the other Slurm utilities to\nmake working with multi-core/multi-threaded systems easier.sinfoThe long version (-l) of the sinfo node listing (-N) has been\nextended to display the sockets, cores, and threads present for each\nnode.  For example:\n\n\n% sinfo -N\nNODELIST     NODES PARTITION STATE\nhydra[12-15]     4    parts* idle\n\n% sinfo -lN\nThu Sep 14 17:47:13 2006\nNODELIST     NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT FEATURES REASON\nhydra[12-15]     4    parts*        idle   8+ 2+:4+:1+   2007    41447      1   (null) none\n\n% sinfo -lNe\nThu Sep 14 17:47:18 2006\nNODELIST     NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT FEATURES REASON\n\nhydra[12-14]     3    parts*        idle    8    2:4:1   2007    41447      1   (null) none\nhydra15          1    parts*        idle   64    8:4:2   2007    41447      1   (null) none\n\nFor user specified output formats (-o/--format) and sorting (-S/--sort),\nthe following identifiers are available:\n\n%X  Number of sockets per node\n%Y  Number of cores per socket\n%Z  Number of threads per core\n%z  Extended processor information: number of\n    sockets, core, threads (S:C:T) per node\n\nFor example:\n\n% sinfo -o '%9P %4c %8z %8X %8Y %8Z'\nPARTITION CPUS S:C:T    SOCKETS  CORES    THREADS\nparts*    4    2:2:1    2        2        1\n\nSee also 'sinfo --help' and 'man sinfo'\n\nsqueue\nFor user specified output formats (-o/--format) and sorting (-S/--sort),\nthe following identifiers are available:\n\n%m  Size of memory (in MB) requested by the job\n%H  Number of requested sockets per node\n%I  Number of requested cores per socket\n%J  Number of requested threads per core\n%z  Extended processor information: number of requested\n    sockets, cores, threads (S:C:T) per node\n\nBelow is an example squeue output after running 7 copies of:\n\n\n% srun -n 4 -B 2:2:1 --mem=1024 sleep 100 &\n\n\n\n% squeue -o '%.5i %.2t %.4M %.5D %7H %6I %7J %6z %R'\nJOBID ST TIME NODES SOCKETS CORES THREADS S:C:T NODELIST(REASON)\n   17 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   18 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   19 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   13  R 1:27     1 2       2     1       2:2:1 hydra12\n   14  R 1:26     1 2       2     1       2:2:1 hydra13\n   15  R 1:26     1 2       2     1       2:2:1 hydra14\n   16  R 1:26     1 2       2     1       2:2:1 hydra15\n\nThe squeue command can also display the memory size of jobs, for example:\n\n% sbatch --mem=123 tmp\nSubmitted batch job 24\n\n$ squeue -o \"%.5i %.2t %.4M %.5D %m\"\nJOBID ST TIME NODES MIN_MEMORY\n  24   R 0:05     1 123\n\nSee also 'squeue --help' and 'man squeue'\n\nscontrol\nThe following job settings can be adjusted using scontrol:\n\n\nRequested Allocation:\n  ReqSockets=<count>  Set the job's count of required sockets\n  ReqCores=<count>    Set the job's count of required cores\n  ReqThreads=<count>  Set the job's count of required threads\n\nFor example:\n\n# scontrol update JobID=17 ReqThreads=2\n# scontrol update JobID=18 ReqCores=4\n# scontrol update JobID=19 ReqSockets=8\n\n% squeue -o '%.5i %.2t %.4M %.5D %9c %7H %6I %8J'\nJOBID ST TIME NODES MIN_PROCS SOCKETS CORES THREADS\n   17 PD 0:00     1 1         4       2     1\n   18 PD 0:00     1 1         8       4     2\n   19 PD 0:00     1 1         4       2     1\n   13  R 1:35     1 0         0       0     0\n   14  R 1:34     1 0         0       0     0\n   15  R 1:34     1 0         0       0     0\n   16  R 1:34     1 0         0       0     0\n\nThe 'scontrol show job' command can be used to display\nthe number of allocated CPUs per node as well as the socket, cores,\nand threads specified in the request and constraints.\n\n\n% srun -N 2 -B 2:1 sleep 100 &\n% scontrol show job 20\nJobId=20 UserId=(30352) GroupId=users(1051)\n   Name=sleep\n   Priority=4294901749 Partition=parts BatchFlag=0\n   AllocNode:Sid=hydra16:3892 TimeLimit=UNLIMITED\n   JobState=RUNNING StartTime=09/25-17:17:30 EndTime=NONE\n   NodeList=hydra[12-14] NodeListIndices=0,2,-1\n   AllocCPUs=1,2,1\n   NumCPUs=4 ReqNodes=2 ReqS:C:T=2:1:*\n   OverSubscribe=0 Contiguous=0 CPUs/task=0\n   MinCPUs=0 MinMemory=0 MinTmpDisk=0 Features=(null)\n   Dependency=0 Account=(null) Reason=None Network=(null)\n   ReqNodeList=(null) ReqNodeListIndices=-1\n   ExcNodeList=(null) ExcNodeListIndices=-1\n   SubmitTime=09/25-17:17:30 SuspendTime=None PreSusTime=0\n\nSee also 'scontrol --help' and 'man scontrol'\n\nConfiguration settings in slurm.conf\n\n\nSeveral slurm.conf settings are available to control the multi-core\nfeatures described above.\n\nIn addition to the description below, also see the \"Task Launch\" and\n\"Resource Selection\" sections if generating slurm.conf\nvia configurator.html.\n\nAs previously mentioned, in order for the affinity to be set, the\ntask/affinity plugin must be first enabled in slurm.conf:\n\n\nTaskPlugin=task/affinity          # enable task affinity\n\nThis setting is part of the task launch specific parameters:\n\n# o Define task launch specific parameters\n#\n#    \"TaskProlog\" : Define a program to be executed as the user before each\n#                   task begins execution.\n#    \"TaskEpilog\" : Define a program to be executed as the user after each\n#                   task terminates.\n#    \"TaskPlugin\" : Define a task launch plugin. This may be used to\n#                   provide resource management within a node (e.g. pinning\n#                   tasks to specific processors). Permissible values are:\n#      \"task/affinity\" : CPU affinity support\n#      \"task/cgroup\"   : bind tasks to resources using Linux cgroup\n#      \"task/none\"     : no task launch actions, the default\n#\n# Example:\n#\n# TaskProlog=/usr/local/slurm/etc/task_prolog # default is none\n# TaskEpilog=/usr/local/slurm/etc/task_epilog # default is none\n# TaskPlugin=task/affinity                    # default is task/none\n\nDeclare the node hardware configuration in slurm.conf:\n\n\nNodeName=dualcore[01-16] CoresPerSocket=2 ThreadsPerCore=1\n\nFor a more complete description of the various node configuration options\nsee the slurm.conf man page.\n\nLast modified 19 May 2023\n"
            },
            {
                "title": "Configuration settings in slurm.conf\n\n",
                "content": "Several slurm.conf settings are available to control the multi-core\nfeatures described above.\n\nIn addition to the description below, also see the \"Task Launch\" and\n\"Resource Selection\" sections if generating slurm.conf\nvia configurator.html.\n\nAs previously mentioned, in order for the affinity to be set, the\ntask/affinity plugin must be first enabled in slurm.conf:\n\n\nTaskPlugin=task/affinity          # enable task affinity\n\nThis setting is part of the task launch specific parameters:\n\n# o Define task launch specific parameters\n#\n#    \"TaskProlog\" : Define a program to be executed as the user before each\n#                   task begins execution.\n#    \"TaskEpilog\" : Define a program to be executed as the user after each\n#                   task terminates.\n#    \"TaskPlugin\" : Define a task launch plugin. This may be used to\n#                   provide resource management within a node (e.g. pinning\n#                   tasks to specific processors). Permissible values are:\n#      \"task/affinity\" : CPU affinity support\n#      \"task/cgroup\"   : bind tasks to resources using Linux cgroup\n#      \"task/none\"     : no task launch actions, the default\n#\n# Example:\n#\n# TaskProlog=/usr/local/slurm/etc/task_prolog # default is none\n# TaskEpilog=/usr/local/slurm/etc/task_epilog # default is none\n# TaskPlugin=task/affinity                    # default is task/none\n\nDeclare the node hardware configuration in slurm.conf:\n\n\nNodeName=dualcore[01-16] CoresPerSocket=2 ThreadsPerCore=1\n\nFor a more complete description of the various node configuration options\nsee the slurm.conf man page.\n\nLast modified 19 May 2023\n"
            },
            {
                "title": "Low-level --cpu-bind=... - Explicit binding interface\n\n",
                "content": "The following srun flag provides a low-level core binding interface:\n--cpu-bind=        Bind tasks to CPUs\n    q[uiet]         quietly bind before task runs (default)\n    v[erbose]       verbosely report binding before task runs\n    no[ne]          don't bind tasks to CPUs (default)\n    rank            bind by task rank\n    map_cpu:<list>  specify a CPU ID binding for each task\n                    where <list> is\n                    <cpuid1>,<cpuid2>,...<cpuidN>\n    mask_cpu:<list> specify a CPU ID binding mask for each\n                    task where <list> is\n                    <mask1>,<mask2>,...<maskN>\n    rank_ldom       bind task by rank to CPUs in a NUMA\n                    locality domain\n    map_ldom:<list> specify a NUMA locality domain ID\n                    for each task where <list> is\n                    <ldom1>,<ldom2>,...<ldomN>\n    rank_ldom       bind task by rank to CPUs in a NUMA\n                    locality domain where <list> is\n                    <ldom1>,<ldom2>,...<ldomN>\n    mask_ldom:<list> specify a NUMA locality domain ID mask\n                    for each task where <list> is\n                    <ldom1>,<ldom2>,...<ldomN>\n    ldoms           auto-generated masks bind to NUMA locality\n                    domains\n    sockets         auto-generated masks bind to sockets\n    cores           auto-generated masks bind to cores\n    threads         auto-generated masks bind to threads\n    help            show this help message\n The affinity can be either set to either a specific logical processor\n(socket, core, threads) or at a coarser granularity than the lowest level\nof logical processor (core or thread).\nIn the later case the processes are allowed to utilize multiple processors\nwithin a specific socket or core.\n\nExamples:\n\n\n srun -n 8 -N 4 --cpu-bind=mask_cpu:0x1,0x4 a.out\n srun -n 8 -N 4 --cpu-bind=mask_cpu:0x3,0xD a.out\n\n\nSee also 'srun --cpu-bind=help' and 'man srun'\n\nHigh-level -B S[:C[:T]] - Automatic mask generation interface\n\n\nWe have updated the node\nselection infrastructure with a mechanism that allows selection of logical\nprocessors at a finer granularity. Users are able to request a specific number\nof nodes, sockets,\u00a0 cores, and threads:\n\n-B, --extra-node-info=S[:C[:T]]            Expands to:\n  --sockets-per-node=S   number of sockets per node to allocate\n  --cores-per-socket=C   number of cores per socket to allocate\n  --threads-per-core=T   number of threads per core to allocate\n                each field can be 'min' or wildcard '*'\n\n     Total cpus requested = (Nodes) x (S x C x T)\n\nExamples:\n\n\n\n srun -n 8 -N 4 -B 2:1 a.out\n srun -n 8 -N 4 -B 2   a.out\n\nnote: compare the above with the previous corresponding --cpu-bind=... examples\n\n srun -n 16 -N 4 a.out\n srun -n 16 -N 4 -B 2:2:1 a.out\n srun -n 16 -N 4 -B 2:2:1 a.out\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0or\n srun -n 16 -N 4 --sockets-per-node=2 --cores-per-socket=2 --threads-per-core=1 a.out\n srun -n 16 -N 2-4 -B '1:*:1' a.out\n srun -n 16 -N 4-2 -B '2:*:1' a.out\n srun -n 16 -N 4-4 -B '1:1' a.out\n\n\nNotes:\n\n Adding --cpu-bind=no to the command line will cause the processes\n      to not be bound the logical processors.\n  Adding --cpu-bind=verbose to the command line (or setting the\n      CPU_BIND environment variable to \"verbose\") will cause each task\n      to report the affinity mask in use\n  Binding is on by default when -B is used. The default binding on\n      multi-core/multi-threaded systems is equivalent to the level of\n      resource enumerated in the -B option.\n\nSee also 'srun --help' and 'man srun'\nTask distribution options: Extensions to -m / --distribution\n\n\nThe -m / --distribution option for distributing processes across nodes\nhas been extended to also describe the distribution within the lowest level\nof logical processors.\nAvailable distributions include:\n\narbitrary | block | cyclic | plane=x | [block|cyclic]:[block|cyclic|fcyclic]\n\nThe  plane distribution (plane=x)\nresults in a block:cyclic distribution of blocksize equal to x.\nIn the following we use \"lowest level of logical processors\"\nto describe sockets, cores or threads depending of the architecture.\nThe distribution divides\nthe cluster into planes (including a number of the lowest level of logical\nprocessors on each node) and then schedule first within each plane and then\nacross planes.\nFor the two dimensional distributions ([block|cyclic]:[block|cyclic|fcyclic]),\nthe second distribution (after \":\") allows users to specify a distribution\nmethod for processes within a node and applies to the lowest level of logical\nprocessors (sockets, core or thread depending on the architecture).\nWhen a task requires more than one CPU, the cyclic will allocate all\nof those CPUs as a group (i.e. within the same socket if possible) while \nfcyclic would distribute each of those CPU of the in a cyclic fashion\nacross sockets.\nThe binding is enabled automatically when high level flags are used as long\nas the task/affinity plug-in is enabled. To disable binding at the job level\nuse --cpu-bind=no.\nThe distribution flags can be combined with the other switches:\n\n\n\nsrun -n 16 -N 4 -B '2:*:1' -m block:cyclic --cpu-bind=socket a.out\nsrun -n 16 -N 4 -B '2:*:1' -m plane=2 --cpu-bind=core a.out\nsrun -n 16 -N 4 -B '2:*:1' -m plane=2 a.out\n\n\nThe default distribution on multi-core/multi-threaded systems is equivalent\nto -m block:cyclic with --cpu-bind=thread.\nSee also 'srun --help'\nMemory as a Consumable Resource\n\n\nThe --mem flag specifies the maximum amount of memory in MB\nneeded by the job per node.  This flag is used to support the memory\nas a consumable resource allocation strategy.\n\n--mem=MB      maximum amount of real memory per node\n              required by the job.\n\nThis flag allows the scheduler to co-allocate jobs on specific nodes\ngiven that their added memory requirement do not exceed the total amount\nof memory on the nodes.\nIn order to use memory as a consumable resource, the select/cons_tres\nplugin must be first enabled in slurm.conf:\n\nSelectType=select/cons_tres     # enable consumable resources\nSelectTypeParameters=CR_Memory  # memory as a consumable resource\n\n Using memory as a consumable resource is typically combined with\nthe CPU, Socket, or Core consumable resources using SelectTypeParameters\nvalues of: CR_CPU_Memory, CR_Socket_Memory or CR_Core_Memory\n\nSee the \"Resource Selection\" section if generating slurm.conf\nvia configurator.html.\n\nSee also 'srun --help' and 'man srun'\nTask invocation as a function of logical processors\n\n\nThe --ntasks-per-{node,socket,core}=ntasks flags\nallow the user to request that no more than ntasks\nbe invoked on each node, socket, or core.\nThis is similar to using --cpus-per-task=ncpus\nbut does not require knowledge of the actual number of cpus on\neach node.  In some cases, it is more convenient to be able to\nrequest that no more than a specific number of ntasks be invoked\non each node, socket, or core.  Examples of this include submitting\nan app where only one \"task/rank\" should be\nassigned to each node while allowing the job to utilize\nall of the parallelism present in the node, or submitting a single\nsetup/cleanup/monitoring job to each node of a pre-existing\nallocation as one step in a larger job script.\nThis can now be specified via the following flags:\n\n--ntasks-per-node=n    number of tasks to invoke on each node\n--ntasks-per-socket=n  number of tasks to invoke on each socket\n--ntasks-per-core=n    number of tasks to invoke on each core\n\nFor example, given a cluster with nodes containing two sockets,\neach containing two cores, the following commands illustrate the\nbehavior of these flags:\n\n% srun -n 4 hostname\nhydra12\nhydra12\nhydra12\nhydra12\n% srun -n 4 --ntasks-per-node=1 hostname\nhydra12\nhydra13\nhydra14\nhydra15\n% srun -n 4 --ntasks-per-node=2 hostname\nhydra12\nhydra12\nhydra13\nhydra13\n% srun -n 4 --ntasks-per-socket=1 hostname\nhydra12\nhydra12\nhydra13\nhydra13\n% srun -n 4 --ntasks-per-core=1 hostname\nhydra12\nhydra12\nhydra12\nhydra12\n\nSee also 'srun --help' and 'man srun'\nApplication hints\n\n\nDifferent applications will have various levels of resource\nrequirements. Some applications tend to be computationally intensive\nbut require little to no inter-process communication. Some applications\nwill be memory bound, saturating the memory bandwidth of a processor\nbefore exhausting the computational capabilities. Other applications\nwill be highly communication intensive causing processes to block\nawaiting messages from other processes. Applications with these\ndifferent properties tend to run well on a multi-core system given\nthe right mappings.\nFor computationally intensive applications, all cores in a multi-core\nsystem would normally be used. For memory bound applications, only\nusing a single core on each socket will result in the highest per\ncore memory bandwidth. For communication intensive applications,\nusing in-core multi-threading (e.g. hyperthreading, SMT, or TMT)\nmay also improve performance.\nThe following command line flags can be used to communicate these\ntypes of application hints to the Slurm multi-core support:\n\n--hint=             Bind tasks according to application hints\n    compute_bound   use all cores in each socket\n    memory_bound    use only one core in each socket\n    [no]multithread [don't] use extra threads with in-core multi-threading\n    help            show this help message\n\nFor example, given a cluster with nodes containing two sockets,\neach containing two cores, the following commands illustrate the\nbehavior of these flags.  In the verbose --cpu-bind output, tasks\nare described as 'hostname, task Global_ID Local_ID [PID]':\n\n% srun -n 4 --hint=compute_bound --cpu-bind=verbose sleep 1\ncpu-bind=MASK - hydra12, task  0  0 [15425]: mask 0x1 set\ncpu-bind=MASK - hydra12, task  1  1 [15426]: mask 0x4 set\ncpu-bind=MASK - hydra12, task  2  2 [15427]: mask 0x2 set\ncpu-bind=MASK - hydra12, task  3  3 [15428]: mask 0x8 set\n\n% srun -n 4 --hint=memory_bound --cpu-bind=verbose sleep 1\ncpu-bind=MASK - hydra12, task  0  0 [15550]: mask 0x1 set\ncpu-bind=MASK - hydra12, task  1  1 [15551]: mask 0x4 set\ncpu-bind=MASK - hydra13, task  2  0 [14974]: mask 0x1 set\ncpu-bind=MASK - hydra13, task  3  1 [14975]: mask 0x4 set\n\nSee also 'srun --hint=help' and 'man srun'\n\nMotivation behind high-level srun flags\n\n\nThe motivation behind allowing users to use higher level srun\nflags instead of --cpu-bind is that the later can be difficult to use. The\nproposed high-level flags are easier to use than --cpu-bind because:\n\nAffinity mask generation happens automatically when using the high-level flags. \nThe length and complexity of the --cpu-bind flag vs. the length\nof the combination of -B and --distribution flags make the high-level\nflags much easier to use.\n\nAlso as illustrated in the example below it is much simpler to specify\na different layout using the high-level flags since users do not have to\nrecalculate mask or CPU IDs. This approach is much simpler than\nrearranging the mask or map.\nGiven a 32-process job and a four node, dual-socket, dual-core\ncluster, we want to use a block distribution across the four nodes and then a\ncyclic distribution within the node across the physical processors. Below we\nshow how to obtain the wanted layout using 1) high-level flags and\n2) --cpubind\nHigh-Level flags\n\n\nUsing Slurm's high-level flag, users can obtain the above layout with\neither of the following submissions since --distribution=block:cyclic\nis the default distribution method.\n\n$ srun -n 32 -N 4 -B 4:2 --distribution=block:cyclic a.out\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0or\n\n$ srun -n 32 -N 4 -B 4:2 a.out\n\nThe cores are shown as c0 and c1 and the processors are shown\nas p0 through p3. The resulting task IDs are: \n\n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\n\nThe computation and assignment of the task IDs is transparent\nto the user. Users don't have to worry about the core numbering (Section\nPinning processes to cores) or any setting any CPU affinities. By default CPU affinity\nwill be set when using multi-core supporting flags. \nLow-level flag --cpu-bind\n\n\nUsing Slurm's --cpu-bind flag, users must compute the CPU IDs or\nmasks as well as make sure they understand the core numbering on their\nsystem. Another problem arises when core numbering is not the same on all\nnodes. The --cpu-bind option only allows users to specify a single\nmask for all the nodes. Using Slurm high-level flags remove this limitation\nsince Slurm will correctly generate the appropriate masks for each requested nodes.\n\nOn a four dual-socket dual-core node cluster with core block numbering\n\n\nThe cores are shown as c0 and c1 and the processors are shown\nas p0 through p3. The CPU IDs within a node in the block numbering are:\n(this information is available from the /proc/cpuinfo file on the system)\n\n\n\n\n \nc0c1\np0 0  1 \np2 4  5 \n\n\n\n\n\n\n \nc0c1\np1 2  3 \np3 6  7 \n\n\n\n\n\u00a0resulting in the following mapping for processor/cores\nand task IDs which users need to calculate: \n\nmapping for processors/cores\n\n\n\n\n\n \nc0c1\np0 0x01  0x02 \np2 0x10  0x20 \n\n\n\n\n\n\n \nc0c1\np1 0x04  0x08 \np3 0x40  0x80 \n\n\n\n\n\n\ntask IDs\n\n\n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\n\nThe above maps and task IDs can be translated into the\nfollowing command:\n\n$ srun -n 32 -N 4 --cpu-bind=mask_cpu:1,4,10,40,2,8,20,80 a.out\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0or\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,2,4,6,1,3,5,7 a.out\n\n\nSame cluster but with its core numbered cyclic instead of block\n\n\nOn a system with cyclically numbered cores, the correct mask\nargument to the srun command looks like: (this will\nachieve the same layout as the command above on a system with core block\nnumbering.)\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,1,2,3,4,5,6,7 a.out\n\nBlock map_cpu on a system with cyclic core numbering\n\n\nIf users do not check their system's core numbering before specifying\nthe map_cpu list and thereby do not realize that the system has cyclic core\nnumbering instead of block numbering then they will not get the expected\nlayout. For example, if they decide to re-use their command from above:\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,2,4,6,1,3,5,7 a.out\n\nthey get the following unintentional task ID layout:\n\n\n\n\n \nc0c1\np0 0  2 \np2 1  3 \n\n\n\n\n\n\n \nc0c1\np1 4  6 \np3 5  7 \n\n\n\n\nsince the processor IDs within a node in the cyclic numbering are:\n\n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\n\nThe important conclusion is that using the --cpu-bind flag is not\ntrivial and that it assumes that users are experts.\n\nExtensions to sinfo/squeue/scontrol\n\n\nSeveral extensions have also been made to the other Slurm utilities to\nmake working with multi-core/multi-threaded systems easier.\n\nsinfo\nThe long version (-l) of the sinfo node listing (-N) has been\nextended to display the sockets, cores, and threads present for each\nnode.  For example:\n\n\n% sinfo -N\nNODELIST     NODES PARTITION STATE\nhydra[12-15]     4    parts* idle\n\n% sinfo -lN\nThu Sep 14 17:47:13 2006\nNODELIST     NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT FEATURES REASON\nhydra[12-15]     4    parts*        idle   8+ 2+:4+:1+   2007    41447      1   (null) none\n\n% sinfo -lNe\nThu Sep 14 17:47:18 2006\nNODELIST     NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT FEATURES REASON\n\nhydra[12-14]     3    parts*        idle    8    2:4:1   2007    41447      1   (null) none\nhydra15          1    parts*        idle   64    8:4:2   2007    41447      1   (null) none\n\nFor user specified output formats (-o/--format) and sorting (-S/--sort),\nthe following identifiers are available:\n\n%X  Number of sockets per node\n%Y  Number of cores per socket\n%Z  Number of threads per core\n%z  Extended processor information: number of\n    sockets, core, threads (S:C:T) per node\n\nFor example:\n\n% sinfo -o '%9P %4c %8z %8X %8Y %8Z'\nPARTITION CPUS S:C:T    SOCKETS  CORES    THREADS\nparts*    4    2:2:1    2        2        1\n\nSee also 'sinfo --help' and 'man sinfo'\n\nsqueue\nFor user specified output formats (-o/--format) and sorting (-S/--sort),\nthe following identifiers are available:\n\n%m  Size of memory (in MB) requested by the job\n%H  Number of requested sockets per node\n%I  Number of requested cores per socket\n%J  Number of requested threads per core\n%z  Extended processor information: number of requested\n    sockets, cores, threads (S:C:T) per node\n\nBelow is an example squeue output after running 7 copies of:\n\n\n% srun -n 4 -B 2:2:1 --mem=1024 sleep 100 &\n\n\n\n% squeue -o '%.5i %.2t %.4M %.5D %7H %6I %7J %6z %R'\nJOBID ST TIME NODES SOCKETS CORES THREADS S:C:T NODELIST(REASON)\n   17 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   18 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   19 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   13  R 1:27     1 2       2     1       2:2:1 hydra12\n   14  R 1:26     1 2       2     1       2:2:1 hydra13\n   15  R 1:26     1 2       2     1       2:2:1 hydra14\n   16  R 1:26     1 2       2     1       2:2:1 hydra15\n\nThe squeue command can also display the memory size of jobs, for example:\n\n% sbatch --mem=123 tmp\nSubmitted batch job 24\n\n$ squeue -o \"%.5i %.2t %.4M %.5D %m\"\nJOBID ST TIME NODES MIN_MEMORY\n  24   R 0:05     1 123\n\nSee also 'squeue --help' and 'man squeue'\n\nscontrol\nThe following job settings can be adjusted using scontrol:\n\n\nRequested Allocation:\n  ReqSockets=<count>  Set the job's count of required sockets\n  ReqCores=<count>    Set the job's count of required cores\n  ReqThreads=<count>  Set the job's count of required threads\n\nFor example:\n\n# scontrol update JobID=17 ReqThreads=2\n# scontrol update JobID=18 ReqCores=4\n# scontrol update JobID=19 ReqSockets=8\n\n% squeue -o '%.5i %.2t %.4M %.5D %9c %7H %6I %8J'\nJOBID ST TIME NODES MIN_PROCS SOCKETS CORES THREADS\n   17 PD 0:00     1 1         4       2     1\n   18 PD 0:00     1 1         8       4     2\n   19 PD 0:00     1 1         4       2     1\n   13  R 1:35     1 0         0       0     0\n   14  R 1:34     1 0         0       0     0\n   15  R 1:34     1 0         0       0     0\n   16  R 1:34     1 0         0       0     0\n\nThe 'scontrol show job' command can be used to display\nthe number of allocated CPUs per node as well as the socket, cores,\nand threads specified in the request and constraints.\n\n\n% srun -N 2 -B 2:1 sleep 100 &\n% scontrol show job 20\nJobId=20 UserId=(30352) GroupId=users(1051)\n   Name=sleep\n   Priority=4294901749 Partition=parts BatchFlag=0\n   AllocNode:Sid=hydra16:3892 TimeLimit=UNLIMITED\n   JobState=RUNNING StartTime=09/25-17:17:30 EndTime=NONE\n   NodeList=hydra[12-14] NodeListIndices=0,2,-1\n   AllocCPUs=1,2,1\n   NumCPUs=4 ReqNodes=2 ReqS:C:T=2:1:*\n   OverSubscribe=0 Contiguous=0 CPUs/task=0\n   MinCPUs=0 MinMemory=0 MinTmpDisk=0 Features=(null)\n   Dependency=0 Account=(null) Reason=None Network=(null)\n   ReqNodeList=(null) ReqNodeListIndices=-1\n   ExcNodeList=(null) ExcNodeListIndices=-1\n   SubmitTime=09/25-17:17:30 SuspendTime=None PreSusTime=0\n\nSee also 'scontrol --help' and 'man scontrol'\n\nConfiguration settings in slurm.conf\n\n\nSeveral slurm.conf settings are available to control the multi-core\nfeatures described above.\n\nIn addition to the description below, also see the \"Task Launch\" and\n\"Resource Selection\" sections if generating slurm.conf\nvia configurator.html.\n\nAs previously mentioned, in order for the affinity to be set, the\ntask/affinity plugin must be first enabled in slurm.conf:\n\n\nTaskPlugin=task/affinity          # enable task affinity\n\nThis setting is part of the task launch specific parameters:\n\n# o Define task launch specific parameters\n#\n#    \"TaskProlog\" : Define a program to be executed as the user before each\n#                   task begins execution.\n#    \"TaskEpilog\" : Define a program to be executed as the user after each\n#                   task terminates.\n#    \"TaskPlugin\" : Define a task launch plugin. This may be used to\n#                   provide resource management within a node (e.g. pinning\n#                   tasks to specific processors). Permissible values are:\n#      \"task/affinity\" : CPU affinity support\n#      \"task/cgroup\"   : bind tasks to resources using Linux cgroup\n#      \"task/none\"     : no task launch actions, the default\n#\n# Example:\n#\n# TaskProlog=/usr/local/slurm/etc/task_prolog # default is none\n# TaskEpilog=/usr/local/slurm/etc/task_epilog # default is none\n# TaskPlugin=task/affinity                    # default is task/none\n\nDeclare the node hardware configuration in slurm.conf:\n\n\nNodeName=dualcore[01-16] CoresPerSocket=2 ThreadsPerCore=1\n\nFor a more complete description of the various node configuration options\nsee the slurm.conf man page.\n\nLast modified 19 May 2023\n"
            },
            {
                "title": "\nHigh-level -B S[:C[:T]] - Automatic mask generation interface\n\n",
                "content": "We have updated the node\nselection infrastructure with a mechanism that allows selection of logical\nprocessors at a finer granularity. Users are able to request a specific number\nof nodes, sockets,\u00a0 cores, and threads:\n-B, --extra-node-info=S[:C[:T]]            Expands to:\n  --sockets-per-node=S   number of sockets per node to allocate\n  --cores-per-socket=C   number of cores per socket to allocate\n  --threads-per-core=T   number of threads per core to allocate\n                each field can be 'min' or wildcard '*'\n\n     Total cpus requested = (Nodes) x (S x C x T)\nExamples:\n\n\n\n srun -n 8 -N 4 -B 2:1 a.out\n srun -n 8 -N 4 -B 2   a.out\n\nnote: compare the above with the previous corresponding --cpu-bind=... examples\n\n srun -n 16 -N 4 a.out\n srun -n 16 -N 4 -B 2:2:1 a.out\n srun -n 16 -N 4 -B 2:2:1 a.out\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0or\n srun -n 16 -N 4 --sockets-per-node=2 --cores-per-socket=2 --threads-per-core=1 a.out\n srun -n 16 -N 2-4 -B '1:*:1' a.out\n srun -n 16 -N 4-2 -B '2:*:1' a.out\n srun -n 16 -N 4-4 -B '1:1' a.out\n\n\nNotes:\n\n Adding --cpu-bind=no to the command line will cause the processes\n      to not be bound the logical processors.\n  Adding --cpu-bind=verbose to the command line (or setting the\n      CPU_BIND environment variable to \"verbose\") will cause each task\n      to report the affinity mask in use\n  Binding is on by default when -B is used. The default binding on\n      multi-core/multi-threaded systems is equivalent to the level of\n      resource enumerated in the -B option.\n\nSee also 'srun --help' and 'man srun'\nTask distribution options: Extensions to -m / --distribution\n\n\nThe -m / --distribution option for distributing processes across nodes\nhas been extended to also describe the distribution within the lowest level\nof logical processors.\nAvailable distributions include:\n\narbitrary | block | cyclic | plane=x | [block|cyclic]:[block|cyclic|fcyclic]\n\nThe  plane distribution (plane=x)\nresults in a block:cyclic distribution of blocksize equal to x.\nIn the following we use \"lowest level of logical processors\"\nto describe sockets, cores or threads depending of the architecture.\nThe distribution divides\nthe cluster into planes (including a number of the lowest level of logical\nprocessors on each node) and then schedule first within each plane and then\nacross planes.\nFor the two dimensional distributions ([block|cyclic]:[block|cyclic|fcyclic]),\nthe second distribution (after \":\") allows users to specify a distribution\nmethod for processes within a node and applies to the lowest level of logical\nprocessors (sockets, core or thread depending on the architecture).\nWhen a task requires more than one CPU, the cyclic will allocate all\nof those CPUs as a group (i.e. within the same socket if possible) while \nfcyclic would distribute each of those CPU of the in a cyclic fashion\nacross sockets.\nThe binding is enabled automatically when high level flags are used as long\nas the task/affinity plug-in is enabled. To disable binding at the job level\nuse --cpu-bind=no.\nThe distribution flags can be combined with the other switches:\n\n\n\nsrun -n 16 -N 4 -B '2:*:1' -m block:cyclic --cpu-bind=socket a.out\nsrun -n 16 -N 4 -B '2:*:1' -m plane=2 --cpu-bind=core a.out\nsrun -n 16 -N 4 -B '2:*:1' -m plane=2 a.out\n\n\nThe default distribution on multi-core/multi-threaded systems is equivalent\nto -m block:cyclic with --cpu-bind=thread.\nSee also 'srun --help'\nMemory as a Consumable Resource\n\n\nThe --mem flag specifies the maximum amount of memory in MB\nneeded by the job per node.  This flag is used to support the memory\nas a consumable resource allocation strategy.\n\n--mem=MB      maximum amount of real memory per node\n              required by the job.\n\nThis flag allows the scheduler to co-allocate jobs on specific nodes\ngiven that their added memory requirement do not exceed the total amount\nof memory on the nodes.\nIn order to use memory as a consumable resource, the select/cons_tres\nplugin must be first enabled in slurm.conf:\n\nSelectType=select/cons_tres     # enable consumable resources\nSelectTypeParameters=CR_Memory  # memory as a consumable resource\n\n Using memory as a consumable resource is typically combined with\nthe CPU, Socket, or Core consumable resources using SelectTypeParameters\nvalues of: CR_CPU_Memory, CR_Socket_Memory or CR_Core_Memory\n\nSee the \"Resource Selection\" section if generating slurm.conf\nvia configurator.html.\n\nSee also 'srun --help' and 'man srun'\nTask invocation as a function of logical processors\n\n\nThe --ntasks-per-{node,socket,core}=ntasks flags\nallow the user to request that no more than ntasks\nbe invoked on each node, socket, or core.\nThis is similar to using --cpus-per-task=ncpus\nbut does not require knowledge of the actual number of cpus on\neach node.  In some cases, it is more convenient to be able to\nrequest that no more than a specific number of ntasks be invoked\non each node, socket, or core.  Examples of this include submitting\nan app where only one \"task/rank\" should be\nassigned to each node while allowing the job to utilize\nall of the parallelism present in the node, or submitting a single\nsetup/cleanup/monitoring job to each node of a pre-existing\nallocation as one step in a larger job script.\nThis can now be specified via the following flags:\n\n--ntasks-per-node=n    number of tasks to invoke on each node\n--ntasks-per-socket=n  number of tasks to invoke on each socket\n--ntasks-per-core=n    number of tasks to invoke on each core\n\nFor example, given a cluster with nodes containing two sockets,\neach containing two cores, the following commands illustrate the\nbehavior of these flags:\n\n% srun -n 4 hostname\nhydra12\nhydra12\nhydra12\nhydra12\n% srun -n 4 --ntasks-per-node=1 hostname\nhydra12\nhydra13\nhydra14\nhydra15\n% srun -n 4 --ntasks-per-node=2 hostname\nhydra12\nhydra12\nhydra13\nhydra13\n% srun -n 4 --ntasks-per-socket=1 hostname\nhydra12\nhydra12\nhydra13\nhydra13\n% srun -n 4 --ntasks-per-core=1 hostname\nhydra12\nhydra12\nhydra12\nhydra12\n\nSee also 'srun --help' and 'man srun'\nApplication hints\n\n\nDifferent applications will have various levels of resource\nrequirements. Some applications tend to be computationally intensive\nbut require little to no inter-process communication. Some applications\nwill be memory bound, saturating the memory bandwidth of a processor\nbefore exhausting the computational capabilities. Other applications\nwill be highly communication intensive causing processes to block\nawaiting messages from other processes. Applications with these\ndifferent properties tend to run well on a multi-core system given\nthe right mappings.\nFor computationally intensive applications, all cores in a multi-core\nsystem would normally be used. For memory bound applications, only\nusing a single core on each socket will result in the highest per\ncore memory bandwidth. For communication intensive applications,\nusing in-core multi-threading (e.g. hyperthreading, SMT, or TMT)\nmay also improve performance.\nThe following command line flags can be used to communicate these\ntypes of application hints to the Slurm multi-core support:\n\n--hint=             Bind tasks according to application hints\n    compute_bound   use all cores in each socket\n    memory_bound    use only one core in each socket\n    [no]multithread [don't] use extra threads with in-core multi-threading\n    help            show this help message\n\nFor example, given a cluster with nodes containing two sockets,\neach containing two cores, the following commands illustrate the\nbehavior of these flags.  In the verbose --cpu-bind output, tasks\nare described as 'hostname, task Global_ID Local_ID [PID]':\n\n% srun -n 4 --hint=compute_bound --cpu-bind=verbose sleep 1\ncpu-bind=MASK - hydra12, task  0  0 [15425]: mask 0x1 set\ncpu-bind=MASK - hydra12, task  1  1 [15426]: mask 0x4 set\ncpu-bind=MASK - hydra12, task  2  2 [15427]: mask 0x2 set\ncpu-bind=MASK - hydra12, task  3  3 [15428]: mask 0x8 set\n\n% srun -n 4 --hint=memory_bound --cpu-bind=verbose sleep 1\ncpu-bind=MASK - hydra12, task  0  0 [15550]: mask 0x1 set\ncpu-bind=MASK - hydra12, task  1  1 [15551]: mask 0x4 set\ncpu-bind=MASK - hydra13, task  2  0 [14974]: mask 0x1 set\ncpu-bind=MASK - hydra13, task  3  1 [14975]: mask 0x4 set\n\nSee also 'srun --hint=help' and 'man srun'\n\nMotivation behind high-level srun flags\n\n\nThe motivation behind allowing users to use higher level srun\nflags instead of --cpu-bind is that the later can be difficult to use. The\nproposed high-level flags are easier to use than --cpu-bind because:\n\nAffinity mask generation happens automatically when using the high-level flags. \nThe length and complexity of the --cpu-bind flag vs. the length\nof the combination of -B and --distribution flags make the high-level\nflags much easier to use.\n\nAlso as illustrated in the example below it is much simpler to specify\na different layout using the high-level flags since users do not have to\nrecalculate mask or CPU IDs. This approach is much simpler than\nrearranging the mask or map.\nGiven a 32-process job and a four node, dual-socket, dual-core\ncluster, we want to use a block distribution across the four nodes and then a\ncyclic distribution within the node across the physical processors. Below we\nshow how to obtain the wanted layout using 1) high-level flags and\n2) --cpubind\nHigh-Level flags\n\n\nUsing Slurm's high-level flag, users can obtain the above layout with\neither of the following submissions since --distribution=block:cyclic\nis the default distribution method.\n\n$ srun -n 32 -N 4 -B 4:2 --distribution=block:cyclic a.out\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0or\n\n$ srun -n 32 -N 4 -B 4:2 a.out\n\nThe cores are shown as c0 and c1 and the processors are shown\nas p0 through p3. The resulting task IDs are: \n\n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\n\nThe computation and assignment of the task IDs is transparent\nto the user. Users don't have to worry about the core numbering (Section\nPinning processes to cores) or any setting any CPU affinities. By default CPU affinity\nwill be set when using multi-core supporting flags. \nLow-level flag --cpu-bind\n\n\nUsing Slurm's --cpu-bind flag, users must compute the CPU IDs or\nmasks as well as make sure they understand the core numbering on their\nsystem. Another problem arises when core numbering is not the same on all\nnodes. The --cpu-bind option only allows users to specify a single\nmask for all the nodes. Using Slurm high-level flags remove this limitation\nsince Slurm will correctly generate the appropriate masks for each requested nodes.\n\nOn a four dual-socket dual-core node cluster with core block numbering\n\n\nThe cores are shown as c0 and c1 and the processors are shown\nas p0 through p3. The CPU IDs within a node in the block numbering are:\n(this information is available from the /proc/cpuinfo file on the system)\n\n\n\n\n \nc0c1\np0 0  1 \np2 4  5 \n\n\n\n\n\n\n \nc0c1\np1 2  3 \np3 6  7 \n\n\n\n\n\u00a0resulting in the following mapping for processor/cores\nand task IDs which users need to calculate: \n\nmapping for processors/cores\n\n\n\n\n\n \nc0c1\np0 0x01  0x02 \np2 0x10  0x20 \n\n\n\n\n\n\n \nc0c1\np1 0x04  0x08 \np3 0x40  0x80 \n\n\n\n\n\n\ntask IDs\n\n\n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\n\nThe above maps and task IDs can be translated into the\nfollowing command:\n\n$ srun -n 32 -N 4 --cpu-bind=mask_cpu:1,4,10,40,2,8,20,80 a.out\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0or\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,2,4,6,1,3,5,7 a.out\n\n\nSame cluster but with its core numbered cyclic instead of block\n\n\nOn a system with cyclically numbered cores, the correct mask\nargument to the srun command looks like: (this will\nachieve the same layout as the command above on a system with core block\nnumbering.)\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,1,2,3,4,5,6,7 a.out\n\nBlock map_cpu on a system with cyclic core numbering\n\n\nIf users do not check their system's core numbering before specifying\nthe map_cpu list and thereby do not realize that the system has cyclic core\nnumbering instead of block numbering then they will not get the expected\nlayout. For example, if they decide to re-use their command from above:\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,2,4,6,1,3,5,7 a.out\n\nthey get the following unintentional task ID layout:\n\n\n\n\n \nc0c1\np0 0  2 \np2 1  3 \n\n\n\n\n\n\n \nc0c1\np1 4  6 \np3 5  7 \n\n\n\n\nsince the processor IDs within a node in the cyclic numbering are:\n\n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\n\nThe important conclusion is that using the --cpu-bind flag is not\ntrivial and that it assumes that users are experts.\n\nExtensions to sinfo/squeue/scontrol\n\n\nSeveral extensions have also been made to the other Slurm utilities to\nmake working with multi-core/multi-threaded systems easier.\n\nsinfo\nThe long version (-l) of the sinfo node listing (-N) has been\nextended to display the sockets, cores, and threads present for each\nnode.  For example:\n\n\n% sinfo -N\nNODELIST     NODES PARTITION STATE\nhydra[12-15]     4    parts* idle\n\n% sinfo -lN\nThu Sep 14 17:47:13 2006\nNODELIST     NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT FEATURES REASON\nhydra[12-15]     4    parts*        idle   8+ 2+:4+:1+   2007    41447      1   (null) none\n\n% sinfo -lNe\nThu Sep 14 17:47:18 2006\nNODELIST     NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT FEATURES REASON\n\nhydra[12-14]     3    parts*        idle    8    2:4:1   2007    41447      1   (null) none\nhydra15          1    parts*        idle   64    8:4:2   2007    41447      1   (null) none\n\nFor user specified output formats (-o/--format) and sorting (-S/--sort),\nthe following identifiers are available:\n\n%X  Number of sockets per node\n%Y  Number of cores per socket\n%Z  Number of threads per core\n%z  Extended processor information: number of\n    sockets, core, threads (S:C:T) per node\n\nFor example:\n\n% sinfo -o '%9P %4c %8z %8X %8Y %8Z'\nPARTITION CPUS S:C:T    SOCKETS  CORES    THREADS\nparts*    4    2:2:1    2        2        1\n\nSee also 'sinfo --help' and 'man sinfo'\n\nsqueue\nFor user specified output formats (-o/--format) and sorting (-S/--sort),\nthe following identifiers are available:\n\n%m  Size of memory (in MB) requested by the job\n%H  Number of requested sockets per node\n%I  Number of requested cores per socket\n%J  Number of requested threads per core\n%z  Extended processor information: number of requested\n    sockets, cores, threads (S:C:T) per node\n\nBelow is an example squeue output after running 7 copies of:\n\n\n% srun -n 4 -B 2:2:1 --mem=1024 sleep 100 &\n\n\n\n% squeue -o '%.5i %.2t %.4M %.5D %7H %6I %7J %6z %R'\nJOBID ST TIME NODES SOCKETS CORES THREADS S:C:T NODELIST(REASON)\n   17 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   18 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   19 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   13  R 1:27     1 2       2     1       2:2:1 hydra12\n   14  R 1:26     1 2       2     1       2:2:1 hydra13\n   15  R 1:26     1 2       2     1       2:2:1 hydra14\n   16  R 1:26     1 2       2     1       2:2:1 hydra15\n\nThe squeue command can also display the memory size of jobs, for example:\n\n% sbatch --mem=123 tmp\nSubmitted batch job 24\n\n$ squeue -o \"%.5i %.2t %.4M %.5D %m\"\nJOBID ST TIME NODES MIN_MEMORY\n  24   R 0:05     1 123\n\nSee also 'squeue --help' and 'man squeue'\n\nscontrol\nThe following job settings can be adjusted using scontrol:\n\n\nRequested Allocation:\n  ReqSockets=<count>  Set the job's count of required sockets\n  ReqCores=<count>    Set the job's count of required cores\n  ReqThreads=<count>  Set the job's count of required threads\n\nFor example:\n\n# scontrol update JobID=17 ReqThreads=2\n# scontrol update JobID=18 ReqCores=4\n# scontrol update JobID=19 ReqSockets=8\n\n% squeue -o '%.5i %.2t %.4M %.5D %9c %7H %6I %8J'\nJOBID ST TIME NODES MIN_PROCS SOCKETS CORES THREADS\n   17 PD 0:00     1 1         4       2     1\n   18 PD 0:00     1 1         8       4     2\n   19 PD 0:00     1 1         4       2     1\n   13  R 1:35     1 0         0       0     0\n   14  R 1:34     1 0         0       0     0\n   15  R 1:34     1 0         0       0     0\n   16  R 1:34     1 0         0       0     0\n\nThe 'scontrol show job' command can be used to display\nthe number of allocated CPUs per node as well as the socket, cores,\nand threads specified in the request and constraints.\n\n\n% srun -N 2 -B 2:1 sleep 100 &\n% scontrol show job 20\nJobId=20 UserId=(30352) GroupId=users(1051)\n   Name=sleep\n   Priority=4294901749 Partition=parts BatchFlag=0\n   AllocNode:Sid=hydra16:3892 TimeLimit=UNLIMITED\n   JobState=RUNNING StartTime=09/25-17:17:30 EndTime=NONE\n   NodeList=hydra[12-14] NodeListIndices=0,2,-1\n   AllocCPUs=1,2,1\n   NumCPUs=4 ReqNodes=2 ReqS:C:T=2:1:*\n   OverSubscribe=0 Contiguous=0 CPUs/task=0\n   MinCPUs=0 MinMemory=0 MinTmpDisk=0 Features=(null)\n   Dependency=0 Account=(null) Reason=None Network=(null)\n   ReqNodeList=(null) ReqNodeListIndices=-1\n   ExcNodeList=(null) ExcNodeListIndices=-1\n   SubmitTime=09/25-17:17:30 SuspendTime=None PreSusTime=0\n\nSee also 'scontrol --help' and 'man scontrol'\n\nConfiguration settings in slurm.conf\n\n\nSeveral slurm.conf settings are available to control the multi-core\nfeatures described above.\n\nIn addition to the description below, also see the \"Task Launch\" and\n\"Resource Selection\" sections if generating slurm.conf\nvia configurator.html.\n\nAs previously mentioned, in order for the affinity to be set, the\ntask/affinity plugin must be first enabled in slurm.conf:\n\n\nTaskPlugin=task/affinity          # enable task affinity\n\nThis setting is part of the task launch specific parameters:\n\n# o Define task launch specific parameters\n#\n#    \"TaskProlog\" : Define a program to be executed as the user before each\n#                   task begins execution.\n#    \"TaskEpilog\" : Define a program to be executed as the user after each\n#                   task terminates.\n#    \"TaskPlugin\" : Define a task launch plugin. This may be used to\n#                   provide resource management within a node (e.g. pinning\n#                   tasks to specific processors). Permissible values are:\n#      \"task/affinity\" : CPU affinity support\n#      \"task/cgroup\"   : bind tasks to resources using Linux cgroup\n#      \"task/none\"     : no task launch actions, the default\n#\n# Example:\n#\n# TaskProlog=/usr/local/slurm/etc/task_prolog # default is none\n# TaskEpilog=/usr/local/slurm/etc/task_epilog # default is none\n# TaskPlugin=task/affinity                    # default is task/none\n\nDeclare the node hardware configuration in slurm.conf:\n\n\nNodeName=dualcore[01-16] CoresPerSocket=2 ThreadsPerCore=1\n\nFor a more complete description of the various node configuration options\nsee the slurm.conf man page.\n\nLast modified 19 May 2023\n"
            },
            {
                "title": "Task distribution options: Extensions to -m / --distribution\n\n",
                "content": "The -m / --distribution option for distributing processes across nodes\nhas been extended to also describe the distribution within the lowest level\nof logical processors.\nAvailable distributions include:\n\narbitrary | block | cyclic | plane=x | [block|cyclic]:[block|cyclic|fcyclic]\nThe  plane distribution (plane=x)\nresults in a block:cyclic distribution of blocksize equal to x.\nIn the following we use \"lowest level of logical processors\"\nto describe sockets, cores or threads depending of the architecture.\nThe distribution divides\nthe cluster into planes (including a number of the lowest level of logical\nprocessors on each node) and then schedule first within each plane and then\nacross planes.For the two dimensional distributions ([block|cyclic]:[block|cyclic|fcyclic]),\nthe second distribution (after \":\") allows users to specify a distribution\nmethod for processes within a node and applies to the lowest level of logical\nprocessors (sockets, core or thread depending on the architecture).\nWhen a task requires more than one CPU, the cyclic will allocate all\nof those CPUs as a group (i.e. within the same socket if possible) while \nfcyclic would distribute each of those CPU of the in a cyclic fashion\nacross sockets.The binding is enabled automatically when high level flags are used as long\nas the task/affinity plug-in is enabled. To disable binding at the job level\nuse --cpu-bind=no.The distribution flags can be combined with the other switches:\n\n\n\nsrun -n 16 -N 4 -B '2:*:1' -m block:cyclic --cpu-bind=socket a.out\nsrun -n 16 -N 4 -B '2:*:1' -m plane=2 --cpu-bind=core a.out\nsrun -n 16 -N 4 -B '2:*:1' -m plane=2 a.out\n\n\nThe default distribution on multi-core/multi-threaded systems is equivalent\nto -m block:cyclic with --cpu-bind=thread.\nSee also 'srun --help'\nMemory as a Consumable Resource\n\n\nThe --mem flag specifies the maximum amount of memory in MB\nneeded by the job per node.  This flag is used to support the memory\nas a consumable resource allocation strategy.\n\n--mem=MB      maximum amount of real memory per node\n              required by the job.\n\nThis flag allows the scheduler to co-allocate jobs on specific nodes\ngiven that their added memory requirement do not exceed the total amount\nof memory on the nodes.\nIn order to use memory as a consumable resource, the select/cons_tres\nplugin must be first enabled in slurm.conf:\n\nSelectType=select/cons_tres     # enable consumable resources\nSelectTypeParameters=CR_Memory  # memory as a consumable resource\n\n Using memory as a consumable resource is typically combined with\nthe CPU, Socket, or Core consumable resources using SelectTypeParameters\nvalues of: CR_CPU_Memory, CR_Socket_Memory or CR_Core_Memory\n\nSee the \"Resource Selection\" section if generating slurm.conf\nvia configurator.html.\n\nSee also 'srun --help' and 'man srun'\nTask invocation as a function of logical processors\n\n\nThe --ntasks-per-{node,socket,core}=ntasks flags\nallow the user to request that no more than ntasks\nbe invoked on each node, socket, or core.\nThis is similar to using --cpus-per-task=ncpus\nbut does not require knowledge of the actual number of cpus on\neach node.  In some cases, it is more convenient to be able to\nrequest that no more than a specific number of ntasks be invoked\non each node, socket, or core.  Examples of this include submitting\nan app where only one \"task/rank\" should be\nassigned to each node while allowing the job to utilize\nall of the parallelism present in the node, or submitting a single\nsetup/cleanup/monitoring job to each node of a pre-existing\nallocation as one step in a larger job script.\nThis can now be specified via the following flags:\n\n--ntasks-per-node=n    number of tasks to invoke on each node\n--ntasks-per-socket=n  number of tasks to invoke on each socket\n--ntasks-per-core=n    number of tasks to invoke on each core\n\nFor example, given a cluster with nodes containing two sockets,\neach containing two cores, the following commands illustrate the\nbehavior of these flags:\n\n% srun -n 4 hostname\nhydra12\nhydra12\nhydra12\nhydra12\n% srun -n 4 --ntasks-per-node=1 hostname\nhydra12\nhydra13\nhydra14\nhydra15\n% srun -n 4 --ntasks-per-node=2 hostname\nhydra12\nhydra12\nhydra13\nhydra13\n% srun -n 4 --ntasks-per-socket=1 hostname\nhydra12\nhydra12\nhydra13\nhydra13\n% srun -n 4 --ntasks-per-core=1 hostname\nhydra12\nhydra12\nhydra12\nhydra12\n\nSee also 'srun --help' and 'man srun'\nApplication hints\n\n\nDifferent applications will have various levels of resource\nrequirements. Some applications tend to be computationally intensive\nbut require little to no inter-process communication. Some applications\nwill be memory bound, saturating the memory bandwidth of a processor\nbefore exhausting the computational capabilities. Other applications\nwill be highly communication intensive causing processes to block\nawaiting messages from other processes. Applications with these\ndifferent properties tend to run well on a multi-core system given\nthe right mappings.\nFor computationally intensive applications, all cores in a multi-core\nsystem would normally be used. For memory bound applications, only\nusing a single core on each socket will result in the highest per\ncore memory bandwidth. For communication intensive applications,\nusing in-core multi-threading (e.g. hyperthreading, SMT, or TMT)\nmay also improve performance.\nThe following command line flags can be used to communicate these\ntypes of application hints to the Slurm multi-core support:\n\n--hint=             Bind tasks according to application hints\n    compute_bound   use all cores in each socket\n    memory_bound    use only one core in each socket\n    [no]multithread [don't] use extra threads with in-core multi-threading\n    help            show this help message\n\nFor example, given a cluster with nodes containing two sockets,\neach containing two cores, the following commands illustrate the\nbehavior of these flags.  In the verbose --cpu-bind output, tasks\nare described as 'hostname, task Global_ID Local_ID [PID]':\n\n% srun -n 4 --hint=compute_bound --cpu-bind=verbose sleep 1\ncpu-bind=MASK - hydra12, task  0  0 [15425]: mask 0x1 set\ncpu-bind=MASK - hydra12, task  1  1 [15426]: mask 0x4 set\ncpu-bind=MASK - hydra12, task  2  2 [15427]: mask 0x2 set\ncpu-bind=MASK - hydra12, task  3  3 [15428]: mask 0x8 set\n\n% srun -n 4 --hint=memory_bound --cpu-bind=verbose sleep 1\ncpu-bind=MASK - hydra12, task  0  0 [15550]: mask 0x1 set\ncpu-bind=MASK - hydra12, task  1  1 [15551]: mask 0x4 set\ncpu-bind=MASK - hydra13, task  2  0 [14974]: mask 0x1 set\ncpu-bind=MASK - hydra13, task  3  1 [14975]: mask 0x4 set\n\nSee also 'srun --hint=help' and 'man srun'\n\nMotivation behind high-level srun flags\n\n\nThe motivation behind allowing users to use higher level srun\nflags instead of --cpu-bind is that the later can be difficult to use. The\nproposed high-level flags are easier to use than --cpu-bind because:\n\nAffinity mask generation happens automatically when using the high-level flags. \nThe length and complexity of the --cpu-bind flag vs. the length\nof the combination of -B and --distribution flags make the high-level\nflags much easier to use.\n\nAlso as illustrated in the example below it is much simpler to specify\na different layout using the high-level flags since users do not have to\nrecalculate mask or CPU IDs. This approach is much simpler than\nrearranging the mask or map.\nGiven a 32-process job and a four node, dual-socket, dual-core\ncluster, we want to use a block distribution across the four nodes and then a\ncyclic distribution within the node across the physical processors. Below we\nshow how to obtain the wanted layout using 1) high-level flags and\n2) --cpubind\nHigh-Level flags\n\n\nUsing Slurm's high-level flag, users can obtain the above layout with\neither of the following submissions since --distribution=block:cyclic\nis the default distribution method.\n\n$ srun -n 32 -N 4 -B 4:2 --distribution=block:cyclic a.out\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0or\n\n$ srun -n 32 -N 4 -B 4:2 a.out\n\nThe cores are shown as c0 and c1 and the processors are shown\nas p0 through p3. The resulting task IDs are: \n\n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\n\nThe computation and assignment of the task IDs is transparent\nto the user. Users don't have to worry about the core numbering (Section\nPinning processes to cores) or any setting any CPU affinities. By default CPU affinity\nwill be set when using multi-core supporting flags. \nLow-level flag --cpu-bind\n\n\nUsing Slurm's --cpu-bind flag, users must compute the CPU IDs or\nmasks as well as make sure they understand the core numbering on their\nsystem. Another problem arises when core numbering is not the same on all\nnodes. The --cpu-bind option only allows users to specify a single\nmask for all the nodes. Using Slurm high-level flags remove this limitation\nsince Slurm will correctly generate the appropriate masks for each requested nodes.\n\nOn a four dual-socket dual-core node cluster with core block numbering\n\n\nThe cores are shown as c0 and c1 and the processors are shown\nas p0 through p3. The CPU IDs within a node in the block numbering are:\n(this information is available from the /proc/cpuinfo file on the system)\n\n\n\n\n \nc0c1\np0 0  1 \np2 4  5 \n\n\n\n\n\n\n \nc0c1\np1 2  3 \np3 6  7 \n\n\n\n\n\u00a0resulting in the following mapping for processor/cores\nand task IDs which users need to calculate: \n\nmapping for processors/cores\n\n\n\n\n\n \nc0c1\np0 0x01  0x02 \np2 0x10  0x20 \n\n\n\n\n\n\n \nc0c1\np1 0x04  0x08 \np3 0x40  0x80 \n\n\n\n\n\n\ntask IDs\n\n\n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\n\nThe above maps and task IDs can be translated into the\nfollowing command:\n\n$ srun -n 32 -N 4 --cpu-bind=mask_cpu:1,4,10,40,2,8,20,80 a.out\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0or\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,2,4,6,1,3,5,7 a.out\n\n\nSame cluster but with its core numbered cyclic instead of block\n\n\nOn a system with cyclically numbered cores, the correct mask\nargument to the srun command looks like: (this will\nachieve the same layout as the command above on a system with core block\nnumbering.)\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,1,2,3,4,5,6,7 a.out\n\nBlock map_cpu on a system with cyclic core numbering\n\n\nIf users do not check their system's core numbering before specifying\nthe map_cpu list and thereby do not realize that the system has cyclic core\nnumbering instead of block numbering then they will not get the expected\nlayout. For example, if they decide to re-use their command from above:\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,2,4,6,1,3,5,7 a.out\n\nthey get the following unintentional task ID layout:\n\n\n\n\n \nc0c1\np0 0  2 \np2 1  3 \n\n\n\n\n\n\n \nc0c1\np1 4  6 \np3 5  7 \n\n\n\n\nsince the processor IDs within a node in the cyclic numbering are:\n\n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\n\nThe important conclusion is that using the --cpu-bind flag is not\ntrivial and that it assumes that users are experts.\n\nExtensions to sinfo/squeue/scontrol\n\n\nSeveral extensions have also been made to the other Slurm utilities to\nmake working with multi-core/multi-threaded systems easier.\n\nsinfo\nThe long version (-l) of the sinfo node listing (-N) has been\nextended to display the sockets, cores, and threads present for each\nnode.  For example:\n\n\n% sinfo -N\nNODELIST     NODES PARTITION STATE\nhydra[12-15]     4    parts* idle\n\n% sinfo -lN\nThu Sep 14 17:47:13 2006\nNODELIST     NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT FEATURES REASON\nhydra[12-15]     4    parts*        idle   8+ 2+:4+:1+   2007    41447      1   (null) none\n\n% sinfo -lNe\nThu Sep 14 17:47:18 2006\nNODELIST     NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT FEATURES REASON\n\nhydra[12-14]     3    parts*        idle    8    2:4:1   2007    41447      1   (null) none\nhydra15          1    parts*        idle   64    8:4:2   2007    41447      1   (null) none\n\nFor user specified output formats (-o/--format) and sorting (-S/--sort),\nthe following identifiers are available:\n\n%X  Number of sockets per node\n%Y  Number of cores per socket\n%Z  Number of threads per core\n%z  Extended processor information: number of\n    sockets, core, threads (S:C:T) per node\n\nFor example:\n\n% sinfo -o '%9P %4c %8z %8X %8Y %8Z'\nPARTITION CPUS S:C:T    SOCKETS  CORES    THREADS\nparts*    4    2:2:1    2        2        1\n\nSee also 'sinfo --help' and 'man sinfo'\n\nsqueue\nFor user specified output formats (-o/--format) and sorting (-S/--sort),\nthe following identifiers are available:\n\n%m  Size of memory (in MB) requested by the job\n%H  Number of requested sockets per node\n%I  Number of requested cores per socket\n%J  Number of requested threads per core\n%z  Extended processor information: number of requested\n    sockets, cores, threads (S:C:T) per node\n\nBelow is an example squeue output after running 7 copies of:\n\n\n% srun -n 4 -B 2:2:1 --mem=1024 sleep 100 &\n\n\n\n% squeue -o '%.5i %.2t %.4M %.5D %7H %6I %7J %6z %R'\nJOBID ST TIME NODES SOCKETS CORES THREADS S:C:T NODELIST(REASON)\n   17 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   18 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   19 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   13  R 1:27     1 2       2     1       2:2:1 hydra12\n   14  R 1:26     1 2       2     1       2:2:1 hydra13\n   15  R 1:26     1 2       2     1       2:2:1 hydra14\n   16  R 1:26     1 2       2     1       2:2:1 hydra15\n\nThe squeue command can also display the memory size of jobs, for example:\n\n% sbatch --mem=123 tmp\nSubmitted batch job 24\n\n$ squeue -o \"%.5i %.2t %.4M %.5D %m\"\nJOBID ST TIME NODES MIN_MEMORY\n  24   R 0:05     1 123\n\nSee also 'squeue --help' and 'man squeue'\n\nscontrol\nThe following job settings can be adjusted using scontrol:\n\n\nRequested Allocation:\n  ReqSockets=<count>  Set the job's count of required sockets\n  ReqCores=<count>    Set the job's count of required cores\n  ReqThreads=<count>  Set the job's count of required threads\n\nFor example:\n\n# scontrol update JobID=17 ReqThreads=2\n# scontrol update JobID=18 ReqCores=4\n# scontrol update JobID=19 ReqSockets=8\n\n% squeue -o '%.5i %.2t %.4M %.5D %9c %7H %6I %8J'\nJOBID ST TIME NODES MIN_PROCS SOCKETS CORES THREADS\n   17 PD 0:00     1 1         4       2     1\n   18 PD 0:00     1 1         8       4     2\n   19 PD 0:00     1 1         4       2     1\n   13  R 1:35     1 0         0       0     0\n   14  R 1:34     1 0         0       0     0\n   15  R 1:34     1 0         0       0     0\n   16  R 1:34     1 0         0       0     0\n\nThe 'scontrol show job' command can be used to display\nthe number of allocated CPUs per node as well as the socket, cores,\nand threads specified in the request and constraints.\n\n\n% srun -N 2 -B 2:1 sleep 100 &\n% scontrol show job 20\nJobId=20 UserId=(30352) GroupId=users(1051)\n   Name=sleep\n   Priority=4294901749 Partition=parts BatchFlag=0\n   AllocNode:Sid=hydra16:3892 TimeLimit=UNLIMITED\n   JobState=RUNNING StartTime=09/25-17:17:30 EndTime=NONE\n   NodeList=hydra[12-14] NodeListIndices=0,2,-1\n   AllocCPUs=1,2,1\n   NumCPUs=4 ReqNodes=2 ReqS:C:T=2:1:*\n   OverSubscribe=0 Contiguous=0 CPUs/task=0\n   MinCPUs=0 MinMemory=0 MinTmpDisk=0 Features=(null)\n   Dependency=0 Account=(null) Reason=None Network=(null)\n   ReqNodeList=(null) ReqNodeListIndices=-1\n   ExcNodeList=(null) ExcNodeListIndices=-1\n   SubmitTime=09/25-17:17:30 SuspendTime=None PreSusTime=0\n\nSee also 'scontrol --help' and 'man scontrol'\n\nConfiguration settings in slurm.conf\n\n\nSeveral slurm.conf settings are available to control the multi-core\nfeatures described above.\n\nIn addition to the description below, also see the \"Task Launch\" and\n\"Resource Selection\" sections if generating slurm.conf\nvia configurator.html.\n\nAs previously mentioned, in order for the affinity to be set, the\ntask/affinity plugin must be first enabled in slurm.conf:\n\n\nTaskPlugin=task/affinity          # enable task affinity\n\nThis setting is part of the task launch specific parameters:\n\n# o Define task launch specific parameters\n#\n#    \"TaskProlog\" : Define a program to be executed as the user before each\n#                   task begins execution.\n#    \"TaskEpilog\" : Define a program to be executed as the user after each\n#                   task terminates.\n#    \"TaskPlugin\" : Define a task launch plugin. This may be used to\n#                   provide resource management within a node (e.g. pinning\n#                   tasks to specific processors). Permissible values are:\n#      \"task/affinity\" : CPU affinity support\n#      \"task/cgroup\"   : bind tasks to resources using Linux cgroup\n#      \"task/none\"     : no task launch actions, the default\n#\n# Example:\n#\n# TaskProlog=/usr/local/slurm/etc/task_prolog # default is none\n# TaskEpilog=/usr/local/slurm/etc/task_epilog # default is none\n# TaskPlugin=task/affinity                    # default is task/none\n\nDeclare the node hardware configuration in slurm.conf:\n\n\nNodeName=dualcore[01-16] CoresPerSocket=2 ThreadsPerCore=1\n\nFor a more complete description of the various node configuration options\nsee the slurm.conf man page.\n\nLast modified 19 May 2023\n"
            },
            {
                "title": "Memory as a Consumable Resource\n\n",
                "content": "The --mem flag specifies the maximum amount of memory in MB\nneeded by the job per node.  This flag is used to support the memory\nas a consumable resource allocation strategy.\n--mem=MB      maximum amount of real memory per node\n              required by the job.\nThis flag allows the scheduler to co-allocate jobs on specific nodes\ngiven that their added memory requirement do not exceed the total amount\nof memory on the nodes.In order to use memory as a consumable resource, the select/cons_tres\nplugin must be first enabled in slurm.conf:\n\nSelectType=select/cons_tres     # enable consumable resources\nSelectTypeParameters=CR_Memory  # memory as a consumable resource\n\n Using memory as a consumable resource is typically combined with\nthe CPU, Socket, or Core consumable resources using SelectTypeParameters\nvalues of: CR_CPU_Memory, CR_Socket_Memory or CR_Core_Memory\n\nSee the \"Resource Selection\" section if generating slurm.conf\nvia configurator.html.\n\nSee also 'srun --help' and 'man srun'\nTask invocation as a function of logical processors\n\n\nThe --ntasks-per-{node,socket,core}=ntasks flags\nallow the user to request that no more than ntasks\nbe invoked on each node, socket, or core.\nThis is similar to using --cpus-per-task=ncpus\nbut does not require knowledge of the actual number of cpus on\neach node.  In some cases, it is more convenient to be able to\nrequest that no more than a specific number of ntasks be invoked\non each node, socket, or core.  Examples of this include submitting\nan app where only one \"task/rank\" should be\nassigned to each node while allowing the job to utilize\nall of the parallelism present in the node, or submitting a single\nsetup/cleanup/monitoring job to each node of a pre-existing\nallocation as one step in a larger job script.\nThis can now be specified via the following flags:\n\n--ntasks-per-node=n    number of tasks to invoke on each node\n--ntasks-per-socket=n  number of tasks to invoke on each socket\n--ntasks-per-core=n    number of tasks to invoke on each core\n\nFor example, given a cluster with nodes containing two sockets,\neach containing two cores, the following commands illustrate the\nbehavior of these flags:\n\n% srun -n 4 hostname\nhydra12\nhydra12\nhydra12\nhydra12\n% srun -n 4 --ntasks-per-node=1 hostname\nhydra12\nhydra13\nhydra14\nhydra15\n% srun -n 4 --ntasks-per-node=2 hostname\nhydra12\nhydra12\nhydra13\nhydra13\n% srun -n 4 --ntasks-per-socket=1 hostname\nhydra12\nhydra12\nhydra13\nhydra13\n% srun -n 4 --ntasks-per-core=1 hostname\nhydra12\nhydra12\nhydra12\nhydra12\n\nSee also 'srun --help' and 'man srun'\nApplication hints\n\n\nDifferent applications will have various levels of resource\nrequirements. Some applications tend to be computationally intensive\nbut require little to no inter-process communication. Some applications\nwill be memory bound, saturating the memory bandwidth of a processor\nbefore exhausting the computational capabilities. Other applications\nwill be highly communication intensive causing processes to block\nawaiting messages from other processes. Applications with these\ndifferent properties tend to run well on a multi-core system given\nthe right mappings.\nFor computationally intensive applications, all cores in a multi-core\nsystem would normally be used. For memory bound applications, only\nusing a single core on each socket will result in the highest per\ncore memory bandwidth. For communication intensive applications,\nusing in-core multi-threading (e.g. hyperthreading, SMT, or TMT)\nmay also improve performance.\nThe following command line flags can be used to communicate these\ntypes of application hints to the Slurm multi-core support:\n\n--hint=             Bind tasks according to application hints\n    compute_bound   use all cores in each socket\n    memory_bound    use only one core in each socket\n    [no]multithread [don't] use extra threads with in-core multi-threading\n    help            show this help message\n\nFor example, given a cluster with nodes containing two sockets,\neach containing two cores, the following commands illustrate the\nbehavior of these flags.  In the verbose --cpu-bind output, tasks\nare described as 'hostname, task Global_ID Local_ID [PID]':\n\n% srun -n 4 --hint=compute_bound --cpu-bind=verbose sleep 1\ncpu-bind=MASK - hydra12, task  0  0 [15425]: mask 0x1 set\ncpu-bind=MASK - hydra12, task  1  1 [15426]: mask 0x4 set\ncpu-bind=MASK - hydra12, task  2  2 [15427]: mask 0x2 set\ncpu-bind=MASK - hydra12, task  3  3 [15428]: mask 0x8 set\n\n% srun -n 4 --hint=memory_bound --cpu-bind=verbose sleep 1\ncpu-bind=MASK - hydra12, task  0  0 [15550]: mask 0x1 set\ncpu-bind=MASK - hydra12, task  1  1 [15551]: mask 0x4 set\ncpu-bind=MASK - hydra13, task  2  0 [14974]: mask 0x1 set\ncpu-bind=MASK - hydra13, task  3  1 [14975]: mask 0x4 set\n\nSee also 'srun --hint=help' and 'man srun'\n\nMotivation behind high-level srun flags\n\n\nThe motivation behind allowing users to use higher level srun\nflags instead of --cpu-bind is that the later can be difficult to use. The\nproposed high-level flags are easier to use than --cpu-bind because:\n\nAffinity mask generation happens automatically when using the high-level flags. \nThe length and complexity of the --cpu-bind flag vs. the length\nof the combination of -B and --distribution flags make the high-level\nflags much easier to use.\n\nAlso as illustrated in the example below it is much simpler to specify\na different layout using the high-level flags since users do not have to\nrecalculate mask or CPU IDs. This approach is much simpler than\nrearranging the mask or map.\nGiven a 32-process job and a four node, dual-socket, dual-core\ncluster, we want to use a block distribution across the four nodes and then a\ncyclic distribution within the node across the physical processors. Below we\nshow how to obtain the wanted layout using 1) high-level flags and\n2) --cpubind\nHigh-Level flags\n\n\nUsing Slurm's high-level flag, users can obtain the above layout with\neither of the following submissions since --distribution=block:cyclic\nis the default distribution method.\n\n$ srun -n 32 -N 4 -B 4:2 --distribution=block:cyclic a.out\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0or\n\n$ srun -n 32 -N 4 -B 4:2 a.out\n\nThe cores are shown as c0 and c1 and the processors are shown\nas p0 through p3. The resulting task IDs are: \n\n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\n\nThe computation and assignment of the task IDs is transparent\nto the user. Users don't have to worry about the core numbering (Section\nPinning processes to cores) or any setting any CPU affinities. By default CPU affinity\nwill be set when using multi-core supporting flags. \nLow-level flag --cpu-bind\n\n\nUsing Slurm's --cpu-bind flag, users must compute the CPU IDs or\nmasks as well as make sure they understand the core numbering on their\nsystem. Another problem arises when core numbering is not the same on all\nnodes. The --cpu-bind option only allows users to specify a single\nmask for all the nodes. Using Slurm high-level flags remove this limitation\nsince Slurm will correctly generate the appropriate masks for each requested nodes.\n\nOn a four dual-socket dual-core node cluster with core block numbering\n\n\nThe cores are shown as c0 and c1 and the processors are shown\nas p0 through p3. The CPU IDs within a node in the block numbering are:\n(this information is available from the /proc/cpuinfo file on the system)\n\n\n\n\n \nc0c1\np0 0  1 \np2 4  5 \n\n\n\n\n\n\n \nc0c1\np1 2  3 \np3 6  7 \n\n\n\n\n\u00a0resulting in the following mapping for processor/cores\nand task IDs which users need to calculate: \n\nmapping for processors/cores\n\n\n\n\n\n \nc0c1\np0 0x01  0x02 \np2 0x10  0x20 \n\n\n\n\n\n\n \nc0c1\np1 0x04  0x08 \np3 0x40  0x80 \n\n\n\n\n\n\ntask IDs\n\n\n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\n\nThe above maps and task IDs can be translated into the\nfollowing command:\n\n$ srun -n 32 -N 4 --cpu-bind=mask_cpu:1,4,10,40,2,8,20,80 a.out\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0or\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,2,4,6,1,3,5,7 a.out\n\n\nSame cluster but with its core numbered cyclic instead of block\n\n\nOn a system with cyclically numbered cores, the correct mask\nargument to the srun command looks like: (this will\nachieve the same layout as the command above on a system with core block\nnumbering.)\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,1,2,3,4,5,6,7 a.out\n\nBlock map_cpu on a system with cyclic core numbering\n\n\nIf users do not check their system's core numbering before specifying\nthe map_cpu list and thereby do not realize that the system has cyclic core\nnumbering instead of block numbering then they will not get the expected\nlayout. For example, if they decide to re-use their command from above:\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,2,4,6,1,3,5,7 a.out\n\nthey get the following unintentional task ID layout:\n\n\n\n\n \nc0c1\np0 0  2 \np2 1  3 \n\n\n\n\n\n\n \nc0c1\np1 4  6 \np3 5  7 \n\n\n\n\nsince the processor IDs within a node in the cyclic numbering are:\n\n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\n\nThe important conclusion is that using the --cpu-bind flag is not\ntrivial and that it assumes that users are experts.\n\nExtensions to sinfo/squeue/scontrol\n\n\nSeveral extensions have also been made to the other Slurm utilities to\nmake working with multi-core/multi-threaded systems easier.\n\nsinfo\nThe long version (-l) of the sinfo node listing (-N) has been\nextended to display the sockets, cores, and threads present for each\nnode.  For example:\n\n\n% sinfo -N\nNODELIST     NODES PARTITION STATE\nhydra[12-15]     4    parts* idle\n\n% sinfo -lN\nThu Sep 14 17:47:13 2006\nNODELIST     NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT FEATURES REASON\nhydra[12-15]     4    parts*        idle   8+ 2+:4+:1+   2007    41447      1   (null) none\n\n% sinfo -lNe\nThu Sep 14 17:47:18 2006\nNODELIST     NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT FEATURES REASON\n\nhydra[12-14]     3    parts*        idle    8    2:4:1   2007    41447      1   (null) none\nhydra15          1    parts*        idle   64    8:4:2   2007    41447      1   (null) none\n\nFor user specified output formats (-o/--format) and sorting (-S/--sort),\nthe following identifiers are available:\n\n%X  Number of sockets per node\n%Y  Number of cores per socket\n%Z  Number of threads per core\n%z  Extended processor information: number of\n    sockets, core, threads (S:C:T) per node\n\nFor example:\n\n% sinfo -o '%9P %4c %8z %8X %8Y %8Z'\nPARTITION CPUS S:C:T    SOCKETS  CORES    THREADS\nparts*    4    2:2:1    2        2        1\n\nSee also 'sinfo --help' and 'man sinfo'\n\nsqueue\nFor user specified output formats (-o/--format) and sorting (-S/--sort),\nthe following identifiers are available:\n\n%m  Size of memory (in MB) requested by the job\n%H  Number of requested sockets per node\n%I  Number of requested cores per socket\n%J  Number of requested threads per core\n%z  Extended processor information: number of requested\n    sockets, cores, threads (S:C:T) per node\n\nBelow is an example squeue output after running 7 copies of:\n\n\n% srun -n 4 -B 2:2:1 --mem=1024 sleep 100 &\n\n\n\n% squeue -o '%.5i %.2t %.4M %.5D %7H %6I %7J %6z %R'\nJOBID ST TIME NODES SOCKETS CORES THREADS S:C:T NODELIST(REASON)\n   17 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   18 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   19 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   13  R 1:27     1 2       2     1       2:2:1 hydra12\n   14  R 1:26     1 2       2     1       2:2:1 hydra13\n   15  R 1:26     1 2       2     1       2:2:1 hydra14\n   16  R 1:26     1 2       2     1       2:2:1 hydra15\n\nThe squeue command can also display the memory size of jobs, for example:\n\n% sbatch --mem=123 tmp\nSubmitted batch job 24\n\n$ squeue -o \"%.5i %.2t %.4M %.5D %m\"\nJOBID ST TIME NODES MIN_MEMORY\n  24   R 0:05     1 123\n\nSee also 'squeue --help' and 'man squeue'\n\nscontrol\nThe following job settings can be adjusted using scontrol:\n\n\nRequested Allocation:\n  ReqSockets=<count>  Set the job's count of required sockets\n  ReqCores=<count>    Set the job's count of required cores\n  ReqThreads=<count>  Set the job's count of required threads\n\nFor example:\n\n# scontrol update JobID=17 ReqThreads=2\n# scontrol update JobID=18 ReqCores=4\n# scontrol update JobID=19 ReqSockets=8\n\n% squeue -o '%.5i %.2t %.4M %.5D %9c %7H %6I %8J'\nJOBID ST TIME NODES MIN_PROCS SOCKETS CORES THREADS\n   17 PD 0:00     1 1         4       2     1\n   18 PD 0:00     1 1         8       4     2\n   19 PD 0:00     1 1         4       2     1\n   13  R 1:35     1 0         0       0     0\n   14  R 1:34     1 0         0       0     0\n   15  R 1:34     1 0         0       0     0\n   16  R 1:34     1 0         0       0     0\n\nThe 'scontrol show job' command can be used to display\nthe number of allocated CPUs per node as well as the socket, cores,\nand threads specified in the request and constraints.\n\n\n% srun -N 2 -B 2:1 sleep 100 &\n% scontrol show job 20\nJobId=20 UserId=(30352) GroupId=users(1051)\n   Name=sleep\n   Priority=4294901749 Partition=parts BatchFlag=0\n   AllocNode:Sid=hydra16:3892 TimeLimit=UNLIMITED\n   JobState=RUNNING StartTime=09/25-17:17:30 EndTime=NONE\n   NodeList=hydra[12-14] NodeListIndices=0,2,-1\n   AllocCPUs=1,2,1\n   NumCPUs=4 ReqNodes=2 ReqS:C:T=2:1:*\n   OverSubscribe=0 Contiguous=0 CPUs/task=0\n   MinCPUs=0 MinMemory=0 MinTmpDisk=0 Features=(null)\n   Dependency=0 Account=(null) Reason=None Network=(null)\n   ReqNodeList=(null) ReqNodeListIndices=-1\n   ExcNodeList=(null) ExcNodeListIndices=-1\n   SubmitTime=09/25-17:17:30 SuspendTime=None PreSusTime=0\n\nSee also 'scontrol --help' and 'man scontrol'\n\nConfiguration settings in slurm.conf\n\n\nSeveral slurm.conf settings are available to control the multi-core\nfeatures described above.\n\nIn addition to the description below, also see the \"Task Launch\" and\n\"Resource Selection\" sections if generating slurm.conf\nvia configurator.html.\n\nAs previously mentioned, in order for the affinity to be set, the\ntask/affinity plugin must be first enabled in slurm.conf:\n\n\nTaskPlugin=task/affinity          # enable task affinity\n\nThis setting is part of the task launch specific parameters:\n\n# o Define task launch specific parameters\n#\n#    \"TaskProlog\" : Define a program to be executed as the user before each\n#                   task begins execution.\n#    \"TaskEpilog\" : Define a program to be executed as the user after each\n#                   task terminates.\n#    \"TaskPlugin\" : Define a task launch plugin. This may be used to\n#                   provide resource management within a node (e.g. pinning\n#                   tasks to specific processors). Permissible values are:\n#      \"task/affinity\" : CPU affinity support\n#      \"task/cgroup\"   : bind tasks to resources using Linux cgroup\n#      \"task/none\"     : no task launch actions, the default\n#\n# Example:\n#\n# TaskProlog=/usr/local/slurm/etc/task_prolog # default is none\n# TaskEpilog=/usr/local/slurm/etc/task_epilog # default is none\n# TaskPlugin=task/affinity                    # default is task/none\n\nDeclare the node hardware configuration in slurm.conf:\n\n\nNodeName=dualcore[01-16] CoresPerSocket=2 ThreadsPerCore=1\n\nFor a more complete description of the various node configuration options\nsee the slurm.conf man page.\n\nLast modified 19 May 2023\n"
            },
            {
                "title": "Task invocation as a function of logical processors\n\n",
                "content": "The --ntasks-per-{node,socket,core}=ntasks flags\nallow the user to request that no more than ntasks\nbe invoked on each node, socket, or core.\nThis is similar to using --cpus-per-task=ncpus\nbut does not require knowledge of the actual number of cpus on\neach node.  In some cases, it is more convenient to be able to\nrequest that no more than a specific number of ntasks be invoked\non each node, socket, or core.  Examples of this include submitting\nan app where only one \"task/rank\" should be\nassigned to each node while allowing the job to utilize\nall of the parallelism present in the node, or submitting a single\nsetup/cleanup/monitoring job to each node of a pre-existing\nallocation as one step in a larger job script.\nThis can now be specified via the following flags:\n--ntasks-per-node=n    number of tasks to invoke on each node\n--ntasks-per-socket=n  number of tasks to invoke on each socket\n--ntasks-per-core=n    number of tasks to invoke on each core\nFor example, given a cluster with nodes containing two sockets,\neach containing two cores, the following commands illustrate the\nbehavior of these flags:\n% srun -n 4 hostname\nhydra12\nhydra12\nhydra12\nhydra12\n% srun -n 4 --ntasks-per-node=1 hostname\nhydra12\nhydra13\nhydra14\nhydra15\n% srun -n 4 --ntasks-per-node=2 hostname\nhydra12\nhydra12\nhydra13\nhydra13\n% srun -n 4 --ntasks-per-socket=1 hostname\nhydra12\nhydra12\nhydra13\nhydra13\n% srun -n 4 --ntasks-per-core=1 hostname\nhydra12\nhydra12\nhydra12\nhydra12\nSee also 'srun --help' and 'man srun'Application hints\n\nDifferent applications will have various levels of resource\nrequirements. Some applications tend to be computationally intensive\nbut require little to no inter-process communication. Some applications\nwill be memory bound, saturating the memory bandwidth of a processor\nbefore exhausting the computational capabilities. Other applications\nwill be highly communication intensive causing processes to block\nawaiting messages from other processes. Applications with these\ndifferent properties tend to run well on a multi-core system given\nthe right mappings.For computationally intensive applications, all cores in a multi-core\nsystem would normally be used. For memory bound applications, only\nusing a single core on each socket will result in the highest per\ncore memory bandwidth. For communication intensive applications,\nusing in-core multi-threading (e.g. hyperthreading, SMT, or TMT)\nmay also improve performance.\nThe following command line flags can be used to communicate these\ntypes of application hints to the Slurm multi-core support:\n--hint=             Bind tasks according to application hints\n    compute_bound   use all cores in each socket\n    memory_bound    use only one core in each socket\n    [no]multithread [don't] use extra threads with in-core multi-threading\n    help            show this help message\nFor example, given a cluster with nodes containing two sockets,\neach containing two cores, the following commands illustrate the\nbehavior of these flags.  In the verbose --cpu-bind output, tasks\nare described as 'hostname, task Global_ID Local_ID [PID]':\n% srun -n 4 --hint=compute_bound --cpu-bind=verbose sleep 1\ncpu-bind=MASK - hydra12, task  0  0 [15425]: mask 0x1 set\ncpu-bind=MASK - hydra12, task  1  1 [15426]: mask 0x4 set\ncpu-bind=MASK - hydra12, task  2  2 [15427]: mask 0x2 set\ncpu-bind=MASK - hydra12, task  3  3 [15428]: mask 0x8 set\n\n% srun -n 4 --hint=memory_bound --cpu-bind=verbose sleep 1\ncpu-bind=MASK - hydra12, task  0  0 [15550]: mask 0x1 set\ncpu-bind=MASK - hydra12, task  1  1 [15551]: mask 0x4 set\ncpu-bind=MASK - hydra13, task  2  0 [14974]: mask 0x1 set\ncpu-bind=MASK - hydra13, task  3  1 [14975]: mask 0x4 set\nSee also 'srun --hint=help' and 'man srun'Motivation behind high-level srun flags\n\nThe motivation behind allowing users to use higher level srun\nflags instead of --cpu-bind is that the later can be difficult to use. The\nproposed high-level flags are easier to use than --cpu-bind because:\nAffinity mask generation happens automatically when using the high-level flags. \nThe length and complexity of the --cpu-bind flag vs. the length\nof the combination of -B and --distribution flags make the high-level\nflags much easier to use.\nAlso as illustrated in the example below it is much simpler to specify\na different layout using the high-level flags since users do not have to\nrecalculate mask or CPU IDs. This approach is much simpler than\nrearranging the mask or map.Given a 32-process job and a four node, dual-socket, dual-core\ncluster, we want to use a block distribution across the four nodes and then a\ncyclic distribution within the node across the physical processors. Below we\nshow how to obtain the wanted layout using 1) high-level flags and\n2) --cpubindHigh-Level flags\n\nUsing Slurm's high-level flag, users can obtain the above layout with\neither of the following submissions since --distribution=block:cyclic\nis the default distribution method.\n$ srun -n 32 -N 4 -B 4:2 --distribution=block:cyclic a.out\n\n$ srun -n 32 -N 4 -B 4:2 a.out\nThe cores are shown as c0 and c1 and the processors are shown\nas p0 through p3. The resulting task IDs are: \n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\nThe computation and assignment of the task IDs is transparent\nto the user. Users don't have to worry about the core numbering (Section\nPinning processes to cores) or any setting any CPU affinities. By default CPU affinity\nwill be set when using multi-core supporting flags. Low-level flag --cpu-bind\n\nUsing Slurm's --cpu-bind flag, users must compute the CPU IDs or\nmasks as well as make sure they understand the core numbering on their\nsystem. Another problem arises when core numbering is not the same on all\nnodes. The --cpu-bind option only allows users to specify a single\nmask for all the nodes. Using Slurm high-level flags remove this limitation\nsince Slurm will correctly generate the appropriate masks for each requested nodes.\nOn a four dual-socket dual-core node cluster with core block numbering\n\nThe cores are shown as c0 and c1 and the processors are shown\nas p0 through p3. The CPU IDs within a node in the block numbering are:\n(this information is available from the /proc/cpuinfo file on the system)\n\n\n\n \nc0c1\np0 0  1 \np2 4  5 \n\n\n\n\n\n\n \nc0c1\np1 2  3 \np3 6  7 \n\n\n\n\u00a0resulting in the following mapping for processor/cores\nand task IDs which users need to calculate: \nmapping for processors/cores\n\n\n\n\n \nc0c1\np0 0x01  0x02 \np2 0x10  0x20 \n\n\n\n\n\n\n \nc0c1\np1 0x04  0x08 \np3 0x40  0x80 \n\n\n\n\n\ntask IDs\n\n\n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\n\nThe above maps and task IDs can be translated into the\nfollowing command:\n\n$ srun -n 32 -N 4 --cpu-bind=mask_cpu:1,4,10,40,2,8,20,80 a.out\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0or\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,2,4,6,1,3,5,7 a.out\n\n\nSame cluster but with its core numbered cyclic instead of block\n\n\nOn a system with cyclically numbered cores, the correct mask\nargument to the srun command looks like: (this will\nachieve the same layout as the command above on a system with core block\nnumbering.)\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,1,2,3,4,5,6,7 a.out\n\nBlock map_cpu on a system with cyclic core numbering\n\n\nIf users do not check their system's core numbering before specifying\nthe map_cpu list and thereby do not realize that the system has cyclic core\nnumbering instead of block numbering then they will not get the expected\nlayout. For example, if they decide to re-use their command from above:\n\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,2,4,6,1,3,5,7 a.out\n\nthey get the following unintentional task ID layout:\n\n\n\n\n \nc0c1\np0 0  2 \np2 1  3 \n\n\n\n\n\n\n \nc0c1\np1 4  6 \np3 5  7 \n\n\n\n\nsince the processor IDs within a node in the cyclic numbering are:\n\n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\n\nThe important conclusion is that using the --cpu-bind flag is not\ntrivial and that it assumes that users are experts.\n\nExtensions to sinfo/squeue/scontrol\n\n\nSeveral extensions have also been made to the other Slurm utilities to\nmake working with multi-core/multi-threaded systems easier.\n\nsinfo\nThe long version (-l) of the sinfo node listing (-N) has been\nextended to display the sockets, cores, and threads present for each\nnode.  For example:\n\n\n% sinfo -N\nNODELIST     NODES PARTITION STATE\nhydra[12-15]     4    parts* idle\n\n% sinfo -lN\nThu Sep 14 17:47:13 2006\nNODELIST     NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT FEATURES REASON\nhydra[12-15]     4    parts*        idle   8+ 2+:4+:1+   2007    41447      1   (null) none\n\n% sinfo -lNe\nThu Sep 14 17:47:18 2006\nNODELIST     NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT FEATURES REASON\n\nhydra[12-14]     3    parts*        idle    8    2:4:1   2007    41447      1   (null) none\nhydra15          1    parts*        idle   64    8:4:2   2007    41447      1   (null) none\n\nFor user specified output formats (-o/--format) and sorting (-S/--sort),\nthe following identifiers are available:\n\n%X  Number of sockets per node\n%Y  Number of cores per socket\n%Z  Number of threads per core\n%z  Extended processor information: number of\n    sockets, core, threads (S:C:T) per node\n\nFor example:\n\n% sinfo -o '%9P %4c %8z %8X %8Y %8Z'\nPARTITION CPUS S:C:T    SOCKETS  CORES    THREADS\nparts*    4    2:2:1    2        2        1\n\nSee also 'sinfo --help' and 'man sinfo'\n\nsqueue\nFor user specified output formats (-o/--format) and sorting (-S/--sort),\nthe following identifiers are available:\n\n%m  Size of memory (in MB) requested by the job\n%H  Number of requested sockets per node\n%I  Number of requested cores per socket\n%J  Number of requested threads per core\n%z  Extended processor information: number of requested\n    sockets, cores, threads (S:C:T) per node\n\nBelow is an example squeue output after running 7 copies of:\n\n\n% srun -n 4 -B 2:2:1 --mem=1024 sleep 100 &\n\n\n\n% squeue -o '%.5i %.2t %.4M %.5D %7H %6I %7J %6z %R'\nJOBID ST TIME NODES SOCKETS CORES THREADS S:C:T NODELIST(REASON)\n   17 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   18 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   19 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   13  R 1:27     1 2       2     1       2:2:1 hydra12\n   14  R 1:26     1 2       2     1       2:2:1 hydra13\n   15  R 1:26     1 2       2     1       2:2:1 hydra14\n   16  R 1:26     1 2       2     1       2:2:1 hydra15\n\nThe squeue command can also display the memory size of jobs, for example:\n\n% sbatch --mem=123 tmp\nSubmitted batch job 24\n\n$ squeue -o \"%.5i %.2t %.4M %.5D %m\"\nJOBID ST TIME NODES MIN_MEMORY\n  24   R 0:05     1 123\n\nSee also 'squeue --help' and 'man squeue'\n\nscontrol\nThe following job settings can be adjusted using scontrol:\n\n\nRequested Allocation:\n  ReqSockets=<count>  Set the job's count of required sockets\n  ReqCores=<count>    Set the job's count of required cores\n  ReqThreads=<count>  Set the job's count of required threads\n\nFor example:\n\n# scontrol update JobID=17 ReqThreads=2\n# scontrol update JobID=18 ReqCores=4\n# scontrol update JobID=19 ReqSockets=8\n\n% squeue -o '%.5i %.2t %.4M %.5D %9c %7H %6I %8J'\nJOBID ST TIME NODES MIN_PROCS SOCKETS CORES THREADS\n   17 PD 0:00     1 1         4       2     1\n   18 PD 0:00     1 1         8       4     2\n   19 PD 0:00     1 1         4       2     1\n   13  R 1:35     1 0         0       0     0\n   14  R 1:34     1 0         0       0     0\n   15  R 1:34     1 0         0       0     0\n   16  R 1:34     1 0         0       0     0\n\nThe 'scontrol show job' command can be used to display\nthe number of allocated CPUs per node as well as the socket, cores,\nand threads specified in the request and constraints.\n\n\n% srun -N 2 -B 2:1 sleep 100 &\n% scontrol show job 20\nJobId=20 UserId=(30352) GroupId=users(1051)\n   Name=sleep\n   Priority=4294901749 Partition=parts BatchFlag=0\n   AllocNode:Sid=hydra16:3892 TimeLimit=UNLIMITED\n   JobState=RUNNING StartTime=09/25-17:17:30 EndTime=NONE\n   NodeList=hydra[12-14] NodeListIndices=0,2,-1\n   AllocCPUs=1,2,1\n   NumCPUs=4 ReqNodes=2 ReqS:C:T=2:1:*\n   OverSubscribe=0 Contiguous=0 CPUs/task=0\n   MinCPUs=0 MinMemory=0 MinTmpDisk=0 Features=(null)\n   Dependency=0 Account=(null) Reason=None Network=(null)\n   ReqNodeList=(null) ReqNodeListIndices=-1\n   ExcNodeList=(null) ExcNodeListIndices=-1\n   SubmitTime=09/25-17:17:30 SuspendTime=None PreSusTime=0\n\nSee also 'scontrol --help' and 'man scontrol'\n\nConfiguration settings in slurm.conf\n\n\nSeveral slurm.conf settings are available to control the multi-core\nfeatures described above.\n\nIn addition to the description below, also see the \"Task Launch\" and\n\"Resource Selection\" sections if generating slurm.conf\nvia configurator.html.\n\nAs previously mentioned, in order for the affinity to be set, the\ntask/affinity plugin must be first enabled in slurm.conf:\n\n\nTaskPlugin=task/affinity          # enable task affinity\n\nThis setting is part of the task launch specific parameters:\n\n# o Define task launch specific parameters\n#\n#    \"TaskProlog\" : Define a program to be executed as the user before each\n#                   task begins execution.\n#    \"TaskEpilog\" : Define a program to be executed as the user after each\n#                   task terminates.\n#    \"TaskPlugin\" : Define a task launch plugin. This may be used to\n#                   provide resource management within a node (e.g. pinning\n#                   tasks to specific processors). Permissible values are:\n#      \"task/affinity\" : CPU affinity support\n#      \"task/cgroup\"   : bind tasks to resources using Linux cgroup\n#      \"task/none\"     : no task launch actions, the default\n#\n# Example:\n#\n# TaskProlog=/usr/local/slurm/etc/task_prolog # default is none\n# TaskEpilog=/usr/local/slurm/etc/task_epilog # default is none\n# TaskPlugin=task/affinity                    # default is task/none\n\nDeclare the node hardware configuration in slurm.conf:\n\n\nNodeName=dualcore[01-16] CoresPerSocket=2 ThreadsPerCore=1\n\nFor a more complete description of the various node configuration options\nsee the slurm.conf man page.\n\nLast modified 19 May 2023\n"
            },
            {
                "title": "\nSame cluster but with its core numbered cyclic instead of block\n\n",
                "content": "On a system with cyclically numbered cores, the correct mask\nargument to the srun command looks like: (this will\nachieve the same layout as the command above on a system with core block\nnumbering.)\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,1,2,3,4,5,6,7 a.out\nBlock map_cpu on a system with cyclic core numbering\n\nIf users do not check their system's core numbering before specifying\nthe map_cpu list and thereby do not realize that the system has cyclic core\nnumbering instead of block numbering then they will not get the expected\nlayout. For example, if they decide to re-use their command from above:\n$ srun -n 32 -N 4 --cpu-bind=map_cpu:0,2,4,6,1,3,5,7 a.out\nthey get the following unintentional task ID layout:\n\n\n\n \nc0c1\np0 0  2 \np2 1  3 \n\n\n\n\n\n\n \nc0c1\np1 4  6 \np3 5  7 \n\n\n\nsince the processor IDs within a node in the cyclic numbering are:\n\n\n\n \nc0c1\np0 0  4 \np2 2  6 \n\n\n\n\n\n\n \nc0c1\np1 1  5 \np3 3  7 \n\n\n\nThe important conclusion is that using the --cpu-bind flag is not\ntrivial and that it assumes that users are experts.Extensions to sinfo/squeue/scontrol\n\nSeveral extensions have also been made to the other Slurm utilities to\nmake working with multi-core/multi-threaded systems easier.sinfoThe long version (-l) of the sinfo node listing (-N) has been\nextended to display the sockets, cores, and threads present for each\nnode.  For example:\n\n\n% sinfo -N\nNODELIST     NODES PARTITION STATE\nhydra[12-15]     4    parts* idle\n\n% sinfo -lN\nThu Sep 14 17:47:13 2006\nNODELIST     NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT FEATURES REASON\nhydra[12-15]     4    parts*        idle   8+ 2+:4+:1+   2007    41447      1   (null) none\n\n% sinfo -lNe\nThu Sep 14 17:47:18 2006\nNODELIST     NODES PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT FEATURES REASON\n\nhydra[12-14]     3    parts*        idle    8    2:4:1   2007    41447      1   (null) none\nhydra15          1    parts*        idle   64    8:4:2   2007    41447      1   (null) none\n\nFor user specified output formats (-o/--format) and sorting (-S/--sort),\nthe following identifiers are available:\n\n%X  Number of sockets per node\n%Y  Number of cores per socket\n%Z  Number of threads per core\n%z  Extended processor information: number of\n    sockets, core, threads (S:C:T) per node\n\nFor example:\n\n% sinfo -o '%9P %4c %8z %8X %8Y %8Z'\nPARTITION CPUS S:C:T    SOCKETS  CORES    THREADS\nparts*    4    2:2:1    2        2        1\n\nSee also 'sinfo --help' and 'man sinfo'\n\nsqueue\nFor user specified output formats (-o/--format) and sorting (-S/--sort),\nthe following identifiers are available:\n\n%m  Size of memory (in MB) requested by the job\n%H  Number of requested sockets per node\n%I  Number of requested cores per socket\n%J  Number of requested threads per core\n%z  Extended processor information: number of requested\n    sockets, cores, threads (S:C:T) per node\n\nBelow is an example squeue output after running 7 copies of:\n\n\n% srun -n 4 -B 2:2:1 --mem=1024 sleep 100 &\n\n\n\n% squeue -o '%.5i %.2t %.4M %.5D %7H %6I %7J %6z %R'\nJOBID ST TIME NODES SOCKETS CORES THREADS S:C:T NODELIST(REASON)\n   17 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   18 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   19 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   13  R 1:27     1 2       2     1       2:2:1 hydra12\n   14  R 1:26     1 2       2     1       2:2:1 hydra13\n   15  R 1:26     1 2       2     1       2:2:1 hydra14\n   16  R 1:26     1 2       2     1       2:2:1 hydra15\n\nThe squeue command can also display the memory size of jobs, for example:\n\n% sbatch --mem=123 tmp\nSubmitted batch job 24\n\n$ squeue -o \"%.5i %.2t %.4M %.5D %m\"\nJOBID ST TIME NODES MIN_MEMORY\n  24   R 0:05     1 123\n\nSee also 'squeue --help' and 'man squeue'\n\nscontrol\nThe following job settings can be adjusted using scontrol:\n\n\nRequested Allocation:\n  ReqSockets=<count>  Set the job's count of required sockets\n  ReqCores=<count>    Set the job's count of required cores\n  ReqThreads=<count>  Set the job's count of required threads\n\nFor example:\n\n# scontrol update JobID=17 ReqThreads=2\n# scontrol update JobID=18 ReqCores=4\n# scontrol update JobID=19 ReqSockets=8\n\n% squeue -o '%.5i %.2t %.4M %.5D %9c %7H %6I %8J'\nJOBID ST TIME NODES MIN_PROCS SOCKETS CORES THREADS\n   17 PD 0:00     1 1         4       2     1\n   18 PD 0:00     1 1         8       4     2\n   19 PD 0:00     1 1         4       2     1\n   13  R 1:35     1 0         0       0     0\n   14  R 1:34     1 0         0       0     0\n   15  R 1:34     1 0         0       0     0\n   16  R 1:34     1 0         0       0     0\n\nThe 'scontrol show job' command can be used to display\nthe number of allocated CPUs per node as well as the socket, cores,\nand threads specified in the request and constraints.\n\n\n% srun -N 2 -B 2:1 sleep 100 &\n% scontrol show job 20\nJobId=20 UserId=(30352) GroupId=users(1051)\n   Name=sleep\n   Priority=4294901749 Partition=parts BatchFlag=0\n   AllocNode:Sid=hydra16:3892 TimeLimit=UNLIMITED\n   JobState=RUNNING StartTime=09/25-17:17:30 EndTime=NONE\n   NodeList=hydra[12-14] NodeListIndices=0,2,-1\n   AllocCPUs=1,2,1\n   NumCPUs=4 ReqNodes=2 ReqS:C:T=2:1:*\n   OverSubscribe=0 Contiguous=0 CPUs/task=0\n   MinCPUs=0 MinMemory=0 MinTmpDisk=0 Features=(null)\n   Dependency=0 Account=(null) Reason=None Network=(null)\n   ReqNodeList=(null) ReqNodeListIndices=-1\n   ExcNodeList=(null) ExcNodeListIndices=-1\n   SubmitTime=09/25-17:17:30 SuspendTime=None PreSusTime=0\n\nSee also 'scontrol --help' and 'man scontrol'\n\nConfiguration settings in slurm.conf\n\n\nSeveral slurm.conf settings are available to control the multi-core\nfeatures described above.\n\nIn addition to the description below, also see the \"Task Launch\" and\n\"Resource Selection\" sections if generating slurm.conf\nvia configurator.html.\n\nAs previously mentioned, in order for the affinity to be set, the\ntask/affinity plugin must be first enabled in slurm.conf:\n\n\nTaskPlugin=task/affinity          # enable task affinity\n\nThis setting is part of the task launch specific parameters:\n\n# o Define task launch specific parameters\n#\n#    \"TaskProlog\" : Define a program to be executed as the user before each\n#                   task begins execution.\n#    \"TaskEpilog\" : Define a program to be executed as the user after each\n#                   task terminates.\n#    \"TaskPlugin\" : Define a task launch plugin. This may be used to\n#                   provide resource management within a node (e.g. pinning\n#                   tasks to specific processors). Permissible values are:\n#      \"task/affinity\" : CPU affinity support\n#      \"task/cgroup\"   : bind tasks to resources using Linux cgroup\n#      \"task/none\"     : no task launch actions, the default\n#\n# Example:\n#\n# TaskProlog=/usr/local/slurm/etc/task_prolog # default is none\n# TaskEpilog=/usr/local/slurm/etc/task_epilog # default is none\n# TaskPlugin=task/affinity                    # default is task/none\n\nDeclare the node hardware configuration in slurm.conf:\n\n\nNodeName=dualcore[01-16] CoresPerSocket=2 ThreadsPerCore=1\n\nFor a more complete description of the various node configuration options\nsee the slurm.conf man page.\n\nLast modified 19 May 2023\n"
            },
            {
                "title": "squeue",
                "content": "For user specified output formats (-o/--format) and sorting (-S/--sort),\nthe following identifiers are available:\n%m  Size of memory (in MB) requested by the job\n%H  Number of requested sockets per node\n%I  Number of requested cores per socket\n%J  Number of requested threads per core\n%z  Extended processor information: number of requested\n    sockets, cores, threads (S:C:T) per node\nBelow is an example squeue output after running 7 copies of:\n\n% srun -n 4 -B 2:2:1 --mem=1024 sleep 100 &\n\n\n% squeue -o '%.5i %.2t %.4M %.5D %7H %6I %7J %6z %R'\nJOBID ST TIME NODES SOCKETS CORES THREADS S:C:T NODELIST(REASON)\n   17 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   18 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   19 PD 0:00     1 2       2     1       2:2:1 (Resources)\n   13  R 1:27     1 2       2     1       2:2:1 hydra12\n   14  R 1:26     1 2       2     1       2:2:1 hydra13\n   15  R 1:26     1 2       2     1       2:2:1 hydra14\n   16  R 1:26     1 2       2     1       2:2:1 hydra15\nThe squeue command can also display the memory size of jobs, for example:\n% sbatch --mem=123 tmp\nSubmitted batch job 24\n\n$ squeue -o \"%.5i %.2t %.4M %.5D %m\"\nJOBID ST TIME NODES MIN_MEMORY\n  24   R 0:05     1 123\nSee also 'squeue --help' and 'man squeue'scontrolThe following job settings can be adjusted using scontrol:\n\n\nRequested Allocation:\n  ReqSockets=<count>  Set the job's count of required sockets\n  ReqCores=<count>    Set the job's count of required cores\n  ReqThreads=<count>  Set the job's count of required threads\n\nFor example:\n\n# scontrol update JobID=17 ReqThreads=2\n# scontrol update JobID=18 ReqCores=4\n# scontrol update JobID=19 ReqSockets=8\n\n% squeue -o '%.5i %.2t %.4M %.5D %9c %7H %6I %8J'\nJOBID ST TIME NODES MIN_PROCS SOCKETS CORES THREADS\n   17 PD 0:00     1 1         4       2     1\n   18 PD 0:00     1 1         8       4     2\n   19 PD 0:00     1 1         4       2     1\n   13  R 1:35     1 0         0       0     0\n   14  R 1:34     1 0         0       0     0\n   15  R 1:34     1 0         0       0     0\n   16  R 1:34     1 0         0       0     0\n\nThe 'scontrol show job' command can be used to display\nthe number of allocated CPUs per node as well as the socket, cores,\nand threads specified in the request and constraints.\n\n\n% srun -N 2 -B 2:1 sleep 100 &\n% scontrol show job 20\nJobId=20 UserId=(30352) GroupId=users(1051)\n   Name=sleep\n   Priority=4294901749 Partition=parts BatchFlag=0\n   AllocNode:Sid=hydra16:3892 TimeLimit=UNLIMITED\n   JobState=RUNNING StartTime=09/25-17:17:30 EndTime=NONE\n   NodeList=hydra[12-14] NodeListIndices=0,2,-1\n   AllocCPUs=1,2,1\n   NumCPUs=4 ReqNodes=2 ReqS:C:T=2:1:*\n   OverSubscribe=0 Contiguous=0 CPUs/task=0\n   MinCPUs=0 MinMemory=0 MinTmpDisk=0 Features=(null)\n   Dependency=0 Account=(null) Reason=None Network=(null)\n   ReqNodeList=(null) ReqNodeListIndices=-1\n   ExcNodeList=(null) ExcNodeListIndices=-1\n   SubmitTime=09/25-17:17:30 SuspendTime=None PreSusTime=0\n\nSee also 'scontrol --help' and 'man scontrol'\n\nConfiguration settings in slurm.conf\n\n\nSeveral slurm.conf settings are available to control the multi-core\nfeatures described above.\n\nIn addition to the description below, also see the \"Task Launch\" and\n\"Resource Selection\" sections if generating slurm.conf\nvia configurator.html.\n\nAs previously mentioned, in order for the affinity to be set, the\ntask/affinity plugin must be first enabled in slurm.conf:\n\n\nTaskPlugin=task/affinity          # enable task affinity\n\nThis setting is part of the task launch specific parameters:\n\n# o Define task launch specific parameters\n#\n#    \"TaskProlog\" : Define a program to be executed as the user before each\n#                   task begins execution.\n#    \"TaskEpilog\" : Define a program to be executed as the user after each\n#                   task terminates.\n#    \"TaskPlugin\" : Define a task launch plugin. This may be used to\n#                   provide resource management within a node (e.g. pinning\n#                   tasks to specific processors). Permissible values are:\n#      \"task/affinity\" : CPU affinity support\n#      \"task/cgroup\"   : bind tasks to resources using Linux cgroup\n#      \"task/none\"     : no task launch actions, the default\n#\n# Example:\n#\n# TaskProlog=/usr/local/slurm/etc/task_prolog # default is none\n# TaskEpilog=/usr/local/slurm/etc/task_epilog # default is none\n# TaskPlugin=task/affinity                    # default is task/none\n\nDeclare the node hardware configuration in slurm.conf:\n\n\nNodeName=dualcore[01-16] CoresPerSocket=2 ThreadsPerCore=1\n\nFor a more complete description of the various node configuration options\nsee the slurm.conf man page.\n\nLast modified 19 May 2023\n"
            }
        ]
    }
]