{
    "url": "https://slurm.schedmd.com/gres_design.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Generic Resource (GRES) Design Guide",
            "content": "OverviewGeneric Resources (GRES) are resources associated with a specific node\nthat can be allocated to jobs and steps. The most obvious example of\nGRES use would be GPUs. GRES are identified by a specific name and use an\noptional plugin to provide device-specific support. This document is meant\nto provide details about Slurm's implementation of GRES support including the\nrelevant data structures. For an overview of GRES configuration and use, see\nGeneric Resource (GRES) Scheduling.\n\nData Structures\nGRES are associated with Slurm nodes, jobs and job steps. You will find\na string variable named gres in those data structures which\nis used to store the GRES configured on a node or required by a job or step\n(e.g. \"gpu:2,nic:1\"). This string is also visible to various Slurm commands\nviewing information about those data structures (e.g. \"scontrol show job\").\nThere is a second variable associated with each of those data structures on\nthe slurmctld daemon\nnamed gres_list that is intended for program use only. Each element\nin the list gres_list provides information about a specific GRES type\n(e.g. one data structure for \"gpu\" and a second structure with information\nabout \"nic\"). The structures on gres_list contain an ID number\n(which is faster to compare than a string) plus a pointer to another structure.\nThis second structure differs somewhat for nodes, jobs, and steps (see\ngres_node_state_t, gres_job_state_t, and gres_step_state_t in\nsrc/common/gres.h for details), but contains various counters and bitmaps.\nSince these data structures differ for various entity types, the functions\nused to work with them are also different. If no GRES are associated with a\nnode, job or step, then both gres and gres_list will be NULL.\n\n------------------------\n|   Job Information    |\n|----------------------|\n| gres = \"gpu:2,nic:1\" |\n|      gres_list       |\n------------------------\n           |\n           +---------------------------------\n           |                                |\n   ------------------               ------------------\n   | List Struct    |               | List Struct    |\n   |----------------|               |----------------|\n   | id = 123 (gpu) |               | id = 124 (nic) |\n   |   gres_data    |               |   gres_data    |\n   ------------------               ------------------\n           |                                |\n           |                              ....\n           |\n           |\n------------------------------------------------\n| gres_job_state_t                             |\n|----------------------------------------------|\n| gres_count = 2                               |\n| node_count = 3                               |\n| gres_bitmap(by node) = 0,1;                  |\n|                        2,3;                  |\n|                        0,2                   |\n| gres_count_allocated_to_steps(by node) = 1;  |\n|                                          1;  |\n|                                          1   |\n| gres_bitmap_allocated_to_steps(by node) = 0; |\n|                                           2; |\n|                                           0  |\n------------------------------------------------\n\nMode of Operation\nAfter the slurmd daemon reads the configuration files, it calls the function\nnode_config_load() for each configured plugin. This can be used to\nvalidate the configuration, for example validate that the appropriate devices\nactually exist. If no GRES plugin exists for that resource type, the information\nin the configuration file is assumed correct. Each node's GRES information is\nreported by slurmd to the slurmctld daemon at node registration time.\nThe slurmctld daemon maintains GRES information in the data structures\ndescribed above for each node, including the number of configured and allocated\nresources. If those resources are identified with a specific device file\nrather than just a count, bitmaps are used record which specific resources have\nbeen allocated to jobs.\nThe slurmctld daemon's GRES information about jobs includes several arrays\nequal in length to the number of allocated nodes. The index into each of the\narrays is the sequence number of the node in that's job's allocation (e.g.\nthe first element is node zero of the job allocation). The job step's\nGRES information is similar to that of a job including the design where the\nindex into arrays is based upon the job's allocation. This means when a job\nstep is allocated or terminates, the required bitmap operations are very\neasy to perform without computing different index values for job and step\ndata structures.\nThe most complex operation on the GRES data structures happens when a job\nchanges size (has nodes added or removed). In that case, the array indexed by\nnode index must be rebuilt, with records shifting as appropriate. Note that\nthe current software is not compatible with having different GRES counts by\nnode (a job can not have 2 GPUs on one node and 1 GPU on a second node),\nalthough that might be addressed at a later time.\nWhen a job or step is initiated, its credential includes allocated GRES information.\nThis can be used by the slurmd daemon to associate those resources with that\njob. Our plan is to use the Linux cgroups logic to bind a job and/or its\ntasks with specific GRES devices, however that logic does not currently exist.\nWhat does exist today is a pair of plugin APIs, job_set_env() and\nstep_set_env() which can be used to set environment variables for the\nprogram directing it to GRES which have been allocated for its use (the CUDA\nlibraries base their GPU selection upon environment variables, so this logic\nshould work for CUDA today if users do not attempt to manipulate the\nenvironment variables reserved for CUDA use).\nIf you want to see how GRES logic is allocating resources, configure\nDebugFlags=GRES to log GRES state changes. Note the resulting output can\nbe quite verbose, especially for larger clusters.\nLast modified 6 August 2021\n"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        },
        {
            "title": "Data Structures",
            "content": "GRES are associated with Slurm nodes, jobs and job steps. You will find\na string variable named gres in those data structures which\nis used to store the GRES configured on a node or required by a job or step\n(e.g. \"gpu:2,nic:1\"). This string is also visible to various Slurm commands\nviewing information about those data structures (e.g. \"scontrol show job\").\nThere is a second variable associated with each of those data structures on\nthe slurmctld daemon\nnamed gres_list that is intended for program use only. Each element\nin the list gres_list provides information about a specific GRES type\n(e.g. one data structure for \"gpu\" and a second structure with information\nabout \"nic\"). The structures on gres_list contain an ID number\n(which is faster to compare than a string) plus a pointer to another structure.\nThis second structure differs somewhat for nodes, jobs, and steps (see\ngres_node_state_t, gres_job_state_t, and gres_step_state_t in\nsrc/common/gres.h for details), but contains various counters and bitmaps.\nSince these data structures differ for various entity types, the functions\nused to work with them are also different. If no GRES are associated with a\nnode, job or step, then both gres and gres_list will be NULL.\n------------------------\n|   Job Information    |\n|----------------------|\n| gres = \"gpu:2,nic:1\" |\n|      gres_list       |\n------------------------\n           |\n           +---------------------------------\n           |                                |\n   ------------------               ------------------\n   | List Struct    |               | List Struct    |\n   |----------------|               |----------------|\n   | id = 123 (gpu) |               | id = 124 (nic) |\n   |   gres_data    |               |   gres_data    |\n   ------------------               ------------------\n           |                                |\n           |                              ....\n           |\n           |\n------------------------------------------------\n| gres_job_state_t                             |\n|----------------------------------------------|\n| gres_count = 2                               |\n| node_count = 3                               |\n| gres_bitmap(by node) = 0,1;                  |\n|                        2,3;                  |\n|                        0,2                   |\n| gres_count_allocated_to_steps(by node) = 1;  |\n|                                          1;  |\n|                                          1   |\n| gres_bitmap_allocated_to_steps(by node) = 0; |\n|                                           2; |\n|                                           0  |\n------------------------------------------------\nMode of OperationAfter the slurmd daemon reads the configuration files, it calls the function\nnode_config_load() for each configured plugin. This can be used to\nvalidate the configuration, for example validate that the appropriate devices\nactually exist. If no GRES plugin exists for that resource type, the information\nin the configuration file is assumed correct. Each node's GRES information is\nreported by slurmd to the slurmctld daemon at node registration time.The slurmctld daemon maintains GRES information in the data structures\ndescribed above for each node, including the number of configured and allocated\nresources. If those resources are identified with a specific device file\nrather than just a count, bitmaps are used record which specific resources have\nbeen allocated to jobs.The slurmctld daemon's GRES information about jobs includes several arrays\nequal in length to the number of allocated nodes. The index into each of the\narrays is the sequence number of the node in that's job's allocation (e.g.\nthe first element is node zero of the job allocation). The job step's\nGRES information is similar to that of a job including the design where the\nindex into arrays is based upon the job's allocation. This means when a job\nstep is allocated or terminates, the required bitmap operations are very\neasy to perform without computing different index values for job and step\ndata structures.The most complex operation on the GRES data structures happens when a job\nchanges size (has nodes added or removed). In that case, the array indexed by\nnode index must be rebuilt, with records shifting as appropriate. Note that\nthe current software is not compatible with having different GRES counts by\nnode (a job can not have 2 GPUs on one node and 1 GPU on a second node),\nalthough that might be addressed at a later time.When a job or step is initiated, its credential includes allocated GRES information.\nThis can be used by the slurmd daemon to associate those resources with that\njob. Our plan is to use the Linux cgroups logic to bind a job and/or its\ntasks with specific GRES devices, however that logic does not currently exist.\nWhat does exist today is a pair of plugin APIs, job_set_env() and\nstep_set_env() which can be used to set environment variables for the\nprogram directing it to GRES which have been allocated for its use (the CUDA\nlibraries base their GPU selection upon environment variables, so this logic\nshould work for CUDA today if users do not attempt to manipulate the\nenvironment variables reserved for CUDA use).If you want to see how GRES logic is allocating resources, configure\nDebugFlags=GRES to log GRES state changes. Note the resulting output can\nbe quite verbose, especially for larger clusters.Last modified 6 August 2021"
        }
    ]
}