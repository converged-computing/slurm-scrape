{
    "url": "https://slurm.schedmd.com/reservations.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Advanced Resource Reservation Guide",
            "content": "Slurm has the ability to reserve resources for jobs\nbeing executed by select users and/or select bank accounts.\nA resource reservation identifies the resources in that reservation\nand a time period during which the reservation is available.\nThe resources which can be reserved include cores, nodes, licenses and/or\nburst buffers.\nA reservation that contains nodes or cores is associated with one partition,\nand can't span resources over multiple partitions.\nThe only exception from this is when\nthe reservation is created with explicitly requested nodes.\nNote that resource reservations are not compatible with Slurm's\ngang scheduler plugin since the termination time of running jobs\ncannot be accurately predicted.Note that reserved burst buffers and licenses are treated somewhat\ndifferently than reserved cores or nodes.\nWhen cores or nodes are reserved, then jobs using that reservation can use only\nthose resources (this behavior can be change using FLEX flag) and no other jobs can use those resources.\nReserved burst buffers and licenses can only be used by jobs associated with\nthat reservation, but licenses not explicitly reserved are available to any job.\nThis eliminates the need to explicitly put licenses into every advanced\nreservation created.Reservations can be created, updated, or destroyed only by user root\nor the configured SlurmUser using the scontrol command.\nThe scontrol and sview commands can be used\nto view reservations. Additionally, root and the configured SlurmUser\nhave access to all reservations, even if they would normally not have access.\nThe man pages for the various commands contain details.Reservation Creation\n\nOne common mode of operation for a reservation would be to reserve\nan entire computer at a particular time for a system down time.\nThe example below shows the creation of a full-system reservation\nat 16:00 hours on 6 February and lasting for 120 minutes.\nThe \"maint\" flag is used to identify the reservation for accounting\npurposes as system maintenance.\nThe \"ignore_jobs\" flag is used to indicate that we can ignore currently\nrunning jobs when creating this reservation.\nBy default, only resources which are not expected to have a running job\nat the start time can be reserved (the time limit of all running\njobs will have been reached).\nIn this case we can manually cancel the running jobs as needed\nto perform system maintenance.\nAs the reservation time approaches,\nonly jobs that can complete by the reservation time will be initiated.\n$ scontrol create reservation starttime=2009-02-06T16:00:00 \\\n   duration=120 user=root flags=maint,ignore_jobs nodes=ALL\nReservation created: root_3\n\n$ scontrol show reservation\nReservationName=root_3 StartTime=2009-02-06T16:00:00\n   EndTime=2009-02-06T18:00:00 Duration=120\n   Nodes=ALL NodeCnt=20\n   Features=(null) PartitionName=(null)\n   Flags=MAINT,SPEC_NODES,IGNORE_JOBS Licenses=(null)\n   BurstBuffers=(null)\n   Users=root Accounts=(null)\nA variation of this would be to configure licenses to represent system\nresources, such as a global file system.\nThe system resource may not require an actual license for use, but\nSlurm licenses can be used to prevent jobs needing the resource from being\nstarted when that resource is unavailable.\nOne could create a reservation for all of those licenses in order to perform\nmaintenance on that resource.\nIn the example below, we create a reservation for 1000 licenses with the name\nof \"lustre\". \nIf there are a total of 1000 lustre licenses configured in this cluster,\nthis reservation will prevent any job specifying the need for a lustre\nlicense from being scheduled on this cluster during this reservation.\n$ scontrol create reservation starttime=2009-04-06T16:00:00 \\\n   duration=120 user=root flags=license_only \\\n   licenses=lustre:1000\nReservation created: root_4\n\n$ scontrol show reservation\nReservationName=root_4 StartTime=2009-04-06T16:00:00\n   EndTime=2009-04-06T18:00:00 Duration=120\n   Nodes= NodeCnt=0\n   Features=(null) PartitionName=(null)\n   Flags=LICENSE_ONLY Licenses=lustre*1000\n   BurstBuffers=(null)\n   Users=root Accounts=(null)\nAnother mode of operation would be to reserve specific nodes\nfor an indefinite period in order to study problems on those\nnodes. This could also be accomplished using a Slurm partition\nspecifically for this purpose, but that would fail to capture\nthe maintenance nature of their use.\n$ scontrol create reservation user=root starttime=now \\\n   duration=infinite flags=maint nodes=sun000\nReservation created: root_5\n\n$ scontrol show res\nReservationName=root_5 StartTime=2009-02-04T16:22:57\n   EndTime=2009-02-04T16:21:57 Duration=4294967295\n   Nodes=sun000 NodeCnt=1\n   Features=(null) PartitionName=(null)\n   Flags=MAINT,SPEC_NODES Licenses=(null)\n   BurstBuffers=(null)\n   Users=root Accounts=(null)\nOur next example is to reserve ten nodes in the default\nSlurm partition starting at noon and with a duration of 60\nminutes occurring daily. The reservation will be available\nonly to users \"alan\" and \"brenda\".\n$ scontrol create reservation user=alan,brenda \\\n   starttime=noon duration=60 flags=daily nodecnt=10\nReservation created: alan_6\n\n$ scontrol show res\nReservationName=alan_6 StartTime=2009-02-05T12:00:00\n   EndTime=2009-02-05T13:00:00 Duration=60\n   Nodes=sun[000-003,007,010-013,017] NodeCnt=10\n   Features=(null) PartitionName=pdebug\n   Flags=DAILY Licenses=(null) BurstBuffers=(null)\n   Users=alan,brenda Accounts=(null)\nOur next example is to reserve 100GB of burst buffer space\nstarting at noon today and with a duration of 60 minutes.\nThe reservation will be available only to users \"alan\" and \"brenda\".\n$ scontrol create reservation user=alan,brenda \\\n   starttime=noon duration=60 flags=any_nodes burstbuffer=100GB\nReservation created: alan_7\n\n$ scontrol show res\nReservationName=alan_7 StartTime=2009-02-05T12:00:00\n   EndTime=2009-02-05T13:00:00 Duration=60\n   Nodes= NodeCnt=0\n   Features=(null) PartitionName=(null)\n   Flags=ANY_NODES Licenses=(null) BurstBuffer=100GB\n   Users=alan,brenda Accounts=(null)\nNote that specific nodes to be associated with the reservation are\nidentified immediately after creation of the reservation. This permits\nusers to stage files to the nodes in preparation for use during the\nreservation. Note that the reservation creation request can also\nidentify the partition from which to select the nodes or _one_\nfeature that every selected node must contain.On a smaller system, one might want to reserve cores rather than\nwhole nodes.\nThis capability permits the administrator to identify the core count to be\nreserved on each node as shown in the examples below.\nNOTE: Core reservations are not available when the system is configured\nto use the select/linear plugin.\n# Create a two core reservation for user alan\n$ scontrol create reservation StartTime=now Duration=60 \\\n  NodeCnt=1 CoreCnt=2 User=alan\n\n# Create a reservation for user brenda with two cores on\n# node tux8 and 4 cores on node tux9\n$ scontrol create reservation StartTime=now Duration=60 \\\n  Nodes=tux8,tux9 CoreCnt=2,4 User=brenda\nReservations can not only be created for the use of specific accounts and\nusers, but specific accounts and/or users can be prevented from using them.\nIn the following example, a reservation is created for account \"foo\", but user\n\"alan\" is prevented from using the reservation even when using the account\n\"foo\".\n$ scontrol create reservation account=foo \\\n   user=-alan partition=pdebug \\\n   starttime=noon duration=60 nodecnt=2k,2k\nReservation created: alan_9\n\n$ scontrol show res\nReservationName=alan_9 StartTime=2011-12-05T13:00:00\n   EndTime=2011-12-05T14:00:00 Duration=60\n   Nodes=bgp[000x011,210x311] NodeCnt=4096\n   Features=(null) PartitionName=pdebug\n   Flags= Licenses=(null) BurstBuffers=(null)\n   Users=-alan Accounts=foo\nWhen creating a reservation, you can request that Slurm include all the\nnodes in a partition by specifying the PartitionName option.\nIf you only want a certain number of nodes or CPUs from that partition\nyou can combine PartitionName with the CoreCnt, NodeCnt\nor TRES options to specify how many of a resource you want.\nIn the following example, a reservation is created in the 'gpu' partition\nthat uses the TRES option to limit the reservation to 24 processors,\ndivided among 4 nodes.\n$ scontrol create reservationname=test start=now duration=1 \\\n   user=user1 partition=gpu tres=cpu=24,node=4\nReservation created: test\n\n$ scontrol show res\nReservationName=test StartTime=2020-08-28T11:07:09\n   EndTime=2020-08-28T11:08:09 Duration=00:01:00\n   Nodes=node[01-04] NodeCnt=4 CoreCnt=24\n   Features=(null) PartitionName=gpu\n     NodeName=node01 CoreIDs=0-5\n     NodeName=node02 CoreIDs=0-5\n     NodeName=node03 CoreIDs=0-5\n     NodeName=node04 CoreIDs=0-5\n   TRES=cpu=24\n   Users=user1 Accounts=(null) Licenses=(null)\n   State=ACTIVE BurstBuffer=(null)\n   MaxStartDelay=(null)\nReservation UseThe reservation create response includes the reservation's name.\nThis name is automatically generated by Slurm based upon the first\nuser or account name and a numeric suffix. In order to use the\nreservation, the job submit request must explicitly specify that\nreservation name. The job must be contained completely within the\nnamed reservation. The job will be canceled after the reservation\nreaches its EndTime. If letting the job continue execution after\nthe reservation EndTime, a configuration option ResvOverRun\nin slurm.conf can be set to control how long the job can continue execution.\n$ sbatch --reservation=alan_6 -N4 my.script\nsbatch: Submitted batch job 65540\nNote that use of a reservation does not alter a job's priority, but it\ndoes act as an enhancement to the job's priority.\nAny job with a reservation is considered for scheduling to resources \nbefore any other job in the same Slurm partition (queue) not associated\nwith a reservation.Reservation Modification\n\nReservations can be modified by user root as desired.\nFor example their duration could be altered or the users\ngranted access changed as shown below:\n$ scontrol update ReservationName=root_3 \\\n   duration=150 users=admin\nReservation updated.\n\nbash-3.00$ scontrol show ReservationName=root_3\nReservationName=root_3 StartTime=2009-02-06T16:00:00\n   EndTime=2009-02-06T18:30:00 Duration=150\n   Nodes=ALL NodeCnt=20 Features=(null)\n   PartitionName=(null) Flags=MAINT,SPEC_NODES\n   Licenses=(null) BurstBuffers=(null)\n   Users=admin Accounts=(null)\nReservation Deletion\n\nReservations are automatically purged after their end time.\nThey may also be manually deleted as shown below.\nNote that a reservation can not be deleted while there are\njobs running in it.\n$ scontrol delete ReservationName=alan_6\n\nNOTE: By default, when a reservation ends the reservation request will be\nremoved from any pending jobs submitted to the reservation and will be put into\na held state.  Use the NO_HOLD_JOBS_AFTER_END reservation flag to let jobs run\noutside of the reservation after the reservation is gone.\nOverlapping Reservations\n\nBy default, reservations must not overlap. They must either include\ndifferent nodes or operate at different times. If specific nodes\nare not specified when a reservation is created, Slurm will\nautomatically select nodes to avoid overlap and ensure that\nthe selected nodes are available when the reservation begins.There is very limited support for overlapping reservations\nwith two specific modes of operation available.\nFor ease of system maintenance, you can create a reservation\nwith the \"maint\" flag that overlaps existing reservations.\nThis permits an administrator to easily create a maintenance\nreservation for an entire cluster without needing to remove\nor reschedule pre-existing reservations. Users requesting access\nto one of these pre-existing reservations will be prevented from\nusing resources that are also in this maintenance reservation.\nFor example, users alan and brenda might have a reservation for\nsome nodes daily from noon until 1PM. If there is a maintenance\nreservation for all nodes starting at 12:30PM, the only jobs they\nmay start in their reservation would have to be completed by 12:30PM,\nwhen the maintenance reservation begins.The second exception operates in the same manner as a maintenance\nreservation except that it is not logged in the accounting system as nodes\nreserved for maintenance.\nIt requires the use of the \"overlap\" flag when creating the second\nreservation.\nThis might be used to ensure availability of resources for a specific\nuser within a group having a reservation.\nUsing the previous example of alan and brenda having a 10 node reservation\nfor 60 minutes, we might want to reserve 4 nodes of that for brenda\nduring the first 30 minutes of the time period.\nIn this case, the creation of one overlapping reservation (for a total of\ntwo reservations) may be simpler than creating three separate reservations,\npartly since the use of any reservation requires the job specification\nof the reservation name.\n\nA six node reservation for both alan and brenda that lasts the full\n60 minutes\nA four node reservation for brenda for the first 30 minutes\nA four node reservation for both alan and brenda that lasts for the\nfinal 30 minutes\nIf the \"maint\" or \"overlap\" flag is used when creating reservations,\none could create a reservation within a reservation within a third\nreservation.\nNote a reservation having a \"maint\" or \"overlap\" flag will not have\nresources removed from it by a subsequent reservation also having a\n\"maint\" or \"overlap\" flag, so nesting of reservations only works to a\ndepth of two.Reservations Floating Through Time\n\nSlurm can be used to create an advanced reservation with a start time that\nremains a fixed period of time in the future.\nThese reservation are not intended to run jobs, but to prevent long running\njobs from being initiated on specific nodes.\nThat node might be placed in a DRAINING state to prevent any new jobs\nfrom being started there.\nAlternately, an advanced reservation might be placed on the node to prevent\njobs exceeding some specific time limit from being started.\nAttempts by users to make use of a reservation with a floating start time will\nbe rejected.\nWhen ready to perform the maintenance, place the node in DRAINING state and\ndelete the previously created advanced reservation.Create the reservation by using the flag value of TIME_FLOAT and a\nstart time that is relative to the current time (use the keyword now).\nThe reservation duration should generally be a value which is large relative\nto typical job run times in order to not adversely impact backfill scheduling\ndecisions.\nAlternately the reservation can have a specific end time, in which case the\nreservation's start time will increase through time until the reservation's\nend time is reached.\nWhen the current time passes the reservation end time then the reservation will\nbe purged.\nIn the example below, node tux8 is prevented from starting any jobs exceeding\na 60 minute time limit.\nThe duration of this reservation is 100 (minutes).\n$ scontrol create reservation user=operator nodes=tux8 \\\n  starttime=now+60minutes duration=100 flags=time_float\nReservations that Replace Allocated Resources\n\nBy default, nodes in a reservation that are DOWN or DRAINED will be replaced,\nbut not nodes that are allocated to jobs. This behavior may be explicitly\nrequested with the REPLACE_DOWN flag.However, you may instruct Slurm to also replace nodes which are allocated to\njobs with new idle nodes. This is done using the REPLACE flag as shown in\nthe example below.\nThe effect of this is to always maintain a constant size pool of resources.\nThis option is not supported for reservations specifying cores which\nspan more than one node, rather than full nodes. (E.g. a 1 core reservation on\nnode \"tux1\" will be moved if node \"tux1\" goes down, but a reservation\ncontaining 2 cores on node \"tux1\" and 3 cores on \"tux2\" will not be moved if\n\"tux1\" goes down.)\n$ scontrol create reservation starttime=now duration=60 \\\n  users=foo nodecnt=2 flags=replace\nReservation created: foo_82\n\n$ scontrol show res\nReservationName=foo_82 StartTime=2014-11-20T16:21:11\n   EndTime=2014-11-20T17:21:11 Duration=01:00:00\n   Nodes=tux[0-1] NodeCnt=2 CoreCnt=12 Features=(null)\n   PartitionName=debug Flags=REPLACE\n   Users=jette Accounts=(null) Licenses=(null) State=ACTIVE\n\n$ sbatch -n4 --reservation=foo_82 tmp\nSubmitted batch job 97\n\n$ scontrol show res\nReservationName=foo_82 StartTime=2014-11-20T16:21:11\n   EndTime=2014-11-20T17:21:11 Duration=01:00:00\n   Nodes=tux[1-2] NodeCnt=2 CoreCnt=12 Features=(null)\n   PartitionName=debug Flags=REPLACE\n   Users=jette Accounts=(null) Licenses=(null) State=ACTIVE\n\n$ sbatch -n4 --reservation=foo_82 tmp\nSubmitted batch job 98\n\n$ scontrol show res\nReservationName=foo_82 StartTime=2014-11-20T16:21:11\n   EndTime=2014-11-20T17:21:11 Duration=01:00:00\n   Nodes=tux[2-3] NodeCnt=2 CoreCnt=12 Features=(null)\n   PartitionName=debug Flags=REPLACE\n   Users=jette Accounts=(null) Licenses=(null) State=ACTIVE\n\n$ squeue\nJOBID PARTITION  NAME  USER ST  TIME  NODES NODELIST(REASON)\n   97     debug   tmp   foo  R  0:09      1 tux0\n   98     debug   tmp   foo  R  0:07      1 tux1\nFLEX ReservationsBy default, jobs that run in reservations must fit within the time and\nsize constraints of the reserved resources. With the FLEX flag jobs\nare able to start before the reservation begins or continue after it ends.\nThey are also able to use the reserved node(s) along with additional nodes if\nrequired and available.\n\nThe default behavior for jobs that request a reservation is that they must\nbe able to run within the confines (time and space) of that reservation.\nThe following example shows that the FLEX flag allows the job to run\nbefore the reservation starts, after it ends, and on a node outside\nof the reservation.\n\n$ scontrol create reservation user=user1 nodes=node01 starttime=now+10minutes duration=10 flags=flex\nReservation created: user1_831\n\n$ sbatch -wnode0[1-2] -t30:00 --reservation=user1_831 test.job\nSubmitted batch job 57996\n\n$ squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n             57996     debug sleepjob    user1  R       0:08      2 node[01-02]\n\nMagnetic Reservations\n\n\nThe default behavior for reservations is that jobs must request a\nreservation in order to run in it. The MAGNETIC flag allows you to\ncreate a reservation that will allow jobs to run in it without requiring that\nthey specify the name of the reservation. The reservation will only \"attract\"\njobs that meet the access control requirements.\nNOTE: Magnetic reservations cannot \"attract\" heterogeneous jobs -\nheterogeneous jobs will only run in magnetic reservations if they explicitly\nrequest the reservation.\nThe following example shows a reservation created on node05. The user\nspecified as being able to access the reservation then submits a job and\nthe job starts on the reserved node.\n\n$ scontrol create reservation user=user1 nodes=node05 starttime=now duration=10 flags=magnetic\nReservation created: user1_850\n\n$ scontrol show res\nReservationName=user1_850 StartTime=2020-07-29T13:44:13 EndTime=2020-07-29T13:54:13 Duration=00:10:00\n   Nodes=node05 NodeCnt=1 CoreCnt=12 Features=(null) PartitionName=(null) Flags=SPEC_NODES,MAGNETIC\n   TRES=cpu=12\n   Users=user1 Accounts=(null) Licenses=(null) State=ACTIVE BurstBuffer=(null)\n   MaxStartDelay=(null)\n\n$ sbatch -N1 -t5:00 test.job\nSubmitted batch job 62297\n\n$ squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n             62297     debug sleepjob    user1  R       0:04      1 node05\n\nReservation Purging After Last Job\n\n\nA reservation may be automatically purged after the last associated job\ncompletes. This is accomplished by using a \"purge_comp\" flag.\nOnce the reservation has been created, it must be populated within 5 minutes\nof its start time or it will be purged before any jobs have been run.\nReservation Accounting\n\n\nJobs executed within a reservation are accounted for using the appropriate\nuser and bank account. If resources within a reservation are not used, those\nresources will be accounted for as being used by all users or bank accounts\nassociated with the reservation on an equal basis (e.g. if two users are\neligible to use a reservation and neither does, each user will be reported\nto have used half of the reserved resources).\nProlog and Epilog\n\n\nSlurm supports both a reservation prolog and epilog.\nThey may be configured using the ResvProlog and ResvEpilog\nconfiguration parameters in the slurm.conf file.\nThese scripts can be used to cancel jobs, modify partition configuration,\netc.\nFuture Work\nReservations made within a partition having gang scheduling assumes\nthe highest level rather than the actual level of time-slicing when\nconsidering the initiation of jobs.\nThis will prevent the initiation of some jobs which would complete execution\nbefore a reservation given fewer jobs to time-slice with.\nLast modified 02 August 2024\n"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        },
        {
            "title": "Magnetic Reservations\n\n",
            "content": "The default behavior for reservations is that jobs must request a\nreservation in order to run in it. The MAGNETIC flag allows you to\ncreate a reservation that will allow jobs to run in it without requiring that\nthey specify the name of the reservation. The reservation will only \"attract\"\njobs that meet the access control requirements.NOTE: Magnetic reservations cannot \"attract\" heterogeneous jobs -\nheterogeneous jobs will only run in magnetic reservations if they explicitly\nrequest the reservation.The following example shows a reservation created on node05. The user\nspecified as being able to access the reservation then submits a job and\nthe job starts on the reserved node.\n$ scontrol create reservation user=user1 nodes=node05 starttime=now duration=10 flags=magnetic\nReservation created: user1_850\n\n$ scontrol show res\nReservationName=user1_850 StartTime=2020-07-29T13:44:13 EndTime=2020-07-29T13:54:13 Duration=00:10:00\n   Nodes=node05 NodeCnt=1 CoreCnt=12 Features=(null) PartitionName=(null) Flags=SPEC_NODES,MAGNETIC\n   TRES=cpu=12\n   Users=user1 Accounts=(null) Licenses=(null) State=ACTIVE BurstBuffer=(null)\n   MaxStartDelay=(null)\n\n$ sbatch -N1 -t5:00 test.job\nSubmitted batch job 62297\n\n$ squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n             62297     debug sleepjob    user1  R       0:04      1 node05\nReservation Purging After Last Job\n\nA reservation may be automatically purged after the last associated job\ncompletes. This is accomplished by using a \"purge_comp\" flag.\nOnce the reservation has been created, it must be populated within 5 minutes\nof its start time or it will be purged before any jobs have been run.Reservation Accounting\n\nJobs executed within a reservation are accounted for using the appropriate\nuser and bank account. If resources within a reservation are not used, those\nresources will be accounted for as being used by all users or bank accounts\nassociated with the reservation on an equal basis (e.g. if two users are\neligible to use a reservation and neither does, each user will be reported\nto have used half of the reserved resources).Prolog and Epilog\n\nSlurm supports both a reservation prolog and epilog.\nThey may be configured using the ResvProlog and ResvEpilog\nconfiguration parameters in the slurm.conf file.\nThese scripts can be used to cancel jobs, modify partition configuration,\netc.Future WorkReservations made within a partition having gang scheduling assumes\nthe highest level rather than the actual level of time-slicing when\nconsidering the initiation of jobs.\nThis will prevent the initiation of some jobs which would complete execution\nbefore a reservation given fewer jobs to time-slice with.Last modified 02 August 2024"
        }
    ]
}