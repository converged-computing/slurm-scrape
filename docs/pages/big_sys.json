{
    "url": "https://slurm.schedmd.com/big_sys.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Large Cluster Administration Guide",
            "content": "This document contains Slurm administrator information specifically\nfor clusters containing 1,024 nodes or more.\nSome examples of large systems currently managed by Slurm are:\n\nFrontier at Oak Ridge National Laboratory (ORNL) with 8,699,904 cores.\nTianhe-2 at the National University of Defense Technology in China with\n4,981,760 cores.\nPerlmutter at National Energy Research Scientific Computing (NERSC) with\n761,856 cores.\n\nSlurm operation on systems orders of magnitude larger has been validated\nusing emulation.\nGetting optimal performance at that scale does require some tuning and\nthis document should help get you off to a good start.\nA working knowledge of Slurm should be considered a prerequisite\nfor this material.PerformanceTimes below are for execution of an MPI program printing \"Hello world\" and\nexiting and includes the time for processing output. Your performance may\nvary due to differences in hardware, software, and configuration.\n1,966,080 tasks on 122,880 compute nodes of a BlueGene/Q: 322 seconds\n30,000 tasks on 15,000 compute nodes of a Linux cluster: 30 seconds\nSystem Configuration\n\nThree system configuration parameters must be set to support a large number\nof open files and TCP connections with large bursts of messages. Changes can\nbe made using the /etc/rc.d/rc.local or /etc/sysctl.conf \nscript to preserve changes after reboot. In either case, you can write values\ndirectly into these files\n(e.g. \"echo 388067 > /proc/sys/fs/file-max\").\n/proc/sys/fs/file-max:\nThe maximum number of concurrently open files. The appropriate amount is highly\ndependent on system specs and workload. We recommend starting with a minimum of\n388067 or the default for your OS, whichever is greater. This may need to be\nadjusted upwards, depending on your needs.\n/proc/sys/net/ipv4/tcp_max_syn_backlog:\nMaximum number of remembered connection requests, which still have not\nreceived an acknowledgment from the connecting client.\nThe default value is 1024 for systems with more than 128Mb of memory, and 128\nfor low memory machines. If server suffers of overload, try to increase this\nnumber.\n/proc/sys/net/core/somaxconn:\nLimit of socket listen() backlog, known in userspace as SOMAXCONN. Defaults to\n128. The value should be raised substantially to support bursts of request.\nFor example, to support a burst of 1024 requests, set somaxconn to 1024.\nThe transmit queue length (txqueuelen) may also need to be modified\nusing the ifconfig command. A value of 4096 has been found to work well for one\nsite with a very large cluster\n(e.g. \"ifconfig  txqueuelen 4096\").Thread/Process Limit\n\nThere is a newly introduced limit in SLES 12 SP2 (used on Cray systems\nwith CLE 6.0UP04, to be released mid-2017).\nThe version of systemd shipped with SLES 12 SP2 contains support for the\n\nPIDs cgroup controller.\nUnder the new systemd version, each init script or systemd service is limited\nto 512 threads/processes by default.\nThis could cause issues for the slurmctld and slurmd daemons on large clusters\nor systems with a high job throughput rate.\nTo increase the limit beyond the default:\nIf using a systemd service file: Add TasksMax=N to the [Service]\n section. N can be a specific number, or special value infinity.\nIf using an init script: Create the file\n/etc/systemd/system/<init script name>.service.d/override.conf\nwith these contents:\n\n  [Service]\n  TasksMax=N\n\nNote: Earlier versions of systemd that don't support the PIDs cgroup\ncontroller simply ignore the TasksMax setting.User LimitsThe ulimit values in effect for the slurmctld daemon should\nbe set quite high for memory size, open file count and stack size.Node Selection Plugin (SelectType)\n\nWhile allocating individual processors within a node is great\nfor smaller clusters, the overhead of keeping track of the individual\nprocessors and memory within each node adds significant overhead.\nFor best scalability, allocate whole nodes using select/linear\nand avoid select/cons_tres.Job Accounting Gather Plugin (JobAcctGatherType)\n\nJob accounting relies upon the slurmstepd daemon on each compute\nnode periodically sampling data.\nThis data collection will take compute cycles away from the application\ninducing what is known as system noise.\nFor large parallel applications, this system noise can detract from\napplication scalability.\nFor optimal application performance, disabling job accounting\nis best (jobacct_gather/none).\nConsider use of job completion records (JobCompType) for accounting\npurposes as this entails far less overhead.\nIf job accounting is required, configure the sampling interval\nto a relatively large size (e.g. JobAcctGatherFrequency=300).\nSome experimentation may be required to deal with collisions\non data transmission.Node Configuration\n\nWhile Slurm can track the amount of memory and disk space actually found\non each compute node and use it for scheduling purposes, this entails\nextra overhead.\nOptimize performance by specifying the expected configuration using\nthe available parameters (RealMemory, CPUs, and\nTmpDisk).\nIf the node is found to contain less resources than configured,\nit will be marked DOWN and not used.\nWhile Slurm can easily handle a heterogeneous cluster, configuring\nthe nodes using the minimal number of lines in slurm.conf\nwill both make for easier administration and better performance.TimersThe EioTimeout configuration parameter controls how long the srun\ncommand will wait for the slurmstepd to close the TCP/IP connection used to\nrelay data between the user application and srun when the user application\nterminates. The default value is 60 seconds. Larger systems and/or slower\nnetworks may need a higher value.If a high throughput of jobs is anticipated (i.e. large numbers of jobs\nwith brief execution times) then configure MinJobAge to the smallest\ninterval practical for your environment. MinJobAge specifies the\nminimum number of seconds that a terminated job will be retained by Slurm's\ncontrol daemon before purging. After this time, information about terminated\njobs will only be available through accounting records.The configuration parameter SlurmdTimeout determines the interval\nat which slurmctld routinely communicates with slurmd.\nCommunications occur at half the SlurmdTimeout value.\nThe purpose of this is to determine when a compute node fails\nand thus should not be allocated work.\nLonger intervals decrease system noise on compute nodes (we do\nsynchronize these requests across the cluster, but there will\nbe some impact upon applications).\nFor really large clusters, SlurmdTimeout values of\n120 seconds or more are reasonable.If MPICH-2 is used, the srun command will manage the key-pairs\nused to bootstrap the application.\nDepending upon the processor speed and architecture, the communication\nof key-pair information may require extra time.\nThis can be done by setting an environment variable PMI_TIME before\nexecuting srun to launch the tasks.\nThe default value of PMI_TIME is 500 and this is the number of\nmicroseconds allotted to transmit each key-pair.\nWe have executed up to 16,000 tasks with a value of PMI_TIME=4000.The individual slurmd daemons on compute nodes will initiate messages\nto the slurmctld daemon only when they start up or when the epilog\ncompletes for a job. When a job allocated a large number of nodes\ncompletes, it can cause a very large number of messages to be sent\nby the slurmd daemons on these nodes to the slurmctld daemon all at\nthe same time. In order to spread this message traffic out over time\nand avoid message loss, The EpilogMsgTime parameter may be\nused. Note that even if messages are lost, they will be retransmitted,\nbut this will result in a delay for reallocating resources to new jobs.OtherSlurm uses hierarchical communications between the slurmd daemons\nin order to increase parallelism and improve performance. The\nTreeWidth configuration parameter controls the fanout of messages.\nThe default value is 16, meaning each slurmd daemon can communicate\nwith up to 16 other slurmd daemons and 4368 nodes can be contacted\nwith three message hops.\nThe default value will work well for most clusters.\nOptimal system performance can typically be achieved if TreeWidth\nis set to the cube root of the number of nodes in the cluster.The srun command automatically increases its open file limit to\nthe hard limit in order to process all of the standard input and output\nconnections to the launched tasks. It is recommended that you set the\nopen file hard limit to 8192 across the cluster.Last modified 22 April 2024"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        }
    ]
}