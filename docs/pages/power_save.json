{
    "url": "https://slurm.schedmd.com/power_save.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Slurm Power Saving Guide",
            "content": "Contents\nOverview\nConfiguration\nNode Lifecycle\nManual Power Saving\nResume and Suspend Programs\nFault Tolerance\nBooting Different Images\nUse of Allocations\nNode Features\nHybrid Cluster\nCloud Accounting\nOverviewSlurm provides an integrated mechanism for nodes being suspended (powered\ndown, placed into power saving mode) and resumed (powered up, restored from\npower saving mode) on demand or by request. Nodes that remain IDLE for\nSuspendTime will be suspended by SuspendProgram and will be\nunavailable for scheduling for SuspendTimeout. Nodes will\nautomatically be resumed by ResumeProgram to complete work allocated\nto them. Nodes that fail to register within ResumeTimeout will become\nDOWN and their allocated jobs are requeued. Node power saving can be\nmanually requested by scontrol update nodename=<nodename>\nstate=power_<down|up>. The rate of nodes being resumed or\nsuspended can be controlled by ResumeRate and SuspendRate.Slurm can be configured to accomplish power saving by managing compute\nresources in any cloud provider (e.g. Amazon\nWeb Services, Google Cloud\nPlatform, Microsoft Azure) via\ntheir API. These resources can be combined with an existing cluster to\nprocess excess workload (cloud bursting) or it can operate as an independent\nand self-contained cluster.To enable Power Saving operation in Slurm, you must configure the\nfollowing:\nResumeProgram and SuspendProgram must be defined. Their\n  value must be a valid path to a program.\nResumeTimeout and SuspendTimeout must be defined, either\n  globally or on at least one partition.\nSuspendTime must be defined, either globally or on at least one\n  partition, and not be INFINITE or -1.\nResumeRate and SuspendRate must be greater than or equal\n  to 0.\nThe Slurm control daemon, slurmctld, must be restarted to initially\nenable Power Saving operation. Changes in the configuration parameters (e.g.\nSuspendTime) will take effect after modifying the slurm.conf\nconfiguration file and executing scontrol reconfigure.ConfigurationThe following configuration parameters of interest include:\nDebugFlags\n\n\nDefines specific subsystems which should provide more detailed event\n    logging. Options of interest include:\n\nPower\n\n\nPower management plugin and power save (suspend/resume programs)\n        details.\n\n\n\nReconfigFlags\n\n\nFlags to control various actions that may be taken when an\n    scontrol reconfigure command is issued. Options of interest\n    include:\n\nKeepPowerSaveSettings\n\n\nIf set, an scontrol reconfigure command will preserve\n        the current state of SuspendExcNodes, SuspendExcParts,\n        and SuspendExcStates.\n\n\n\nResumeFailProgram\n\n\nProgram to be executed when nodes fail to resume by\n    ResumeTimeout. The argument to the program will be the names of\n    the failed nodes (using Slurm's hostlist expression format).\n\nResumeProgram\n\n\nProgram to be executed to restore nodes from power saving mode. The\n    program executes as SlurmUser (as configured in\n    slurm.conf). The argument to the program will be the names of\n    nodes to be restored from power savings mode (using Slurm's hostlist\n    expression format).\nIf the slurmd daemon fails to respond within the configured\n    ResumeTimeout value with an updated BootTime, the node will be\n    placed in a DOWN state and the job requesting the node will be requeued.\n    If the node isn't actually rebooted (e.g. when multiple-slurmd is\n    configured) you can start slurmd with the \"-b\" option to report the node\n    boot time as now.\nA job to node mapping is available in JSON format by reading the\n    temporary file specified by the SLURM_RESUME_FILE environment\n    variable. This file should be used at the beginning of\n    ResumeProgram - see the Fault Tolerance\n    section for more details. This program may use the scontrol show\n    nodename command to ensure that a node has booted and the\n    slurmd daemon started.\n\nSLURM_RESUME_FILE=/proc/1647372/fd/7:\n{\n  \"all_nodes_resume\" : \"cloud[1-3,7-8]\",\n  \"jobs\" : [\n    {\n      \"extra\" : \"An arbitrary string from --extra\",\n      \"features\" : \"c1,c2\",\n      \"job_id\" : 140814,\n      \"nodes_alloc\" : \"cloud[1-4]\",\n      \"nodes_resume\" : \"cloud[1-3]\",\n      \"oversubscribe\" : \"OK\",\n      \"partition\" : \"cloud\",\n      \"reservation\" : \"resv_1234\"\n    },\n    {\n      \"extra\" : null,\n      \"features\" : \"c1,c2\",\n      \"job_id\" : 140815,\n      \"nodes_alloc\" : \"cloud[1-2]\",\n      \"nodes_resume\" : \"cloud[1-2]\",\n      \"oversubscribe\" : \"OK\",\n      \"partition\" : \"cloud\",\n      \"reservation\" : null\n    },\n    {\n      \"extra\" : null,\n      \"features\" : null\n      \"job_id\" : 140816,\n      \"nodes_alloc\" : \"cloud[7-8]\",\n      \"nodes_resume\" : \"cloud[7-8]\",\n      \"oversubscribe\" : \"NO\",\n      \"partition\" : \"cloud_exclusive\",\n      \"reservation\" : null\n    }\n  ]\n}\n\nSee the squeue man page\n    for possible values for oversubscribe.\nNOTE: The SLURM_RESUME_FILE will only exist and be\n    usable if Slurm was compiled with the JSON-C serializer library.\n\nResumeRate\n\n\nMaximum number of nodes to be removed from power saving mode per\n    minute. A value of zero results in no limits being imposed. The default\n    value is 300. Use this to prevent rapid increases in power\n    consumption.\n\nResumeTimeout\n\n\nMaximum time permitted (in seconds) between when a node resume request\n    is issued and when the node is actually available for use. Nodes which\n    fail to respond in this time frame will be marked DOWN and the jobs\n    scheduled on the node requeued. Nodes which reboot after this time frame\n    will be marked DOWN with a reason of \"Node unexpectedly rebooted.\" The\n    default value is 60 seconds.\n\nSchedulerParameters\n\n\nThe interpretation of this parameter varies by SchedulerType. Multiple\n    options may be comma separated. Options of interest include:\n\nsalloc_wait_nodes\n\n\nIf defined, the salloc command will wait until all allocated nodes\n        are ready for use (i.e. booted) before the command returns. By\n        default, salloc will return as soon as the resource allocation has\n        been made. The salloc command can use the\n        --wait-all-nodes option to override this configuration\n        parameter.\n\nsbatch_wait_nodes\n\n\nIf defined, the sbatch script will wait until all allocated nodes\n        are ready for use (i.e. booted) before the initiation. By default,\n        the sbatch script will be initiated as soon as the first node in the\n        job allocation is ready. The sbatch command can use the\n        --wait-all-nodes option to override this configuration\n        parameter.\n\n\n\nSlurmctldParameters\n\n\nComma-separated options identifying slurmctld options. Options of\n    interest include:\n\ncloud_dns\n\n\nBy default, Slurm expects that the network addresses for cloud\n        nodes won't be known until creation of the node and that Slurm will\n        be notified of the node's address upon registration. Since Slurm\n        communications rely on the node configuration found in the\n        slurm.conf, Slurm will tell the client command, after waiting for all\n        nodes to boot, each node's IP address. However, in environments where\n        the nodes are in DNS, this step can be avoided by configuring this\n        option.\n\nidle_on_node_suspend\n\n\nMark nodes as idle, regardless of current state, when suspending\n        nodes with SuspendProgram so that nodes will be eligible to be\n        resumed at a later time.\n\nnode_reg_mem_percent=#\n\n\nPercentage of memory a node is allowed to register with without\n        being marked as invalid with low memory. Default is 100. For\n        State=CLOUD nodes, the default is 90.\n\npower_save_interval=#\n\n\nHow often the power_save thread looks to resume and suspend nodes.\n        The power_save thread will do work sooner if there are node state\n        changes. Default is 10 seconds.\n\npower_save_min_interval=#\n\n\nHow often the power_save thread, at a minimum, looks to resume and\n        suspend nodes. Default is 0.\n\n\n\nSuspendExcNodes\n\n\nNodes not subject to suspend/resume logic. This may be used to avoid\n    suspending and resuming nodes which are not in the cloud. Alternately the\n    suspend/resume programs can treat local nodes differently from nodes\n    being provisioned from cloud. Use Slurm's hostlist expression to identify\n    nodes with an optional \":\" separator and count of nodes to exclude from\n    the preceding range. For example nid[10-20]:4 will prevent 4\n    usable nodes (i.e IDLE and not DOWN, DRAINING or already powered down) in\n    the set nid[10-20] from being powered down. Multiple sets of\n    nodes can be specified with or without counts in a comma separated list\n    (e.g nid[10-20]:4,nid[80-90]:2). By default, no nodes are\n    excluded. This value may be updated with scontrol. See\n    ReconfigFlags=KeepPowerSaveSettings for setting persistence.\n\nSuspendExcParts\n\n\nList of partitions with nodes to never place in power saving mode.\n    Multiple partitions may be specified using a comma separator. By default,\n    no nodes are excluded. This value may be updated with scontrol. See\n    ReconfigFlags=KeepPowerSaveSettings for setting persistence.\n\nSuspendExcStates\n\n\nSpecifies node states that are not to be powered down automatically.\n    Valid states include CLOUD, DOWN, DRAIN, DYNAMIC_FUTURE, DYNAMIC_NORM,\n    FAIL, INVALID_REG, MAINTENANCE, NOT_RESPONDING, PERFCTRS, PLANNED, and\n    RESERVED. By default, any of these states, if idle for\n    SuspendTime, would be powered down. This value may be updated with\n    scontrol. See ReconfigFlags=KeepPowerSaveSettings for setting\n    persistence.\n\nSuspendProgram\n\n\nProgram to be executed to place nodes into power saving mode. The\n    program executes as SlurmUser (as configured in\n    slurm.conf). The argument to the program will be the names of\n    nodes to be placed into power savings mode (using Slurm's hostlist\n    expression format).\n\nSuspendRate\n\n\nMaximum number of nodes to be placed into power saving mode per\n    minute. A value of zero results in no limits being imposed. The default\n    value is 60. Use this to prevent rapid drops in power consumption.\n\nSuspendTime\n\n\nNodes becomes eligible for power saving mode after being idle or down\n    for this number of seconds. A negative number disables power saving mode.\n    The default value is -1 (disabled).\n\nSuspendTimeout\n\n\nMaximum time permitted (in second) between when a node suspend request\n    is issued and when the node shutdown is complete. At that time the node\n    must ready for a resume request to be issued as needed for new workload.\n    The default value is 30 seconds.\n\nNode ConfigurationNode parameters of interest include:\nFeature\n\n\nA node feature can be associated with resources acquired from the\n    cloud and user jobs can specify their preference for resource use with\n    the --constraint option.\n\nNodeName\n\n\nThis is the name by which Slurm refers to the node. A name containing\n    a numeric suffix is recommended for convenience.\n\nState\n\n\nNodes which are to be added on demand should have a state of\n    CLOUD.\n\nWeight\n\n\nEach node can be configured with a weight indicating the desirability\n    of using that resource. Nodes with lower weights are used before those\n    with higher weights. The default value is 1.\n\nPartition ConfigurationPartition parameters of interest include:\nPowerDownOnIdle\n\n\nIf set to YES and power saving is enabled for the partition,\n    then nodes allocated from this partition will be requested to power down\n    after being allocated at least one job. These nodes will not power down\n    until they transition from COMPLETING to IDLE. If set to NO then\n    power saving will operate as configured for the partition. The default\n    value is NO.\nThe following will cause a transition from COMPLETING to\n    IDLE:\n\nCompleting all running jobs without additional jobs being\n      allocated.\nExclusiveUser=YES and after all running jobs complete but\n      before another user's job is allocated.\nOverSubscribe=EXCLUSIVE and after the running job completes\n      but before another job is allocated.\n\nNOTE: Nodes are still subject to powering down when being IDLE\n    for SuspendTime when PowerDownOnIdle is set to NO.\n\nResumeTimeout\n\n\nMaximum time permitted (in seconds) between when a node resume request\n    is issued and when the node is actually available for use. Nodes which\n    fail to respond in this time frame will be marked DOWN and the jobs\n    scheduled on the node requeued. Nodes which reboot after this time frame\n    will be marked DOWN with a reason of \"Node unexpectedly rebooted.\" The\n    default value is 60 seconds.\nFor nodes that are in multiple partitions with this option set, the\n    highest time will take effect. If not set on any partition, the node will\n    use the ResumeTimeout value set for the entire cluster.\n\nSuspendTime\n\n\nNodes which remain idle or down for this number of seconds will be\n    placed into power saving mode by SuspendProgram.\nFor nodes that are in multiple partitions with this option set, the\n    highest time will take effect. If not set on any partition, the node will\n    use the SuspendTime value set for the entire cluster. Setting\n    SuspendTime to INFINITE will disable suspending of nodes in\n    this partition.\n\nSuspendTimeout\n\n\nMaximum time permitted (in second) between when a node suspend request\n    is issued and when the node shutdown is complete. At that time the node\n    must ready for a resume request to be issued as needed for new workload.\n    The default value is 30 seconds.\nFor nodes that are in multiple partitions with this option set, the\n    highest time will take effect. If not set on any partition, the node will\n    use the SuspendTimeout value set for the entire cluster.\n\nNode LifecycleWhen Slurm is configured for Power Saving operation, nodes have an\nexpanded set of states associated with them. States associated with Power\nSaving are generally labeled with a symbol when viewing node details with\nsinfo.\n\n  Figure 1. Node Lifecycle\nNode states of interest:\n\n\n\nSTATE\n\nPower Saving Symbol\n\nDescription\n\n\n\nPOWER_DOWN\n!\nPower down request. When the node is no longer running job(s),\n        run the SuspendProgram.\n\n\nPOWER_UP\n\u00a0\nPower up request. When possible, run the\n        ResumeProgram.\n\n\nPOWERED_DOWN\n~\nThe node is powered down or in power saving mode.\n\n\nPOWERING_DOWN\n%\nThe node is in the process of powering down, or being put into\n        power saving mode, and is not capable of running any jobs for\n        SuspendTimeout.\n\n\nPOWERING_UP\n#\nThe node is in the process of powering up, or being restored from\n        power saving mode.\n\n\n\nManual Power SavingA node can be manually powered up and down by setting the state of the\nnode to the following states using scontrol:\nscontrol update nodename=<nodename> state=power_<down|down_asap|down_force|up>\nscontrol update command actions/states of interest:\nPOWER_DOWN\n\nWill use the configured SuspendProgram program to explicitly\n  place a node in power saving mode. If a node is already in the process of\n  being powered down, the command will only change the state of the node but\n  won't have any effect until the configured SuspendTimeout is\n  reached.\nPOWER_DOWN_ASAP\n\nWill drain the node and mark it for power down. Currently running jobs\n  will complete first and no additional jobs will be allocated to the\n  node.\nPOWER_DOWN_FORCE\n\nWill cancel all jobs on the node, power it down, and reset its state to\n  IDLE.\nPOWER_UP\n\nWill use the configured ResumeProgram program to explicitly move\n  a node out of power saving mode. If a node is already in the process of\n  being powered up, the command will only change the state of the node but\n  won't have any effect until the configured ResumeTimeout is\n  reached.\nRESUME\n\nNot an actual node state, but will change a node state from DRAIN,\n  DRAINING, DOWN or REBOOT to IDLE and NoResp. slurmctld will then attempt to\n  contact slurmd to request that the node register itself. Once registered,\n  the node state will then remove the NoResp flag and will resume normal\n  operations. It will also clear the POWERING_DOWN state of a node and make\n  it eligible to be allocated.\nResume and Suspend ProgramsThe ResumeProgram and SuspendProgram execute as\nSlurmUser on the node where the slurmctld daemon runs (primary\nand backup server nodes). Use of sudo may be required for\nSlurmUser to power down and restart nodes. If you need to convert\nSlurm's hostlist expression into individual node names, the scontrol\nshow hostnames command may prove useful. The commands used to boot or\nshut down nodes will depend upon your cluster management tools.The ResumeProgram and SuspendProgram are not subject to any\ntime limits but must have Fault Tolerance. They\nshould perform the required action, ideally verify the action (e.g. node boot\nand start the slurmd daemon, thus the node is no longer non-responsive\nto slurmctld) and terminate. Long running programs will be logged by\nslurmctld, but not aborted.Example ResumeProgram:\n#!/bin/bash\n# Example ResumeProgram\nhosts=$(scontrol show hostnames \"$1\")\nlogfile=/var/log/power_save.log\necho \"$(date) Resume invoked $0 $*\" >>$logfile\nfor host in $hosts\ndo\n        sudo node_startup \"$host\"\ndone\nexit 0\nExample SuspendProgram:\n#!/bin/bash\n# Example SuspendProgram\nhosts=$(scontrol show hostnames \"$1\")\nlogfile=/var/log/power_save.log\necho \"$(date) Suspend invoked $0 $*\" >>$logfile\nfor host in $hosts\ndo\n        sudo node_shutdown \"$host\"\ndone\nexit 0\nNOTE: the stderr and stdout of the suspend and resume programs are\nnot logged. If logging is desired, then it should be added to the\nscripts.Fault ToleranceIf the slurmctld daemon is terminated gracefully, it will wait up\nto ten seconds (or the maximum of SuspendTimeout or\nResumeTimeout if less than ten seconds) for any spawned\nSuspendProgram or ResumeProgram to terminate before the daemon\nterminates. If the spawned program does not terminate within that time\nperiod, the event will be logged and slurmctld will exit in order to\npermit another slurmctld daemon to be initiated. Any spawned\nSuspendProgram or ResumeProgram will continue to run.When the slurmctld daemon shuts down, any SLURM_RESUME_FILE\ntemporary files are no longer available, even once slurmctld restarts.\nTherefore, ResumeProgram should use SLURM_RESUME_FILE within\nten seconds of starting to guarantee that it still exists.Booting Different ImagesIf you want ResumeProgram to boot various images according to job\nspecifications, it will need to be a fairly sophisticated program and perform\nthe following actions:\nDetermine which jobs are associated with the nodes to be booted.\n  SLURM_RESUME_FILE will help with this step.\nDetermine which image is required for each job. Images can be mapped\n  with NodeFeaturesPlugins.\n  \nBoot the appropriate image for each node.\nUse of AllocationsA resource allocation request will be granted as soon as resources are\nselected for use, possibly before the nodes are all available for use. The\nlaunching of job steps will be delayed until the required nodes have been\nrestored to service (it prints a warning about waiting for nodes to become\navailable and periodically retries until they are available).In the case of an sbatch command, the batch program will start when\nnode zero of the allocation is ready for use and pre-processing can be\nperformed as needed before using srun to launch job steps. The\nsbatch --wait-all-nodes=<value> option can be used\nto override this behavior on a per-job basis and a system-wide default can be\nset with the SchedulerParameters=sbatch_wait_nodes option.In the case of the salloc command, once the allocation is made a\nnew shell will be created on the login node. The salloc\n--wait-all-nodes=<value> option can be used to override\nthis behavior on a per-job basis and a system-wide default can be set with\nthe SchedulerParameters=salloc_wait_nodes option.Node FeaturesFeatures defined by NodeFeaturesPlugins, and\nassociated to cloud nodes in the slurm.conf, will be available but not\nactive when the node is powered down. If a job requests available nut not\nactive features, the controller will allocate nodes that are powered down and\nhave the features as available. At allocation, the features will be made\nactive. A cloud node will remain with the active features until the node is\npowered down (i.e. the node can't be rebooted to get other features until the\nnode is powered down). When the node is powered down, the those features\nbecome available but not active. Any feature not defined by\nNodeFeaturesPlugins are always active.Example:\nslurm.conf:\nNodeFeaturesPlugins=node_features/helpers\n\nNodeName=cloud[1-5] ... State=CLOUD Feature=f1,f2,l1\nNodeName=cloud[6-10] ... State=CLOUD Feature=f3,f4,l2\n\nhelpers.conf:\nNodeName=cloud[1-5] Feature=f1,f2 Helper=/bin/true\nNodeName=cloud[6-10] Feature=f3,f4 Helper=/bin/true\nFeatures f1, f2, f3, and f4 are changeable features and are defined on the\nnode lines in the slurm.conf because CLOUD nodes do not register\nbefore being allocated. By setting the Helper script to /bin/true, the\nslurmd's will not report any active features to the controller and the\ncontroller will manage all the active features. If the Helper is set\nto a script that reports the active features, the controller will validate\nthat the reported active features are a super set of the node's active\nchangeable features in the controller. Features l1 and l2 will always be\nactive and can be used as selectable labels.Hybrid ClusterCloud nodes to be acquired on demand can be placed into their own Slurm\npartition. This mode of operation can be used to use these nodes only if so\nrequested by the user. Note that jobs can be submitted to multiple partitions\nand will use resources from whichever partition permits faster initiation. A\nsample configuration in which nodes are added from the cloud when the\nworkload exceeds available resources. Users can explicitly request local\nresources or resources from the cloud by using the --constraint\noption.Example:\n# Excerpt of slurm.conf\nSelectType=select/cons_tres\nSelectTypeParameters=CR_CORE_Memory\n\nSuspendProgram=/usr/sbin/slurm_suspend\nResumeProgram=/usr/sbin/slurm_resume\nSuspendTime=600\nSuspendExcNodes=tux[0-127]\nTreeWidth=128\n\nNodeName=DEFAULT    Sockets=1 CoresPerSocket=4 ThreadsPerCore=2\nNodeName=tux[0-127] Weight=1 Feature=local State=UNKNOWN\nNodeName=ec[0-127]  Weight=8 Feature=cloud State=CLOUD\nPartitionName=debug MaxTime=1:00:00 Nodes=tux[0-32] Default=YES\nPartitionName=batch MaxTime=8:00:00 Nodes=tux[0-127],ec[0-127]\nWhen SuspendTime is set globally, Slurm attempts to suspend all\nnodes unless excluded by SuspendExcNodes or SuspendExcParts. It\ncan be tricky to have to remember to add on-premise nodes to the excluded\noptions. By setting the global SuspendTime to INFINITE and\nconfiguring SuspendTime on cloud specific partitions, you can avoid\nhaving to exclude nodes.Example:\n# Excerpt of slurm.conf\nSelectType=select/cons_tres\nSelectTypeParameters=CR_CORE_Memory\n\nSuspendProgram=/usr/sbin/slurm_suspend\nResumeProgram=/usr/sbin/slurm_resume\nTreeWidth=128\n\nNodeName=DEFAULT    Sockets=1 CoresPerSocket=4 ThreadsPerCore=2\nNodeName=tux[0-127] Weight=1 Feature=local State=UNKNOWN\nNodeName=ec[0-127]  Weight=8 Feature=cloud State=CLOUD\nPartitionName=debug MaxTime=1:00:00 Nodes=tux[0-32] Default=YES\nPartitionName=batch MaxTime=8:00:00 Nodes=tux[0-127],ec[0-127]\nPartitionName=cloud Nodes=ec[0-127] SuspendTime=600\nHere we have configured a partition with only cloud nodes and defined\nSuspendTime on that partition. Doing so will allow us to control when\nthose nodes power down without affecting our on-premise nodes, therefore\nSuspendExcNodes or SuspendExcParts are not needed in this\nsetup.Cloud AccountingInformation about cloud instances can be stored in the database. This can\nbe done by configuring instance id/type upon slurmd startup or with\nscontrol update. The node's \"extra\" field will also be stored in the\ndatabase.Configuring cloud information on slurmd startup:\n$ slurmd --instance-id=12345 --instance-type=m7g.medium --extra=\"arbitrary string\" . . .\nConfiguring cloud information with scontrol update:\n$ scontrol update nodename=n1 instanceid=12345 instancetype=m7g.medium extra=\"arbitrary string\"\nThis data can then be seen on the controller with scontrol show\nnode. Past and current data can be seen in the database with sacctmgr\nshow instance, as well as through slurmrestd with the /instance\nand /instances endpoints.Showing cloud information on the controller with scontrol:\n$ scontrol show nodes n1 | grep \"NodeName\\|Extra\\|Instance\"\nNodeName=n1 Arch=x86_64 CoresPerSocket=4\n   Extra=arbitrary string\n   InstanceId=12345 InstanceType=m7g.medium\nShowing cloud information from the database with sacctmgr:\n$ sacctmgr show instance format=nodename,instanceid,instancetype,extra\nNodeName                  InstanceId         InstanceType                Extra\n--------------- -------------------- -------------------- --------------------\nn1                             12345           m7g.medium     arbitrary string\nShowing cloud information from the database with slurmrestd:\n$ curl -k -s \\\n        --request GET \\\n        -H X-SLURM-USER-NAME:$(whoami) \\\n        -H X-SLURM-USER-TOKEN:$SLURM_JWT \\\n        -H \"Content-Type: application/json\" \\\n        --url localhost:8080/slurmdb/v0.0.40/instances \\\n        | jq \".instances\"\n\n[\n  {\n    \"cluster\": \"c1\",\n    \"extra\": \"arbitrary string\",\n    \"instance_id\": \"12345\",\n    \"instance_type\": \"m7g.medium\",\n    \"node_name\": \"n1\",\n    \"time\": {\n      \"time_end\": 0,\n      \"time_start\": 1687213177\n    }\n  }\n]\nLast modified 02 February 2024"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        }
    ]
}