{
    "url": "https://slurm.schedmd.com/quickstart_admin.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Quick Start Administrator Guide",
            "content": "Contents\nOverview\nSuper Quick Start\n\nBuilding and Installing Slurm\n\nInstalling Prerequisites\nBuilding RPMs\nInstalling RPMs\nBuilding Debian Packages\nInstalling Debian Packages\nBuilding Manually\n\n\nDaemons\nInfrastructure\nConfiguration\nSecurity\nStarting the Daemons\nAdministration Examples\nUpgrades\nFreeBSD\nOverviewPlease see the Quick Start User Guide for a\ngeneral overview.Also see Platforms for a list of supported\ncomputer platforms.For information on performing an upgrade, please see the\nUpgrade Guide.Super Quick Start\n\n\nMake sure the clocks, users and groups (UIDs and GIDs) are synchronized\nacross the cluster.\nInstall MUNGE for\nauthentication. Make sure that all nodes in your cluster have the\nsame munge.key. Make sure the MUNGE daemon, munged,\nis started before you start the Slurm daemons.\nDownload the latest\nversion of Slurm.\nInstall Slurm using one of the following methods:\n\nBuild RPM or DEB packages\n(recommended for production)\nBuild Manually from source\n(for developers or advanced users)\nNOTE: Some Linux distributions may have unofficial\nSlurm packages available in software repositories. SchedMD does not maintain\nor recommend these packages.\n\n\nBuild a configuration file using your favorite web browser and the\nSlurm Configuration Tool.\nNOTE: The SlurmUser must exist prior to starting Slurm\nand must exist on all nodes of the cluster.\nNOTE: The parent directories for Slurm's log files, process ID files,\nstate save directories, etc. are not created by Slurm.\nThey must be created and made writable by SlurmUser as needed prior to\nstarting Slurm daemons.\nNOTE: If any parent directories are created during the installation\nprocess (for the executable files, libraries, etc.),\nthose directories will have access rights equal to read/write/execute for\neveryone minus the umask value (e.g. umask=0022 generates directories with\npermissions of \"drwxr-r-x\" and mask=0000 generates directories with\npermissions of \"drwxrwrwx\" which is a security problem).\nInstall the configuration file in <sysconfdir>/slurm.conf.\nNOTE: You will need to install this configuration file on all nodes of\nthe cluster.\nsystemd (optional): enable the appropriate services on each system:\n\nController: systemctl enable slurmctld\nDatabase: systemctl enable slurmdbd\nCompute Nodes: systemctl enable slurmd\n\nStart the slurmctld and slurmd daemons.\nFreeBSD administrators should see the FreeBSD section below.Building and Installing Slurm\n\nInstalling Prerequisites\n\nBefore building Slurm, consider which plugins you will need for your\ninstallation.  Which plugins are built can vary based on the libraries that\nare available when running configure.  Refer to the below list of possible\nplugins and what is required to build them.\n auth/Slurm The auth/slurm plugin will be built if the jwt\n\t\tdevelopment library is installed. This is an alternative to the\n\t\ttraditional MUNGE authentication mechanism.\n AMD GPU Support Autodetection of AMD GPUs will be available\n\t\tif the ROCm development library is installed.\n\t\t\n cgroup Task Constraining The task/cgroup plugin will be built\n\t\tif the hwloc development library is present. cgroup/v2\n\t\tsupport also requires the bpf and dbus development\n\t\tlibraries.\n HDF5 Job Profiling The acct_gather_profile/hdf5 job profiling\n\t\tplugin will be built if the hdf5 development library is\n\t\tpresent.\n HTML Man Pages HTML versions of the man pages will be generated if\n\t\tthe man2html command is present.\n HPE Slingshot The switch/hpe_slingshot plugin will be built\n\t\tif the cray-libcxi, curl, and json-c\n\t\tdevelopment libraries are present.\n InfiniBand Accounting The acct_gather_interconnect/ofed\n\t\tInfiniBand accounting plugin will be built if the\n\t\tlibibmad and libibumad development libraries are\n\t\tpresent.\n Intel GPU Support Autodetection of Intel GPUs will be available\n\t\tif the libvpl development library is installed.\n\t\t\n IPMI Energy Consumption The acct_gather_energy/ipmi\n\t\taccounting plugin will be built if the freeipmi\n\t\tdevelopment library is present. When building the RPM,\n\t\trpmbuild ... --with freeipmi can be\n\t\tspecified to explicitly check for these dependencies.\n Lua Support The lua API will be available in various plugins if the\n\t\tlua development library is present.\n MUNGE The auth/munge plugin will be built if the MUNGE\n\t\tauthentication development library is installed. MUNGE is used\n\t\tas the default authentication mechanism.\n MySQL MySQL support for accounting will be built if the\n\t\tMySQL or MariaDB development library is present.\n\t\tA currently supported version of MySQL or MariaDB should be\n\t\tused.\n NUMA Affinity NUMA support in the task/affinity plugin will be\n\t\tavailable if the numa development library is installed.\n\t\t\n NVIDIA GPU Support Autodetection of NVIDIA GPUs will be available\n\t\tif the libnvidia-ml development library is installed.\n\t\t\n PAM Support PAM support will be added if the PAM development\n\t\tlibrary is installed.\n PMIx PMIx support will be added if the pmix development\n\t\tlibrary is installed and the --with-pmix flag is\n\t\tprovided at build time.\n Readline Support Readline support in scontrol and sacctmgr's\n\t\tinteractive modes will be available if the readline\n\t\tdevelopment library is present.\n REST API Support for Slurm's REST API will be built if the\n\t\thttp-parser and json-c development libraries\n\t\tare installed. Additional functionality will be included\n\t\tif the optional yaml and jwt development\n\t\tlibraries are installed.\n sview The sview command will be built only if gtk+-2.0\n\t\tis installed.\nPlease see the Related Software page for\nreferences to required software to build these plugins.If required libraries or header files are in non-standard locations, set\nCFLAGS and LDFLAGS environment variables accordingly.\nBuilding RPMsTo build RPMs directly, copy the distributed tarball into a directory\nand execute (substituting the appropriate Slurm version\nnumber):rpmbuild -ta slurm-23.02.7.tar.bz2$(HOME)/rpmbuildYou can control some aspects of the RPM built with a .rpmmacros\nfile in your home directory. Special macro definitions will likely\nonly be required if files are installed in unconventional locations.\nA full list of rpmbuild options can be found near the top of the\nslurm.spec file.\nSome macro definitions that may be used in building Slurm include:\n\n_enable_debug\nSpecify if debugging logic within Slurm is to be enabled\n_prefix\nPathname of directory to contain the Slurm files\n_slurm_sysconfdir\nPathname of directory containing the slurm.conf configuration file (default\n/etc/slurm)\nwith_munge\nSpecifies the MUNGE (authentication library) installation location\n\nAn example .rpmmacros file:\n\n# .rpmmacros\n# Override some RPM macros from /usr/lib/rpm/macros\n# Set Slurm-specific macros for unconventional file locations\n#\n%_enable_debug     \"--with-debug\"\n%_prefix           /opt/slurm\n%_slurm_sysconfdir %{_prefix}/etc/slurm\n%_defaultdocdir    %{_prefix}/doc\n%with_munge        \"--with-munge=/opt/munge\"\n\nRPMs Installed\nThe RPMs needed on the head node, compute nodes, and slurmdbd node can vary\nby configuration, but here is a suggested starting point:\n\nHead Node (where the slurmctld daemon runs),\n    Compute and Login Nodes\n\t\nslurm\nslurm-perlapi\nslurm-slurmctld (only on the head node)\nslurm-slurmd (only on the compute nodes)\n\n\nSlurmDBD Node\n\t\nslurm\nslurm-slurmdbd\n\n\n\nBuilding Debian Packages\n\n\nBeginning with Slurm 23.11.0, Slurm includes the files required to build\nDebian packages. These packages conflict with the packages shipped with Debian\nbased distributions, and are named distinctly to differentiate them. After\ndownloading the desired version of Slurm, the following can be done to build\nthe packages:\n\nInstall basic Debian package build requirements:\napt-get install build-essential fakeroot devscripts equivs\n\nUnpack the distributed tarball:\ntar -xaf slurm*tar.bz2\n\ncd to the directory containing the Slurm source\nInstall the Slurm package dependencies:\nmk-build-deps -i debian/control\n\nBuild the Slurm packages:\ndebuild -b -uc -us\n\n\nThe packages will be in the parent directory after debuild completes.\nInstalling Debian Packages\n\n\nThe packages needed on the head node, compute nodes, and slurmdbd node can\nvary site to site, but this is a good starting point:\n\nSlurmDBD Node\n\t\nslurm-smd\nslurm-smd-slurmdbd\n\n\nHead Node (slurmctld node)\n\t\nslurm-smd\nslurm-smd-slurmctld\nslurm-smd-client\n\n\nCompute Nodes (slurmd node)\n\t\nslurm-smd\nslurm-smd-slurmd\nslurm-smd-client\n\n\nLogin Nodes\n\t\nslurm-smd\nslurm-smd-client\n\n\n\nBuilding Manually\n\n\nInstructions to build and install Slurm manually are shown below.\nThis is significantly more complicated to manage than the RPM and DEB build\nprocedures, so this approach is only recommended for developers or\nadvanced users who are looking for a more customized install.\nSee the README and INSTALL files in the source distribution for more details.\n\n\nUnpack the distributed tarball:\ntar -xaf slurm*tar.bz2\ncd to the directory containing the Slurm source and type\n./configure with appropriate options (see below).\nType make install to compile and install the programs,\ndocumentation, libraries, header files, etc.\nType ldconfig -n <library_location> so that the Slurm\nlibraries can be found by applications that intend to use Slurm APIs directly.\nThe library location will be a subdirectory of PREFIX (described below) and\ndepend upon the system type and configuration, typically lib or lib64.\nFor example, if PREFIX is \"/usr\" and the subdirectory is \"lib64\" then you would\nfind that a file named \"/usr/lib64/libslurm.so\" was installed and the command\nldconfig -n /usr/lib64 should be executed.\n\nA full list of configure options will be returned by the\ncommand configure --help. The most commonly used arguments\nto the configure command include:\n--enable-debug\nEnable additional debugging logic within Slurm.\n--prefix=PREFIX\nInstall architecture-independent files in PREFIX; default value is /usr/local.\n--sysconfdir=DIR\nSpecify location of Slurm configuration file. The default value is PREFIX/etc\nDaemons\nslurmctld is sometimes called the \"controller\".\nIt orchestrates Slurm activities, including queuing of jobs,\nmonitoring node states, and allocating resources to jobs. There is an\noptional backup controller that automatically assumes control in the\nevent the primary controller fails (see the High\nAvailability section below).  The primary controller resumes\ncontrol whenever it is restored to service. The controller saves its\nstate to disk whenever there is a change in state (see\n\"StateSaveLocation\" in Configuration\nsection below).  This state can be recovered by the controller at\nstartup time.  State changes are saved so that jobs and other state\ninformation can be preserved when the controller moves (to or from a\nbackup controller) or is restarted.\nWe recommend that you create a Unix user slurm for use by\nslurmctld. This user name will also be specified using the\nSlurmUser in the slurm.conf configuration file.\nThis user must exist on all nodes of the cluster.\nNote that files and directories used by slurmctld will need to be\nreadable or writable by the user SlurmUser (the Slurm configuration\nfiles must be readable; the log file directory and state save directory\nmust be writable).\nThe slurmd daemon executes on every compute node. It resembles a\nremote shell daemon to export control to Slurm. Because slurmd initiates and\nmanages user jobs, it must execute as the user root.\nIf you want to archive job accounting records to a database, the\nslurmdbd (Slurm DataBase Daemon) should be used. We recommend that\nyou defer adding accounting support until after basic Slurm functionality is\nestablished on your system. An Accounting web\npage contains more information.\nslurmctld and/or slurmd should be initiated at node startup\ntime per the Slurm configuration.\nThe slurmrestd daemon was introduced in version 20.02 and allows\nclients to communicate with Slurm via the REST API. This is installed by\ndefault, assuming the prerequisites are met.\nIt has two run modes,\nallowing you to have it run as a traditional Unix service and always listen\nfor TCP connections, or you can have it run as an Inet service and only have\nit active when in use.\nHigh Availability\nMultiple SlurmctldHost entries can be configured, with any entry beyond the\nfirst being treated as a backup host. Any backup hosts configured should be on\na different node than the node hosting the primary slurmctld. However, all\nhosts should mount a common file system containing the state information (see\n\"StateSaveLocation\" in the Configuration\nsection below).\nIf more than one host is specified, when the primary fails the second listed\nSlurmctldHost will take over for it. When the primary returns to service, it\nnotifies the backup.  The backup then saves the state and returns to backup\nmode. The primary reads the saved state and resumes normal operation. Likewise,\nif both of the first two listed hosts fail the third SlurmctldHost will take\nover until the primary returns to service. Other than a brief period of non-\nresponsiveness, the transition back and forth should go undetected.\nPrior to 18.08, Slurm used the \n\"BackupAddr\" and \n\"BackupController\" parameters for High Availability. These\nparameters have been deprecated and are replaced by\n\"SlurmctldHost\".\nAlso see \"\nSlurmctldPrimaryOnProg\" and\n\"\nSlurmctldPrimaryOffProg\" to adjust the actions taken when machines\ntransition between being the primary controller.\nAny time the slurmctld daemon or hardware fails before state information\nreaches disk can result in lost state.\nSlurmctld writes state frequently (every five seconds by default), but with\nlarge numbers of jobs, the formatting and writing of records can take seconds\nand recent changes might not be written to disk.\nAnother example is if the state information is written to file, but that\ninformation is cached in memory rather than written to disk when the node fails.\nThe interval between state saves being written to disk can be configured at\nbuild time by defining SAVE_MAX_WAIT to a different value than five.\nA backup instance of slurmdbd can also be configured by specifying\n\nAccountingStorageBackupHost in slurm.conf, as well as\nDbdBackupHost in\nslurmdbd.conf. The backup host should be on a different machine than the one\nhosting the primary instance of slurmdbd. Both instances of slurmdbd should\nhave access to the same database. The\nnetwork page has a visual representation\nof how this might look.\nInfrastructure\n\n\nUser and Group Identification\n\n\nThere must be a uniform user and group name space (including\nUIDs and GIDs) across the cluster.\nIt is not necessary to permit user logins to the control hosts\n(SlurmctldHost), but the\nusers and groups must be resolvable on those hosts.\nAuthentication of Slurm communications\n\n\nAll communications between Slurm components are authenticated. The\nauthentication infrastructure is provided by a dynamically loaded\nplugin chosen at runtime via the AuthType keyword in the Slurm\nconfiguration file. Until 23.11.0, the only supported authentication type was\nmunge, which requires the\ninstallation of the MUNGE package.\nWhen using MUNGE, all nodes in the cluster must be configured with the\nsame munge.key file. The MUNGE daemon, munged, must also be\nstarted before Slurm daemons. Note that MUNGE does require clocks to be\nsynchronized throughout the cluster, usually done by NTP.\nAs of 23.11.0, AuthType can also be set to\nslurm, an internal authentication\nplugin. This plugin has similar requirements to MUNGE, requiring a key file\nshared to all Slurm daemons. The auth/slurm plugin requires installation of the\njwt package.\nMUNGE is currently the default and recommended option.\nThe configure script in the top-level directory of this distribution will\ndetermine which authentication plugins may be built.\nThe configuration file specifies which of the available plugins will be\nutilized.\nMPI support\nSlurm supports many different MPI implementations.\nFor more information, see MPI.\n\nScheduler support\n\n\nSlurm can be configured with rather simple or quite sophisticated\nscheduling algorithms depending upon your needs and willingness to\nmanage the configuration (much of which requires a database).\nThe first configuration parameter of interest is PriorityType\nwith two options available: basic (first-in-first-out) and\nmultifactor.\nThe multifactor plugin will assign a priority to jobs based upon\na multitude of configuration parameters (age, size, fair-share allocation,\netc.) and its details are beyond the scope of this document.\nSee the Multifactor Job Priority Plugin\ndocument for details.\nThe SchedType configuration parameter controls how queued\njobs are scheduled and several options are available.\n\nbuiltin will initiate jobs strictly in their priority order,\ntypically (first-in-first-out) \nbackfill will initiate a lower-priority job if doing so does\nnot delay the expected initiation time of higher priority jobs; essentially\nusing smaller jobs to fill holes in the resource allocation plan. Effective\nbackfill scheduling does require users to specify job time limits.\ngang time-slices jobs in the same partition/queue and can be\nused to preempt jobs from lower-priority queues in order to execute\njobs in higher priority queues.\n\nFor more information about scheduling options see\nGang Scheduling,\nPreemption,\nResource Reservation Guide,\nResource Limits and\nSharing Consumable Resources.\nResource selection\n\n\nThe resource selection mechanism used by Slurm is controlled by the\nSelectType configuration parameter.\nIf you want to execute multiple jobs per node, but track and manage allocation\nof the processors, memory and other resources, the cons_tres (consumable\ntrackable resources) plugin is recommended.\nFor more information, please see\nConsumable Resources in Slurm.\nLogging\nSlurm uses syslog to record events if the SlurmctldLogFile and\nSlurmdLogFile locations are not set.\nAccounting\nSlurm supports accounting records being written to a simple text file,\ndirectly to a database (MySQL or MariaDB), or to a daemon securely\nmanaging accounting data for multiple clusters. For more information\nsee Accounting. \nCompute node access\n\n\nSlurm does not by itself limit access to allocated compute nodes,\nbut it does provide mechanisms to accomplish this.\nThere is a Pluggable Authentication Module (PAM) for restricting access\nto compute nodes available for download.\nWhen installed, the Slurm PAM module will prevent users from logging\ninto any node that has not be assigned to that user.\nOn job termination, any processes initiated by the user outside of\nSlurm's control may be killed using an Epilog script configured\nin slurm.conf.\nConfiguration\nThe Slurm configuration file includes a wide variety of parameters.\nThis configuration file must be available on each node of the cluster and\nmust have consistent contents. A full\ndescription of the parameters is included in the slurm.conf man page. Rather than\nduplicate that information, a minimal sample configuration file is shown below.\nYour slurm.conf file should define at least the configuration parameters defined\nin this sample and likely additional ones. Any text\nfollowing a \"#\" is considered a comment. The keywords in the file are\nnot case sensitive, although the argument typically is (e.g., \"SlurmUser=slurm\"\nmight be specified as \"slurmuser=slurm\"). The control machine, like\nall other machine specifications, can include both the host name and the name\nused for communications. In this case, the host's name is \"mcri\" and\nthe name \"emcri\" is used for communications.\nIn this case \"emcri\" is the private management network interface\nfor the host \"mcri\". Port numbers to be used for\ncommunications are specified as well as various timer values.\nThe SlurmUser must be created as needed prior to starting Slurm\nand must exist on all nodes in your cluster.\nThe parent directories for Slurm's log files, process ID files,\nstate save directories, etc. are not created by Slurm.\nThey must be created and made writable by SlurmUser as needed prior to\nstarting Slurm daemons.\nThe StateSaveLocation is used to store information about the current\nstate of the cluster, including information about queued, running and recently\ncompleted jobs. The directory used should be on a low-latency local disk to\nprevent file system delays from affecting Slurm performance. If using a backup\nhost, the StateSaveLocation should reside on a file system shared by the two\nhosts. We do not recommend using NFS to make the directory accessible to both\nhosts, but do recommend a shared mount that is accessible to the two\ncontrollers and allows low-latency reads and writes to the disk. If a\ncontroller comes up without access to the state information, queued and\nrunning jobs will be cancelled.\nA description of the nodes and their grouping into partitions is required.\nA simple node range expression may optionally be used to specify\nranges of nodes to avoid building a configuration file with large\nnumbers of entries. The node range expression can contain one\npair of square brackets with a sequence of comma separated\nnumbers and/or ranges of numbers separated by a \"-\"\n(e.g. \"linux[0-64,128]\", or \"lx[15,18,32-33]\").\nUp to two numeric ranges can be included in the expression\n(e.g. \"rack[0-63]_blade[0-41]\").\nIf one or more numeric expressions are included, one of them\nmust be at the end of the name (e.g. \"unit[0-31]rack\" is invalid),\nbut arbitrary names can always be used in a comma separated list.\nNode names can have up to three name specifications:\nNodeName is the name used by all Slurm tools when referring to the node,\nNodeAddr is the name or IP address Slurm uses to communicate with the node, and\nNodeHostname is the name returned by the command /bin/hostname -s.\nOnly NodeName is required (the others default to the same name),\nalthough supporting all three parameters provides complete control over\nnaming and addressing the nodes.  See the slurm.conf man page for\ndetails on all configuration parameters.\nNodes can be in more than one partition and each partition can have different\nconstraints (permitted users, time limits, job size limits, etc.).\nEach partition can thus be considered a separate queue.\nPartition and node specifications use node range expressions to identify\nnodes in a concise fashion. This configuration file defines a 1154-node cluster\nfor Slurm, but it might be used for a much larger cluster by just changing a few\nnode range expressions. Specify the minimum processor count (CPUs), real memory\nspace (RealMemory, megabytes), and temporary disk space (TmpDisk, megabytes) that\na node should have to be considered available for use. Any node lacking these\nminimum configuration values will be considered DOWN and not scheduled.\nNote that a more extensive sample configuration file is provided in\netc/slurm.conf.example. We also have a web-based\nconfiguration tool which can\nbe used to build a simple configuration file, which can then be\nmanually edited for more complex configurations.\n\n#\n# Sample /etc/slurm.conf for mcr.llnl.gov\n#\nSlurmctldHost=mcri(12.34.56.78)\nSlurmctldHost=mcrj(12.34.56.79)\n#\nAuthType=auth/munge\nEpilog=/usr/local/slurm/etc/epilog\nJobCompLoc=/var/tmp/jette/slurm.job.log\nJobCompType=jobcomp/filetxt\nPluginDir=/usr/local/slurm/lib/slurm\nProlog=/usr/local/slurm/etc/prolog\nSchedulerType=sched/backfill\nSelectType=select/linear\nSlurmUser=slurm\nSlurmctldPort=7002\nSlurmctldTimeout=300\nSlurmdPort=7003\nSlurmdSpoolDir=/var/spool/slurmd.spool\nSlurmdTimeout=300\nStateSaveLocation=/var/spool/slurm.state\nTreeWidth=16\n#\n# Node Configurations\n#\nNodeName=DEFAULT CPUs=2 RealMemory=2000 TmpDisk=64000 State=UNKNOWN\nNodeName=mcr[0-1151] NodeAddr=emcr[0-1151]\n#\n# Partition Configurations\n#\nPartitionName=DEFAULT State=UP\nPartitionName=pdebug Nodes=mcr[0-191] MaxTime=30 MaxNodes=32 Default=YES\nPartitionName=pbatch Nodes=mcr[192-1151]\n\nSecurity\nBesides authentication of Slurm communications based upon the value\nof the AuthType, digital signatures are used in job step\ncredentials.\nThis signature is used by slurmctld to construct a job step\ncredential, which is sent to srun and then forwarded to\nslurmd to initiate job steps.\nThis design offers improved performance by removing much of the\njob step initiation overhead from the  slurmctld  daemon.\nThe digital signature mechanism is specified by the CredType\nconfiguration parameter and the default mechanism is MUNGE. \nPluggable Authentication Module (PAM) support\n\n\nA PAM module (Pluggable Authentication Module) is available for Slurm that\ncan prevent a user from accessing a node which he has not been allocated,\nif that mode of operation is desired.\nStarting the Daemons\n\n\nFor testing purposes you may want to start by just running slurmctld and slurmd\non one node. By default, they execute in the background. Use the -D\noption for each daemon to execute them in the foreground and logging will be done\nto your terminal. The -v option will log events\nin more detail with more v's increasing the level of detail (e.g. -vvvvvv).\nYou can use one window to execute \"slurmctld -D -vvvvvv\",\na second window to execute \"slurmd -D -vvvvv\".\nYou may see errors such as \"Connection refused\" or \"Node X not responding\"\nwhile one daemon is operative and the other is being started, but the\ndaemons can be started in any order and proper communications will be\nestablished once both daemons complete initialization.\nYou can use a third window to execute commands such as\n\"srun -N1 /bin/hostname\" to confirm functionality.\nAnother important option for the daemons is \"-c\"\nto clear previous state information. Without the \"-c\"\noption, the daemons will restore any previously saved state information: node\nstate, job state, etc. With the \"-c\" option all\npreviously running jobs will be purged and node state will be restored to the\nvalues specified in the configuration file. This means that a node configured\ndown manually using the scontrol command will\nbe returned to service unless noted as being down in the configuration file.\nIn practice, Slurm consistently restarts with preservation.\nAdministration Examples\n\n\nscontrol can be used to print all system information\nand modify most of it. Only a few examples are shown below. Please see the scontrol\nman page for full details. The commands and options are all case insensitive.\nPrint detailed state of all jobs in the system.\n\nadev0: scontrol\nscontrol: show job\nJobId=475 UserId=bob(6885) Name=sleep JobState=COMPLETED\n   Priority=4294901286 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:53:41 EndTime=03/19-12:53:59\n   NodeList=adev8 NodeListIndecies=-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n\nJobId=476 UserId=bob(6885) Name=sleep JobState=RUNNING\n   Priority=4294901285 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:54:01 EndTime=NONE\n   NodeList=adev8 NodeListIndecies=8,8,-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n Print the detailed state of job 477 and change its priority to\nzero. A priority of zero prevents a job from being initiated (it is held in \"pending\"\nstate).\n\nadev0: scontrol\nscontrol: show job 477\nJobId=477 UserId=bob(6885) Name=sleep JobState=PENDING\n   Priority=4294901286 Partition=batch BatchFlag=0\n   more data removed....\nscontrol: update JobId=477 Priority=0\n\nPrint the state of node adev13 and drain it. To drain a node, specify a new\nstate of DRAIN, DRAINED, or DRAINING. Slurm will automatically set it to the appropriate\nvalue of either DRAINING or DRAINED depending on whether the node is allocated\nor not. Return it to service later.\n\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=ALLOCATED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=DRAIN\nscontrol: show node adev13\nNodeName=adev13 State=DRAINING CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: quit\nLater\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=DRAINED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=IDLE\n Reconfigure all Slurm daemons on all nodes. This should\nbe done after changing the Slurm configuration file.\n\nadev0: scontrol reconfig\n Print the current Slurm configuration. This also reports if the\nprimary and secondary controllers (slurmctld daemons) are responding. To just\nsee the state of the controllers, use the command ping.\n\nadev0: scontrol show config\nConfiguration data as of 2019-03-29T12:20:45\n...\nSlurmctldAddr           = eadevi\nSlurmctldDebug          = info\nSlurmctldHost[0]        = adevi\nSlurmctldHost[1]        = adevj\nSlurmctldLogFile        = /var/log/slurmctld.log\n...\n\nSlurmctld(primary) at adevi is UP\nSlurmctld(backup) at adevj is UP\n Shutdown all Slurm daemons on all nodes.\n\nadev0: scontrol shutdown\n\nUpgrades\nSlurm supports in-place upgrades between certain versions. Important details\nabout the steps necessary to perform an upgrade and the potential complications\nto prepare for are contained on this page:\nUpgrade Guide\nFreeBSD\nFreeBSD administrators can install the latest stable Slurm as a binary\npackage using:\n\npkg install slurm-wlm\n\nOr, it can be built and installed from source using:\n\ncd /usr/ports/sysutils/slurm-wlm && make install\n\nThe binary package installs a minimal Slurm configuration suitable for\ntypical compute nodes.  Installing from source allows the user to enable\noptions such as mysql and gui tools via a configuration menu.\nLast modified 16 August 2024\n"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        },
        {
            "title": "Daemons",
            "content": "slurmctld is sometimes called the \"controller\".\nIt orchestrates Slurm activities, including queuing of jobs,\nmonitoring node states, and allocating resources to jobs. There is an\noptional backup controller that automatically assumes control in the\nevent the primary controller fails (see the High\nAvailability section below).  The primary controller resumes\ncontrol whenever it is restored to service. The controller saves its\nstate to disk whenever there is a change in state (see\n\"StateSaveLocation\" in Configuration\nsection below).  This state can be recovered by the controller at\nstartup time.  State changes are saved so that jobs and other state\ninformation can be preserved when the controller moves (to or from a\nbackup controller) or is restarted.We recommend that you create a Unix user slurm for use by\nslurmctld. This user name will also be specified using the\nSlurmUser in the slurm.conf configuration file.\nThis user must exist on all nodes of the cluster.\nNote that files and directories used by slurmctld will need to be\nreadable or writable by the user SlurmUser (the Slurm configuration\nfiles must be readable; the log file directory and state save directory\nmust be writable).The slurmd daemon executes on every compute node. It resembles a\nremote shell daemon to export control to Slurm. Because slurmd initiates and\nmanages user jobs, it must execute as the user root.If you want to archive job accounting records to a database, the\nslurmdbd (Slurm DataBase Daemon) should be used. We recommend that\nyou defer adding accounting support until after basic Slurm functionality is\nestablished on your system. An Accounting web\npage contains more information.slurmctld and/or slurmd should be initiated at node startup\ntime per the Slurm configuration.The slurmrestd daemon was introduced in version 20.02 and allows\nclients to communicate with Slurm via the REST API. This is installed by\ndefault, assuming the prerequisites are met.\nIt has two run modes,\nallowing you to have it run as a traditional Unix service and always listen\nfor TCP connections, or you can have it run as an Inet service and only have\nit active when in use.High AvailabilityMultiple SlurmctldHost entries can be configured, with any entry beyond the\nfirst being treated as a backup host. Any backup hosts configured should be on\na different node than the node hosting the primary slurmctld. However, all\nhosts should mount a common file system containing the state information (see\n\"StateSaveLocation\" in the Configuration\nsection below).If more than one host is specified, when the primary fails the second listed\nSlurmctldHost will take over for it. When the primary returns to service, it\nnotifies the backup.  The backup then saves the state and returns to backup\nmode. The primary reads the saved state and resumes normal operation. Likewise,\nif both of the first two listed hosts fail the third SlurmctldHost will take\nover until the primary returns to service. Other than a brief period of non-\nresponsiveness, the transition back and forth should go undetected.Prior to 18.08, Slurm used the \n\"BackupAddr\" and \n\"BackupController\" parameters for High Availability. These\nparameters have been deprecated and are replaced by\n\"SlurmctldHost\".\nAlso see \"\nSlurmctldPrimaryOnProg\" and\n\"\nSlurmctldPrimaryOffProg\" to adjust the actions taken when machines\ntransition between being the primary controller.Any time the slurmctld daemon or hardware fails before state information\nreaches disk can result in lost state.\nSlurmctld writes state frequently (every five seconds by default), but with\nlarge numbers of jobs, the formatting and writing of records can take seconds\nand recent changes might not be written to disk.\nAnother example is if the state information is written to file, but that\ninformation is cached in memory rather than written to disk when the node fails.\nThe interval between state saves being written to disk can be configured at\nbuild time by defining SAVE_MAX_WAIT to a different value than five.A backup instance of slurmdbd can also be configured by specifying\n\nAccountingStorageBackupHost in slurm.conf, as well as\nDbdBackupHost in\nslurmdbd.conf. The backup host should be on a different machine than the one\nhosting the primary instance of slurmdbd. Both instances of slurmdbd should\nhave access to the same database. The\nnetwork page has a visual representation\nof how this might look.Infrastructure\n\nUser and Group Identification\n\nThere must be a uniform user and group name space (including\nUIDs and GIDs) across the cluster.\nIt is not necessary to permit user logins to the control hosts\n(SlurmctldHost), but the\nusers and groups must be resolvable on those hosts.Authentication of Slurm communications\n\nAll communications between Slurm components are authenticated. The\nauthentication infrastructure is provided by a dynamically loaded\nplugin chosen at runtime via the AuthType keyword in the Slurm\nconfiguration file. Until 23.11.0, the only supported authentication type was\nmunge, which requires the\ninstallation of the MUNGE package.\nWhen using MUNGE, all nodes in the cluster must be configured with the\nsame munge.key file. The MUNGE daemon, munged, must also be\nstarted before Slurm daemons. Note that MUNGE does require clocks to be\nsynchronized throughout the cluster, usually done by NTP.As of 23.11.0, AuthType can also be set to\nslurm, an internal authentication\nplugin. This plugin has similar requirements to MUNGE, requiring a key file\nshared to all Slurm daemons. The auth/slurm plugin requires installation of the\njwt package.MUNGE is currently the default and recommended option.\nThe configure script in the top-level directory of this distribution will\ndetermine which authentication plugins may be built.\nThe configuration file specifies which of the available plugins will be\nutilized.MPI supportSlurm supports many different MPI implementations.\nFor more information, see MPI.\n\nScheduler support\n\n\nSlurm can be configured with rather simple or quite sophisticated\nscheduling algorithms depending upon your needs and willingness to\nmanage the configuration (much of which requires a database).\nThe first configuration parameter of interest is PriorityType\nwith two options available: basic (first-in-first-out) and\nmultifactor.\nThe multifactor plugin will assign a priority to jobs based upon\na multitude of configuration parameters (age, size, fair-share allocation,\netc.) and its details are beyond the scope of this document.\nSee the Multifactor Job Priority Plugin\ndocument for details.\nThe SchedType configuration parameter controls how queued\njobs are scheduled and several options are available.\n\nbuiltin will initiate jobs strictly in their priority order,\ntypically (first-in-first-out) \nbackfill will initiate a lower-priority job if doing so does\nnot delay the expected initiation time of higher priority jobs; essentially\nusing smaller jobs to fill holes in the resource allocation plan. Effective\nbackfill scheduling does require users to specify job time limits.\ngang time-slices jobs in the same partition/queue and can be\nused to preempt jobs from lower-priority queues in order to execute\njobs in higher priority queues.\n\nFor more information about scheduling options see\nGang Scheduling,\nPreemption,\nResource Reservation Guide,\nResource Limits and\nSharing Consumable Resources.\nResource selection\n\n\nThe resource selection mechanism used by Slurm is controlled by the\nSelectType configuration parameter.\nIf you want to execute multiple jobs per node, but track and manage allocation\nof the processors, memory and other resources, the cons_tres (consumable\ntrackable resources) plugin is recommended.\nFor more information, please see\nConsumable Resources in Slurm.\nLogging\nSlurm uses syslog to record events if the SlurmctldLogFile and\nSlurmdLogFile locations are not set.\nAccounting\nSlurm supports accounting records being written to a simple text file,\ndirectly to a database (MySQL or MariaDB), or to a daemon securely\nmanaging accounting data for multiple clusters. For more information\nsee Accounting. \nCompute node access\n\n\nSlurm does not by itself limit access to allocated compute nodes,\nbut it does provide mechanisms to accomplish this.\nThere is a Pluggable Authentication Module (PAM) for restricting access\nto compute nodes available for download.\nWhen installed, the Slurm PAM module will prevent users from logging\ninto any node that has not be assigned to that user.\nOn job termination, any processes initiated by the user outside of\nSlurm's control may be killed using an Epilog script configured\nin slurm.conf.\nConfiguration\nThe Slurm configuration file includes a wide variety of parameters.\nThis configuration file must be available on each node of the cluster and\nmust have consistent contents. A full\ndescription of the parameters is included in the slurm.conf man page. Rather than\nduplicate that information, a minimal sample configuration file is shown below.\nYour slurm.conf file should define at least the configuration parameters defined\nin this sample and likely additional ones. Any text\nfollowing a \"#\" is considered a comment. The keywords in the file are\nnot case sensitive, although the argument typically is (e.g., \"SlurmUser=slurm\"\nmight be specified as \"slurmuser=slurm\"). The control machine, like\nall other machine specifications, can include both the host name and the name\nused for communications. In this case, the host's name is \"mcri\" and\nthe name \"emcri\" is used for communications.\nIn this case \"emcri\" is the private management network interface\nfor the host \"mcri\". Port numbers to be used for\ncommunications are specified as well as various timer values.\nThe SlurmUser must be created as needed prior to starting Slurm\nand must exist on all nodes in your cluster.\nThe parent directories for Slurm's log files, process ID files,\nstate save directories, etc. are not created by Slurm.\nThey must be created and made writable by SlurmUser as needed prior to\nstarting Slurm daemons.\nThe StateSaveLocation is used to store information about the current\nstate of the cluster, including information about queued, running and recently\ncompleted jobs. The directory used should be on a low-latency local disk to\nprevent file system delays from affecting Slurm performance. If using a backup\nhost, the StateSaveLocation should reside on a file system shared by the two\nhosts. We do not recommend using NFS to make the directory accessible to both\nhosts, but do recommend a shared mount that is accessible to the two\ncontrollers and allows low-latency reads and writes to the disk. If a\ncontroller comes up without access to the state information, queued and\nrunning jobs will be cancelled.\nA description of the nodes and their grouping into partitions is required.\nA simple node range expression may optionally be used to specify\nranges of nodes to avoid building a configuration file with large\nnumbers of entries. The node range expression can contain one\npair of square brackets with a sequence of comma separated\nnumbers and/or ranges of numbers separated by a \"-\"\n(e.g. \"linux[0-64,128]\", or \"lx[15,18,32-33]\").\nUp to two numeric ranges can be included in the expression\n(e.g. \"rack[0-63]_blade[0-41]\").\nIf one or more numeric expressions are included, one of them\nmust be at the end of the name (e.g. \"unit[0-31]rack\" is invalid),\nbut arbitrary names can always be used in a comma separated list.\nNode names can have up to three name specifications:\nNodeName is the name used by all Slurm tools when referring to the node,\nNodeAddr is the name or IP address Slurm uses to communicate with the node, and\nNodeHostname is the name returned by the command /bin/hostname -s.\nOnly NodeName is required (the others default to the same name),\nalthough supporting all three parameters provides complete control over\nnaming and addressing the nodes.  See the slurm.conf man page for\ndetails on all configuration parameters.\nNodes can be in more than one partition and each partition can have different\nconstraints (permitted users, time limits, job size limits, etc.).\nEach partition can thus be considered a separate queue.\nPartition and node specifications use node range expressions to identify\nnodes in a concise fashion. This configuration file defines a 1154-node cluster\nfor Slurm, but it might be used for a much larger cluster by just changing a few\nnode range expressions. Specify the minimum processor count (CPUs), real memory\nspace (RealMemory, megabytes), and temporary disk space (TmpDisk, megabytes) that\na node should have to be considered available for use. Any node lacking these\nminimum configuration values will be considered DOWN and not scheduled.\nNote that a more extensive sample configuration file is provided in\netc/slurm.conf.example. We also have a web-based\nconfiguration tool which can\nbe used to build a simple configuration file, which can then be\nmanually edited for more complex configurations.\n\n#\n# Sample /etc/slurm.conf for mcr.llnl.gov\n#\nSlurmctldHost=mcri(12.34.56.78)\nSlurmctldHost=mcrj(12.34.56.79)\n#\nAuthType=auth/munge\nEpilog=/usr/local/slurm/etc/epilog\nJobCompLoc=/var/tmp/jette/slurm.job.log\nJobCompType=jobcomp/filetxt\nPluginDir=/usr/local/slurm/lib/slurm\nProlog=/usr/local/slurm/etc/prolog\nSchedulerType=sched/backfill\nSelectType=select/linear\nSlurmUser=slurm\nSlurmctldPort=7002\nSlurmctldTimeout=300\nSlurmdPort=7003\nSlurmdSpoolDir=/var/spool/slurmd.spool\nSlurmdTimeout=300\nStateSaveLocation=/var/spool/slurm.state\nTreeWidth=16\n#\n# Node Configurations\n#\nNodeName=DEFAULT CPUs=2 RealMemory=2000 TmpDisk=64000 State=UNKNOWN\nNodeName=mcr[0-1151] NodeAddr=emcr[0-1151]\n#\n# Partition Configurations\n#\nPartitionName=DEFAULT State=UP\nPartitionName=pdebug Nodes=mcr[0-191] MaxTime=30 MaxNodes=32 Default=YES\nPartitionName=pbatch Nodes=mcr[192-1151]\n\nSecurity\nBesides authentication of Slurm communications based upon the value\nof the AuthType, digital signatures are used in job step\ncredentials.\nThis signature is used by slurmctld to construct a job step\ncredential, which is sent to srun and then forwarded to\nslurmd to initiate job steps.\nThis design offers improved performance by removing much of the\njob step initiation overhead from the  slurmctld  daemon.\nThe digital signature mechanism is specified by the CredType\nconfiguration parameter and the default mechanism is MUNGE. \nPluggable Authentication Module (PAM) support\n\n\nA PAM module (Pluggable Authentication Module) is available for Slurm that\ncan prevent a user from accessing a node which he has not been allocated,\nif that mode of operation is desired.\nStarting the Daemons\n\n\nFor testing purposes you may want to start by just running slurmctld and slurmd\non one node. By default, they execute in the background. Use the -D\noption for each daemon to execute them in the foreground and logging will be done\nto your terminal. The -v option will log events\nin more detail with more v's increasing the level of detail (e.g. -vvvvvv).\nYou can use one window to execute \"slurmctld -D -vvvvvv\",\na second window to execute \"slurmd -D -vvvvv\".\nYou may see errors such as \"Connection refused\" or \"Node X not responding\"\nwhile one daemon is operative and the other is being started, but the\ndaemons can be started in any order and proper communications will be\nestablished once both daemons complete initialization.\nYou can use a third window to execute commands such as\n\"srun -N1 /bin/hostname\" to confirm functionality.\nAnother important option for the daemons is \"-c\"\nto clear previous state information. Without the \"-c\"\noption, the daemons will restore any previously saved state information: node\nstate, job state, etc. With the \"-c\" option all\npreviously running jobs will be purged and node state will be restored to the\nvalues specified in the configuration file. This means that a node configured\ndown manually using the scontrol command will\nbe returned to service unless noted as being down in the configuration file.\nIn practice, Slurm consistently restarts with preservation.\nAdministration Examples\n\n\nscontrol can be used to print all system information\nand modify most of it. Only a few examples are shown below. Please see the scontrol\nman page for full details. The commands and options are all case insensitive.\nPrint detailed state of all jobs in the system.\n\nadev0: scontrol\nscontrol: show job\nJobId=475 UserId=bob(6885) Name=sleep JobState=COMPLETED\n   Priority=4294901286 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:53:41 EndTime=03/19-12:53:59\n   NodeList=adev8 NodeListIndecies=-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n\nJobId=476 UserId=bob(6885) Name=sleep JobState=RUNNING\n   Priority=4294901285 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:54:01 EndTime=NONE\n   NodeList=adev8 NodeListIndecies=8,8,-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n Print the detailed state of job 477 and change its priority to\nzero. A priority of zero prevents a job from being initiated (it is held in \"pending\"\nstate).\n\nadev0: scontrol\nscontrol: show job 477\nJobId=477 UserId=bob(6885) Name=sleep JobState=PENDING\n   Priority=4294901286 Partition=batch BatchFlag=0\n   more data removed....\nscontrol: update JobId=477 Priority=0\n\nPrint the state of node adev13 and drain it. To drain a node, specify a new\nstate of DRAIN, DRAINED, or DRAINING. Slurm will automatically set it to the appropriate\nvalue of either DRAINING or DRAINED depending on whether the node is allocated\nor not. Return it to service later.\n\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=ALLOCATED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=DRAIN\nscontrol: show node adev13\nNodeName=adev13 State=DRAINING CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: quit\nLater\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=DRAINED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=IDLE\n Reconfigure all Slurm daemons on all nodes. This should\nbe done after changing the Slurm configuration file.\n\nadev0: scontrol reconfig\n Print the current Slurm configuration. This also reports if the\nprimary and secondary controllers (slurmctld daemons) are responding. To just\nsee the state of the controllers, use the command ping.\n\nadev0: scontrol show config\nConfiguration data as of 2019-03-29T12:20:45\n...\nSlurmctldAddr           = eadevi\nSlurmctldDebug          = info\nSlurmctldHost[0]        = adevi\nSlurmctldHost[1]        = adevj\nSlurmctldLogFile        = /var/log/slurmctld.log\n...\n\nSlurmctld(primary) at adevi is UP\nSlurmctld(backup) at adevj is UP\n Shutdown all Slurm daemons on all nodes.\n\nadev0: scontrol shutdown\n\nUpgrades\nSlurm supports in-place upgrades between certain versions. Important details\nabout the steps necessary to perform an upgrade and the potential complications\nto prepare for are contained on this page:\nUpgrade Guide\nFreeBSD\nFreeBSD administrators can install the latest stable Slurm as a binary\npackage using:\n\npkg install slurm-wlm\n\nOr, it can be built and installed from source using:\n\ncd /usr/ports/sysutils/slurm-wlm && make install\n\nThe binary package installs a minimal Slurm configuration suitable for\ntypical compute nodes.  Installing from source allows the user to enable\noptions such as mysql and gui tools via a configuration menu.\nLast modified 16 August 2024\n"
        },
        {
            "title": "Configuration",
            "content": "The Slurm configuration file includes a wide variety of parameters.\nThis configuration file must be available on each node of the cluster and\nmust have consistent contents. A full\ndescription of the parameters is included in the slurm.conf man page. Rather than\nduplicate that information, a minimal sample configuration file is shown below.\nYour slurm.conf file should define at least the configuration parameters defined\nin this sample and likely additional ones. Any text\nfollowing a \"#\" is considered a comment. The keywords in the file are\nnot case sensitive, although the argument typically is (e.g., \"SlurmUser=slurm\"\nmight be specified as \"slurmuser=slurm\"). The control machine, like\nall other machine specifications, can include both the host name and the name\nused for communications. In this case, the host's name is \"mcri\" and\nthe name \"emcri\" is used for communications.\nIn this case \"emcri\" is the private management network interface\nfor the host \"mcri\". Port numbers to be used for\ncommunications are specified as well as various timer values.The SlurmUser must be created as needed prior to starting Slurm\nand must exist on all nodes in your cluster.\nThe parent directories for Slurm's log files, process ID files,\nstate save directories, etc. are not created by Slurm.\nThey must be created and made writable by SlurmUser as needed prior to\nstarting Slurm daemons.The StateSaveLocation is used to store information about the current\nstate of the cluster, including information about queued, running and recently\ncompleted jobs. The directory used should be on a low-latency local disk to\nprevent file system delays from affecting Slurm performance. If using a backup\nhost, the StateSaveLocation should reside on a file system shared by the two\nhosts. We do not recommend using NFS to make the directory accessible to both\nhosts, but do recommend a shared mount that is accessible to the two\ncontrollers and allows low-latency reads and writes to the disk. If a\ncontroller comes up without access to the state information, queued and\nrunning jobs will be cancelled.A description of the nodes and their grouping into partitions is required.\nA simple node range expression may optionally be used to specify\nranges of nodes to avoid building a configuration file with large\nnumbers of entries. The node range expression can contain one\npair of square brackets with a sequence of comma separated\nnumbers and/or ranges of numbers separated by a \"-\"\n(e.g. \"linux[0-64,128]\", or \"lx[15,18,32-33]\").\nUp to two numeric ranges can be included in the expression\n(e.g. \"rack[0-63]_blade[0-41]\").\nIf one or more numeric expressions are included, one of them\nmust be at the end of the name (e.g. \"unit[0-31]rack\" is invalid),\nbut arbitrary names can always be used in a comma separated list.Node names can have up to three name specifications:\nNodeName is the name used by all Slurm tools when referring to the node,\nNodeAddr is the name or IP address Slurm uses to communicate with the node, and\nNodeHostname is the name returned by the command /bin/hostname -s.\nOnly NodeName is required (the others default to the same name),\nalthough supporting all three parameters provides complete control over\nnaming and addressing the nodes.  See the slurm.conf man page for\ndetails on all configuration parameters.Nodes can be in more than one partition and each partition can have different\nconstraints (permitted users, time limits, job size limits, etc.).\nEach partition can thus be considered a separate queue.\nPartition and node specifications use node range expressions to identify\nnodes in a concise fashion. This configuration file defines a 1154-node cluster\nfor Slurm, but it might be used for a much larger cluster by just changing a few\nnode range expressions. Specify the minimum processor count (CPUs), real memory\nspace (RealMemory, megabytes), and temporary disk space (TmpDisk, megabytes) that\na node should have to be considered available for use. Any node lacking these\nminimum configuration values will be considered DOWN and not scheduled.\nNote that a more extensive sample configuration file is provided in\netc/slurm.conf.example. We also have a web-based\nconfiguration tool which can\nbe used to build a simple configuration file, which can then be\nmanually edited for more complex configurations.\n#\n# Sample /etc/slurm.conf for mcr.llnl.gov\n#\nSlurmctldHost=mcri(12.34.56.78)\nSlurmctldHost=mcrj(12.34.56.79)\n#\nAuthType=auth/munge\nEpilog=/usr/local/slurm/etc/epilog\nJobCompLoc=/var/tmp/jette/slurm.job.log\nJobCompType=jobcomp/filetxt\nPluginDir=/usr/local/slurm/lib/slurm\nProlog=/usr/local/slurm/etc/prolog\nSchedulerType=sched/backfill\nSelectType=select/linear\nSlurmUser=slurm\nSlurmctldPort=7002\nSlurmctldTimeout=300\nSlurmdPort=7003\nSlurmdSpoolDir=/var/spool/slurmd.spool\nSlurmdTimeout=300\nStateSaveLocation=/var/spool/slurm.state\nTreeWidth=16\n#\n# Node Configurations\n#\nNodeName=DEFAULT CPUs=2 RealMemory=2000 TmpDisk=64000 State=UNKNOWN\nNodeName=mcr[0-1151] NodeAddr=emcr[0-1151]\n#\n# Partition Configurations\n#\nPartitionName=DEFAULT State=UP\nPartitionName=pdebug Nodes=mcr[0-191] MaxTime=30 MaxNodes=32 Default=YES\nPartitionName=pbatch Nodes=mcr[192-1151]\nSecurityBesides authentication of Slurm communications based upon the value\nof the AuthType, digital signatures are used in job step\ncredentials.\nThis signature is used by slurmctld to construct a job step\ncredential, which is sent to srun and then forwarded to\nslurmd to initiate job steps.\nThis design offers improved performance by removing much of the\njob step initiation overhead from the  slurmctld  daemon.\nThe digital signature mechanism is specified by the CredType\nconfiguration parameter and the default mechanism is MUNGE. Pluggable Authentication Module (PAM) support\n\nA PAM module (Pluggable Authentication Module) is available for Slurm that\ncan prevent a user from accessing a node which he has not been allocated,\nif that mode of operation is desired.Starting the Daemons\n\nFor testing purposes you may want to start by just running slurmctld and slurmd\non one node. By default, they execute in the background. Use the -D\noption for each daemon to execute them in the foreground and logging will be done\nto your terminal. The -v option will log events\nin more detail with more v's increasing the level of detail (e.g. -vvvvvv).\nYou can use one window to execute \"slurmctld -D -vvvvvv\",\na second window to execute \"slurmd -D -vvvvv\".\nYou may see errors such as \"Connection refused\" or \"Node X not responding\"\nwhile one daemon is operative and the other is being started, but the\ndaemons can be started in any order and proper communications will be\nestablished once both daemons complete initialization.\nYou can use a third window to execute commands such as\n\"srun -N1 /bin/hostname\" to confirm functionality.Another important option for the daemons is \"-c\"\nto clear previous state information. Without the \"-c\"\noption, the daemons will restore any previously saved state information: node\nstate, job state, etc. With the \"-c\" option all\npreviously running jobs will be purged and node state will be restored to the\nvalues specified in the configuration file. This means that a node configured\ndown manually using the scontrol command will\nbe returned to service unless noted as being down in the configuration file.\nIn practice, Slurm consistently restarts with preservation.Administration Examples\n\nscontrol can be used to print all system information\nand modify most of it. Only a few examples are shown below. Please see the scontrol\nman page for full details. The commands and options are all case insensitive.Print detailed state of all jobs in the system.\nadev0: scontrol\nscontrol: show job\nJobId=475 UserId=bob(6885) Name=sleep JobState=COMPLETED\n   Priority=4294901286 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:53:41 EndTime=03/19-12:53:59\n   NodeList=adev8 NodeListIndecies=-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n\nJobId=476 UserId=bob(6885) Name=sleep JobState=RUNNING\n   Priority=4294901285 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:54:01 EndTime=NONE\n   NodeList=adev8 NodeListIndecies=8,8,-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\nPrint the detailed state of job 477 and change its priority to\nzero. A priority of zero prevents a job from being initiated (it is held in \"pending\"\nstate).\nadev0: scontrol\nscontrol: show job 477\nJobId=477 UserId=bob(6885) Name=sleep JobState=PENDING\n   Priority=4294901286 Partition=batch BatchFlag=0\n   more data removed....\nscontrol: update JobId=477 Priority=0\nPrint the state of node adev13 and drain it. To drain a node, specify a new\nstate of DRAIN, DRAINED, or DRAINING. Slurm will automatically set it to the appropriate\nvalue of either DRAINING or DRAINED depending on whether the node is allocated\nor not. Return it to service later.\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=ALLOCATED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=DRAIN\nscontrol: show node adev13\nNodeName=adev13 State=DRAINING CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: quit\nLater\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=DRAINED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=IDLE\nReconfigure all Slurm daemons on all nodes. This should\nbe done after changing the Slurm configuration file.\nadev0: scontrol reconfig\nPrint the current Slurm configuration. This also reports if the\nprimary and secondary controllers (slurmctld daemons) are responding. To just\nsee the state of the controllers, use the command ping.\nadev0: scontrol show config\nConfiguration data as of 2019-03-29T12:20:45\n...\nSlurmctldAddr           = eadevi\nSlurmctldDebug          = info\nSlurmctldHost[0]        = adevi\nSlurmctldHost[1]        = adevj\nSlurmctldLogFile        = /var/log/slurmctld.log\n...\n\nSlurmctld(primary) at adevi is UP\nSlurmctld(backup) at adevj is UP\nShutdown all Slurm daemons on all nodes.\nadev0: scontrol shutdown\nUpgradesSlurm supports in-place upgrades between certain versions. Important details\nabout the steps necessary to perform an upgrade and the potential complications\nto prepare for are contained on this page:\nUpgrade GuideFreeBSDFreeBSD administrators can install the latest stable Slurm as a binary\npackage using:\npkg install slurm-wlm\nOr, it can be built and installed from source using:\ncd /usr/ports/sysutils/slurm-wlm && make install\nThe binary package installs a minimal Slurm configuration suitable for\ntypical compute nodes.  Installing from source allows the user to enable\noptions such as mysql and gui tools via a configuration menu.Last modified 16 August 2024"
        },
        {
            "title": "RPMs Installed",
            "content": "The RPMs needed on the head node, compute nodes, and slurmdbd node can vary\nby configuration, but here is a suggested starting point:\n\nHead Node (where the slurmctld daemon runs),\n    Compute and Login Nodes\n\t\nslurm\nslurm-perlapi\nslurm-slurmctld (only on the head node)\nslurm-slurmd (only on the compute nodes)\n\n\nSlurmDBD Node\n\t\nslurm\nslurm-slurmdbd\n\n\n\nBuilding Debian Packages\n\n\nBeginning with Slurm 23.11.0, Slurm includes the files required to build\nDebian packages. These packages conflict with the packages shipped with Debian\nbased distributions, and are named distinctly to differentiate them. After\ndownloading the desired version of Slurm, the following can be done to build\nthe packages:\n\nInstall basic Debian package build requirements:\napt-get install build-essential fakeroot devscripts equivs\n\nUnpack the distributed tarball:\ntar -xaf slurm*tar.bz2\n\ncd to the directory containing the Slurm source\nInstall the Slurm package dependencies:\nmk-build-deps -i debian/control\n\nBuild the Slurm packages:\ndebuild -b -uc -us\n\n\nThe packages will be in the parent directory after debuild completes.\nInstalling Debian Packages\n\n\nThe packages needed on the head node, compute nodes, and slurmdbd node can\nvary site to site, but this is a good starting point:\n\nSlurmDBD Node\n\t\nslurm-smd\nslurm-smd-slurmdbd\n\n\nHead Node (slurmctld node)\n\t\nslurm-smd\nslurm-smd-slurmctld\nslurm-smd-client\n\n\nCompute Nodes (slurmd node)\n\t\nslurm-smd\nslurm-smd-slurmd\nslurm-smd-client\n\n\nLogin Nodes\n\t\nslurm-smd\nslurm-smd-client\n\n\n\nBuilding Manually\n\n\nInstructions to build and install Slurm manually are shown below.\nThis is significantly more complicated to manage than the RPM and DEB build\nprocedures, so this approach is only recommended for developers or\nadvanced users who are looking for a more customized install.\nSee the README and INSTALL files in the source distribution for more details.\n\n\nUnpack the distributed tarball:\ntar -xaf slurm*tar.bz2\ncd to the directory containing the Slurm source and type\n./configure with appropriate options (see below).\nType make install to compile and install the programs,\ndocumentation, libraries, header files, etc.\nType ldconfig -n <library_location> so that the Slurm\nlibraries can be found by applications that intend to use Slurm APIs directly.\nThe library location will be a subdirectory of PREFIX (described below) and\ndepend upon the system type and configuration, typically lib or lib64.\nFor example, if PREFIX is \"/usr\" and the subdirectory is \"lib64\" then you would\nfind that a file named \"/usr/lib64/libslurm.so\" was installed and the command\nldconfig -n /usr/lib64 should be executed.\n\nA full list of configure options will be returned by the\ncommand configure --help. The most commonly used arguments\nto the configure command include:\n--enable-debug\nEnable additional debugging logic within Slurm.\n--prefix=PREFIX\nInstall architecture-independent files in PREFIX; default value is /usr/local.\n--sysconfdir=DIR\nSpecify location of Slurm configuration file. The default value is PREFIX/etc\nDaemons\nslurmctld is sometimes called the \"controller\".\nIt orchestrates Slurm activities, including queuing of jobs,\nmonitoring node states, and allocating resources to jobs. There is an\noptional backup controller that automatically assumes control in the\nevent the primary controller fails (see the High\nAvailability section below).  The primary controller resumes\ncontrol whenever it is restored to service. The controller saves its\nstate to disk whenever there is a change in state (see\n\"StateSaveLocation\" in Configuration\nsection below).  This state can be recovered by the controller at\nstartup time.  State changes are saved so that jobs and other state\ninformation can be preserved when the controller moves (to or from a\nbackup controller) or is restarted.\nWe recommend that you create a Unix user slurm for use by\nslurmctld. This user name will also be specified using the\nSlurmUser in the slurm.conf configuration file.\nThis user must exist on all nodes of the cluster.\nNote that files and directories used by slurmctld will need to be\nreadable or writable by the user SlurmUser (the Slurm configuration\nfiles must be readable; the log file directory and state save directory\nmust be writable).\nThe slurmd daemon executes on every compute node. It resembles a\nremote shell daemon to export control to Slurm. Because slurmd initiates and\nmanages user jobs, it must execute as the user root.\nIf you want to archive job accounting records to a database, the\nslurmdbd (Slurm DataBase Daemon) should be used. We recommend that\nyou defer adding accounting support until after basic Slurm functionality is\nestablished on your system. An Accounting web\npage contains more information.\nslurmctld and/or slurmd should be initiated at node startup\ntime per the Slurm configuration.\nThe slurmrestd daemon was introduced in version 20.02 and allows\nclients to communicate with Slurm via the REST API. This is installed by\ndefault, assuming the prerequisites are met.\nIt has two run modes,\nallowing you to have it run as a traditional Unix service and always listen\nfor TCP connections, or you can have it run as an Inet service and only have\nit active when in use.\nHigh Availability\nMultiple SlurmctldHost entries can be configured, with any entry beyond the\nfirst being treated as a backup host. Any backup hosts configured should be on\na different node than the node hosting the primary slurmctld. However, all\nhosts should mount a common file system containing the state information (see\n\"StateSaveLocation\" in the Configuration\nsection below).\nIf more than one host is specified, when the primary fails the second listed\nSlurmctldHost will take over for it. When the primary returns to service, it\nnotifies the backup.  The backup then saves the state and returns to backup\nmode. The primary reads the saved state and resumes normal operation. Likewise,\nif both of the first two listed hosts fail the third SlurmctldHost will take\nover until the primary returns to service. Other than a brief period of non-\nresponsiveness, the transition back and forth should go undetected.\nPrior to 18.08, Slurm used the \n\"BackupAddr\" and \n\"BackupController\" parameters for High Availability. These\nparameters have been deprecated and are replaced by\n\"SlurmctldHost\".\nAlso see \"\nSlurmctldPrimaryOnProg\" and\n\"\nSlurmctldPrimaryOffProg\" to adjust the actions taken when machines\ntransition between being the primary controller.\nAny time the slurmctld daemon or hardware fails before state information\nreaches disk can result in lost state.\nSlurmctld writes state frequently (every five seconds by default), but with\nlarge numbers of jobs, the formatting and writing of records can take seconds\nand recent changes might not be written to disk.\nAnother example is if the state information is written to file, but that\ninformation is cached in memory rather than written to disk when the node fails.\nThe interval between state saves being written to disk can be configured at\nbuild time by defining SAVE_MAX_WAIT to a different value than five.\nA backup instance of slurmdbd can also be configured by specifying\n\nAccountingStorageBackupHost in slurm.conf, as well as\nDbdBackupHost in\nslurmdbd.conf. The backup host should be on a different machine than the one\nhosting the primary instance of slurmdbd. Both instances of slurmdbd should\nhave access to the same database. The\nnetwork page has a visual representation\nof how this might look.\nInfrastructure\n\n\nUser and Group Identification\n\n\nThere must be a uniform user and group name space (including\nUIDs and GIDs) across the cluster.\nIt is not necessary to permit user logins to the control hosts\n(SlurmctldHost), but the\nusers and groups must be resolvable on those hosts.\nAuthentication of Slurm communications\n\n\nAll communications between Slurm components are authenticated. The\nauthentication infrastructure is provided by a dynamically loaded\nplugin chosen at runtime via the AuthType keyword in the Slurm\nconfiguration file. Until 23.11.0, the only supported authentication type was\nmunge, which requires the\ninstallation of the MUNGE package.\nWhen using MUNGE, all nodes in the cluster must be configured with the\nsame munge.key file. The MUNGE daemon, munged, must also be\nstarted before Slurm daemons. Note that MUNGE does require clocks to be\nsynchronized throughout the cluster, usually done by NTP.\nAs of 23.11.0, AuthType can also be set to\nslurm, an internal authentication\nplugin. This plugin has similar requirements to MUNGE, requiring a key file\nshared to all Slurm daemons. The auth/slurm plugin requires installation of the\njwt package.\nMUNGE is currently the default and recommended option.\nThe configure script in the top-level directory of this distribution will\ndetermine which authentication plugins may be built.\nThe configuration file specifies which of the available plugins will be\nutilized.\nMPI support\nSlurm supports many different MPI implementations.\nFor more information, see MPI.\n\nScheduler support\n\n\nSlurm can be configured with rather simple or quite sophisticated\nscheduling algorithms depending upon your needs and willingness to\nmanage the configuration (much of which requires a database).\nThe first configuration parameter of interest is PriorityType\nwith two options available: basic (first-in-first-out) and\nmultifactor.\nThe multifactor plugin will assign a priority to jobs based upon\na multitude of configuration parameters (age, size, fair-share allocation,\netc.) and its details are beyond the scope of this document.\nSee the Multifactor Job Priority Plugin\ndocument for details.\nThe SchedType configuration parameter controls how queued\njobs are scheduled and several options are available.\n\nbuiltin will initiate jobs strictly in their priority order,\ntypically (first-in-first-out) \nbackfill will initiate a lower-priority job if doing so does\nnot delay the expected initiation time of higher priority jobs; essentially\nusing smaller jobs to fill holes in the resource allocation plan. Effective\nbackfill scheduling does require users to specify job time limits.\ngang time-slices jobs in the same partition/queue and can be\nused to preempt jobs from lower-priority queues in order to execute\njobs in higher priority queues.\n\nFor more information about scheduling options see\nGang Scheduling,\nPreemption,\nResource Reservation Guide,\nResource Limits and\nSharing Consumable Resources.\nResource selection\n\n\nThe resource selection mechanism used by Slurm is controlled by the\nSelectType configuration parameter.\nIf you want to execute multiple jobs per node, but track and manage allocation\nof the processors, memory and other resources, the cons_tres (consumable\ntrackable resources) plugin is recommended.\nFor more information, please see\nConsumable Resources in Slurm.\nLogging\nSlurm uses syslog to record events if the SlurmctldLogFile and\nSlurmdLogFile locations are not set.\nAccounting\nSlurm supports accounting records being written to a simple text file,\ndirectly to a database (MySQL or MariaDB), or to a daemon securely\nmanaging accounting data for multiple clusters. For more information\nsee Accounting. \nCompute node access\n\n\nSlurm does not by itself limit access to allocated compute nodes,\nbut it does provide mechanisms to accomplish this.\nThere is a Pluggable Authentication Module (PAM) for restricting access\nto compute nodes available for download.\nWhen installed, the Slurm PAM module will prevent users from logging\ninto any node that has not be assigned to that user.\nOn job termination, any processes initiated by the user outside of\nSlurm's control may be killed using an Epilog script configured\nin slurm.conf.\nConfiguration\nThe Slurm configuration file includes a wide variety of parameters.\nThis configuration file must be available on each node of the cluster and\nmust have consistent contents. A full\ndescription of the parameters is included in the slurm.conf man page. Rather than\nduplicate that information, a minimal sample configuration file is shown below.\nYour slurm.conf file should define at least the configuration parameters defined\nin this sample and likely additional ones. Any text\nfollowing a \"#\" is considered a comment. The keywords in the file are\nnot case sensitive, although the argument typically is (e.g., \"SlurmUser=slurm\"\nmight be specified as \"slurmuser=slurm\"). The control machine, like\nall other machine specifications, can include both the host name and the name\nused for communications. In this case, the host's name is \"mcri\" and\nthe name \"emcri\" is used for communications.\nIn this case \"emcri\" is the private management network interface\nfor the host \"mcri\". Port numbers to be used for\ncommunications are specified as well as various timer values.\nThe SlurmUser must be created as needed prior to starting Slurm\nand must exist on all nodes in your cluster.\nThe parent directories for Slurm's log files, process ID files,\nstate save directories, etc. are not created by Slurm.\nThey must be created and made writable by SlurmUser as needed prior to\nstarting Slurm daemons.\nThe StateSaveLocation is used to store information about the current\nstate of the cluster, including information about queued, running and recently\ncompleted jobs. The directory used should be on a low-latency local disk to\nprevent file system delays from affecting Slurm performance. If using a backup\nhost, the StateSaveLocation should reside on a file system shared by the two\nhosts. We do not recommend using NFS to make the directory accessible to both\nhosts, but do recommend a shared mount that is accessible to the two\ncontrollers and allows low-latency reads and writes to the disk. If a\ncontroller comes up without access to the state information, queued and\nrunning jobs will be cancelled.\nA description of the nodes and their grouping into partitions is required.\nA simple node range expression may optionally be used to specify\nranges of nodes to avoid building a configuration file with large\nnumbers of entries. The node range expression can contain one\npair of square brackets with a sequence of comma separated\nnumbers and/or ranges of numbers separated by a \"-\"\n(e.g. \"linux[0-64,128]\", or \"lx[15,18,32-33]\").\nUp to two numeric ranges can be included in the expression\n(e.g. \"rack[0-63]_blade[0-41]\").\nIf one or more numeric expressions are included, one of them\nmust be at the end of the name (e.g. \"unit[0-31]rack\" is invalid),\nbut arbitrary names can always be used in a comma separated list.\nNode names can have up to three name specifications:\nNodeName is the name used by all Slurm tools when referring to the node,\nNodeAddr is the name or IP address Slurm uses to communicate with the node, and\nNodeHostname is the name returned by the command /bin/hostname -s.\nOnly NodeName is required (the others default to the same name),\nalthough supporting all three parameters provides complete control over\nnaming and addressing the nodes.  See the slurm.conf man page for\ndetails on all configuration parameters.\nNodes can be in more than one partition and each partition can have different\nconstraints (permitted users, time limits, job size limits, etc.).\nEach partition can thus be considered a separate queue.\nPartition and node specifications use node range expressions to identify\nnodes in a concise fashion. This configuration file defines a 1154-node cluster\nfor Slurm, but it might be used for a much larger cluster by just changing a few\nnode range expressions. Specify the minimum processor count (CPUs), real memory\nspace (RealMemory, megabytes), and temporary disk space (TmpDisk, megabytes) that\na node should have to be considered available for use. Any node lacking these\nminimum configuration values will be considered DOWN and not scheduled.\nNote that a more extensive sample configuration file is provided in\netc/slurm.conf.example. We also have a web-based\nconfiguration tool which can\nbe used to build a simple configuration file, which can then be\nmanually edited for more complex configurations.\n\n#\n# Sample /etc/slurm.conf for mcr.llnl.gov\n#\nSlurmctldHost=mcri(12.34.56.78)\nSlurmctldHost=mcrj(12.34.56.79)\n#\nAuthType=auth/munge\nEpilog=/usr/local/slurm/etc/epilog\nJobCompLoc=/var/tmp/jette/slurm.job.log\nJobCompType=jobcomp/filetxt\nPluginDir=/usr/local/slurm/lib/slurm\nProlog=/usr/local/slurm/etc/prolog\nSchedulerType=sched/backfill\nSelectType=select/linear\nSlurmUser=slurm\nSlurmctldPort=7002\nSlurmctldTimeout=300\nSlurmdPort=7003\nSlurmdSpoolDir=/var/spool/slurmd.spool\nSlurmdTimeout=300\nStateSaveLocation=/var/spool/slurm.state\nTreeWidth=16\n#\n# Node Configurations\n#\nNodeName=DEFAULT CPUs=2 RealMemory=2000 TmpDisk=64000 State=UNKNOWN\nNodeName=mcr[0-1151] NodeAddr=emcr[0-1151]\n#\n# Partition Configurations\n#\nPartitionName=DEFAULT State=UP\nPartitionName=pdebug Nodes=mcr[0-191] MaxTime=30 MaxNodes=32 Default=YES\nPartitionName=pbatch Nodes=mcr[192-1151]\n\nSecurity\nBesides authentication of Slurm communications based upon the value\nof the AuthType, digital signatures are used in job step\ncredentials.\nThis signature is used by slurmctld to construct a job step\ncredential, which is sent to srun and then forwarded to\nslurmd to initiate job steps.\nThis design offers improved performance by removing much of the\njob step initiation overhead from the  slurmctld  daemon.\nThe digital signature mechanism is specified by the CredType\nconfiguration parameter and the default mechanism is MUNGE. \nPluggable Authentication Module (PAM) support\n\n\nA PAM module (Pluggable Authentication Module) is available for Slurm that\ncan prevent a user from accessing a node which he has not been allocated,\nif that mode of operation is desired.\nStarting the Daemons\n\n\nFor testing purposes you may want to start by just running slurmctld and slurmd\non one node. By default, they execute in the background. Use the -D\noption for each daemon to execute them in the foreground and logging will be done\nto your terminal. The -v option will log events\nin more detail with more v's increasing the level of detail (e.g. -vvvvvv).\nYou can use one window to execute \"slurmctld -D -vvvvvv\",\na second window to execute \"slurmd -D -vvvvv\".\nYou may see errors such as \"Connection refused\" or \"Node X not responding\"\nwhile one daemon is operative and the other is being started, but the\ndaemons can be started in any order and proper communications will be\nestablished once both daemons complete initialization.\nYou can use a third window to execute commands such as\n\"srun -N1 /bin/hostname\" to confirm functionality.\nAnother important option for the daemons is \"-c\"\nto clear previous state information. Without the \"-c\"\noption, the daemons will restore any previously saved state information: node\nstate, job state, etc. With the \"-c\" option all\npreviously running jobs will be purged and node state will be restored to the\nvalues specified in the configuration file. This means that a node configured\ndown manually using the scontrol command will\nbe returned to service unless noted as being down in the configuration file.\nIn practice, Slurm consistently restarts with preservation.\nAdministration Examples\n\n\nscontrol can be used to print all system information\nand modify most of it. Only a few examples are shown below. Please see the scontrol\nman page for full details. The commands and options are all case insensitive.\nPrint detailed state of all jobs in the system.\n\nadev0: scontrol\nscontrol: show job\nJobId=475 UserId=bob(6885) Name=sleep JobState=COMPLETED\n   Priority=4294901286 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:53:41 EndTime=03/19-12:53:59\n   NodeList=adev8 NodeListIndecies=-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n\nJobId=476 UserId=bob(6885) Name=sleep JobState=RUNNING\n   Priority=4294901285 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:54:01 EndTime=NONE\n   NodeList=adev8 NodeListIndecies=8,8,-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n Print the detailed state of job 477 and change its priority to\nzero. A priority of zero prevents a job from being initiated (it is held in \"pending\"\nstate).\n\nadev0: scontrol\nscontrol: show job 477\nJobId=477 UserId=bob(6885) Name=sleep JobState=PENDING\n   Priority=4294901286 Partition=batch BatchFlag=0\n   more data removed....\nscontrol: update JobId=477 Priority=0\n\nPrint the state of node adev13 and drain it. To drain a node, specify a new\nstate of DRAIN, DRAINED, or DRAINING. Slurm will automatically set it to the appropriate\nvalue of either DRAINING or DRAINED depending on whether the node is allocated\nor not. Return it to service later.\n\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=ALLOCATED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=DRAIN\nscontrol: show node adev13\nNodeName=adev13 State=DRAINING CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: quit\nLater\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=DRAINED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=IDLE\n Reconfigure all Slurm daemons on all nodes. This should\nbe done after changing the Slurm configuration file.\n\nadev0: scontrol reconfig\n Print the current Slurm configuration. This also reports if the\nprimary and secondary controllers (slurmctld daemons) are responding. To just\nsee the state of the controllers, use the command ping.\n\nadev0: scontrol show config\nConfiguration data as of 2019-03-29T12:20:45\n...\nSlurmctldAddr           = eadevi\nSlurmctldDebug          = info\nSlurmctldHost[0]        = adevi\nSlurmctldHost[1]        = adevj\nSlurmctldLogFile        = /var/log/slurmctld.log\n...\n\nSlurmctld(primary) at adevi is UP\nSlurmctld(backup) at adevj is UP\n Shutdown all Slurm daemons on all nodes.\n\nadev0: scontrol shutdown\n\nUpgrades\nSlurm supports in-place upgrades between certain versions. Important details\nabout the steps necessary to perform an upgrade and the potential complications\nto prepare for are contained on this page:\nUpgrade Guide\nFreeBSD\nFreeBSD administrators can install the latest stable Slurm as a binary\npackage using:\n\npkg install slurm-wlm\n\nOr, it can be built and installed from source using:\n\ncd /usr/ports/sysutils/slurm-wlm && make install\n\nThe binary package installs a minimal Slurm configuration suitable for\ntypical compute nodes.  Installing from source allows the user to enable\noptions such as mysql and gui tools via a configuration menu.\nLast modified 16 August 2024\n"
        },
        {
            "title": "Building Debian Packages\n\n",
            "content": "Beginning with Slurm 23.11.0, Slurm includes the files required to build\nDebian packages. These packages conflict with the packages shipped with Debian\nbased distributions, and are named distinctly to differentiate them. After\ndownloading the desired version of Slurm, the following can be done to build\nthe packages:\nInstall basic Debian package build requirements:\napt-get install build-essential fakeroot devscripts equivs\n\nUnpack the distributed tarball:\ntar -xaf slurm*tar.bz2\n\ncd to the directory containing the Slurm source\nInstall the Slurm package dependencies:\nmk-build-deps -i debian/control\n\nBuild the Slurm packages:\ndebuild -b -uc -us\n\nThe packages will be in the parent directory after debuild completes.Installing Debian Packages\n\nThe packages needed on the head node, compute nodes, and slurmdbd node can\nvary site to site, but this is a good starting point:\nSlurmDBD Node\n\t\nslurm-smd\nslurm-smd-slurmdbd\n\n\nHead Node (slurmctld node)\n\t\nslurm-smd\nslurm-smd-slurmctld\nslurm-smd-client\n\n\nCompute Nodes (slurmd node)\n\t\nslurm-smd\nslurm-smd-slurmd\nslurm-smd-client\n\n\nLogin Nodes\n\t\nslurm-smd\nslurm-smd-client\n\n\nBuilding Manually\n\nInstructions to build and install Slurm manually are shown below.\nThis is significantly more complicated to manage than the RPM and DEB build\nprocedures, so this approach is only recommended for developers or\nadvanced users who are looking for a more customized install.\nSee the README and INSTALL files in the source distribution for more details.\n\nUnpack the distributed tarball:\ntar -xaf slurm*tar.bz2\ncd to the directory containing the Slurm source and type\n./configure with appropriate options (see below).\nType make install to compile and install the programs,\ndocumentation, libraries, header files, etc.\nType ldconfig -n <library_location> so that the Slurm\nlibraries can be found by applications that intend to use Slurm APIs directly.\nThe library location will be a subdirectory of PREFIX (described below) and\ndepend upon the system type and configuration, typically lib or lib64.\nFor example, if PREFIX is \"/usr\" and the subdirectory is \"lib64\" then you would\nfind that a file named \"/usr/lib64/libslurm.so\" was installed and the command\nldconfig -n /usr/lib64 should be executed.\nA full list of configure options will be returned by the\ncommand configure --help. The most commonly used arguments\nto the configure command include:--enable-debug\nEnable additional debugging logic within Slurm.--prefix=PREFIX\nInstall architecture-independent files in PREFIX; default value is /usr/local.--sysconfdir=DIR\nSpecify location of Slurm configuration file. The default value is PREFIX/etcDaemonsslurmctld is sometimes called the \"controller\".\nIt orchestrates Slurm activities, including queuing of jobs,\nmonitoring node states, and allocating resources to jobs. There is an\noptional backup controller that automatically assumes control in the\nevent the primary controller fails (see the High\nAvailability section below).  The primary controller resumes\ncontrol whenever it is restored to service. The controller saves its\nstate to disk whenever there is a change in state (see\n\"StateSaveLocation\" in Configuration\nsection below).  This state can be recovered by the controller at\nstartup time.  State changes are saved so that jobs and other state\ninformation can be preserved when the controller moves (to or from a\nbackup controller) or is restarted.We recommend that you create a Unix user slurm for use by\nslurmctld. This user name will also be specified using the\nSlurmUser in the slurm.conf configuration file.\nThis user must exist on all nodes of the cluster.\nNote that files and directories used by slurmctld will need to be\nreadable or writable by the user SlurmUser (the Slurm configuration\nfiles must be readable; the log file directory and state save directory\nmust be writable).The slurmd daemon executes on every compute node. It resembles a\nremote shell daemon to export control to Slurm. Because slurmd initiates and\nmanages user jobs, it must execute as the user root.If you want to archive job accounting records to a database, the\nslurmdbd (Slurm DataBase Daemon) should be used. We recommend that\nyou defer adding accounting support until after basic Slurm functionality is\nestablished on your system. An Accounting web\npage contains more information.slurmctld and/or slurmd should be initiated at node startup\ntime per the Slurm configuration.The slurmrestd daemon was introduced in version 20.02 and allows\nclients to communicate with Slurm via the REST API. This is installed by\ndefault, assuming the prerequisites are met.\nIt has two run modes,\nallowing you to have it run as a traditional Unix service and always listen\nfor TCP connections, or you can have it run as an Inet service and only have\nit active when in use.High AvailabilityMultiple SlurmctldHost entries can be configured, with any entry beyond the\nfirst being treated as a backup host. Any backup hosts configured should be on\na different node than the node hosting the primary slurmctld. However, all\nhosts should mount a common file system containing the state information (see\n\"StateSaveLocation\" in the Configuration\nsection below).If more than one host is specified, when the primary fails the second listed\nSlurmctldHost will take over for it. When the primary returns to service, it\nnotifies the backup.  The backup then saves the state and returns to backup\nmode. The primary reads the saved state and resumes normal operation. Likewise,\nif both of the first two listed hosts fail the third SlurmctldHost will take\nover until the primary returns to service. Other than a brief period of non-\nresponsiveness, the transition back and forth should go undetected.Prior to 18.08, Slurm used the \n\"BackupAddr\" and \n\"BackupController\" parameters for High Availability. These\nparameters have been deprecated and are replaced by\n\"SlurmctldHost\".\nAlso see \"\nSlurmctldPrimaryOnProg\" and\n\"\nSlurmctldPrimaryOffProg\" to adjust the actions taken when machines\ntransition between being the primary controller.Any time the slurmctld daemon or hardware fails before state information\nreaches disk can result in lost state.\nSlurmctld writes state frequently (every five seconds by default), but with\nlarge numbers of jobs, the formatting and writing of records can take seconds\nand recent changes might not be written to disk.\nAnother example is if the state information is written to file, but that\ninformation is cached in memory rather than written to disk when the node fails.\nThe interval between state saves being written to disk can be configured at\nbuild time by defining SAVE_MAX_WAIT to a different value than five.A backup instance of slurmdbd can also be configured by specifying\n\nAccountingStorageBackupHost in slurm.conf, as well as\nDbdBackupHost in\nslurmdbd.conf. The backup host should be on a different machine than the one\nhosting the primary instance of slurmdbd. Both instances of slurmdbd should\nhave access to the same database. The\nnetwork page has a visual representation\nof how this might look.Infrastructure\n\nUser and Group Identification\n\nThere must be a uniform user and group name space (including\nUIDs and GIDs) across the cluster.\nIt is not necessary to permit user logins to the control hosts\n(SlurmctldHost), but the\nusers and groups must be resolvable on those hosts.Authentication of Slurm communications\n\nAll communications between Slurm components are authenticated. The\nauthentication infrastructure is provided by a dynamically loaded\nplugin chosen at runtime via the AuthType keyword in the Slurm\nconfiguration file. Until 23.11.0, the only supported authentication type was\nmunge, which requires the\ninstallation of the MUNGE package.\nWhen using MUNGE, all nodes in the cluster must be configured with the\nsame munge.key file. The MUNGE daemon, munged, must also be\nstarted before Slurm daemons. Note that MUNGE does require clocks to be\nsynchronized throughout the cluster, usually done by NTP.As of 23.11.0, AuthType can also be set to\nslurm, an internal authentication\nplugin. This plugin has similar requirements to MUNGE, requiring a key file\nshared to all Slurm daemons. The auth/slurm plugin requires installation of the\njwt package.MUNGE is currently the default and recommended option.\nThe configure script in the top-level directory of this distribution will\ndetermine which authentication plugins may be built.\nThe configuration file specifies which of the available plugins will be\nutilized.MPI supportSlurm supports many different MPI implementations.\nFor more information, see MPI.\n\nScheduler support\n\n\nSlurm can be configured with rather simple or quite sophisticated\nscheduling algorithms depending upon your needs and willingness to\nmanage the configuration (much of which requires a database).\nThe first configuration parameter of interest is PriorityType\nwith two options available: basic (first-in-first-out) and\nmultifactor.\nThe multifactor plugin will assign a priority to jobs based upon\na multitude of configuration parameters (age, size, fair-share allocation,\netc.) and its details are beyond the scope of this document.\nSee the Multifactor Job Priority Plugin\ndocument for details.\nThe SchedType configuration parameter controls how queued\njobs are scheduled and several options are available.\n\nbuiltin will initiate jobs strictly in their priority order,\ntypically (first-in-first-out) \nbackfill will initiate a lower-priority job if doing so does\nnot delay the expected initiation time of higher priority jobs; essentially\nusing smaller jobs to fill holes in the resource allocation plan. Effective\nbackfill scheduling does require users to specify job time limits.\ngang time-slices jobs in the same partition/queue and can be\nused to preempt jobs from lower-priority queues in order to execute\njobs in higher priority queues.\n\nFor more information about scheduling options see\nGang Scheduling,\nPreemption,\nResource Reservation Guide,\nResource Limits and\nSharing Consumable Resources.\nResource selection\n\n\nThe resource selection mechanism used by Slurm is controlled by the\nSelectType configuration parameter.\nIf you want to execute multiple jobs per node, but track and manage allocation\nof the processors, memory and other resources, the cons_tres (consumable\ntrackable resources) plugin is recommended.\nFor more information, please see\nConsumable Resources in Slurm.\nLogging\nSlurm uses syslog to record events if the SlurmctldLogFile and\nSlurmdLogFile locations are not set.\nAccounting\nSlurm supports accounting records being written to a simple text file,\ndirectly to a database (MySQL or MariaDB), or to a daemon securely\nmanaging accounting data for multiple clusters. For more information\nsee Accounting. \nCompute node access\n\n\nSlurm does not by itself limit access to allocated compute nodes,\nbut it does provide mechanisms to accomplish this.\nThere is a Pluggable Authentication Module (PAM) for restricting access\nto compute nodes available for download.\nWhen installed, the Slurm PAM module will prevent users from logging\ninto any node that has not be assigned to that user.\nOn job termination, any processes initiated by the user outside of\nSlurm's control may be killed using an Epilog script configured\nin slurm.conf.\nConfiguration\nThe Slurm configuration file includes a wide variety of parameters.\nThis configuration file must be available on each node of the cluster and\nmust have consistent contents. A full\ndescription of the parameters is included in the slurm.conf man page. Rather than\nduplicate that information, a minimal sample configuration file is shown below.\nYour slurm.conf file should define at least the configuration parameters defined\nin this sample and likely additional ones. Any text\nfollowing a \"#\" is considered a comment. The keywords in the file are\nnot case sensitive, although the argument typically is (e.g., \"SlurmUser=slurm\"\nmight be specified as \"slurmuser=slurm\"). The control machine, like\nall other machine specifications, can include both the host name and the name\nused for communications. In this case, the host's name is \"mcri\" and\nthe name \"emcri\" is used for communications.\nIn this case \"emcri\" is the private management network interface\nfor the host \"mcri\". Port numbers to be used for\ncommunications are specified as well as various timer values.\nThe SlurmUser must be created as needed prior to starting Slurm\nand must exist on all nodes in your cluster.\nThe parent directories for Slurm's log files, process ID files,\nstate save directories, etc. are not created by Slurm.\nThey must be created and made writable by SlurmUser as needed prior to\nstarting Slurm daemons.\nThe StateSaveLocation is used to store information about the current\nstate of the cluster, including information about queued, running and recently\ncompleted jobs. The directory used should be on a low-latency local disk to\nprevent file system delays from affecting Slurm performance. If using a backup\nhost, the StateSaveLocation should reside on a file system shared by the two\nhosts. We do not recommend using NFS to make the directory accessible to both\nhosts, but do recommend a shared mount that is accessible to the two\ncontrollers and allows low-latency reads and writes to the disk. If a\ncontroller comes up without access to the state information, queued and\nrunning jobs will be cancelled.\nA description of the nodes and their grouping into partitions is required.\nA simple node range expression may optionally be used to specify\nranges of nodes to avoid building a configuration file with large\nnumbers of entries. The node range expression can contain one\npair of square brackets with a sequence of comma separated\nnumbers and/or ranges of numbers separated by a \"-\"\n(e.g. \"linux[0-64,128]\", or \"lx[15,18,32-33]\").\nUp to two numeric ranges can be included in the expression\n(e.g. \"rack[0-63]_blade[0-41]\").\nIf one or more numeric expressions are included, one of them\nmust be at the end of the name (e.g. \"unit[0-31]rack\" is invalid),\nbut arbitrary names can always be used in a comma separated list.\nNode names can have up to three name specifications:\nNodeName is the name used by all Slurm tools when referring to the node,\nNodeAddr is the name or IP address Slurm uses to communicate with the node, and\nNodeHostname is the name returned by the command /bin/hostname -s.\nOnly NodeName is required (the others default to the same name),\nalthough supporting all three parameters provides complete control over\nnaming and addressing the nodes.  See the slurm.conf man page for\ndetails on all configuration parameters.\nNodes can be in more than one partition and each partition can have different\nconstraints (permitted users, time limits, job size limits, etc.).\nEach partition can thus be considered a separate queue.\nPartition and node specifications use node range expressions to identify\nnodes in a concise fashion. This configuration file defines a 1154-node cluster\nfor Slurm, but it might be used for a much larger cluster by just changing a few\nnode range expressions. Specify the minimum processor count (CPUs), real memory\nspace (RealMemory, megabytes), and temporary disk space (TmpDisk, megabytes) that\na node should have to be considered available for use. Any node lacking these\nminimum configuration values will be considered DOWN and not scheduled.\nNote that a more extensive sample configuration file is provided in\netc/slurm.conf.example. We also have a web-based\nconfiguration tool which can\nbe used to build a simple configuration file, which can then be\nmanually edited for more complex configurations.\n\n#\n# Sample /etc/slurm.conf for mcr.llnl.gov\n#\nSlurmctldHost=mcri(12.34.56.78)\nSlurmctldHost=mcrj(12.34.56.79)\n#\nAuthType=auth/munge\nEpilog=/usr/local/slurm/etc/epilog\nJobCompLoc=/var/tmp/jette/slurm.job.log\nJobCompType=jobcomp/filetxt\nPluginDir=/usr/local/slurm/lib/slurm\nProlog=/usr/local/slurm/etc/prolog\nSchedulerType=sched/backfill\nSelectType=select/linear\nSlurmUser=slurm\nSlurmctldPort=7002\nSlurmctldTimeout=300\nSlurmdPort=7003\nSlurmdSpoolDir=/var/spool/slurmd.spool\nSlurmdTimeout=300\nStateSaveLocation=/var/spool/slurm.state\nTreeWidth=16\n#\n# Node Configurations\n#\nNodeName=DEFAULT CPUs=2 RealMemory=2000 TmpDisk=64000 State=UNKNOWN\nNodeName=mcr[0-1151] NodeAddr=emcr[0-1151]\n#\n# Partition Configurations\n#\nPartitionName=DEFAULT State=UP\nPartitionName=pdebug Nodes=mcr[0-191] MaxTime=30 MaxNodes=32 Default=YES\nPartitionName=pbatch Nodes=mcr[192-1151]\n\nSecurity\nBesides authentication of Slurm communications based upon the value\nof the AuthType, digital signatures are used in job step\ncredentials.\nThis signature is used by slurmctld to construct a job step\ncredential, which is sent to srun and then forwarded to\nslurmd to initiate job steps.\nThis design offers improved performance by removing much of the\njob step initiation overhead from the  slurmctld  daemon.\nThe digital signature mechanism is specified by the CredType\nconfiguration parameter and the default mechanism is MUNGE. \nPluggable Authentication Module (PAM) support\n\n\nA PAM module (Pluggable Authentication Module) is available for Slurm that\ncan prevent a user from accessing a node which he has not been allocated,\nif that mode of operation is desired.\nStarting the Daemons\n\n\nFor testing purposes you may want to start by just running slurmctld and slurmd\non one node. By default, they execute in the background. Use the -D\noption for each daemon to execute them in the foreground and logging will be done\nto your terminal. The -v option will log events\nin more detail with more v's increasing the level of detail (e.g. -vvvvvv).\nYou can use one window to execute \"slurmctld -D -vvvvvv\",\na second window to execute \"slurmd -D -vvvvv\".\nYou may see errors such as \"Connection refused\" or \"Node X not responding\"\nwhile one daemon is operative and the other is being started, but the\ndaemons can be started in any order and proper communications will be\nestablished once both daemons complete initialization.\nYou can use a third window to execute commands such as\n\"srun -N1 /bin/hostname\" to confirm functionality.\nAnother important option for the daemons is \"-c\"\nto clear previous state information. Without the \"-c\"\noption, the daemons will restore any previously saved state information: node\nstate, job state, etc. With the \"-c\" option all\npreviously running jobs will be purged and node state will be restored to the\nvalues specified in the configuration file. This means that a node configured\ndown manually using the scontrol command will\nbe returned to service unless noted as being down in the configuration file.\nIn practice, Slurm consistently restarts with preservation.\nAdministration Examples\n\n\nscontrol can be used to print all system information\nand modify most of it. Only a few examples are shown below. Please see the scontrol\nman page for full details. The commands and options are all case insensitive.\nPrint detailed state of all jobs in the system.\n\nadev0: scontrol\nscontrol: show job\nJobId=475 UserId=bob(6885) Name=sleep JobState=COMPLETED\n   Priority=4294901286 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:53:41 EndTime=03/19-12:53:59\n   NodeList=adev8 NodeListIndecies=-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n\nJobId=476 UserId=bob(6885) Name=sleep JobState=RUNNING\n   Priority=4294901285 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:54:01 EndTime=NONE\n   NodeList=adev8 NodeListIndecies=8,8,-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n Print the detailed state of job 477 and change its priority to\nzero. A priority of zero prevents a job from being initiated (it is held in \"pending\"\nstate).\n\nadev0: scontrol\nscontrol: show job 477\nJobId=477 UserId=bob(6885) Name=sleep JobState=PENDING\n   Priority=4294901286 Partition=batch BatchFlag=0\n   more data removed....\nscontrol: update JobId=477 Priority=0\n\nPrint the state of node adev13 and drain it. To drain a node, specify a new\nstate of DRAIN, DRAINED, or DRAINING. Slurm will automatically set it to the appropriate\nvalue of either DRAINING or DRAINED depending on whether the node is allocated\nor not. Return it to service later.\n\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=ALLOCATED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=DRAIN\nscontrol: show node adev13\nNodeName=adev13 State=DRAINING CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: quit\nLater\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=DRAINED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=IDLE\n Reconfigure all Slurm daemons on all nodes. This should\nbe done after changing the Slurm configuration file.\n\nadev0: scontrol reconfig\n Print the current Slurm configuration. This also reports if the\nprimary and secondary controllers (slurmctld daemons) are responding. To just\nsee the state of the controllers, use the command ping.\n\nadev0: scontrol show config\nConfiguration data as of 2019-03-29T12:20:45\n...\nSlurmctldAddr           = eadevi\nSlurmctldDebug          = info\nSlurmctldHost[0]        = adevi\nSlurmctldHost[1]        = adevj\nSlurmctldLogFile        = /var/log/slurmctld.log\n...\n\nSlurmctld(primary) at adevi is UP\nSlurmctld(backup) at adevj is UP\n Shutdown all Slurm daemons on all nodes.\n\nadev0: scontrol shutdown\n\nUpgrades\nSlurm supports in-place upgrades between certain versions. Important details\nabout the steps necessary to perform an upgrade and the potential complications\nto prepare for are contained on this page:\nUpgrade Guide\nFreeBSD\nFreeBSD administrators can install the latest stable Slurm as a binary\npackage using:\n\npkg install slurm-wlm\n\nOr, it can be built and installed from source using:\n\ncd /usr/ports/sysutils/slurm-wlm && make install\n\nThe binary package installs a minimal Slurm configuration suitable for\ntypical compute nodes.  Installing from source allows the user to enable\noptions such as mysql and gui tools via a configuration menu.\nLast modified 16 August 2024\n"
        },
        {
            "title": "Scheduler support\n\n",
            "content": "Slurm can be configured with rather simple or quite sophisticated\nscheduling algorithms depending upon your needs and willingness to\nmanage the configuration (much of which requires a database).\nThe first configuration parameter of interest is PriorityType\nwith two options available: basic (first-in-first-out) and\nmultifactor.\nThe multifactor plugin will assign a priority to jobs based upon\na multitude of configuration parameters (age, size, fair-share allocation,\netc.) and its details are beyond the scope of this document.\nSee the Multifactor Job Priority Plugin\ndocument for details.The SchedType configuration parameter controls how queued\njobs are scheduled and several options are available.\n\nbuiltin will initiate jobs strictly in their priority order,\ntypically (first-in-first-out) \nbackfill will initiate a lower-priority job if doing so does\nnot delay the expected initiation time of higher priority jobs; essentially\nusing smaller jobs to fill holes in the resource allocation plan. Effective\nbackfill scheduling does require users to specify job time limits.\ngang time-slices jobs in the same partition/queue and can be\nused to preempt jobs from lower-priority queues in order to execute\njobs in higher priority queues.\n\nFor more information about scheduling options see\nGang Scheduling,\nPreemption,\nResource Reservation Guide,\nResource Limits and\nSharing Consumable Resources.\nResource selection\n\n\nThe resource selection mechanism used by Slurm is controlled by the\nSelectType configuration parameter.\nIf you want to execute multiple jobs per node, but track and manage allocation\nof the processors, memory and other resources, the cons_tres (consumable\ntrackable resources) plugin is recommended.\nFor more information, please see\nConsumable Resources in Slurm.\nLogging\nSlurm uses syslog to record events if the SlurmctldLogFile and\nSlurmdLogFile locations are not set.\nAccounting\nSlurm supports accounting records being written to a simple text file,\ndirectly to a database (MySQL or MariaDB), or to a daemon securely\nmanaging accounting data for multiple clusters. For more information\nsee Accounting. \nCompute node access\n\n\nSlurm does not by itself limit access to allocated compute nodes,\nbut it does provide mechanisms to accomplish this.\nThere is a Pluggable Authentication Module (PAM) for restricting access\nto compute nodes available for download.\nWhen installed, the Slurm PAM module will prevent users from logging\ninto any node that has not be assigned to that user.\nOn job termination, any processes initiated by the user outside of\nSlurm's control may be killed using an Epilog script configured\nin slurm.conf.\nConfiguration\nThe Slurm configuration file includes a wide variety of parameters.\nThis configuration file must be available on each node of the cluster and\nmust have consistent contents. A full\ndescription of the parameters is included in the slurm.conf man page. Rather than\nduplicate that information, a minimal sample configuration file is shown below.\nYour slurm.conf file should define at least the configuration parameters defined\nin this sample and likely additional ones. Any text\nfollowing a \"#\" is considered a comment. The keywords in the file are\nnot case sensitive, although the argument typically is (e.g., \"SlurmUser=slurm\"\nmight be specified as \"slurmuser=slurm\"). The control machine, like\nall other machine specifications, can include both the host name and the name\nused for communications. In this case, the host's name is \"mcri\" and\nthe name \"emcri\" is used for communications.\nIn this case \"emcri\" is the private management network interface\nfor the host \"mcri\". Port numbers to be used for\ncommunications are specified as well as various timer values.\nThe SlurmUser must be created as needed prior to starting Slurm\nand must exist on all nodes in your cluster.\nThe parent directories for Slurm's log files, process ID files,\nstate save directories, etc. are not created by Slurm.\nThey must be created and made writable by SlurmUser as needed prior to\nstarting Slurm daemons.\nThe StateSaveLocation is used to store information about the current\nstate of the cluster, including information about queued, running and recently\ncompleted jobs. The directory used should be on a low-latency local disk to\nprevent file system delays from affecting Slurm performance. If using a backup\nhost, the StateSaveLocation should reside on a file system shared by the two\nhosts. We do not recommend using NFS to make the directory accessible to both\nhosts, but do recommend a shared mount that is accessible to the two\ncontrollers and allows low-latency reads and writes to the disk. If a\ncontroller comes up without access to the state information, queued and\nrunning jobs will be cancelled.\nA description of the nodes and their grouping into partitions is required.\nA simple node range expression may optionally be used to specify\nranges of nodes to avoid building a configuration file with large\nnumbers of entries. The node range expression can contain one\npair of square brackets with a sequence of comma separated\nnumbers and/or ranges of numbers separated by a \"-\"\n(e.g. \"linux[0-64,128]\", or \"lx[15,18,32-33]\").\nUp to two numeric ranges can be included in the expression\n(e.g. \"rack[0-63]_blade[0-41]\").\nIf one or more numeric expressions are included, one of them\nmust be at the end of the name (e.g. \"unit[0-31]rack\" is invalid),\nbut arbitrary names can always be used in a comma separated list.\nNode names can have up to three name specifications:\nNodeName is the name used by all Slurm tools when referring to the node,\nNodeAddr is the name or IP address Slurm uses to communicate with the node, and\nNodeHostname is the name returned by the command /bin/hostname -s.\nOnly NodeName is required (the others default to the same name),\nalthough supporting all three parameters provides complete control over\nnaming and addressing the nodes.  See the slurm.conf man page for\ndetails on all configuration parameters.\nNodes can be in more than one partition and each partition can have different\nconstraints (permitted users, time limits, job size limits, etc.).\nEach partition can thus be considered a separate queue.\nPartition and node specifications use node range expressions to identify\nnodes in a concise fashion. This configuration file defines a 1154-node cluster\nfor Slurm, but it might be used for a much larger cluster by just changing a few\nnode range expressions. Specify the minimum processor count (CPUs), real memory\nspace (RealMemory, megabytes), and temporary disk space (TmpDisk, megabytes) that\na node should have to be considered available for use. Any node lacking these\nminimum configuration values will be considered DOWN and not scheduled.\nNote that a more extensive sample configuration file is provided in\netc/slurm.conf.example. We also have a web-based\nconfiguration tool which can\nbe used to build a simple configuration file, which can then be\nmanually edited for more complex configurations.\n\n#\n# Sample /etc/slurm.conf for mcr.llnl.gov\n#\nSlurmctldHost=mcri(12.34.56.78)\nSlurmctldHost=mcrj(12.34.56.79)\n#\nAuthType=auth/munge\nEpilog=/usr/local/slurm/etc/epilog\nJobCompLoc=/var/tmp/jette/slurm.job.log\nJobCompType=jobcomp/filetxt\nPluginDir=/usr/local/slurm/lib/slurm\nProlog=/usr/local/slurm/etc/prolog\nSchedulerType=sched/backfill\nSelectType=select/linear\nSlurmUser=slurm\nSlurmctldPort=7002\nSlurmctldTimeout=300\nSlurmdPort=7003\nSlurmdSpoolDir=/var/spool/slurmd.spool\nSlurmdTimeout=300\nStateSaveLocation=/var/spool/slurm.state\nTreeWidth=16\n#\n# Node Configurations\n#\nNodeName=DEFAULT CPUs=2 RealMemory=2000 TmpDisk=64000 State=UNKNOWN\nNodeName=mcr[0-1151] NodeAddr=emcr[0-1151]\n#\n# Partition Configurations\n#\nPartitionName=DEFAULT State=UP\nPartitionName=pdebug Nodes=mcr[0-191] MaxTime=30 MaxNodes=32 Default=YES\nPartitionName=pbatch Nodes=mcr[192-1151]\n\nSecurity\nBesides authentication of Slurm communications based upon the value\nof the AuthType, digital signatures are used in job step\ncredentials.\nThis signature is used by slurmctld to construct a job step\ncredential, which is sent to srun and then forwarded to\nslurmd to initiate job steps.\nThis design offers improved performance by removing much of the\njob step initiation overhead from the  slurmctld  daemon.\nThe digital signature mechanism is specified by the CredType\nconfiguration parameter and the default mechanism is MUNGE. \nPluggable Authentication Module (PAM) support\n\n\nA PAM module (Pluggable Authentication Module) is available for Slurm that\ncan prevent a user from accessing a node which he has not been allocated,\nif that mode of operation is desired.\nStarting the Daemons\n\n\nFor testing purposes you may want to start by just running slurmctld and slurmd\non one node. By default, they execute in the background. Use the -D\noption for each daemon to execute them in the foreground and logging will be done\nto your terminal. The -v option will log events\nin more detail with more v's increasing the level of detail (e.g. -vvvvvv).\nYou can use one window to execute \"slurmctld -D -vvvvvv\",\na second window to execute \"slurmd -D -vvvvv\".\nYou may see errors such as \"Connection refused\" or \"Node X not responding\"\nwhile one daemon is operative and the other is being started, but the\ndaemons can be started in any order and proper communications will be\nestablished once both daemons complete initialization.\nYou can use a third window to execute commands such as\n\"srun -N1 /bin/hostname\" to confirm functionality.\nAnother important option for the daemons is \"-c\"\nto clear previous state information. Without the \"-c\"\noption, the daemons will restore any previously saved state information: node\nstate, job state, etc. With the \"-c\" option all\npreviously running jobs will be purged and node state will be restored to the\nvalues specified in the configuration file. This means that a node configured\ndown manually using the scontrol command will\nbe returned to service unless noted as being down in the configuration file.\nIn practice, Slurm consistently restarts with preservation.\nAdministration Examples\n\n\nscontrol can be used to print all system information\nand modify most of it. Only a few examples are shown below. Please see the scontrol\nman page for full details. The commands and options are all case insensitive.\nPrint detailed state of all jobs in the system.\n\nadev0: scontrol\nscontrol: show job\nJobId=475 UserId=bob(6885) Name=sleep JobState=COMPLETED\n   Priority=4294901286 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:53:41 EndTime=03/19-12:53:59\n   NodeList=adev8 NodeListIndecies=-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n\nJobId=476 UserId=bob(6885) Name=sleep JobState=RUNNING\n   Priority=4294901285 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:54:01 EndTime=NONE\n   NodeList=adev8 NodeListIndecies=8,8,-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n Print the detailed state of job 477 and change its priority to\nzero. A priority of zero prevents a job from being initiated (it is held in \"pending\"\nstate).\n\nadev0: scontrol\nscontrol: show job 477\nJobId=477 UserId=bob(6885) Name=sleep JobState=PENDING\n   Priority=4294901286 Partition=batch BatchFlag=0\n   more data removed....\nscontrol: update JobId=477 Priority=0\n\nPrint the state of node adev13 and drain it. To drain a node, specify a new\nstate of DRAIN, DRAINED, or DRAINING. Slurm will automatically set it to the appropriate\nvalue of either DRAINING or DRAINED depending on whether the node is allocated\nor not. Return it to service later.\n\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=ALLOCATED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=DRAIN\nscontrol: show node adev13\nNodeName=adev13 State=DRAINING CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: quit\nLater\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=DRAINED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=IDLE\n Reconfigure all Slurm daemons on all nodes. This should\nbe done after changing the Slurm configuration file.\n\nadev0: scontrol reconfig\n Print the current Slurm configuration. This also reports if the\nprimary and secondary controllers (slurmctld daemons) are responding. To just\nsee the state of the controllers, use the command ping.\n\nadev0: scontrol show config\nConfiguration data as of 2019-03-29T12:20:45\n...\nSlurmctldAddr           = eadevi\nSlurmctldDebug          = info\nSlurmctldHost[0]        = adevi\nSlurmctldHost[1]        = adevj\nSlurmctldLogFile        = /var/log/slurmctld.log\n...\n\nSlurmctld(primary) at adevi is UP\nSlurmctld(backup) at adevj is UP\n Shutdown all Slurm daemons on all nodes.\n\nadev0: scontrol shutdown\n\nUpgrades\nSlurm supports in-place upgrades between certain versions. Important details\nabout the steps necessary to perform an upgrade and the potential complications\nto prepare for are contained on this page:\nUpgrade Guide\nFreeBSD\nFreeBSD administrators can install the latest stable Slurm as a binary\npackage using:\n\npkg install slurm-wlm\n\nOr, it can be built and installed from source using:\n\ncd /usr/ports/sysutils/slurm-wlm && make install\n\nThe binary package installs a minimal Slurm configuration suitable for\ntypical compute nodes.  Installing from source allows the user to enable\noptions such as mysql and gui tools via a configuration menu.\nLast modified 16 August 2024\n"
        },
        {
            "title": "Resource selection\n\n",
            "content": "The resource selection mechanism used by Slurm is controlled by the\nSelectType configuration parameter.\nIf you want to execute multiple jobs per node, but track and manage allocation\nof the processors, memory and other resources, the cons_tres (consumable\ntrackable resources) plugin is recommended.\nFor more information, please see\nConsumable Resources in Slurm.LoggingSlurm uses syslog to record events if the SlurmctldLogFile and\nSlurmdLogFile locations are not set.AccountingSlurm supports accounting records being written to a simple text file,\ndirectly to a database (MySQL or MariaDB), or to a daemon securely\nmanaging accounting data for multiple clusters. For more information\nsee Accounting. Compute node access\n\nSlurm does not by itself limit access to allocated compute nodes,\nbut it does provide mechanisms to accomplish this.\nThere is a Pluggable Authentication Module (PAM) for restricting access\nto compute nodes available for download.\nWhen installed, the Slurm PAM module will prevent users from logging\ninto any node that has not be assigned to that user.\nOn job termination, any processes initiated by the user outside of\nSlurm's control may be killed using an Epilog script configured\nin slurm.conf.ConfigurationThe Slurm configuration file includes a wide variety of parameters.\nThis configuration file must be available on each node of the cluster and\nmust have consistent contents. A full\ndescription of the parameters is included in the slurm.conf man page. Rather than\nduplicate that information, a minimal sample configuration file is shown below.\nYour slurm.conf file should define at least the configuration parameters defined\nin this sample and likely additional ones. Any text\nfollowing a \"#\" is considered a comment. The keywords in the file are\nnot case sensitive, although the argument typically is (e.g., \"SlurmUser=slurm\"\nmight be specified as \"slurmuser=slurm\"). The control machine, like\nall other machine specifications, can include both the host name and the name\nused for communications. In this case, the host's name is \"mcri\" and\nthe name \"emcri\" is used for communications.\nIn this case \"emcri\" is the private management network interface\nfor the host \"mcri\". Port numbers to be used for\ncommunications are specified as well as various timer values.The SlurmUser must be created as needed prior to starting Slurm\nand must exist on all nodes in your cluster.\nThe parent directories for Slurm's log files, process ID files,\nstate save directories, etc. are not created by Slurm.\nThey must be created and made writable by SlurmUser as needed prior to\nstarting Slurm daemons.The StateSaveLocation is used to store information about the current\nstate of the cluster, including information about queued, running and recently\ncompleted jobs. The directory used should be on a low-latency local disk to\nprevent file system delays from affecting Slurm performance. If using a backup\nhost, the StateSaveLocation should reside on a file system shared by the two\nhosts. We do not recommend using NFS to make the directory accessible to both\nhosts, but do recommend a shared mount that is accessible to the two\ncontrollers and allows low-latency reads and writes to the disk. If a\ncontroller comes up without access to the state information, queued and\nrunning jobs will be cancelled.A description of the nodes and their grouping into partitions is required.\nA simple node range expression may optionally be used to specify\nranges of nodes to avoid building a configuration file with large\nnumbers of entries. The node range expression can contain one\npair of square brackets with a sequence of comma separated\nnumbers and/or ranges of numbers separated by a \"-\"\n(e.g. \"linux[0-64,128]\", or \"lx[15,18,32-33]\").\nUp to two numeric ranges can be included in the expression\n(e.g. \"rack[0-63]_blade[0-41]\").\nIf one or more numeric expressions are included, one of them\nmust be at the end of the name (e.g. \"unit[0-31]rack\" is invalid),\nbut arbitrary names can always be used in a comma separated list.Node names can have up to three name specifications:\nNodeName is the name used by all Slurm tools when referring to the node,\nNodeAddr is the name or IP address Slurm uses to communicate with the node, and\nNodeHostname is the name returned by the command /bin/hostname -s.\nOnly NodeName is required (the others default to the same name),\nalthough supporting all three parameters provides complete control over\nnaming and addressing the nodes.  See the slurm.conf man page for\ndetails on all configuration parameters.Nodes can be in more than one partition and each partition can have different\nconstraints (permitted users, time limits, job size limits, etc.).\nEach partition can thus be considered a separate queue.\nPartition and node specifications use node range expressions to identify\nnodes in a concise fashion. This configuration file defines a 1154-node cluster\nfor Slurm, but it might be used for a much larger cluster by just changing a few\nnode range expressions. Specify the minimum processor count (CPUs), real memory\nspace (RealMemory, megabytes), and temporary disk space (TmpDisk, megabytes) that\na node should have to be considered available for use. Any node lacking these\nminimum configuration values will be considered DOWN and not scheduled.\nNote that a more extensive sample configuration file is provided in\netc/slurm.conf.example. We also have a web-based\nconfiguration tool which can\nbe used to build a simple configuration file, which can then be\nmanually edited for more complex configurations.\n#\n# Sample /etc/slurm.conf for mcr.llnl.gov\n#\nSlurmctldHost=mcri(12.34.56.78)\nSlurmctldHost=mcrj(12.34.56.79)\n#\nAuthType=auth/munge\nEpilog=/usr/local/slurm/etc/epilog\nJobCompLoc=/var/tmp/jette/slurm.job.log\nJobCompType=jobcomp/filetxt\nPluginDir=/usr/local/slurm/lib/slurm\nProlog=/usr/local/slurm/etc/prolog\nSchedulerType=sched/backfill\nSelectType=select/linear\nSlurmUser=slurm\nSlurmctldPort=7002\nSlurmctldTimeout=300\nSlurmdPort=7003\nSlurmdSpoolDir=/var/spool/slurmd.spool\nSlurmdTimeout=300\nStateSaveLocation=/var/spool/slurm.state\nTreeWidth=16\n#\n# Node Configurations\n#\nNodeName=DEFAULT CPUs=2 RealMemory=2000 TmpDisk=64000 State=UNKNOWN\nNodeName=mcr[0-1151] NodeAddr=emcr[0-1151]\n#\n# Partition Configurations\n#\nPartitionName=DEFAULT State=UP\nPartitionName=pdebug Nodes=mcr[0-191] MaxTime=30 MaxNodes=32 Default=YES\nPartitionName=pbatch Nodes=mcr[192-1151]\nSecurityBesides authentication of Slurm communications based upon the value\nof the AuthType, digital signatures are used in job step\ncredentials.\nThis signature is used by slurmctld to construct a job step\ncredential, which is sent to srun and then forwarded to\nslurmd to initiate job steps.\nThis design offers improved performance by removing much of the\njob step initiation overhead from the  slurmctld  daemon.\nThe digital signature mechanism is specified by the CredType\nconfiguration parameter and the default mechanism is MUNGE. Pluggable Authentication Module (PAM) support\n\nA PAM module (Pluggable Authentication Module) is available for Slurm that\ncan prevent a user from accessing a node which he has not been allocated,\nif that mode of operation is desired.Starting the Daemons\n\nFor testing purposes you may want to start by just running slurmctld and slurmd\non one node. By default, they execute in the background. Use the -D\noption for each daemon to execute them in the foreground and logging will be done\nto your terminal. The -v option will log events\nin more detail with more v's increasing the level of detail (e.g. -vvvvvv).\nYou can use one window to execute \"slurmctld -D -vvvvvv\",\na second window to execute \"slurmd -D -vvvvv\".\nYou may see errors such as \"Connection refused\" or \"Node X not responding\"\nwhile one daemon is operative and the other is being started, but the\ndaemons can be started in any order and proper communications will be\nestablished once both daemons complete initialization.\nYou can use a third window to execute commands such as\n\"srun -N1 /bin/hostname\" to confirm functionality.Another important option for the daemons is \"-c\"\nto clear previous state information. Without the \"-c\"\noption, the daemons will restore any previously saved state information: node\nstate, job state, etc. With the \"-c\" option all\npreviously running jobs will be purged and node state will be restored to the\nvalues specified in the configuration file. This means that a node configured\ndown manually using the scontrol command will\nbe returned to service unless noted as being down in the configuration file.\nIn practice, Slurm consistently restarts with preservation.Administration Examples\n\nscontrol can be used to print all system information\nand modify most of it. Only a few examples are shown below. Please see the scontrol\nman page for full details. The commands and options are all case insensitive.Print detailed state of all jobs in the system.\nadev0: scontrol\nscontrol: show job\nJobId=475 UserId=bob(6885) Name=sleep JobState=COMPLETED\n   Priority=4294901286 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:53:41 EndTime=03/19-12:53:59\n   NodeList=adev8 NodeListIndecies=-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\n\nJobId=476 UserId=bob(6885) Name=sleep JobState=RUNNING\n   Priority=4294901285 Partition=batch BatchFlag=0\n   AllocNode:Sid=adevi:21432 TimeLimit=UNLIMITED\n   StartTime=03/19-12:54:01 EndTime=NONE\n   NodeList=adev8 NodeListIndecies=8,8,-1\n   NumCPUs=0 MinNodes=0 OverSubscribe=0 Contiguous=0\n   MinCPUs=0 MinMemory=0 Features=(null) MinTmpDisk=0\n   ReqNodeList=(null) ReqNodeListIndecies=-1\nPrint the detailed state of job 477 and change its priority to\nzero. A priority of zero prevents a job from being initiated (it is held in \"pending\"\nstate).\nadev0: scontrol\nscontrol: show job 477\nJobId=477 UserId=bob(6885) Name=sleep JobState=PENDING\n   Priority=4294901286 Partition=batch BatchFlag=0\n   more data removed....\nscontrol: update JobId=477 Priority=0\nPrint the state of node adev13 and drain it. To drain a node, specify a new\nstate of DRAIN, DRAINED, or DRAINING. Slurm will automatically set it to the appropriate\nvalue of either DRAINING or DRAINED depending on whether the node is allocated\nor not. Return it to service later.\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=ALLOCATED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=DRAIN\nscontrol: show node adev13\nNodeName=adev13 State=DRAINING CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: quit\nLater\nadev0: scontrol\nscontrol: show node adev13\nNodeName=adev13 State=DRAINED CPUs=2 RealMemory=3448 TmpDisk=32000\n   Weight=16 Partition=debug Features=(null)\nscontrol: update NodeName=adev13 State=IDLE\nReconfigure all Slurm daemons on all nodes. This should\nbe done after changing the Slurm configuration file.\nadev0: scontrol reconfig\nPrint the current Slurm configuration. This also reports if the\nprimary and secondary controllers (slurmctld daemons) are responding. To just\nsee the state of the controllers, use the command ping.\nadev0: scontrol show config\nConfiguration data as of 2019-03-29T12:20:45\n...\nSlurmctldAddr           = eadevi\nSlurmctldDebug          = info\nSlurmctldHost[0]        = adevi\nSlurmctldHost[1]        = adevj\nSlurmctldLogFile        = /var/log/slurmctld.log\n...\n\nSlurmctld(primary) at adevi is UP\nSlurmctld(backup) at adevj is UP\nShutdown all Slurm daemons on all nodes.\nadev0: scontrol shutdown\nUpgradesSlurm supports in-place upgrades between certain versions. Important details\nabout the steps necessary to perform an upgrade and the potential complications\nto prepare for are contained on this page:\nUpgrade GuideFreeBSDFreeBSD administrators can install the latest stable Slurm as a binary\npackage using:\npkg install slurm-wlm\nOr, it can be built and installed from source using:\ncd /usr/ports/sysutils/slurm-wlm && make install\nThe binary package installs a minimal Slurm configuration suitable for\ntypical compute nodes.  Installing from source allows the user to enable\noptions such as mysql and gui tools via a configuration menu.Last modified 16 August 2024"
        }
    ]
}