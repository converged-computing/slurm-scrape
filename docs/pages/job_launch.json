{
    "url": "https://slurm.schedmd.com/job_launch.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Job Launch Design Guide",
            "content": "OverviewThis guide describes at a high level the processes which occur in order\nto initiate a job including the daemons and plugins involved in the process.\nIt describes the process of job allocation, step allocation, task launch and\njob termination. The functionality of tens of thousands of lines of code\nhas been distilled here to a couple of pages of text, so much detail is\nmissing.Job Allocation\n\nThe first step of the process is to create a job allocation, which is\na claim on compute resources. A job allocation can be created using the\nsalloc, sbatch or srun command. The salloc and\nsbatch commands create resource allocations while the srun\ncommand will create a resource allocation (if not already running within one)\nplus launch tasks. Each of these commands will fill in a data structure\nidentifying the specifications of the job allocation requirement (e.g. node\ncount, task count, etc.) based upon command line options and environment\nvariables and send the RPC to the slurmctld daemon. The UID and GID of\nthe user launching the job will be included in a credential which will be used\nlater to restrict access to the job, so further steps run in the allocation\nwill need to be launched using the same UID and GID as the one used to create\nthe allocation. If the new job\nrequest is the highest priority, the slurmctld daemon will attempt\nto select resources for it immediately, otherwise it will validate that the job\nrequest can be satisfied at some time and queue the request. In either case\nthe request will receive a response almost immediately containing one of the\nfollowing:\nA job ID and the resource allocation specification (nodes, cpus, etc.)\nA job ID and notification of the job being in a queued state OR\nAn error code\nThe process of selecting resources for a job request involves multiple steps,\nsome of which involve plugins. The process is as follows:\nCall job_submit plugins to modify the request as appropriate\nValidate that the options are valid for this user (e.g. valid partition\nname, valid limits, etc.)\nDetermine if this job is the highest priority runnable job, if so then\nreally try to allocate resources for it now, otherwise only validate that it\ncould run if no other jobs existed\nDetermine which nodes could be used for the job. If the feature\nspecification uses an exclusive OR option, then multiple iterations of the\nselection process below will be required with disjoint sets of nodes\nCall the select plugin to select the best resources for the request\nThe select plugin will consider network topology and the topology within\na node (e.g. sockets, cores, and threads) to select the best resources for the\njob\nIf the job can not be initiated using available resources and preemption\nsupport is configured, the select plugin will also determine if the job\ncan be initiated after preempting lower priority jobs. If so then initiate\npreemption as needed to start the job\nStep Allocation\n\nThe srun command is always used for job step creation. It fills in\na job step request RPC using information from the command line and environment\nvariables then sends that request to the slurmctld daemon. It is\nimportant to note that many of the srun options are intended for job\nallocation and are not supported by the job step request RPC (for example the\nsocket, core and thread information is not supported). If a job step uses\nall of the resources allocated to the job then the lack of support for some\noptions is not important. If one wants to execute multiple job steps using\nvarious subsets of resources allocated to the job, this shortcoming could\nprove problematic. It is also worth noting that the logic used to select\nresources for a job step is relatively simple and entirely contained within\nthe slurmctld daemon code (the select plugin is not used for job\nsteps). If the request can not be immediately satisfied due to a request for\nexclusive access to resources, the appropriate error message will be sent and\nthe srun command will retry the request on a periodic basis.\n(NOTE: It would be desirable to queue the job step requests to support\njob step dependencies and better performance in the initiation of job steps,\nbut that is not currently supported.)\nIf the request can be satisfied, the response contains a digitally signed\ncredential (by the cred plugin) identifying the resources to be used.Task Launch\n\nThe srun command builds a task launch request data structure\nincluding the credential, executable name, file names, etc. and sends it to\nthe slurmd daemon on node zero of the job step allocation. The\nslurmd daemon validates the signature and forwards the request to the\nslurmd daemons on other nodes to launch tasks for that job step. The\ndegree of fanout in this message forwarding is configurable using the\nTreeWidth parameter. Each slurmd daemon tests that the job has\nnot been cancelled since the credential was issued (due to a possible race \ncondition) and spawns a slurmstepd program to manage the job step.\nNote that the slurmctld daemon is not directly involved in task\nlaunch in order to minimize the overhead on this critical resource.Each slurmstepd program executes a single job step.\nBesides the functions listed below, the slurmstepd program also\nexecutes several SPANK plugin functions at various times.\nPerforms MPI setup (using the appropriate plugin)\nCalls the switch plugin to perform any needed network configuration\nCreates a container for the job step using a proctrack plugin\nChange user ID to that of the user\nConfigures I/O for the tasks (either using files or a socket connection back\nto the srun command\nSets up environment variables for the tasks including many task-specific\nenvironment variables\nFork/exec the tasks\nJob Step Termination\n\nThere are several ways in which a job step or job can terminate, each with\nslight variation in the logic executed. The simplest case is if the tasks run\nto completion. The srun will note the termination of output from the\ntasks and notify the slurmctld daemon that the job step has completed.\nslurmctld will simply log the job step termination. The job step can\nalso be explicitly cancelled by a user, reach the end of its time limit, etc.\nand those follow a sequence of steps very similar to that for job termination,\nwhich is described below.Job Termination\n\nJob termination can either be user initiated (e.g. scancel command) or system\ninitiated (e.g. time limit reached). The termination ultimately requires\nthe slurmctld daemon to notify the slurmd daemons on allocated\nnodes that the job is to be ended. The slurmd daemon does the following:\n\nSend a SIGCONT and SIGTERM signal to any user tasks\nWait KillWait seconds if there are any user tasks\nSend a SIGKILL signal to any user tasks\nWait for all tasks to complete\nExecute any Epilog program\nSend an epilog_complete RPC to the slurmctld daemon\n\nJob Accounting Records\n\n\nWhen Slurm is configured to use SlurmDBD to store job records (i.e.\nAccountingStorageType=accounting_storage=slurmdbd), there are multiple\nrecords that get stored for each job. There is a record for the job as a\nwhole as well as entries for the following types of job steps:\n\nextern step \u2014 A step created for each job as long as you have\nPrologFlags=contain in your slurm.conf. Each node in the job will\nhave a slurmstepd process created for the extern step.\npam_slurm_adopt uses this step to contain\nexternal connections.\nbatch step \u2014 A step created for jobs that were submitted with\nsbatch. The batch host, or the primary node for the job, will run an instance\nof slurmstepd for the batch step, which is used to run the script provided\nto sbatch.\ninteractive step \u2014 A step created for jobs that were\nsubmitted with salloc when LaunchParameters=use_interactive_step is\nconfigured in your slurm.conf. The node on which you have the interactive\nshell will run an instance of slurmstepd to run the shell or the command\nprovided to salloc.\nnormal step \u2014 A job can have multiple normal steps, which will\nappear in sacct as <job_id>.<step_id>. These steps\nare created when srun is called from inside the job and the slurmstepd created\nwill run the command passed to srun. Each step will have one instance of\nslurmstepd created per node in the step and each instance of slurmstepd can\nrun multiple tasks in the same step.\n\nLast modified 1 August 2022\n"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        },
        {
            "title": "Job Accounting Records\n\n",
            "content": "When Slurm is configured to use SlurmDBD to store job records (i.e.\nAccountingStorageType=accounting_storage=slurmdbd), there are multiple\nrecords that get stored for each job. There is a record for the job as a\nwhole as well as entries for the following types of job steps:\n\nextern step \u2014 A step created for each job as long as you have\nPrologFlags=contain in your slurm.conf. Each node in the job will\nhave a slurmstepd process created for the extern step.\npam_slurm_adopt uses this step to contain\nexternal connections.\nbatch step \u2014 A step created for jobs that were submitted with\nsbatch. The batch host, or the primary node for the job, will run an instance\nof slurmstepd for the batch step, which is used to run the script provided\nto sbatch.\ninteractive step \u2014 A step created for jobs that were\nsubmitted with salloc when LaunchParameters=use_interactive_step is\nconfigured in your slurm.conf. The node on which you have the interactive\nshell will run an instance of slurmstepd to run the shell or the command\nprovided to salloc.\nnormal step \u2014 A job can have multiple normal steps, which will\nappear in sacct as <job_id>.<step_id>. These steps\nare created when srun is called from inside the job and the slurmstepd created\nwill run the command passed to srun. Each step will have one instance of\nslurmstepd created per node in the step and each instance of slurmstepd can\nrun multiple tasks in the same step.\n\nLast modified 1 August 2022\n"
        }
    ]
}