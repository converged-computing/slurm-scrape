{
    "url": "https://slurm.schedmd.com/intel_knl.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Intel Knights Landing (KNL) User and Administrator Guide",
            "content": "OverviewThis document describes the unique features of Slurm on the computers with\nthe Intel Knights Landing processor.\nYou should be familiar with the Slurm's mode of operation on Linux clusters\nbefore studying the relatively few differences in Intel KNL system operation\ndescribed in this document.User Tools\n\nThe desired NUMA and MCDRAM modes for a KNL processor should be specified\nusing the -C or --constraint option of Slurm's job submission commands: salloc,\nsbatch, and srun. Currently available NUMA and MCDRAM modes are shown in the\ntable below. Each node's available and current NUMA and MCDRAM modes are\nvisible in the \"available features\" and \"active features\" fields respectively,\nwhich may be seen using the scontrol, sinfo, or sview commands.\nNote that a node may need to be rebooted to get the desired NUMA and MCDRAM\nmodes and nodes may only be rebooted when they contain no running jobs\n(i.e. sufficient resources may be available to run a pending job, but until\nthe node is idle and can be rebooted, the pending job may not be allocated\nresources). Also note that the job will be charged for resources from the time\nof resource allocation, which may include time to reboot a node into the\ndesired NUMA and MCDRAM configuration.Slurm supports a very rich set of options for the node constraint options\n(exclusive OR, node counts for each constraint, etc.).\nSee the man pages for the salloc, sbatch and srun commands for more information\nabout the constraint syntax.\nJobs may specify their desired NUMA and/or MCDRAM configuration. If no\nNUMA and/or MCDRAM configuration is specified, then a node with any possible\nvalue for that configuration will be used.\n\nType\nName\nDescription\n\nMCDRAMcacheAll of MCDRAM to be used as cache\nMCDRAMequalMCDRAM to be used partly as cache and partly combined with primary memory\nMCDRAMflatMCDRAM to be combined with primary memory into a \"flat\" memory space\nNUMAa2aAll to all\nNUMAhemiHemisphere\nNUMAsnc2Sub-NUMA cluster 2\nNUMAsnc4Sub-NUMA cluster 4 (NOTE)\nNUMAquadQuadrant (NOTE)\nJobs requiring some or all of the KNL high bandwidth memory (HBM) should\nexplicitly request that memory using Slurm's Generic RESource (GRES) options.\nThe HBM will always be known by Slurm GRES name of \"hbm\".\nExamples below demonstrate use of HBM.Sorting of the free cache pages at step startup using Intel's zonesort\nmodule can be configured as the default for all steps using the\n\"LaunchParameters=mem_sort\" option in the slurm.conf file.\nIndividual job steps can enable or disable sorting using the \"--mem-bind=sort\"\nor \"--mem-bind=nosort\" command line options for srun.\nSorting will be performed only for the NUMA nodes allocated to the job step.NOTE: Slurm provides limited support\nfor restricting use of HBM. At some point in the future, the amount of HBM\nrequested by the job will be compared with the total amount of HBM and number of\nmemory-containing NUMA nodes available on the KNL processor. The job will then\nbe bound to specific NUMA nodes in order to limit the total amount of HBM\navailable to the job, and thus reserve the remaining HBM for other jobs running\non that KNL processor.NOTE: Slurm can only\nsupport homogeneous nodes (e.g. the same number of cores per NUMA node).\nKNL processors with 68 cores (a subset of KNL models) will not have\nhomogeneous NUMA nodes in snc4 mode, but each NUMA node will have\neither 16 or 18 cores. This will result in Slurm using the lower core count,\nfinding a total of 256 threads rather than 272 threads and setting the node\nto a DOWN state.AccountingIf a node requires rebooting for a job's required configuration, the job\nwill be charged for the resource allocation from the time of allocation through\nthe lifetime of the job, including the time consumed for booting the nodes.\nThe job's time limit will be calculated from the time that all nodes are ready\nfor use.\nFor example, a job with a 10 minute time limit may be allocated resources at\n10:00:00.\nIf the nodes require rebooting, they might not be available for use until\n10:20:00, 20 minutes after allocation, and the job will begin execution at\nthat time.\nThe job must complete no later than 10:30:00 in order to satisfy its time limit\n(10 minutes after execution actually begins).\nHowever, the job will be charged for 30 minutes of resource use, which includes\nthe boot time.Sample Use Cases\n\n\n$ sbatch -C flat,a2a -N2 --gres=hbm:8g --exclusive my.script\n$ srun --constraint=hemi,cache -n36 a.out\n$ srun --constraint=flat --gres=hbm:2g -n36 a.out\n\n$ sinfo -o \"%30N %20b %f\"\nNODELIST       ACTIVE_FEATURES  AVAIL_FEATURES\nnid000[10-11]\nnid000[12-35]  flat,a2a         flat,a2a,snc2,hemi\nnid000[36-43]  cache,a2a        flat,equal,cache,a2a,hemi\nNetwork Topology\n\nSlurm will optimize performance using those resources available without\nrebooting. If node rebooting is required, then it will optimize layout with\nrespect to network bandwidth using both nodes currently in the desired\nconfiguration and those which can be made available after rebooting.\nThis can result in more nodes being rebooted than strictly needed, but will\nimprove application performance.Users can specify they want all resources allocated on a specific count of\nleaf switches (Dragonfly group) using Slurm's --switches option.\nThey can also specify how much additional time they are willing to wait for\nsuch a configuration. If the desired configuration can not be made available\nwithin the specified time interval, the job will be allocated nodes optimized\nwith respect to network bandwidth to the extent possible. On a Dragonfly\nnetwork, this means allocating resources over either single group or\ndistributed evenly over as many groups as possible. For example:\nsrun --switches=1@10:00 N16 a.out\nNote that system administrators can disable use of the --switches\noption or limit the amount of time the job can be deferred using the\nSchedulerParameters max-switch-wait option.Booting Problems\n\nIf node boots fail, those nodes are drained and the job is requeued so that\nit can be allocated a different set of nodes. The nodes originally allocated\nto the job will remain available to the job, so likely a small number of\nadditional nodes will be required.System Administration\n\nFour important components are required to use Slurm on an Intel KNL system.\nSlurm needs a mechanism to determine the node's current topology (e.g.\nhow many NUMA exist and which cores are associated with each NUMA). Slurm\nrelies upon \nPortable Hardware Locality (HWLOC) for this functionality. Please install\nHWLOC before building Slurm.\nThe node features plugin manages the available and active features\ninformation available for each KNL node.\nA configuration file is used to define various timeouts, default\nconfiguration, etc. The configuration file name and contents will depend upon\nthe node features plugins used. See the knl.conf\nman page for more information.\nA mechanism is required to boot nodes in the desired configuration. This\nmechanism must be integrated with existing Slurm infrastructure for\nrebooting nodes on user request (--reboot).\nIn addition, there is a DebugFlags option of \"NodeFeatures\" which will\ngenerate detailed information about KNL operations.The KNL-specific available and active features are configured differently\nbased upon the plugin configured.\nFor the knl_generic plugin, KNL-specific features should be defined\nin the \"slurm.conf\" configuration file. When the slurmd daemon starts on each\ncompute node, it will update the available and active features as needed.\nFeatures which are not KNL-specific (e.g. rack number, \"knl\", etc.) will be\ncopied from the node's \"Features\" configuration in \"slurm.conf\" to both the\navailable and active feature fields and not modified by the NodeFeatures\nplugin.NOTE: For Dell KNL systems you must also include the\nSystemType=Dell option for successful operation and will likely need to\nincrease the SyscfgTimeout to allow enough time for the command to\nsuccessfully complete.  Experience at one site has shown that a 10 second\ntimeout may be necessary, configured as SyscfgTimeout=10000.Slurm does not support the concept of multiple NUMA nodes\nwithin a single socket. If a KNL node is booted with multiple NUMA, then each\nNUMA node will appear in Slurm as a separate socket.\nIn the slurm.conf configuration file, set node socket and\ncore counts to values which are appropriate for some NUMA mode to be used on the\nnode. When the node boots and the slurmd daemon on the node starts, it will\nreport to the slurmctld daemon the node's actual socket (NUMA) and core counts,\nwhich will update Slurm data structures for the node to the values which are\ncurrently configured.\nNote that Slurm currently does not support the concept of\ndiffering numbers of cores in each socket (or NUMA node). We are currently\nworking to address these issues.Mode of Operation\n\n\nThe node's configured \"Features\" are copied to the available and active\nfeature fields.\nThe node features plugin determines the node's current MCDRAM and NUMA\nvalues as well as those which are available and adds those values to the node's\nactive and available feature fields respectively. Note that these values may\nnot be available until the node has booted and the slurmd daemon on the\ncompute node sends that information to the slurmctld daemon.\nJobs will be allocated nodes already in the requested MCDRAM and NUMA mode\nif possible. If insufficient resources are available with the requested\nconfiguration then other nodes will be selected and booted into the desired\nconfiguration once no other jobs are active on the node. Until a node is idle,\nits configuration can not be changed. Note that node reboot time is roughly\non the order of 20 minutes.\nGeneric Cluster Configuration\n\nAll other clusters should have NodeFeaturesPlugins configured to \"knl_generic\".\nThis plugin performs all operations directly on the compute nodes using Intel's\nsyscfg program to get and modify the node's MCDRAM and NUMA mode and\nuses the Linux reboot program to reboot the compute node in order for\nmodifications in MCDRAM and/or NUMA mode to take effect.\nMake sure that RebootProgram is defined in the slurm.conf file.\nThis plugin currently does not permit the specification of ResumeProgram,\nSuspendProgram, SuspendTime, etc. in slurm.conf, however that limitation may\nbe removed in the future (the ResumeProgram currently has no means of changing\nthe node's MCDRAM and/or NUMA mode prior to reboot).NOTE: The syscfg program reports the MCDRAM and NUMA mode to be used\nwhen the node is next booted. If the syscfg program is used to modify the MCDRAM\nor NUMA mode of a node, but it is not rebooted, then Slurm will be making\nscheduling decisions based upon incorrect state information. If you want to\nchange node state information outside of Slurm then use the following procedure:\n\nDrain the nodes of interest\nChange their MCDRAM and/or NUMA mode\nReboot the nodes, then\nRestore them to service in Slurm\n\nSample knl_generic.conf File\n# Sample knl_generic.conf\nSyscfgPath=/usr/bin/syscfg\nDefaultNUMA=a2a         # NUMA=all2all\nAllowNUMA=a2a,snc2,hemi\nDefaultMCDRAM=cache     # MCDRAM=cache\nSample slurm.conf File\n# Sample slurm.conf\nNodeFeaturesPlugins=knl_generic\nDebugFlags=NodeFeatures\nGresTypes=hbm\nRebootProgram=/sbin/reboot\n...\nNodename=default Sockets=1 CoresPerSocket=68 ThreadsPerCore=4 RealMemory=128000 Feature=knl\nNodeName=nid[00000-00127] State=UNKNOWN\nLast modified 13 March 2024"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        }
    ]
}