{
    "url": "https://slurm.schedmd.com/prolog_epilog.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Prolog and Epilog Guide",
            "content": "Slurm supports a multitude of prolog and epilog programs.\nNote that for security reasons, these programs do not have a search path set.\nEither specify fully qualified path names in the program or set the\nPATH\nenvironment variable.\nThe first table below identifies what prologs and epilogs are available for job\nallocations, when and where they run.\n\n\n\n\n\n\n\n\nParameter\n\t\t\t\t\n\n\nLocation\n\t\t\t\t\n\n\nInvoked by\n\t\t\t\t\n\n\nUser\n\t\t\t\t\n\n\nWhen executed\n\t\t\t\t\n\n\n\n\n\n\t\t\t\tProlog (from slurm.conf)\n\n\n\n\t\t\t\tCompute or front end node\n\n\n\n\t\t\t\tslurmd daemon\n\n\n\n\t\t\t\tSlurmdUser (normally user root)\n\n\n\n\t\t\t\tFirst job or job step initiation on that node (by default);\n\t\t\t\tPrologFlags=Alloc will force the script to be executed at\n\t\t\t\tjob allocation\n\n\n\n\n\n\t\t\t\tPrologSlurmctld (from slurm.conf)\n\n\n\n\t\t\t\tHead node (where slurmctld daemon runs)\n\n\n\n\t\t\t\tslurmctld daemon\n\n\n\n\t\t\t\tSlurmctldUser\n\n\n\n\t\t\t\tAt job allocation\n\n\n\n\n\n\t\t\t\tEpilog (from slurm.conf)\n\n\n\n\t\t\t\tCompute or front end node\n\n\n\n\t\t\t\tslurmd daemon\n\n\n\n\t\t\t\tSlurmdUser (normally user root)\n\n\n\n\t\t\t\tAt job termination\n\n\n\n\n\n\t\t\t\tEpilogSlurmctld (from slurm.conf)\n\n\n\n\t\t\t\tHead node (where slurmctld daemon runs)\n\n\n\n\t\t\t\tslurmctld daemon\n\n\n\n\t\t\t\tSlurmctldUser\n\n\n\n\t\t\t\tAt job termination\n\n\n\nThis second table below identifies what prologs and epilogs are available for job\nstep allocations, when and where they run.\n\n\n\n\n\n\n\n\nParameter\n\t\t\t\t\n\n\nLocation\n\t\t\t\t\n\n\nInvoked by\n\t\t\t\t\n\n\nUser\n\t\t\t\t\n\n\nWhen executed\n\t\t\t\t\n\n\n\n\n\n\t\t\t\tSrunProlog (from slurm.conf) or srun --prolog\n\n\n\n\t\t\t\tsrun invocation node\n\n\n\n\t\t\t\tsrun command\n\n\n\n\t\t\t\tUser invoking srun command\n\n\n\n\t\t\t\tPrior to launching job step\n\n\n\n\n\n\t\t\t\tTaskProlog (from slurm.conf)\n\n\n\n\t\t\t\tCompute node\n\n\n\n\t\t\t\tslurmstepd daemon\n\n\n\n\t\t\t\tUser invoking srun command\n\n\n\n\t\t\t\tPrior to launching job step\n\n\n\n\n\n\t\t\t\tsrun --task-prolog\n\n\n\n\t\t\t\tCompute node\n\n\n\n\t\t\t\tslurmstepd daemon\n\n\n\n\t\t\t\tUser invoking srun command\n\n\n\n\t\t\t\tPrior to launching job step\n\n\n\n\n\n\t\t\t\tTaskEpilog (from slurm.conf)\n\n\n\n\t\t\t\tCompute node\n\n\n\n\t\t\t\tslurmstepd daemon\n\n\n\n\t\t\t\tUser invoking srun command\n\n\n\n\t\t\t\tCompletion job step\n\n\n\n\n\n\t\t\t\tsrun --task-epilog\n\n\n\n\t\t\t\tCompute node\n\n\n\n\t\t\t\tslurmstepd daemon\n\n\n\n\t\t\t\tUser invoking srun command\n\n\n\n\t\t\t\tCompletion job step\n\n\n\n\n\n\t\t\t\tSrunEpilog (from slurm.conf) or srun --epilog\n\n\n\n\t\t\t\tsrun invocation node\n\n\n\n\t\t\t\tsrun command\n\n\n\n\t\t\t\tUser invoking srun command\n\n\n\n\t\t\t\tCompletion job step\n\n\n\nBy default the Prolog script is only run on any individual\nnode when it first sees a job step from a new allocation; it does not\nrun the Prolog immediately when an allocation is granted.  If no job steps\nfrom an allocation are run on a node, it will never run the Prolog for that\nallocation.  This Prolog behavior can be changed by the\nPrologFlags parameter.  The Epilog, on the other hand, always\nruns on every node of an allocation when the allocation is released.If multiple prolog and/or epilog scripts are specified,\n(e.g. \"/etc/slurm/prolog.d/*\") they will run in reverse alphabetical order\n(z-a -> Z-A -> 9-0).Prolog and Epilog scripts should be designed to be as short as possible\nand should not call Slurm commands (e.g. squeue, scontrol, sacctmgr, etc).\nLong running scripts can cause scheduling problems when jobs take a long time\nto start or finish. Slurm commands in these scripts can potentially lead to\nperformance issues and should not be used.The task prolog is executed with the same environment as the user tasks to\nbe initiated. The standard output of that program is read and processed as\nfollows:\nexport name=value\nsets an environment variable for the user task\nunset name\nclears an environment variable from the user task\nprint ...\nwrites to the task's standard output.A TaskProlog script can just be a bash script. Here is a very basic example:\n\n#!/bin/bash\n\n# The TaskProlog script can be used for any preliminary work needed\n# before running a job step, and it can also be used to modify the\n# user's environment. There are two main mechanisms for that, which\n# rely on printing commands to stdout:\n\n# Make a variable available for the user\necho \"export VARIABLE_1=HelloWorld\"\n\n# Unset variables for the user\necho \"unset MANPATH\"\n\n# We can also print messages if needed\necho \"print This message has been printed with TaskProlog\"\nSpecial treatment is given to the SLURM_PROLOG_CPU_MASK variable when\nset in the task prolog. The variable is interpreted as a coma separated list\nof hex maps. It allows you to specify the CPU(s) that will be bound to a\ntask and is applied using sched_setaffinity.\nThe above functionality is limited to the task prolog script.Unless otherwise specified, these environment variables are available\nto all of the programs.\nCUDA_MPS_ACTIVE_THREAD_PERCENTAGE\nSpecifies the percentage of a GPU that should be allocated to the job.\nThe value is set only if the gres/mps plugin is configured and the job\nrequests those resources.\nAvailable in Prolog and Epilog only.\nCUDA_VISIBLE_DEVICES\nSpecifies the GPU devices for the job allocation.\nThe value is set only if the gres/gpu or gres/mps plugin is configured and the\njob requests those resources.\nNote that the environment variable set for the job may differ from that set for\nthe Prolog and Epilog if Slurm is configured to constrain the device files\nvisible to a job using Linux cgroup.\nThis is because the Prolog and Epilog programs run outside of any Linux\ncgroup while the job runs inside of the cgroup and may thus have a\ndifferent set of visible devices.\nFor example, if a job is allocated the device \"/dev/nvidia1\", then\nCUDA_VISIBLE_DEVICES will be set to a value of\n\"1\" in the Prolog and Epilog while the job's value of\nCUDA_VISIBLE_DEVICES will be set to a\nvalue of \"0\" (i.e. the first GPU device visible to the job).\nCUDA_VISIBLE_DEVICES will be set unless\notherwise excluded via the Flags or AutoDetect options in\ngres.conf. See also SLURM_JOB_GPUS.\nAvailable in Prolog and Epilog only.\nGPU_DEVICE_ORDINAL\nSpecifies the GPU devices for the job allocation. The considerations for\nCUDA_VISIBLE_DEVICES also apply to\nGPU_DEVICE_ORDINAL.\n\nROCR_VISIBLE_DEVICES\nSpecifies the GPU devices for the job allocation. The considerations for\nCUDA_VISIBLE_DEVICES also apply to\nROCR_VISIBLE_DEVICES.\n\nSLURM_ARRAY_JOB_ID\nIf this job is part of a job array, this will be set to the job ID.\nOtherwise it will not be set.\nTo reference this specific task of a job array, combine\nSLURM_ARRAY_JOB_ID with\nSLURM_ARRAY_TASK_ID\n(e.g. scontrol update\n${SLURM_ARRAY_JOB_ID}_{$SLURM_ARRAY_TASK_ID} ...);\nAvailable in PrologSlurmctld, SrunProlog, TaskProlog, EpilogSlurmctld,\nSrunEpilog and TaskEpilog.\nSLURM_ARRAY_TASK_COUNT\nIf this job is part of a job array, this will be set to the number of\ntasks in the array. Otherwise it will not be set.\nAvailable in PrologSlurmctld, SrunProlog, TaskProlog, EpilogSlurmctld,\nSrunEpilog and TaskEpilog.\nSLURM_ARRAY_TASK_ID\nIf this job is part of a job array, this will be set to the task ID.\nOtherwise it will not be set.\nTo reference this specific task of a job array, combine\nSLURM_ARRAY_JOB_ID with\nSLURM_ARRAY_TASK_ID\n(e.g. scontrol update\n${SLURM_ARRAY_JOB_ID}_{$SLURM_ARRAY_TASK_ID} ...);\nAvailable in PrologSlurmctld, SrunProlog, TaskProlog, EpilogSlurmctld,\nSrunEpilog and TaskEpilog.\nSLURM_ARRAY_TASK_MAX\nIf this job is part of a job array, this will be set to the maximum\ntask ID.\nOtherwise it will not be set.\nAvailable in PrologSlurmctld, SrunProlog, TaskProlog, EpilogSlurmctld,\nSrunEpilog and TaskEpilog.\nSLURM_ARRAY_TASK_MIN\nIf this job is part of a job array, this will be set to the minimum\ntask ID.\nOtherwise it will not be set.\nAvailable in PrologSlurmctld, SrunProlog, TaskProlog, EpilogSlurmctld,\nSrunEpilog and TaskEpilog.\nSLURM_ARRAY_TASK_STEP\nIf this job is part of a job array, this will be set to the step\nsize of task IDs.\nOtherwise it will not be set.\nAvailable in PrologSlurmctld, SrunProlog, TaskProlog, EpilogSlurmctld,\nSrunEpilog and TaskEpilog.\nSLURM_CLUSTER_NAME\nName of the cluster executing the job. Available in Prolog, PrologSlurmctld,\nEpilog and EpilogSlurmctld.\nSLURM_CONF\nLocation of the slurm.conf file. Available in Prolog, SrunProlog, TaskProlog,\nEpilog, SrunEpilog and TaskEpilog.\nSLURM_CPUS_ON_NODE\nCount of processors available to the job on current node. Available in\nSrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_DISTRIBUTION\nDistribution type for the job. Available in SrunProlog, TaskProlog, SrunEpilog\nand TaskEpilog.\nSLURMD_NODENAME\nName of the node running the task. In the case of a parallel job executing\non multiple compute nodes, the various tasks will have this environment\nvariable set to different values on each compute node. Available in Prolog,\nSrunProlog, TaskProlog, Epilog, SrunEpilog and TaskEpilog.\nSLURM_GPUS\nCount of the GPUs available to the job. Available in SrunProlog, TaskProlog,\nSrunEpilog and TaskEpilog.\nSLURM_GTID\nGlobal Task IDs running on this node. Zero origin and comma separated.\nAvailable in SrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_JOB_ACCOUNT\nAccount name used for the job.\nSLURM_JOB_COMMENT\nComment added to the job.\nAvailable in Prolog, PrologSlurmctld, Epilog and EpilogSlurmctld.\nSLURM_JOB_CONSTRAINTS\nFeatures required to run the job.\nAvailable in Prolog, PrologSlurmctld, Epilog and EpilogSlurmctld.\nSLURM_JOB_CPUS_PER_NODE\nCount of processors available per node.\nSLURM_JOB_DERIVED_EC\nThe highest exit code of all of the job steps.\nAvailable in Epilog and EpilogSlurmctld.\nSLURM_JOB_END_TIME\nThe UNIX timestamp for a job's end time.\nSLURM_JOB_EXIT_CODE\nThe exit code of the job script (or salloc). The value is the status\nas returned by the wait() system call\n(See wait(2)).\nAvailable in Epilog and EpilogSlurmctld.\nSLURM_JOB_EXIT_CODE2\nThe exit code of the job script (or salloc). The value has the format\n<exit>:<sig>.\nThe first number is the exit code, typically as set by the\nexit() function.\nThe second number is the signal that caused the process to\nterminate if it was terminated by a signal.\nAvailable in Epilog and EpilogSlurmctld.\nSLURM_JOB_EXTRA\nExtra field added to the job.\nAvailable in Prolog, PrologSlurmctld, Epilog, EpilogSlurmctld, and\nResumeProgram (via SLURM_RESUME_FILE).\nSLURM_JOB_GID\nGroup ID of the job's owner.\nSLURM_JOB_GPUS\nThe GPU IDs of GPUs in the job allocation (if any).\nAvailable in the Prolog, SrunProlog, TaskProlog, Epilog, SrunEpilog and\nTaskProlog.\nSLURM_JOB_GROUP\nGroup name of the job's owner.\nAvailable in PrologSlurmctld and EpilogSlurmctld.\nSLURM_JOB_ID\nJob ID.\nSLURM_JOBID\nJob ID.\nSLURM_JOB_LICENSES\nName and count of any license(s) requested.\nSLURM_JOB_NAME\nName of the job.\nAvailable in PrologSlurmctld, SrunProlog, TaskProlog, EpilogSlurmctld,\nSrunEpilog and TaskEpilog.\nSLURM_JOB_NODELIST\nNodes assigned to job. A Slurm hostlist expression.\nscontrol show hostnames\ncan be used to convert this to a\nlist of individual host names.\nSLURM_NTASKS\nNumber of tasks requested by the job.\nAvailable in SrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_JOB_NUM_NODES\nNumber of nodes assigned to a job.\nSLURM_JOB_OVERSUBSCRIBE\nJob OverSubscribe status.\nSee the squeue man page for\npossible values.\nAvailable in Prolog, PrologSlurmctld, Epilog and EpilogSlurmctld.\nSLURM_JOB_PARTITION\nPartition that job runs in.\nSLURM_JOB_QOS\nQOS assigned to job. Available in SrunProlog, TaskProlog, SrunEpilog\nand TaskEpilog.\nSLURM_JOB_RESERVATION\nReservation requested for the job.\nSLURM_JOB_RESTART_COUNT\nNumber of times the job has been restarted.\nAvailable in Prolog, PrologSlurmctld, Epilog and EpilogSlurmctld.\nSLURM_JOB_START_TIME\nThe UNIX timestamp of a job's start time.\nSLURM_JOB_STDERR\nJob's stderr path.\nAvailable in Prolog, PrologSlurmctld, Epilog and EpilogSlurmctld.\nSLURM_JOB_STDIN\nJob's stdin path.\nAvailable in Prolog, PrologSlurmctld, Epilog and EpilogSlurmctld.\nSLURM_JOB_STDOUT\nJob's stdout path.\nAvailable in Prolog, PrologSlurmctld, Epilog and EpilogSlurmctld.\nSLURM_JOB_UID\nUser ID of the job's owner.\nSLURM_JOB_USER\nUser name of the job's owner.\nSLURM_JOB_WORK_DIR\nJob's working directory. Available in Prolog, PrologSlurmctld, Epilog,\nEpilogSlurmctld.\nSLURM_LOCAL_GLOBALS_FILE\nGlobals file used to set up the environment for the testsuite. Available\nin SrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_LOCALID\nNode local task ID for the process within a job. Available in SrunProlog,\nTaskProlog, SrunEpilog and TaskEpilog.\nSLURM_NNODES\nNumber of nodes assigned to a job. Available in SrunProlog, TaskProlog,\nSrunEpilog and TaskEpilog.\nSLURM_NODEID\nID of current node relative to other nodes in a multi-node job. Available in\nSrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_PRIO_PROCESS\nScheduling priority (nice value) at the time of submission. Available in\nSrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_PROCID\nThe MPI rank (or relative process ID) of the current process. Available in\nSrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_RESTART_COUNT\nNumber of times the job has been restarted. This is only set if the job\nhas been restarted at least once. Available in SrunProlog, TaskProlog,\nSrunEpilog and TaskEpilog.\nSLURM_RLIMIT_AS\nResource limit on the job's address space. Available in SrunProlog,\nTaskProlog, SrunEpilog and TaskEpilog.\nSLURM_RLIMIT_CORE\nResource limit on the size of a core file the job is able to produce.\nAvailable in SrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_RLIMIT_CPU\nResource limit on the amount of CPU time a job is able to use. Available\nin SrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_RLIMIT_DATA\nResource limit on the size of a job's data segment. Available in SrunProlog,\nTaskProlog, SrunEpilog and TaskEpilog.\nSLURM_RLIMIT_FSIZE\nResource limit on the maximum size of files a job may create. Available in\nSrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_RLIMIT_MEMLOCK\nResource limit on the bytes of data that may be locked into RAM. Available\nin SrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_RLIMIT_NOFILE\nResource limit on the number of file descriptors that can be opened by the\njob. Available in SrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_RLIMIT_NPROC\nResource limit on the number of processes that can be opened by the calling\nprocess. Available in SrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_RLIMIT_RSS\nResource limit on the job's resident set size. Available in SrunProlog,\nTaskProlog, SrunEpilog and TaskEpilog.\nSLURM_RLIMIT_STACK\nResource limit on the job's process stack. Available in SrunProlog,\nTaskProlog, SrunEpilog and TaskEpilog.\nSLURM_SCRIPT_CONTEXT\nIdentifies which epilog or prolog program is currently running.\nThe value is one of the following:\n\nprolog_slurmctld\nepilog_slurmctld\nprolog_slurmd\nepilog_slurmd\nprolog_task\nepilog_task\nprolog_srun\nepilog_srun\n\n\nSLURM_STEP_ID\nStep ID of the current job. Available in SrunProlog, TaskProlog, SrunEpilog\nand TaskEpilog.\nSLURM_STEPID\nStep ID of the current job. Available in SrunProlog and SrunEpilog.\nSLURM_SUBMIT_DIR\nDirectory from which the job was submitted or, if applicable, the directory\nspecified by the -D, --chdir option. Available in SrunProlog,\nTaskprolog, SrunEpilog and TaskEpilog.\nSLURM_SUBMIT_HOST\nHost from which the job was submitted. Available in SrunProlog, TaskProlog,\nSrunEpilog and TaskEpilog.\nSLURM_TASK_PID\nProcess ID of the process started for the task. Available in SrunProlog,\nTaskProlog, SrunEpilog and TaskEpilog.\nSLURM_TASKS_PER_NODE\nNumber of tasks per node. Available in SrunProlog, TaskProlog, SrunEpilog\nand TaskEpilog.\nSLURM_TOPOLOGY_ADDR\nSet to the names of network switches or nodes that may be involved in the\njob's communications. Starts with the top level switch down to the node name.\nA period is used to separate each hardware component name. Available in\nSrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_TOPOLOGY_ADDR_PATTERN\nSet to the network component types that corresponds with the list of names\nfrom SLURM_TOPOLOGY_ADDR. Each component will be identified as either\nswitch or node. A period is used to separate each component\ntype. Available in SrunProlog, TaskProlog, SrunEpilog and TaskEpilog.\nSLURM_WCKEY\nUser name of the job's wckey (if any).\nAvailable in PrologSlurmctld and EpilogSlurmctld only.\nPlugin functions may also be useful to execute logic at various well-defined\npoints.\nSPANK is another mechanism that may be useful\nto invoke logic in the user commands, slurmd daemon, and slurmstepd daemon.\nFailure Handling\n\n\nIf the Epilog fails (returns a non-zero exit code), this will result in the\nnode being set to a DRAIN state.\nIf the EpilogSlurmctld fails (returns a non-zero exit code), this will only\nbe logged.\nIf the Prolog fails (returns a non-zero exit code), this will result in the\nnode being set to a DRAIN state and the job requeued. The job will be placed\nin a held state unless nohold_on_prolog_fail is configured in\nSchedulerParameters.\nIf the PrologSlurmctld fails (returns a non-zero exit code), this will cause\nthe job to be requeued. Only batch jobs can be requeued. Interactive jobs\n(salloc and srun) will be cancelled if the PrologSlurmctld fails.\nIf a task epilog or srun epilog fails (returns a non-zero exit code) this\nwill only be logged.\nIf a task prolog fails (returns a non-zero exit code), the task will be\ncanceled.\nIf the srun prolog fails (returns a non-zero exit code), the step will be\ncanceled.\n\nBased upon work by Jason Sollom, Cray Inc. and used by permission.\nLast modified 23 September 2024\n"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        },
        {
            "title": "Failure Handling\n\n",
            "content": "If the Epilog fails (returns a non-zero exit code), this will result in the\nnode being set to a DRAIN state.\nIf the EpilogSlurmctld fails (returns a non-zero exit code), this will only\nbe logged.\nIf the Prolog fails (returns a non-zero exit code), this will result in the\nnode being set to a DRAIN state and the job requeued. The job will be placed\nin a held state unless nohold_on_prolog_fail is configured in\nSchedulerParameters.\nIf the PrologSlurmctld fails (returns a non-zero exit code), this will cause\nthe job to be requeued. Only batch jobs can be requeued. Interactive jobs\n(salloc and srun) will be cancelled if the PrologSlurmctld fails.If a task epilog or srun epilog fails (returns a non-zero exit code) this\nwill only be logged.\nIf a task prolog fails (returns a non-zero exit code), the task will be\ncanceled.\nIf the srun prolog fails (returns a non-zero exit code), the step will be\ncanceled.Based upon work by Jason Sollom, Cray Inc. and used by permission.Last modified 23 September 2024"
        }
    ]
}