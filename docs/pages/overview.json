{
    "url": "https://slurm.schedmd.com/overview.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Overview",
            "content": "Slurm is an open source,\nfault-tolerant, and highly scalable cluster management and job scheduling system\nfor large and small Linux clusters. Slurm requires no kernel modifications for\nits operation and is relatively self-contained. As a cluster workload manager,\nSlurm has three key functions. First, it allocates exclusive and/or non-exclusive\naccess to resources (compute nodes) to users for some duration of time so they\ncan perform work. Second, it provides a framework for starting, executing, and\nmonitoring work (normally a parallel job) on the set of allocated nodes.\nFinally, it arbitrates contention for resources by managing a queue of\npending work.\nOptional plugins can be used for\naccounting,\nadvanced reservation,\ngang scheduling (time sharing for\nparallel jobs), backfill scheduling,\ntopology optimized resource selection,\nresource limits by user or bank account,\nand sophisticated  multifactor job\nprioritization algorithms.\n\nArchitecture\n\n\nSlurm has a centralized manager, slurmctld, to monitor resources and\nwork. There may also be a backup manager to assume those responsibilities in the\nevent of failure. Each compute server (node) has a slurmd daemon, which\ncan be compared to a remote shell: it waits for work, executes that work, returns\nstatus, and waits for more work.\nThe slurmd daemons provide fault-tolerant hierarchical communications.\nThere is an optional slurmdbd (Slurm DataBase Daemon) which can be used\nto record accounting information for multiple Slurm-managed clusters in a\nsingle database.\nThere is an optional\nslurmrestd (Slurm REST API Daemon)\nwhich can be used to interact with Slurm through its\n\nREST API.\nUser tools include srun to initiate jobs,\nscancel to terminate queued or running jobs,\nsinfo to report system status,\nsqueue to report the status of jobs, and\nsacct to get information about jobs and job steps that are running or have completed.\nThe sview commands graphically reports system and\njob status including network topology.\nThere is an administrative tool scontrol available to monitor\nand/or modify configuration and state information on the cluster.\nThe administrative tool used to manage the database is sacctmgr.\nIt can be used to identify the clusters, valid users, valid bank accounts, etc.\nAPIs are available for all functions.\n\n\n  Figure 1. Slurm components\n\nSlurm has a general-purpose plugin mechanism available to easily support various\ninfrastructures. This permits a wide variety of Slurm configurations using a\nbuilding block approach. These plugins presently include:\n\nAccounting Storage:\n  Primarily Used to store historical data about jobs.  When used with\n  SlurmDBD (Slurm Database Daemon), it can also supply a\n  limits based system along with historical system status.\n\nAccount Gather Energy:\n  Gather energy consumption data per job or nodes in the system.\n  This plugin is integrated with the\n  Accounting Storage and Job Account Gather plugins.\n\nAuthentication of communications:\n  Provides authentication mechanism between various components of Slurm.\n\nContainers:\n  HPC workload container support and implementations.\n\nCredential (Digital Signature Generation):\n  Mechanism used to generate a digital signature, which is used to validate\n  that job step is authorized to execute on specific nodes.\n  This is distinct from the plugin used for\n  Authentication since the job step\n  request is sent from the user's srun command rather than directly from the\n  slurmctld daemon, which generates the job step credential and its\n  digital signature.\n\nGeneric Resources: Provide interface to\n  control generic resources, including Graphical Processing Units (GPUs).\n\nJob Submit:\n  Custom plugin to allow site specific control over job requirements at\n  submission and update.\n\nJob Accounting Gather:\n  Gather job step resource utilization data.\n\nJob Completion Logging:\n  Log a job's termination data. This is typically a subset of data stored by\n  an Accounting Storage Plugin.\n\nLaunchers:\n  Controls the mechanism used by the 'srun' command\n  to launch the tasks.\n\nMPI:\n  Provides different hooks for the various MPI implementations.\n  For example, this can set MPI specific environment variables.\n\nPreempt:\n  Determines which jobs can preempt other jobs and the preemption mechanism\n  to be used.\n\nPriority:\n  Assigns priorities to jobs upon submission and on an ongoing basis\n  (e.g. as they age).\n\nProcess tracking (for signaling):\n  Provides a mechanism for identifying the processes associated with each job.\n  Used for job accounting and signaling.\n\nScheduler:\n  Plugin determines how and when Slurm schedules jobs.\n\nNode selection:\n  Plugin used to determine the resources used for a job allocation.\n\nSite Factor (Priority):\n  Assigns a specific site_factor component of a job's multifactor priority to\n  jobs upon submission and on an ongoing basis (e.g. as they age).\n\nSwitch or interconnect:\n  Plugin to interface with a switch or interconnect.\n  For most systems (Ethernet or InfiniBand) this is not needed.\n\nTask Affinity:\n  Provides mechanism to bind a job and its individual tasks to specific\n  processors.\n\nNetwork Topology:\n  Optimizes resource selection based upon the network topology.\n  Used for both job allocations and advanced reservation.\n\n\nThe entities managed by these Slurm daemons, shown in Figure 2, include nodes,\nthe compute resource in Slurm, partitions, which group nodes into logical\nsets, jobs, or allocations of resources assigned to a user for\na specified amount of time, and job steps, which are sets of (possibly\nparallel) tasks within a job.\nThe partitions can be considered job queues, each of which has an assortment of\nconstraints such as job size limit, job time limit, users permitted to use it, etc.\nPriority-ordered jobs are allocated nodes within a partition until the resources\n(nodes, processors, memory, etc.) within that partition are exhausted. Once\na job is assigned a set of nodes, the user is able to initiate parallel work in\nthe form of job steps in any configuration within the allocation. For instance,\na single job step may be started that utilizes all nodes allocated to the job,\nor several job steps may independently use a portion of the allocation.\nSlurm provides resource management for the processors allocated to a job,\nso that multiple job steps can be simultaneously submitted and queued until\nthere are available resources within the job's allocation.\n\n\n  Figure 2. Slurm entities\n\nConfigurability\n\n\nNode state monitored include: count of processors, size of real memory, size\nof temporary disk space, and state (UP, DOWN, etc.). Additional node information\nincludes weight (preference in being allocated work) and features (arbitrary information\nsuch as processor speed or type).\nNodes are grouped into partitions, which may contain overlapping nodes so they are\nbest thought of as job queues.\nPartition information includes: name, list of associated nodes, state (UP or DOWN),\nmaximum job time limit, maximum node count per job, group access list,\npriority (important if nodes are in multiple partitions) and shared node access policy\nwith optional over-subscription level for gang scheduling (e.g. YES, NO or FORCE:2).\nBit maps are used to represent nodes and scheduling\ndecisions can be made by performing a small number of comparisons and a series\nof fast bit map manipulations. A sample (partial. Slurm configuration file follows.\n\n#\n# Sample /etc/slurm.conf\n#\nSlurmctldHost=linux0001  # Primary server\nSlurmctldHost=linux0002  # Backup server\n#\nAuthType=auth/munge\nEpilog=/usr/local/slurm/sbin/epilog\nPluginDir=/usr/local/slurm/lib\nProlog=/usr/local/slurm/sbin/prolog\nSlurmctldPort=7002\nSlurmctldTimeout=120\nSlurmdPort=7003\nSlurmdSpoolDir=/var/tmp/slurmd.spool\nSlurmdTimeout=120\nStateSaveLocation=/usr/local/slurm/slurm.state\nTmpFS=/tmp\n#\n# Node Configurations\n#\nNodeName=DEFAULT CPUs=4 TmpDisk=16384 State=IDLE\nNodeName=lx[0001-0002] State=DRAINED\nNodeName=lx[0003-8000] RealMemory=2048 Weight=2\nNodeName=lx[8001-9999] RealMemory=4096 Weight=6 Feature=video\n#\n# Partition Configurations\n#\nPartitionName=DEFAULT MaxTime=30 MaxNodes=2\nPartitionName=login Nodes=lx[0001-0002] State=DOWN\nPartitionName=debug Nodes=lx[0003-0030] State=UP Default=YES\nPartitionName=class Nodes=lx[0031-0040] AllowGroups=students\nPartitionName=DEFAULT MaxTime=UNLIMITED MaxNodes=4096\nPartitionName=batch Nodes=lx[0041-9999]\n\nLast modified 6 August 2021\n"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        },
        {
            "title": "Architecture\n\n",
            "content": "Slurm has a centralized manager, slurmctld, to monitor resources and\nwork. There may also be a backup manager to assume those responsibilities in the\nevent of failure. Each compute server (node) has a slurmd daemon, which\ncan be compared to a remote shell: it waits for work, executes that work, returns\nstatus, and waits for more work.\nThe slurmd daemons provide fault-tolerant hierarchical communications.\nThere is an optional slurmdbd (Slurm DataBase Daemon) which can be used\nto record accounting information for multiple Slurm-managed clusters in a\nsingle database.\nThere is an optional\nslurmrestd (Slurm REST API Daemon)\nwhich can be used to interact with Slurm through its\n\nREST API.\nUser tools include srun to initiate jobs,\nscancel to terminate queued or running jobs,\nsinfo to report system status,\nsqueue to report the status of jobs, and\nsacct to get information about jobs and job steps that are running or have completed.\nThe sview commands graphically reports system and\njob status including network topology.\nThere is an administrative tool scontrol available to monitor\nand/or modify configuration and state information on the cluster.\nThe administrative tool used to manage the database is sacctmgr.\nIt can be used to identify the clusters, valid users, valid bank accounts, etc.\nAPIs are available for all functions.\n\n  Figure 1. Slurm components\nSlurm has a general-purpose plugin mechanism available to easily support various\ninfrastructures. This permits a wide variety of Slurm configurations using a\nbuilding block approach. These plugins presently include:\n\nAccounting Storage:\n  Primarily Used to store historical data about jobs.  When used with\n  SlurmDBD (Slurm Database Daemon), it can also supply a\n  limits based system along with historical system status.\n\nAccount Gather Energy:\n  Gather energy consumption data per job or nodes in the system.\n  This plugin is integrated with the\n  Accounting Storage and Job Account Gather plugins.\n\nAuthentication of communications:\n  Provides authentication mechanism between various components of Slurm.\n\nContainers:\n  HPC workload container support and implementations.\n\nCredential (Digital Signature Generation):\n  Mechanism used to generate a digital signature, which is used to validate\n  that job step is authorized to execute on specific nodes.\n  This is distinct from the plugin used for\n  Authentication since the job step\n  request is sent from the user's srun command rather than directly from the\n  slurmctld daemon, which generates the job step credential and its\n  digital signature.\n\nGeneric Resources: Provide interface to\n  control generic resources, including Graphical Processing Units (GPUs).\n\nJob Submit:\n  Custom plugin to allow site specific control over job requirements at\n  submission and update.\n\nJob Accounting Gather:\n  Gather job step resource utilization data.\n\nJob Completion Logging:\n  Log a job's termination data. This is typically a subset of data stored by\n  an Accounting Storage Plugin.\n\nLaunchers:\n  Controls the mechanism used by the 'srun' command\n  to launch the tasks.\n\nMPI:\n  Provides different hooks for the various MPI implementations.\n  For example, this can set MPI specific environment variables.\n\nPreempt:\n  Determines which jobs can preempt other jobs and the preemption mechanism\n  to be used.\n\nPriority:\n  Assigns priorities to jobs upon submission and on an ongoing basis\n  (e.g. as they age).\n\nProcess tracking (for signaling):\n  Provides a mechanism for identifying the processes associated with each job.\n  Used for job accounting and signaling.\n\nScheduler:\n  Plugin determines how and when Slurm schedules jobs.\n\nNode selection:\n  Plugin used to determine the resources used for a job allocation.\n\nSite Factor (Priority):\n  Assigns a specific site_factor component of a job's multifactor priority to\n  jobs upon submission and on an ongoing basis (e.g. as they age).\n\nSwitch or interconnect:\n  Plugin to interface with a switch or interconnect.\n  For most systems (Ethernet or InfiniBand) this is not needed.\n\nTask Affinity:\n  Provides mechanism to bind a job and its individual tasks to specific\n  processors.\n\nNetwork Topology:\n  Optimizes resource selection based upon the network topology.\n  Used for both job allocations and advanced reservation.\n\n\nThe entities managed by these Slurm daemons, shown in Figure 2, include nodes,\nthe compute resource in Slurm, partitions, which group nodes into logical\nsets, jobs, or allocations of resources assigned to a user for\na specified amount of time, and job steps, which are sets of (possibly\nparallel) tasks within a job.\nThe partitions can be considered job queues, each of which has an assortment of\nconstraints such as job size limit, job time limit, users permitted to use it, etc.\nPriority-ordered jobs are allocated nodes within a partition until the resources\n(nodes, processors, memory, etc.) within that partition are exhausted. Once\na job is assigned a set of nodes, the user is able to initiate parallel work in\nthe form of job steps in any configuration within the allocation. For instance,\na single job step may be started that utilizes all nodes allocated to the job,\nor several job steps may independently use a portion of the allocation.\nSlurm provides resource management for the processors allocated to a job,\nso that multiple job steps can be simultaneously submitted and queued until\nthere are available resources within the job's allocation.\n\n\n  Figure 2. Slurm entities\n\nConfigurability\n\n\nNode state monitored include: count of processors, size of real memory, size\nof temporary disk space, and state (UP, DOWN, etc.). Additional node information\nincludes weight (preference in being allocated work) and features (arbitrary information\nsuch as processor speed or type).\nNodes are grouped into partitions, which may contain overlapping nodes so they are\nbest thought of as job queues.\nPartition information includes: name, list of associated nodes, state (UP or DOWN),\nmaximum job time limit, maximum node count per job, group access list,\npriority (important if nodes are in multiple partitions) and shared node access policy\nwith optional over-subscription level for gang scheduling (e.g. YES, NO or FORCE:2).\nBit maps are used to represent nodes and scheduling\ndecisions can be made by performing a small number of comparisons and a series\nof fast bit map manipulations. A sample (partial. Slurm configuration file follows.\n\n#\n# Sample /etc/slurm.conf\n#\nSlurmctldHost=linux0001  # Primary server\nSlurmctldHost=linux0002  # Backup server\n#\nAuthType=auth/munge\nEpilog=/usr/local/slurm/sbin/epilog\nPluginDir=/usr/local/slurm/lib\nProlog=/usr/local/slurm/sbin/prolog\nSlurmctldPort=7002\nSlurmctldTimeout=120\nSlurmdPort=7003\nSlurmdSpoolDir=/var/tmp/slurmd.spool\nSlurmdTimeout=120\nStateSaveLocation=/usr/local/slurm/slurm.state\nTmpFS=/tmp\n#\n# Node Configurations\n#\nNodeName=DEFAULT CPUs=4 TmpDisk=16384 State=IDLE\nNodeName=lx[0001-0002] State=DRAINED\nNodeName=lx[0003-8000] RealMemory=2048 Weight=2\nNodeName=lx[8001-9999] RealMemory=4096 Weight=6 Feature=video\n#\n# Partition Configurations\n#\nPartitionName=DEFAULT MaxTime=30 MaxNodes=2\nPartitionName=login Nodes=lx[0001-0002] State=DOWN\nPartitionName=debug Nodes=lx[0003-0030] State=UP Default=YES\nPartitionName=class Nodes=lx[0031-0040] AllowGroups=students\nPartitionName=DEFAULT MaxTime=UNLIMITED MaxNodes=4096\nPartitionName=batch Nodes=lx[0041-9999]\n\nLast modified 6 August 2021\n"
        },
        {
            "title": "Configurability\n\n",
            "content": "Node state monitored include: count of processors, size of real memory, size\nof temporary disk space, and state (UP, DOWN, etc.). Additional node information\nincludes weight (preference in being allocated work) and features (arbitrary information\nsuch as processor speed or type).\nNodes are grouped into partitions, which may contain overlapping nodes so they are\nbest thought of as job queues.\nPartition information includes: name, list of associated nodes, state (UP or DOWN),\nmaximum job time limit, maximum node count per job, group access list,\npriority (important if nodes are in multiple partitions) and shared node access policy\nwith optional over-subscription level for gang scheduling (e.g. YES, NO or FORCE:2).\nBit maps are used to represent nodes and scheduling\ndecisions can be made by performing a small number of comparisons and a series\nof fast bit map manipulations. A sample (partial. Slurm configuration file follows.\n#\n# Sample /etc/slurm.conf\n#\nSlurmctldHost=linux0001  # Primary server\nSlurmctldHost=linux0002  # Backup server\n#\nAuthType=auth/munge\nEpilog=/usr/local/slurm/sbin/epilog\nPluginDir=/usr/local/slurm/lib\nProlog=/usr/local/slurm/sbin/prolog\nSlurmctldPort=7002\nSlurmctldTimeout=120\nSlurmdPort=7003\nSlurmdSpoolDir=/var/tmp/slurmd.spool\nSlurmdTimeout=120\nStateSaveLocation=/usr/local/slurm/slurm.state\nTmpFS=/tmp\n#\n# Node Configurations\n#\nNodeName=DEFAULT CPUs=4 TmpDisk=16384 State=IDLE\nNodeName=lx[0001-0002] State=DRAINED\nNodeName=lx[0003-8000] RealMemory=2048 Weight=2\nNodeName=lx[8001-9999] RealMemory=4096 Weight=6 Feature=video\n#\n# Partition Configurations\n#\nPartitionName=DEFAULT MaxTime=30 MaxNodes=2\nPartitionName=login Nodes=lx[0001-0002] State=DOWN\nPartitionName=debug Nodes=lx[0003-0030] State=UP Default=YES\nPartitionName=class Nodes=lx[0031-0040] AllowGroups=students\nPartitionName=DEFAULT MaxTime=UNLIMITED MaxNodes=4096\nPartitionName=batch Nodes=lx[0041-9999]\nLast modified 6 August 2021"
        }
    ]
}