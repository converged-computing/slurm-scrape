{
    "url": "https://slurm.schedmd.com/cpu_management.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": " CPU Management User and Administrator Guide",
            "content": "OverviewThe purpose of this guide is to assist Slurm users and administrators in selecting configuration options\nand composing command lines to manage the use of CPU resources by jobs, steps and tasks. The document\nis divided into the following sections:\nOverview\nCPU Management Steps performed by Slurm\nGetting Information about CPU usage by Jobs/Steps/Tasks\nCPU Management and Slurm Accounting\nCPU Management Examples\nCPU Management through user commands is constrained by the configuration parameters\nchosen by the Slurm administrator. The interactions between different CPU management options are complex\nand often difficult to predict. Some experimentation may be required to discover the exact combination\nof options  needed to produce a desired outcome. Users and administrators should refer to the man pages\nfor slurm.conf, cgroup.conf,\nsalloc,\nsbatch and srun for detailed explanations of each\noption. The following html documents may also be useful:\nConsumable Resources in Slurm\nSharing Consumable Resources\nSupport for Multi-core/Multi-thread\nArchitectures\nPlane distributionCPU Management Steps performed by Slurm\n\nSlurm uses four basic steps to manage CPU resources for a job/step:\nStep 1: Selection of Nodes\nStep 2: Allocation of CPUs from the selected Nodes\nStep 3: Distribution of Tasks to the selected Nodes\nStep 4: Optional Distribution and Binding of Tasks to CPUs within a Node\n\nStep 1: Selection of Nodes\n\nIn Step 1, Slurm selects the set of nodes from which CPU resources are to be allocated to a job or\njob step.  Node selection is therefore influenced by many of the configuration and command line options\nthat control the allocation of CPUs (Step 2 below).\nIf \nSelectType=select/linear is configured, all resources on the selected nodes will be allocated\nto the job/step. If SelectType is configured to be\nselect/cons_tres,\nindividual sockets, cores and threads may be allocated from the selected nodes as\nconsumable resources. The consumable resource type is defined by\nSelectTypeParameters.\n\n\nStep 1 is performed by slurmctld and the select plugin.\n\n\n\nslurm.conf options that control Step 1\n\n\n\n\n\nslurm.conf\n\t\t\t\tparameter\n\n\nPossible values\n\n\nDescription\n\n\n\n\n\nNodeName\n\n\n<name of the node>\n\t\t\t\tPlus additional parameters. See man page for details.\n\n\nDefines\n\t\t\t\ta node. This includes the number and layout of boards, sockets, cores,\n\t\t\t\tthreads and processors (logical CPUs) on the node.\n\n\n\n\n\nPartitionName\n\n\n<name of the partition>\n\t\t\t\tPlus additional parameters. See man page for details.\n\n\nDefines\n\t\t\t\ta partition. Several parameters of the partition definition\n\t\t\t\taffect the selection of nodes (e.g., Nodes,\n\t\t\t\tOverSubscribe, MaxNodes)\n\n\n\n\n\nSlurmdParameters\n\n\nconfig_overrides\n\n\nControls\n\t\t\t\thow the information in a node definition is used.\n\n\n\n\n\nSelectType\n\n\n\nselect/linear | select/cons_tres\n\n\nControls\n\t\t\t\twhether CPU resources are allocated to jobs and job steps in\n\t\t\t\tunits of whole nodes or as consumable resources (sockets, cores\n\t\t\t\tor threads).\n\n\n\n\n\nSelectTypeParameters\n\n\nCR_CPU | CR_CPU_Memory | CR_Core |\nCR_Core_Memory | CR_Socket | CR_Socket_MemoryPlus additional options.  See man page for details.\n\n\nDefines\n\t\t\t\tthe consumable resource type and controls other aspects of CPU\n\t\t\t\tresource allocation by the select plugin.\n\n\n\n\n\nsrun/salloc/sbatch command line options that control Step 1\n\n\n\n\n\n\nCommand\n\t\t\t\tline option\n\n\n\nPossible values\n\n\nDescription\n\n\n\n\n\n-B, --extra-node-info\n\n\n\n\n\t\t\t\t<sockets[:cores[:threads]]>\n\n\nRestricts\n\t\t\t\tnode selection to nodes with a specified layout of sockets, cores\n\t\t\t\tand threads.\n\n\n\n\n\n-C, --constraint\n\n\n<list>\n\t\t\t\t\n\n\nRestricts\n\t\t\t\tnode selection to nodes with specified attributes\n\n\n\n\n\n--contiguous\n\n\nN/A\n\n\nRestricts\n\t\t\t\tnode selection to contiguous nodes\n\n\n\n\n\n--cores-per-socket\n\n\n<cores>\n\n\nRestricts\n\t\t\t\tnode selection to nodes with at least the specified number of cores per socket\n\n\n\n\n\n-c, --cpus-per-task\n\n\n<ncpus>\n\n\nControls\n\t\t\t\tthe number of CPUs allocated per task\n\n\n\n\n\n--exclusive\n\n\nN/A\n\n\nPrevents\n\t\t\t\tsharing of allocated nodes with other jobs. Suballocates CPUs to job steps.\n\n\n\n\n\n-F, --nodefile\n\n\n<node file>\n\n\nFile\n\t\t\t\tcontaining a list of specific nodes to be selected for the job (salloc and sbatch only)\n\n\n\n\n\n--hint\n\n\ncompute_bound |\n\t\t\t\tmemory_bound | [no]multithread\n\n\nAdditional\n\t\t\t\tcontrols on allocation of CPU resources\n\n\n\n\n\n--mincpus\n\n\n<n>\n\n\nControls\n\t\t\t\tthe minimum number of CPUs allocated per node\n\n\n\n\n\n-N, --nodes\n\n\n\n\t\t\t\t<minnodes[-maxnodes]>\n\n\nControls\n\t\t\t\tthe minimum/maximum number of nodes allocated to the job\n\n\n\n\n\n-n, --ntasks\n\n\n<number>\n\n\nControls\n\t\t\t\tthe number of tasks to be created for the job\n\n\n\n\n\n--ntasks-per-core\n\n\n<number>\n\n\nControls\n\t\t\t\tthe maximum number of tasks per allocated core\n\n\n\n\n\n--ntasks-per-socket\n\n\n<number>\n\n\nControls\n\t\t\t\tthe maximum number of tasks per allocated socket\n\n\n\n\n\n--ntasks-per-node\n\n\n<number>\n\n\nControls\n\t\t\t\tthe maximum number of tasks per allocated node\n\n\n\n\n\n-O, --overcommit\n\n\nN/A\n\n\nAllows\n\t\t\t\tfewer CPUs to be allocated than the number of tasks\n\n\n\n\n\n-p, --partition\n\n\n\n\t\t\t\t<partition_names>\n\n\nControls\n\t\t\t\twhich partition is used for the job\n\n\n\n\n\n-s, --oversubscribe\n\n\nN/A\n\n\nAllows\n\t\t\t\tsharing of allocated nodes with other jobs\n\n\n\n\n\n--sockets-per-node\n\n\n<sockets>\n\n\nRestricts\n\t\t\t\tnode selection to nodes with at least the specified number of sockets\n\n\n\n\n\n--threads-per-core\n\n\n<threads>\n\n\nRestricts\n\t\t\t\tnode selection to nodes with at least the specified number of threads per core\n\n\n\n\n\n-w, --nodelist\n\n\n\n\t\t\t\t<host1,host2,... or filename>\n\n\nList\n\t\t\t\tof specific nodes to be allocated to the job\n\n\n\n\n\n-x, --exclude\n\n\n\n\t\t\t\t<host1,host2,... or filename>\n\n\nList\n\t\t\t\tof specific nodes to be excluded from allocation to the job\n\n\n\n\n\n-Z, --no-allocate\n\n\nN/A\n\n\nBypass\n\t\t\t\tnormal allocation (privileged option available to users\n\t\t\t\t\u201cSlurmUser\u201d and \u201croot\u201d only)\n\n\n\nStep 2: Allocation of CPUs from the selected Nodes\n\nIn Step 2, Slurm allocates CPU resources to a job/step from the set of nodes selected\nin Step 1. CPU allocation is therefore influenced by the configuration and command line options\nthat relate to node selection.\nIf \nSelectType=select/linear is configured, all resources on the selected nodes will be allocated\nto the job/step. If SelectType is configured to be\nselect/cons_tres,\nindividual sockets, cores and threads may be allocated from the selected nodes as\nconsumable resources. The consumable resource type is defined by\nSelectTypeParameters.\n\nWhen using a SelectType of\nselect/cons_tres,\nthe default allocation method across nodes is block allocation (allocate all available CPUs in\na node before using another node). The default allocation method within a node is cyclic\nallocation (allocate available CPUs in a round-robin fashion across the sockets within a node).\nUsers may override the default behavior using the appropriate command\nline options described below.  The choice of allocation methods may influence which specific\nCPUs are allocated to the job/step.\n\nStep 2 is performed by slurmctld and the select plugin.\n\n\n\nslurm.conf options that control Step 2\n\n\n\n\n\nslurm.conf\n\t\t\t\tparameter\n\n\nPossible values\n\n\nDescription\n\n\n\n\n\nNodeName\n\n\n<name of the node>\n\t\t\t\tPlus additional parameters. See man page for details.\n\n\nDefines\n\t\t\t\ta node. This includes the number and layout of boards, sockets, cores,\n\t\t\t\tthreads and processors (logical CPUs) on the node.\n\n\n\n\n\nPartitionName\n\n\n<name of the partition>\n\t\t\t\tPlus additional parameters. See man page for details.\n\n\nDefines\n\t\t\t\ta partition. Several parameters of the partition definition\n\t\t\t\taffect the allocation of CPU resources to jobs (e.g., Nodes,\n\t\t\t\tOverSubscribe, MaxNodes)\n\n\n\n\n\nSlurmdParameters\n\n\nconfig_overrides\n\n\nControls\n\t\t\t\thow the information in a node definition is used.\n\n\n\n\n\nSelectType\n\n\n\nselect/linear | select/cons_tres\n\n\nControls\n\t\t\t\twhether CPU resources are allocated to jobs and job steps in\n\t\t\t\tunits of whole nodes or as consumable resources (sockets, cores\n\t\t\t\tor threads).\n\n\n\n\n\nSelectTypeParameters\n\n\nCR_CPU | CR_CPU_Memory | CR_Core |\nCR_Core_Memory | CR_Socket | CR_Socket_MemoryPlus additional options.  See man page for details.\n\n\nDefines\n\t\t\t\tthe consumable resource type and controls other aspects of CPU\n\t\t\t\tresource allocation by the select plugin.\n\n\n\n\n\nsrun/salloc/sbatch command line options that control Step 2\n\n\n\n\n\n\nCommand\n\t\t\t\tline option\n\n\n\nPossible values\n\n\nDescription\n\n\n\n\n\n-B, --extra-node-info\n\n\n\n\n\t\t\t\t<sockets[:cores[:threads]]>\n\n\nRestricts\n\t\t\t\tnode selection to nodes with a specified layout of sockets, cores\n\t\t\t\tand threads.\n\n\n\n\n\n-C, --constraint\n\n\n<list>\n\t\t\t\t\n\n\nRestricts\n\t\t\t\tnode selection to nodes with specified attributes\n\n\n\n\n\n--contiguous\n\n\nN/A\n\n\nRestricts\n\t\t\t\tnode selection to contiguous nodes\n\n\n\n\n\n--cores-per-socket\n\n\n<cores>\n\n\nRestricts\n\t\t\t\tnode selection to nodes with at least the specified number of cores per socket\n\n\n\n\n\n-c, --cpus-per-task\n\n\n<ncpus>\n\n\nControls\n\t\t\t\tthe number of CPUs allocated per task\n\n\n\n\n\n--distribution, -m\n\n\n\n\t\t\t\tblock|cyclic|arbitrary|plane=<options>[:block|cyclic]\n\n\nThe second specified distribution (after the \":\")\n\t\t\t\tcan be used to override the default allocation method within nodes\n\n\n\n\n\n--exclusive\n\n\nN/A\n\n\nPrevents\n\t\t\t\tsharing of allocated nodes with other jobs\n\n\n\n\n\n-F, --nodefile\n\n\n<node file>\n\n\nFile\n\t\t\t\tcontaining a list of specific nodes to be selected for the job (salloc and sbatch only)\n\n\n\n\n\n--hint\n\n\ncompute_bound |\n\t\t\t\tmemory_bound | [no]multithread\n\n\nAdditional\n\t\t\t\tcontrols on allocation of CPU resources\n\n\n\n\n\n--mincpus\n\n\n<n>\n\n\nControls\n\t\t\t\tthe minimum number of CPUs allocated per node\n\n\n\n\n\n-N, --nodes\n\n\n\n\t\t\t\t<minnodes[-maxnodes]>\n\n\nControls\n\t\t\t\tthe minimum/maximum number of nodes allocated to the job\n\n\n\n\n\n-n, --ntasks\n\n\n<number>\n\n\nControls\n\t\t\t\tthe number of tasks to be created for the job\n\n\n\n\n\n--ntasks-per-core\n\n\n<number>\n\n\nControls\n\t\t\t\tthe maximum number of tasks per allocated core\n\n\n\n\n\n--ntasks-per-socket\n\n\n<number>\n\n\nControls\n\t\t\t\tthe maximum number of tasks per allocated socket\n\n\n\n\n\n--ntasks-per-node\n\n\n<number>\n\n\nControls\n\t\t\t\tthe maximum number of tasks per allocated node\n\n\n\n\n\n-O, --overcommit\n\n\nN/A\n\n\nAllows\n\t\t\t\tfewer CPUs to be allocated than the number of tasks\n\n\n\n\n\n-p, --partition\n\n\n\n\t\t\t\t<partition_names>\n\n\nControls\n\t\t\t\twhich partition is used for the job\n\n\n\n\n\n-s, --oversubscribe\n\n\nN/A\n\n\nAllows\n\t\t\t\tsharing of allocated nodes with other jobs\n\n\n\n\n\n--sockets-per-node\n\n\n<sockets>\n\n\nRestricts\n\t\t\t\tnode selection to nodes with at least the specified number of sockets\n\n\n\n\n\n--threads-per-core\n\n\n<threads>\n\n\nRestricts\n\t\t\t\tnode selection to nodes with at least the specified number of threads per core\n\n\n\n\n\n-w, --nodelist\n\n\n\n\t\t\t\t<host1,host2,... or filename>\n\n\nList\n\t\t\t\tof specific nodes to be allocated to the job\n\n\n\n\n\n-x, --exclude\n\n\n\n\t\t\t\t<host1,host2,... or filename>\n\n\nList\n\t\t\t\tof specific nodes to be excluded from allocation to the job\n\n\n\n\n\n-Z, --no-allocate\n\n\nN/A\n\n\nBypass\n\t\t\t\tnormal allocation (privileged option available to users\n\t\t\t\t\u201cSlurmUser\u201d and \u201croot\u201d only)\n\n\n\nStep 3: Distribution of Tasks to the selected Nodes\n\nIn Step 3, Slurm distributes tasks to the nodes that were selected for\nthe job/step in Step 1. Each task is distributed to only one node, but more than one\ntask may be distributed to each node.  Unless overcommitment of CPUs to tasks is\nspecified for the job, the number of tasks distributed to a node is\nconstrained by the number of CPUs allocated on the node and the number of CPUs per\ntask. If consumable resources is configured, or resource sharing is allowed, tasks from\nmore than one job/step may run on the same node concurrently.\n\nStep 3 is performed by slurmctld.\n\n\n\nslurm.conf options that control Step 3\n\n\n\n\n\nslurm.conf\n\t\t\t\tparameter\n\n\nPossible values\n\n\nDescription\n\n\n\n\n\nMaxTasksPerNode\n\n\n<number>\n\n\n\n\t\t\t\tControls the maximum number of tasks that a job step can spawn on a single node\n\t\t\t\t\n\n\n\n\n\nsrun/salloc/sbatch command line options that control Step 3\n\n\n\n\n\n\nCommand\n\t\t\t\tline option\n\n\n\nPossible values\n\n\nDescription\n\n\n\n\n\n--distribution, -m\n\n\n\n\t\t\t\tblock|cyclic|arbitrary|plane=<options>[:block|cyclic]\n\n\nThe first specified distribution (before the \":\")\n\t\t\t\tcontrols the sequence in which tasks are distributed to each of the selected nodes. Note that\n\t\t\t\tthis option does not affect the number of tasks distributed to each node, but only the sequence of\n\t\t\t\tdistribution.\n\n\n\n\n\n--ntasks-per-core\n\n\n<number>\n\t\t\t\t\n\n\n\n\t\t\t\tControls the maximum number of tasks per allocated core\n\n\n\n\n\n--ntasks-per-socket\n\n\n<number>\n\n\n\n\t\t\t\tControls the maximum number of tasks per allocated socket\n\n\n\n\n\n--ntasks-per-node\n\n\n<number>\n\n\n\n\t\t\t\tControls the maximum number of tasks per allocated node\n\n\n\n\n\n-r, --relative\n\n\nN/A\n\n\nControls\n\t\t\t\twhich node is used for a job step\n\n\n\n\nStep 4: Optional Distribution and Binding of Tasks to CPUs within a Node\n\nIn optional Step 4, Slurm distributes and binds each task to a specified subset of\nthe allocated CPUs on the node to which the task was distributed in Step 3. Different\ntasks distributed to the same node may be bound to the same subset of CPUs or to\ndifferent subsets. This step is known as task affinity or task/CPU binding.\n\nStep 4 is performed by slurmd and the task plugin.\n\n\n\nslurm.conf options that control Step 4\n\n\n\n\n\nslurm.conf\n\t\t\t\tparameter\n\n\nPossible values\n\n\nDescription\n\n\n\n\n\nTaskPlugin\n\n\n\n\t\t\t\ttask/none | task/affinity | task/cgroup\n\n\n\n\t\t\t\tControls whether this step is enabled and which task plugin to use\n\t\t\t\t\n\n\n\n\n\ncgroup.conf options that control Step 4 (task/cgroup plugin only)\n\n\n\n\n\ncgroup.conf\n\t\t\t\tparameter\n\n\nPossible values\n\n\nDescription\n\n\n\n\n\nConstrainCores\n\n\n\n\t\t\t\tyes|no\n\n\n\n\t\t\t\tControls whether jobs are constrained to their allocated CPUs\n\t\t\t\t\n\n\n\n\n\nsrun/salloc/sbatch command line options that control Step 4\n\n\n\n\n\n\nCommand\n\t\t\t\tline option\n\n\n\nPossible values\n\n\nDescription\n\n\n\n\n\n--cpu-bind\n\n\n\n\t\t\t\tSee man page\n\n\nControls binding of tasks to CPUs\n\t\t\t\t(srun only)\n\n\n\n\n\n--ntasks-per-core\n\n\n<number>\n\t\t\t\t\n\n\n\n\t\t\t\tControls the maximum number of tasks per allocated core\n\n\n\n\n\n--distribution, -m\n\n\n\n\t\t\t\tblock|cyclic|arbitrary|plane=<options>[:block|cyclic]\n\n\n\n\t\t\t\tThe second specified distribution (after the \":\") controls the sequence in which tasks are\n\t\t\t\tdistributed to allocated CPUs within a node for binding of tasks to CPUs\n\n\n\nAdditional Notes on CPU Management Steps\n\nFor consumable resources, it is important for users to understand the difference between\ncpu allocation (Step 2) and task affinity/binding (Step 4).  Exclusive (unshared) allocation\nof CPUs as consumable resources limits the number of jobs/steps/tasks that\ncan use a node concurrently.  But it does not limit the set of CPUs on the node that each\ntask distributed to the node can use.  Unless some form of CPU/task binding is used\n(e.g., a task or spank plugin), all tasks distributed to a node can use all of\nthe CPUs on the node, including CPUs not allocated to their job/step.  This may have\nunexpected adverse effects on performance, since it allows one job to use CPUs allocated\nexclusively to another job.  For this reason, it may not be advisable to configure\nconsumable resources without also configuring task affinity.  Note that task affinity\ncan also be useful when select/linear (whole node allocation) is configured, to improve\nperformance by restricting each task to a particular socket or other subset of CPU\nresources on a node.Getting Information about CPU usage by Jobs/Steps/Tasks\n\nThere is no easy way to generate a comprehensive set of CPU management information\nfor a job/step (allocation, distribution and binding). However, several\ncommands/options provide limited information about CPU usage.\n\n\n\n\nCommand/Option\n\n\nInformation\n\n\n\n\n\nscontrol show job option:\n--details\n\n\n\nThis option provides a list of the nodes selected for the job and the CPU ids allocated to the job on each\nnode. Note that the CPU ids reported by this command are Slurm abstract CPU ids, not Linux/hardware CPU ids\n(as reported by, for example, /proc/cpuinfo).\n\n\n\n\n\n\nLinux command: env\n\n\n\nMan. Slurm environment variables provide information related to node and CPU usage:\n\n\nSLURM_JOB_CPUS_PER_NODE\nSLURM_CPUS_PER_TASK\nSLURM_CPU_BIND\nSLURM_DISTRIBUTION\nSLURM_JOB_NODELIST\nSLURM_TASKS_PER_NODE\nSLURM_STEP_NODELIST\nSLURM_STEP_NUM_NODES\nSLURM_STEP_NUM_TASKS\nSLURM_STEP_TASKS_PER_NODE\nSLURM_JOB_NUM_NODES\nSLURM_NTASKS\nSLURM_NPROCS\nSLURM_CPUS_ON_NODE\nSLURM_NODEID\nSLURMD_NODENAME\n\n\n\n\n\n\n\nsrun option:\n--cpu-bind=verbose\n\n\n\nThis option provides a list of the CPU masks used by task affinity to bind tasks to CPUs.\nNote that the CPU ids represented by these masks are Linux/hardware CPU ids, not Slurm\nabstract CPU ids as reported by scontrol, etc.\n\n\n\n\n\n\nsrun/salloc/sbatch option:\n-l\n\n\n\nThis option adds the task id as a prefix to each line of output from a task sent to stdout/stderr.\nThis can be useful for distinguishing node-related and CPU-related information by task id\nfor multi-task jobs/steps.\n\n\n\n\n\n\nLinux command:\ncat /proc/<pid>/status | grep Cpus_allowed_list\n\n\n\nGiven a task's pid (or \"self\" if the command is executed by the task itself), this command\nproduces a list of the CPU ids bound to the task. This is the same information that is\nprovided by --cpu-bind=verbose, but in a more readable format.\n\n\n\n\nA Note on CPU Numbering\n\nThe number and layout of logical CPUs known to Slurm is described in the node definitions in slurm.conf. This may\ndiffer from the physical CPU layout on the actual hardware.  For this reason, Slurm generates its own internal, or\n\"abstract\", CPU numbers.  These numbers may not match the physical, or \"machine\", CPU numbers known to Linux.CPU Management and Slurm Accounting\n\nCPU management by Slurm users is subject to limits imposed by Slurm Accounting. Accounting limits may be applied on CPU\nusage at the level of users, groups and clusters. For details, see the sacctmgr man page.CPU Management Examples\n\nThe following examples illustrate some scenarios for managing CPU\nresources using Slurm. Many additional scenarios are possible. In\neach example, it is assumed that all CPUs on each node are available\nfor allocation.\nExample Node and Partition Configuration\nExample 1: Allocation of whole nodes\nExample 2: Simple allocation of cores as consumable resources\nExample 3: Consumable resources with balanced allocation across nodes\nExample 4: Consumable resources with minimization of resource fragmentation\nExample 5: Consumable resources with cyclic distribution of tasks to nodes\nExample 6: Consumable resources with default allocation and plane distribution of tasks to nodes\nExample 7: Consumable resources with overcommitment of CPUs to tasks\nExample 8: Consumable resources with resource sharing between jobs\nExample 9: Consumable resources on multithreaded node, allocating only one thread per core\nExample 10: Consumable resources with task affinity and core binding\nExample 11: Consumable resources with task affinity and socket binding, Case 1\nExample 12: Consumable resources with task affinity and socket binding, Case 2\nExample 13: Consumable resources with task affinity and socket binding, Case 3\nExample 14: Consumable resources with task affinity and customized allocation and distribution\nExample 15: Consumable resources with task affinity to optimize the performance of a multi-task,\nmulti-thread job\nExample 16: Consumable resources with task cgroup\nExample Node and Partition Configuration\n\nFor these examples, the Slurm cluster contains the following nodes:\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\nn1\n\n\nn2\n\n\nn3\n\n\n\n\nNumber\n\t\t\t\tof Sockets\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n\n\nNumber\n\t\t\t\tof Cores per Socket\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n\n\nTotal\n\t\t\t\tNumber of Cores\n\n\n8\n\n\n8\n\n\n8\n\n\n8\n\n\n\n\nNumber\n\t\t\t\tof Threads (CPUs) per Core\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n\n\nTotal\n\t\t\t\tNumber of CPUs\n\n\n8\n\n\n8\n\n\n8\n\n\n16\n\n\n\nAnd the following partitions:\n\n\n\n\n\n\nPartitionName\n\n\nregnodes\n\n\nhypernode\n\n\n\n\nNodes\n\n\nn0\n\t\t\t\t n1  n2\n\n\nn3\n\n\n\n\nDefault\n\n\nYES\n\n\n-\n\n\n\nThese entities are defined in slurm.conf as follows:Nodename=n0 NodeAddr=node0 Sockets=2 CoresPerSocket=4 ThreadsPerCore=1 Procs=8\nNodename=n1 NodeAddr=node1 Sockets=2 CoresPerSocket=4 ThreadsPerCore=1 Procs=8 State=IDLE\nNodename=n2 NodeAddr=node2 Sockets=2 CoresPerSocket=4 ThreadsPerCore=1 Procs=8 State=IDLE\nNodename=n3 NodeAddr=node3 Sockets=2 CoresPerSocket=4 ThreadsPerCore=2 Procs=16 State=IDLE\nPartitionName=regnodes Nodes=n0,n1,n2 OverSubscribe=YES Default=YES State=UP\nPartitionName=hypernode Nodes=n3 State=UP\nThese examples show the use of the\ncons_tres plugin.Example 1: Allocation of whole nodes\n\nAllocate a minimum of two whole nodes to a job.slurm.conf options:SelectType=select/linear\nCommand line:srun --nodes=2 ...\nComments:SelectType=select/linear --nodes=2Example 2: Simple allocation of cores as consumable resources\n\nA job requires 6 CPUs (2 tasks and 3 CPUs per task with no overcommitment). Allocate the 6 CPUs as consumable resources\nfrom a single node in the default partition.slurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core\nCommand line:srun --nodes=1-1 --ntasks=2 --cpus-per-task=3 ...\nComments:The SelectType configuration options define cores as consumable resources.\nThe --nodes=1-1 srun option\n restricts the job to a single node. The following table shows a possible pattern of allocation\n  for this job.\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\nn1\n\n\nn2\n\n\n\n\nNumber\n\t\t\t\tof Allocated CPUs\n\n\n6\n\n\n0\n\n\n0\n\n\n\n\nNumber\n\t\t\t\tof Tasks\n\n\n2\n\n\n0\n\n\n0\n\n\n\n\nExample 3: Consumable resources with balanced allocation across nodes\n\nA job requires 9 CPUs (3 tasks and 3 CPUs per task with no overcommitment).\nAllocate 3 CPUs from each of the 3 nodes in the default partition.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core\nCommand line:srun --nodes=3-3 --ntasks=3 --cpus-per-task=3 ...\nComments:The options specify the following conditions for the job: 3 tasks, 3 unique CPUs\n per task, using exactly 3 nodes. To satisfy these conditions, Slurm must\n  allocate 3 CPUs from each node. The following table shows the allocation\n   for this job.\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\nn1\n\n\nn2\n\n\n\n\nNumber\n\t\t\t\tof Allocated CPUs\n\n\n3\n\n\n3\n\n\n3\n\n\n\n\nNumber\n\t\t\t\tof Tasks\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\nExample 4: Consumable resources with minimization of resource fragmentation\n\nA job requires 12 CPUs (12 tasks and 1 CPU per task with no overcommitment). Allocate\nCPUs using the minimum number of nodes and the minimum number of sockets required for\nthe job in order to minimize fragmentation of allocated/unallocated CPUs in the cluster.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core,CR_CORE_DEFAULT_DIST_BLOCK\nCommand line:srun --ntasks=12 ...\nComments:The default allocation method across nodes is block. This minimizes the number of nodes\n used for the job. The configuration option \n CR_CORE_DEFAULT_DIST_BLOCK sets the default allocation method within a\n node to block. This minimizes the number of sockets used for the job within a node.\n The combination of these two methods causes Slurm to allocate the 12 CPUs using the\n minimum required number of nodes (2 nodes) and sockets (3 sockets).The following\n table shows a possible pattern of allocation for this job.\n\n\n\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\nn1\n\n\nn2\n\n\n\n\nSocket id\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n4\n\n\n4\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\nNumber of\n\t\t\t\tTasks\n\n\n8\n\n\n4\n\n\n0\n\n\n\n\nExample 5: Consumable resources with cyclic distribution of tasks to nodes\n\nA job requires 12 CPUs (6 tasks and 2 CPUs per task with no overcommitment). Allocate\n6 CPUs each from 2 nodes in the default partition. Distribute tasks to nodes cyclically.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core\nCommand line:srun --nodes=2-2 --ntasks-per-node=3 --distribution=cyclic \n--ntasks=6 --cpus-per-task=2 ...\nComments:The options specify the following conditions for the job: 6 tasks, 2 unique CPUs per task,\nusing exactly 2 nodes, and with 3 tasks per node. To satisfy these conditions, Slurm\nmust allocate 6 CPUs from each of the 2 nodes. The \n--distribution=cyclic option causes the tasks to be distributed to the nodes in a\nround-robin fashion. The following table shows a possible pattern of allocation and\ndistribution for this job.\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\nn1\n\n\nn2\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n6\n\n\n6\n\n\n0\n\n\n\n\nNumber of\n\t\t\t\tTasks\n\n\n3\n\n\n3\n\n\n0\n\n\n\n\nDistribution\n\t\t\t\tof Tasks to Nodes, by Task id\n\n\n\n\t\t\t\t024\n\n\n\t\t\t\t135\n\n-\n\n\n\nExample 6: Consumable resources with default allocation and\nplane distribution of tasks to nodes\n\nA job requires 16 CPUs (8 tasks and 2 CPUs per task with no overcommitment).\nUse all 3 nodes in the default partition. Distribute tasks to each node in blocks of two in a round-robin fashion.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core\nCommand line:srun --nodes=3-3 --distribution=plane=2 --ntasks=8 --cpus-per-task=2 ...\nComments:The options specify the following conditions for the job: 8 tasks, 2 unique CPUs\nper task, using all 3 nodes in the partition. To satisfy these conditions using\nthe default allocation method across nodes (block), Slurm allocates 8 CPUs from\nthe first node, 6 CPUs from the second node and 2 CPUs from the third node.\nThe --distribution=plane=2 option causes Slurm\nto distribute tasks in blocks of two to each of the nodes in a round-robin fashion,\nsubject to the number of CPUs allocated on each node.  So, for example, only 1 task\nis distributed to the third node because only 2 CPUs were allocated on that node and\neach task requires 2 CPUs. The following table shows a possible pattern of allocation\nand distribution for this job.\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\nn1\n\n\nn2\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n8\n\n\n6\n\n\n2\n\n\n\n\nNumber of\n\t\t\t\tTasks\n\n\n4\n\n\n3\n\n\n1\n\n\n\n\nDistribution\n\t\t\t\tof Tasks to Nodes, by Task id\n\n\n0\n\t\t\t\t 15 6\n\n\n2\n\t\t\t\t 37\n\n\n4\n\n\n\n\n\nExample 7: Consumable resources with overcommitment of CPUs to tasks\n\nA job has 20 tasks. Run the job in a single node.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core\nCommand line:srun --nodes=1-1 --ntasks=20 --overcommit ...\nComments:The\n--overcommit option allows the job to\nrun in only one node by overcommitting CPUs to tasks.The following table shows\n a possible pattern of allocation and distribution for this job.\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\nn1\n\n\nn2\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n8\n\n\n0\n\n\n0\n\n\n\n\nNumber of\n\t\t\t\tTasks\n\n\n20\n\n\n0\n\n\n0\n\n\n\n\nDistribution\n\t\t\t\tof Tasks to Nodes, by Task id\n\n\n0\n\t\t\t\t- 19\n\n\n-\n\n\n-\n\n\n\n\nExample 8: Consumable resources with resource sharing between jobs\n\n2 jobs each require 6 CPUs (6 tasks per job with no overcommitment).\nRun both jobs simultaneously in a single node.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core\nCommand line:srun --nodes=1-1 --nodelist=n0 --ntasks=6 --oversubscribe ...\nsrun --nodes=1-1 --nodelist=n0 --ntasks=6 --oversubscribe ...\nComments:The --nodes=1-1 and --nodelist=n0\nsrun options together restrict both jobs to node n0. The\nOverSubscribe=YES option in the partition definition plus\nthe --oversubscribe srun option allows the two\njobs to oversubscribe CPUs on the node.\nExample 9: Consumable resources on multithreaded node,\nallocating only one thread per core\n\nA job requires 8 CPUs (8 tasks with no overcommitment). Run the job on node n3,\nallocating only one thread per core.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_CPU\nCommand line:srun --partition=hypernode --ntasks=8 --hint=nomultithread ...\nComments:The CR_CPU configuration\noption enables the allocation of only one thread per core.\nThe --hint=nomultithread\nsrun option causes Slurm to allocate only one thread from each core to\nthis job. The following table shows a possible pattern of allocation\nfor this job.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNodename\n\n\nn3\n\n\n\n\nSocket id\n\n\n0\n\n\n1\n\n\n\n\nCore id\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n\n\nCPU id\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n4\n\n\n4\n\n\n\n\nAllocated\n\t\t\t\tCPU ids\n\n\n0\n\t\t\t\t  2   4   6\n\n\n8\n\t\t\t\t 10  12  14\n\n\n\n\nExample 10: Consumable resources with task affinity and core binding\n\nA job requires 6 CPUs (6 tasks with no overcommitment). Run the job in a\nsingle node in the default partition. Apply core binding to each task.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core\nTaskPlugin=task/affinity\nCommand line:srun --nodes=1-1 --ntasks=6 --cpu-bind=cores ...\nComments:Using the default allocation method within nodes (cyclic), Slurm allocates\n3 CPUs on each socket of 1 node. Using the default distribution method\nwithin nodes (cyclic), Slurm distributes and binds each task to an allocated\ncore in a round-robin fashion across the sockets. The following table shows\na possible pattern of allocation, distribution and binding for this job.\nFor example, task id 2 is bound to CPU id 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\n\n\nSocket id\n\n\n0\n\n\n1\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n3\n\n\n3\n\n\n\n\nAllocated\n\t\t\t\tCPU ids\n\n\n0 1 2\n\n\n4 5 6\n\n\n\n\nBinding of\n\t\t\t\tTasks to CPUs\n\n\nCPU id\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n\nTask id\n\n\n0\n\n\n2\n\n\n4\n\n\n-\n\n\n1\n\n\n3\n\n\n5\n\n\n-\n\n\n\n\nExample 11: Consumable resources with task affinity and socket binding, Case 1\n\nA job requires 6 CPUs (6 tasks with no overcommitment). Run the job in\na single node in the default partition. Apply socket binding to each task.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core\nTaskPlugin=task/affinity\nCommand line:srun --nodes=1-1 --ntasks=6 --cpu-bind=sockets ...\nComments:Using the default allocation method within nodes (cyclic), Slurm allocates 3\nCPUs on each socket of 1 node. Using the default distribution method within nodes\n(cyclic), Slurm distributes and binds each task to all of the allocated CPUs in\none socket in a round-robin fashion across the sockets. The following table shows\na possible pattern of allocation, distribution and binding for this job. For\nexample, task ids 1, 3 and 5 are all bound to CPU ids 4, 5 and 6.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\n\n\nSocket id\n\n\n0\n\n\n1\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n3\n\n\n3\n\n\n\n\nAllocated\n\t\t\t\tCPU ids\n\n\n0 1 2\n\n\n4 5 6\n\n\n\n\nBinding of\n\t\t\t\tTasks to CPUs\n\n\nCPU id\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n\nTask ids\n\n\n0\n\t\t\t\t 2  4\n\n\n-\n\n\n1\n\t\t\t\t 3  5\n\n\n-\n\n\n\n\nExample 12: Consumable resources with task affinity and socket binding, Case 2\n\nA job requires 6 CPUs (2 tasks with 3 cpus per task and no overcommitment). Run the job in\na single node in the default partition. Allocate cores using the block allocation method.\nDistribute cores using the block distribution method. Apply socket binding to each task.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core,CR_CORE_DEFAULT_DIST_BLOCK\nTaskPlugin=task/affinity\nCommand line:srun --nodes=1-1 --ntasks=2 --cpus-per-task=3 --cpu-bind=sockets \n--distribution=block:block ...\nComments:Using the block allocation method, Slurm allocates 4\nCPUs on one socket and 2 CPUs on the other socket of one node. Using the block distribution method within\nnodes, Slurm distributes 3 CPUs to each task.  Applying socket binding, Slurm binds each task to all\nallocated CPUs in all sockets in which the task has a distributed CPU. The following table shows\na possible pattern of allocation, distribution and binding for this job. In this example, using the\nblock allocation method CPU ids 0-3 are allocated on socket id 0 and CPU ids 4-5 are allocated on\nsocket id 1.  Using the block distribution method, CPU ids 0-2 were distributed to task id 0, and CPU ids\n3-5 were distributed to task id 1.  Applying socket binding, task id 0 is therefore bound to the allocated\nCPUs on socket 0, and task id 1 is bound to the allocated CPUs on both sockets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\n\n\nSocket id\n\n\n0\n\n\n1\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n4\n\n\n2\n\n\n\n\nAllocated\n\t\t\t\tCPU ids\n\n\n0 1 2 3\n\n\n4 5\n\n\n\n\nBinding of\n\t\t\t\tTasks to CPUs\n\n\nCPU id\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n\nTask ids\n\n\n0 1\n\n\n1\n\n\n-\n\n\n\n\nExample 13: Consumable resources with task affinity and socket binding, Case 3\n\nA job requires 6 CPUs (2 tasks with 3 cpus per task and no overcommitment). Run the job in\na single node in the default partition. Allocate cores using the block allocation method.\nDistribute cores using the cyclic distribution method. Apply socket binding to each task.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core,CR_CORE_DEFAULT_DIST_BLOCK\nTaskPlugin=task/affinity\nCommand line:srun --nodes=1-1 --ntasks=2 --cpus-per-task=3 --cpu-bind=sockets \n--distribution=block:cyclic ...\nComments:Using the block allocation method, Slurm allocates 4\nCPUs on one socket and 2 CPUs on the other socket of one node. Using the cyclic distribution method within\nnodes, Slurm distributes 3 CPUs to each task.  Applying socket binding, Slurm binds each task to all\nallocated CPUs in all sockets in which the task has a distributed CPU. The following table shows\na possible pattern of allocation, distribution and binding for this job. In this example, using the\nblock allocation method CPU ids 0-3 are allocated on socket id 0 and CPU ids 4-5 are allocated on\nsocket id 1.  Using the cyclic distribution method, CPU ids 0, 1 and 4 were distributed to task id 0, and CPU ids\n2, 3 and 5 were distributed to task id 1.  Applying socket binding, both tasks are therefore bound to the\nallocated CPUs on both sockets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\n\n\nSocket id\n\n\n0\n\n\n1\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n4\n\n\n2\n\n\n\n\nAllocated\n\t\t\t\tCPU ids\n\n\n0 1 2 3\n\n\n4 5\n\n\n\n\nBinding of\n\t\t\t\tTasks to CPUs\n\n\nCPU id\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n\nTask ids\n\n\n0 1\n\n\n0 1\n\n\n-\n\n\n\nExample 14: Consumable resources with task affinity and\ncustomized allocation and distribution\n\nA job requires 18 CPUs (18 tasks with no overcommitment). Run the job in the\ndefault partition. Allocate 6 CPUs on each node using block allocation within\nnodes. Use cyclic distribution of tasks to nodes and block distribution of\ntasks for CPU binding.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core,CR_CORE_DEFAULT_DIST_BLOCK\nTaskPlugin=task/affinity\nCommand line:srun --nodes=3-3 --ntasks=18 --ntasks-per-node=6 \n--distribution=cyclic:block --cpu-bind=cores ...\nComments:This example shows the use of task affinity with customized allocation of CPUs and\ndistribution of tasks across nodes and within nodes for binding. The srun options\nspecify the following conditions for the job: 18 tasks, 1 unique CPU per task, using\nall 3 nodes in the partition, with 6 tasks per node.\nThe CR_CORE_DEFAULT_DIST_BLOCK\nconfiguration option specifies block allocation within nodes. To satisfy these\nconditions, Slurm allocates 6 CPUs on each node, with 4 CPUs allocated on one socket\nand 2 CPUs on the other socket. The \n--distribution=cyclic:block option specifies cyclic distribution of\ntasks to nodes and block distribution of tasks to CPUs within nodes for binding.\nThe following table shows a possible pattern of allocation, distribution and binding\nfor this job. For example, task id 10 is bound to CPU id 3 on node n1.\n\n\n\n\nNodename\n\n\nn0\n\n\nn1\n\n\nn2\n\n\n\n\nSocket id\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n4\n\n\n2\n\n\n4\n\n\n2\n\n\n4\n\n\n2\n\n\n\n\nAllocated\n\t\t\t\tCPU ids\n\n\n0 1 2 3 4 5\n\n\n0 1 2 3 4 5\n\n\n0 1 2 3 4 5\n\n\n\n\nNumber of\n\t\t\t\tTasks\n\n\n6\n\n\n6\n\n\n6\n\n\n\n\nDistribution\n\t\t\t\tof Tasks to Nodes, by Task id\n\n\n0\n\t\t\t3\n\t\t\t6\n\t\t\t9\n\t\t\t12\n\t\t\t15\n\t\t\t\n\n\n1\n\t\t\t4\n\t\t\t7\n\t\t\t10\n\t\t\t13\n\t\t\t16\n\t\t\t\n\n\n2\n\t\t\t5\n\t\t\t8\n\t\t\t11\n\t\t\t14\n\t\t\t17\n\t\t\t \n\n\n\n\nBinding of\n\t\t\t\tTasks to CPUs\n\n\nCPU id\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n\nTask id\n\n\n0\n\n\n3\n\n\n6\n\n\n9\n\n\n12\n\n\n15\n\n\n-\n\n\n-\n\n\n1\n\n\n4\n\n\n7\n\n\n10\n\n\n13\n\n\n16\n\n\n-\n\n\n-\n\n\n2\n\n\n5\n\n\n8\n\n\n11\n\n\n14\n\n\n17\n\n\n-\n\n\n-\n\n\n\nExample 15: Consumable resources with task affinity to\noptimize the performance of a multi-task, multi-thread job\n\nA job requires 9 CPUs (3 tasks and 3 CPUs per task with no overcommitment). Run\nthe job in the default partition, managing the CPUs to optimize the performance\nof the job.slurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core,CR_CORE_DEFAULT_DIST_BLOCK\nTaskPlugin=task/affinity\nCommand line:srun --ntasks=3 --cpus-per-task=3 --ntasks-per-node=1 --cpu-bind=cores ...\nComments:To optimize the performance of this job, the user wishes to allocate 3 CPUs from each of\n3 sockets and bind each task to the 3 CPUs in a single socket. The\nSelectTypeParameters configuration option specifies\na consumable resource type of cores and block allocation within nodes. The\nTaskPlugin\nconfiguration option enables task affinity. The srun options specify the following\nconditions for the job: 3 tasks, with 3 unique CPUs per task, with 1 task per node. To satisfy\nthese conditions, Slurm allocates 3 CPUs from one socket in each of the 3 nodes in the default partition. The\n--cpu-bind=cores option causes Slurm to bind\neach task to the 3 allocated CPUs on the node to which it is distributed. The\nfollowing table shows a possible pattern of allocation, distribution and binding\nfor this job. For example, task id 2 is bound to CPU ids 0, 1 and 2 on socket id 0 of node n2.\n\n\n\n\nNodename\n\n\nn0\n\n\nn1\n\n\nn2\n\n\n\n\nSocket id\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n3\n\n\n0\n\n\n3\n\n\n0\n\n\n3\n\n\n0\n\n\n\n\nAllocated\n\t\t\t\tCPU ids\n\n\n0 1 2\n\n\n0 1 2\n\n\n0 1 2\n\n\n\n\nNumber of\n\t\t\t\tTasks\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\nDistribution\n\t\t\t\tof Tasks to Nodes, by Task id\n\n\n0\n\n\n1\n\n\n2\n\n\n\n\nBinding of\n\t\t\t\tTasks to CPUs\n\n\nCPU id\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n\nTask id\n\n\n0\n\n\n-\n\n\n1\n\n\n-\n\n\n2\n\n\n--\n\n\n\n\nExample 16: Consumable resources with task cgroup\n\nA job requires 6 CPUs (6 tasks with no overcommitment). Run the job in a\nsingle node in the default partition.\nslurm.conf options:SelectType=select/cons_tres\nSelectTypeParameters=CR_Core\nTaskPlugin=task/cgroup\ncgroup.conf options:ConstrainCores=yes\nCommand line:srun --nodes=1-1 --ntasks=6 ...\nComments:The task/cgroup plugin currently supports only the block method for\nallocating cores within nodes. Slurm distributes tasks to the cores but\nwithout cpu binding, each task has access to all the allocated CPUs.\nThe following table shows a possible pattern of allocation, distribution\nand binding for this job.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNodename\n\n\nn0\n\n\n\n\nSocket id\n\n\n0\n\n\n1\n\n\n\n\nNumber of\n\t\t\t\tAllocated CPUs\n\n\n4\n\n\n2\n\n\n\n\nAllocated\n\t\t\t\tCPU ids\n\n\n0 1 2 3\n\n\n4 5\n\n\n\n\nBinding of\n\t\t\t\tTasks to CPUs\n\n\nCPU id\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n\nTask id\n\n\n0-5\n\n\n0-5\n\n\n-\n\n\n\n\nThe task/cgroup plugin does not bind tasks to CPUs. To bind tasks to CPUs and\nfor access to all task distribution options, the task/affinity plugin can be\nused with the task/cgroup plugin:\nTaskPlugin=task/cgroup,task/affinityLast modified 04 January 2024"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        }
    ]
}