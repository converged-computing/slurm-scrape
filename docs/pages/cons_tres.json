{
    "url": "https://slurm.schedmd.com/cons_tres.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Consumable Resources in Slurm",
            "content": "Slurm, using the default node allocation plug-in, allocates nodes to jobs in\nexclusive mode.  This means that even when all the resources within a node are\nnot utilized by a given job, another job will not have access to these resources.\nNodes possess resources such as processors, memory, swap, local\ndisk, etc. and jobs consume these resources. The exclusive use default policy\nin Slurm can result in inefficient utilization of the cluster and of its nodes\nresources.\nSlurm's cons_tres plugin is available to\nmanage resources on a much more fine-grained basis as described below.\nUsing the Consumable Trackable Resource Plugin: select/cons_tres\n\nThe Consumable Trackable Resources (cons_tres) plugin has been built\nto work with several resources. It can track a Board, Socket, Core, CPU, Memory\nas well as any combination of the logical processors with Memory:\nCPU (CR_CPU): CPU as a consumable resource.\n\nNo notion of sockets, cores, or threads.\nOn a multi-core system CPUs will be cores.\nOn a multi-core/hyperthread system CPUs will be threads.\nOn a single-core system CPUs are CPUs.\n\nBoard (CR_Board): Baseboard as a consumable resource.\nSocket (CR_Socket): Socket as a consumable resource.\nCore (CR_Core): Core as a consumable resource.\nMemory (CR_Memory) Memory only as a\n  consumable resource. NOTE: CR_Memory assumes OverSubscribe=Yes\nSocket and Memory (CR_Socket_Memory): Socket\n  and Memory as consumable resources.\nCore and Memory (CR_Core_Memory): Core and\n  Memory as consumable resources.\nCPU and Memory (CR_CPU_Memory) CPU and Memory\n  as consumable resources.\nAll CR_* parameters assume OverSubscribe=No or\nOverSubscribe=Force EXCEPT for CR_MEMORY which assumes\nOverSubscribe=Yes.The cons_tres plugin also provides functionality specifically\nrelated to GPUs.Additional parameters available for the cons_tres plugin:\nDefCpuPerGPU: Default number of CPUs allocated per GPU.\nDefMemPerGPU: Default amount of memory allocated per GPU.\nAdditional job submit options available for the cons_tres plugin:\n--cpus-per-gpu=: Number of CPUs for every GPU.\n--gpus=: Count of GPUs for entire job allocation.\n--gpu-bind=: Bind task to specific GPU(s).\n--gpu-freq=: Request specific GPU/memory frequencies.\n--gpus-per-node=: Number of GPUs per node.\n--gpus-per-socket=: Number of GPUs per socket.\n--gpus-per-task=: Number of GPUs per task.\n--mem-per-gpu=: Amount of memory for each GPU.\nsrun's -B extension for sockets, cores, and threads is\nignored within the node allocation mechanism when CR_CPU or\nCR_CPU_MEMORY is selected. It is used to compute the total\nnumber of tasks when -n is not specified.In the cases where Memory is a consumable resource, the RealMemory\nparameter must be set in the slurm.conf to define a node's amount of real\nmemory.The job submission commands (salloc, sbatch and srun) support the options\n--mem=MB and --mem-per-cpu=MB, permitting users to specify\nthe maximum amount of real memory required per node or per allocated CPU.\nThis option is required in the environments where Memory is a consumable\nresource. It is important to specify enough memory since Slurm will not allow\nthe application to use more than the requested amount of real memory. The\ndefault value for --mem is inherited from DefMemPerNode. See\nsrun(1) for more details.Using --overcommit or -O is allowed. When the process to\nlogical processor pinning is enabled by using an appropriate TaskPlugin\nconfiguration parameter, the extra processes will time share the allocated\nresources.The Consumable Trackable Resource plugin is enabled via the SelectType\nparameter in the slurm.conf.\n# Excerpt from sample slurm.conf file\nSelectType=select/cons_tres\nGeneral CommentsSlurm's default select/linear plugin is using a best fit algorithm\nbased on number of consecutive nodes.The select/cons_tres plugin is enabled or disabled cluster-wide.In the case where select/linear is enabled, the normal Slurm\nbehaviors are not disrupted. The major change users see when using the\nselect/cons_tres plugin is that jobs can be\nco-scheduled on nodes when resources permit it. Generic resources (such as GPUs)\ncan also be tracked individually with this plugin.\nThe rest of Slurm, such as srun and its options (except srun -s ...), etc. are not\naffected by this plugin. Slurm is, from the user's point of view, working the\nsame way as when using the default node selection scheme.The --exclusive srun option allows users to request nodes in\nexclusive mode even when consumable resources is enabled. See\nsrun(1) for details. srun's -s or --oversubscribe is incompatible with the consumable\nresource environment and will therefore not be honored. Since this\nenvironment's nodes are shared by default, --exclusive allows users to\nobtain dedicated nodes.The --oversubscribe and --exclusive options are mutually\nexclusive when used at job submission. If both options are set when submitting\na job, the job submission command used will fatal.Examples of CR_Memory, CR_Socket_Memory, and CR_CPU_Memory\ntype consumable resources\n\n\n# sinfo -lNe\nNODELIST     NODES PARTITION  STATE  CPUS  S:C:T MEMORY\nhydra[12-16]     5 allNodes*  ...       4  2:2:1   2007\nUsing select/cons_tres plug-in with CR_Memory\nExample:\n# srun -N 5 -n 20 --mem=1000 sleep 100 &  <-- running\n# srun -N 5 -n 20 --mem=10 sleep 100 &    <-- running\n# srun -N 5 -n 10 --mem=1000 sleep 100 &  <-- queued and waiting for resources\n\n# squeue\nJOBID PARTITION   NAME   USER ST  TIME  NODES NODELIST(REASON)\n 1820  allNodes  sleep sballe PD  0:00      5 (Resources)\n 1818  allNodes  sleep sballe  R  0:17      5 hydra[12-16]\n 1819  allNodes  sleep sballe  R  0:11      5 hydra[12-16]\nUsing select/cons_tres plug-in with CR_Socket_Memory (2 sockets/node)\nExample 1:\n# srun -N 5 -n 5 --mem=1000 sleep 100 &        <-- running\n# srun -n 1 -w hydra12 --mem=2000 sleep 100 &  <-- queued and waiting for resources\n\n# squeue\nJOBID PARTITION   NAME   USER ST  TIME  NODES NODELIST(REASON)\n 1890  allNodes  sleep sballe PD  0:00      1 (Resources)\n 1889  allNodes  sleep sballe  R  0:08      5 hydra[12-16]\n\nExample 2:\n# srun -N 5 -n 10 --mem=10 sleep 100 & <-- running\n# srun -n 1 --mem=10 sleep 100 & <-- queued and waiting for resourcessqueue\n\n# squeue\nJOBID PARTITION   NAME   USER ST  TIME  NODES NODELIST(REASON)\n 1831  allNodes  sleep sballe PD  0:00      1 (Resources)\n 1830  allNodes  sleep sballe  R  0:07      5 hydra[12-16]\nUsing select/cons_tres plug-in with CR_CPU_Memory (4 CPUs/node)\nExample 1:\n# srun -N 5 -n 5 --mem=1000 sleep 100 &  <-- running\n# srun -N 5 -n 5 --mem=10 sleep 100 &    <-- running\n# srun -N 5 -n 5 --mem=1000 sleep 100 &  <-- queued and waiting for resources\n\n# squeue\nJOBID PARTITION   NAME   USER ST  TIME  NODES NODELIST(REASON)\n 1835  allNodes  sleep sballe PD  0:00      5 (Resources)\n 1833  allNodes  sleep sballe  R  0:10      5 hydra[12-16]\n 1834  allNodes  sleep sballe  R  0:07      5 hydra[12-16]\n\nExample 2:\n# srun -N 5 -n 20 --mem=10 sleep 100 & <-- running\n# srun -n 1 --mem=10 sleep 100 &       <-- queued and waiting for resources\n\n# squeue\nJOBID PARTITION   NAME   USER ST  TIME  NODES NODELIST(REASON)\n 1837  allNodes  sleep sballe PD  0:00      1 (Resources)\n 1836  allNodes  sleep sballe  R  0:11      5 hydra[12-16]\n\nExample of Node Allocations Using Consumable Resource Plugin\n\nThe following example illustrates the different ways four jobs\nare allocated across a cluster using (1) Slurm's default allocation method\n(exclusive mode) and (2) a processor as consumable resource\napproach.It is important to understand that the example listed below is a\ncontrived example and is only given here to illustrate the use of CPUs as\nconsumable resources. Job 2 and Job 3 call for the node count to equal\nthe processor count. This would typically be done because\nthat one task per node requires all of the memory, disk space, etc. The\nbottleneck would not be processor count.Trying to execute more than one job per node will almost certainly severely\nimpact a parallel job's performance.\nThe biggest beneficiary of CPUs as consumable resources will be serial jobs or\njobs with modest parallelism, which can effectively share resources. On many\nsystems with larger processor count, jobs typically run one fewer task than\nthere are processors to minimize interference by the kernel and daemons.The example cluster is composed of 4 nodes (10 CPUs in total):\nlinux01 (with 2 processors), \nlinux02 (with 2 processors), \nlinux03 (with 2 processors), and\nlinux04 (with 4 processors). \nThe four jobs are the following:\n[2] srun -n 4 -N 4 sleep 120 &\n[3] srun -n 3 -N 3 sleep 120 &\n[4] srun -n 1 sleep 120 &\n[5] srun -n 3 sleep 120 &\nThe user launches them in the same order as listed above.Using Slurm's Default Node Allocation (Non-shared Mode)\n\nThe four jobs have been launched and 3 of the jobs are now\npending, waiting to get resources allocated to them. Only Job 2 is running\nsince it uses one CPU on all 4 nodes. This means that linux01 to linux03 each\nhave one idle CPU and linux04 has 3 idle CPUs.\n# squeue\nJOBID PARTITION   NAME  USER  ST  TIME  NODES NODELIST(REASON)\n    3       lsf  sleep  root  PD  0:00      3 (Resources)\n    4       lsf  sleep  root  PD  0:00      1 (Resources)\n    5       lsf  sleep  root  PD  0:00      1 (Resources)\n    2       lsf  sleep  root   R  0:14      4 linux[01-04]\nOnce Job 2 is finished, Job 3 is scheduled and runs on\nlinux01, linux02, and linux03. Job 3 is only using one CPU on each of the 3\nnodes. Job 4 can be allocated onto the remaining idle node (linux04) so Job 3\nand Job 4 can run concurrently on the cluster.Job 5 has to wait for idle nodes to be able to run.\n# squeue\nJOBID PARTITION   NAME  USER  ST  TIME  NODES NODELIST(REASON)\n    5       lsf  sleep  root  PD  0:00      1 (Resources)\n    3       lsf  sleep  root   R  0:11      3 linux[01-03]\n    4       lsf  sleep  root   R  0:11      1 linux04\nOnce Job 3 finishes, Job 5 is allocated resources and can run.The advantage of the exclusive mode scheduling policy is\nthat the a job gets all the resources of the assigned nodes for optimal\nparallel performance. The drawback is\nthat jobs can tie up large amount of resources that it does not use and which\ncannot be shared with other jobs.Using a Processor Consumable Resource Approach\n\nWe will run through the same scenario again using the cons_tres\nplugin and CPUs as the consumable resource. The output of squeue shows that we\nhave 3 out of the 4 jobs allocated and running. This is a 2 running job\nincrease over the default Slurm approach. Job 2 is running on nodes linux01\nto linux04. Job 2's allocation is the same as for Slurm's default allocation\nwhich is that it uses one CPU on each of the 4 nodes. Once Job 2 is scheduled\nand running, nodes linux01, linux02 and linux03 still have one idle CPU each\nand node linux04 has 3 idle CPUs. The main difference between this approach and\nthe exclusive mode approach described above is that idle CPUs within a node\nare now allowed to be assigned to other jobs.It is important to note that\nassigned doesn't mean oversubscription. The consumable resource approach\ntracks how much of each available resource (in our case CPUs) must be dedicated\nto a given job. This allows us to prevent per node oversubscription of\nresources (CPUs).Once Job 2 is running, Job 3 is\nscheduled onto node linux01, linux02, and Linux03 (using one CPU on each of the\nnodes) and Job 4 is scheduled onto one of the remaining idle CPUs on Linux04.Job 2, Job 3, and Job 4 are now running concurrently on the cluster.\n# squeue\nJOBID PARTITION   NAME  USER  ST  TIME  NODES NODELIST(REASON)\n    5       lsf  sleep  root  PD  0:00      1 (Resources)\n    2       lsf  sleep  root   R  0:13      4 linux[01-04]\n    3       lsf  sleep  root   R  0:09      3 linux[01-03]\n    4       lsf  sleep  root   R  0:05      1 linux04\n\n# sinfo -lNe\nNODELIST     NODES PARTITION       STATE CPUS MEMORY TMP_DISK WEIGHT FEATURES REASON\nlinux[01-03]     3      lsf*   allocated    2   2981        1      1   (null) none\nlinux04          1      lsf*   allocated    4   3813        1      1   (null) none\nOnce Job 2 finishes, Job 5, which was pending, is allocated available resources and is then\nrunning as illustrated below:\n# squeue\nJOBID PARTITION   NAME  USER  ST  TIME  NODES NODELIST(REASON)\n   3       lsf   sleep  root   R  1:58      3 linux[01-03]\n   4       lsf   sleep  root   R  1:54      1 linux04\n   5       lsf   sleep  root   R  0:02      3 linux[01-03]\n# sinfo -lNe\nNODELIST     NODES PARTITION       STATE CPUS MEMORY TMP_DISK WEIGHT FEATURES REASON\nlinux[01-03]     3      lsf*   allocated    2   2981        1      1   (null) none\nlinux04          1      lsf*        idle    4   3813        1      1   (null) none\nJob 3, Job 4, and Job 5 are now running concurrently on the cluster.\n# squeue\nJOBID PARTITION   NAME  USER  ST  TIME  NODES NODELIST(REASON)\n    5       lsf  sleep  root   R  1:52      3 linux[01-03]\nJob 3 and Job 4 have finished and Job 5 is still running on nodes linux[01-03].The advantage of the consumable resource scheduling policy\nis that the job throughput can increase dramatically. The overall job\nthroughput and productivity of the cluster increases, thereby reducing the\namount of time users have to wait for their job to complete as well as\nincreasing the overall efficiency of the use of the cluster. The drawback is\nthat users do not have entire nodes dedicated to their jobs by default.We have added the --exclusive option to srun (see\nsrun(1) for more details),\nwhich allows users to specify that they would like\ntheir nodes to be allocated in exclusive mode.\nThis is to accommodate users who might have mpi/threaded/openMP\nprograms that will take advantage of all the CPUs on a node but only need\none mpi process per node.Last modified 18 May 2023"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        }
    ]
}