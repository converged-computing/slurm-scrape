{
    "url": "https://slurm.schedmd.com/publications.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Presentations",
            "content": "Note that older presentations may contain outdated information.Presentations from Cray User Group, May 2024\nSlurm 24.08 24.05, and Beyond,\nTim Wickberg, SchedMD\nPresentations from SC23, November 2023\nSlurm and/or/vs Kubernetes,\nTim Wickberg, SchedMD\nSlurm 23.02, 23.11, and Beyond,\nTim Wickberg, SchedMD\nPresentations from Slurm User Group Meeting, September 2023\n\nKeynote: Improving quinoa through the development of genetic and genomic resources,\nDavid Jarvis, Brigham Young University\nNever use Slurm HA again: Solve all your problems with Kubernetes,\nChris Samuel and Doug Jacobsen, NERSC\n\nContainers in Slurm,\nScott Hilton, SchedMD\n\nOptimizing Diverse Workloads and Resource Usage with Slurm,\nChansup Byun et al, LLSC\nSlurm's REST API,\nNathan Rini, SchedMD\nBuild a flexible and powerful High Performance Computing foundation with\u00a0Google Cloud,\nVolker Eyrich (Google) and Joshua Fryer (Recursion)\nDemand Driven Cluster Elasticity,\nMike Fazio, Dow\nField Notes 7 \u2013 How to make the most of Slurm and avoid common issues,\nJason Booth, SchedMD\n\nAccelerating Genomics Research Machine Learning with Slurm,\nWilly Markuske, San Diego Supercomputing Center (SDSC)\nSaving Power with Slurm,\nOle Nielsen, Technical University of Denmark (DTU)\nRunning Flux in Slurm,\nRyan Day, LLNL\nSite Report: CINECA experience with Slurm,\nAlessandro Marani, CINECA\nStep Management Enhancements,\nBrian Christiansen, SchedMD\nSystem and Job Scheduling Simulation For Enhancing Production HPC,\nVivian Hafener, LANL\nSite Update: Georgia Institute of Technology,\nMarian Zvada and Aaron Jezghani, Georgia Tech\nBuilding Blocks in the Cloud: Scaling LEGO Engineering with AWS High-Performance Computing,\nBrian Skjerven and Matt Vaughn, AWS\nSlurm 23.02, 23.11, and Beyond (Roadmap),\nTim Wickberg, SchedMD\nPresentations from Dell HPC Community, September 2023\nSlurm and/or/vs Kubernetes,\nTim Wickberg, SchedMD\nPresentations from Cray User Group, May 2023\nSlurm 23.02, 23.11, and Beyond,\nTim Wickberg, SchedMD\nPresentations from SC22, November 2022\nSlurm \u2665 Containers,\nNate Rini & Tim Wickberg, SchedMD\nDoing More with Slurm Advanced Capabilities,\nShawn Hoopes, SchedMD\nSlurm 22.05, 23.02, and Beyond,\nTim Wickberg, SchedMD\nSlurm and/or/versus Kubernetes,\nTim Wickberg, SchedMD\nAccelerating HPC and AI with Slurm and SchedMD,\nNick Ihli, SchedMD\nPresentations from the HPC Containers Advisory Working Group, November 2022\nSlurm \u2665 Containers, Nathan Rini, SchedMD\nPresentations from CNCF Research End User Group, October 2022\nSlurm Container Support,\nVideo,\nNathan Rini, SchedMD\n\nPresentations from Slurm User Group Meeting, September 2022\nField Notes 6: From The Frontlines of Slurm Support,\nVideo,\nJason Booth, SchedMD\nPathfinding into the clouds,\nVideo,\nOle Nielsen, Technical University of Denmark (DTU)\nOCI Containers with scrun\nVideo,\nNathan Rini, SchedMD\nLBNL Site Report,\nVideo,\nWei Feinstein, Lawrence Berkeley National Laboratory\nCloudy, With a Chance of Dynamic Nodes,\nVideo,\nNick Ihli, SchedMD\nBurst Buffer Lua Plugin For Lustre,\nVideo,\nKota Tsuyuzaki / Rikimaru Honjo / Yusuke Kaneko / Kohei Tahara, NTT Computer and Data Science Laboratory / NTT TechnoCross Corporation\n22.05, 23.02, and Beyond,\nVideo,\nTim Wickberg, SchedMD\nEDA Slurm Cluster on AWS,\nVideo,\nAllan Carter, AWS\nPresentations from NHR Container Workshop, December 2021\nContainers and Slurm,\nVideo,\nNathan Rini, SchedMD\nPresentations from SC21, November 2021\nBOF: Slurm Birds of a Feather,\nVideo,\nTim Wickberg, SchedMD\nPresentations from Slurm User Group Meeting, September 2021\nField Notes 5: From The Frontlines of Slurm Support,\nVideo,\nJason Booth, SchedMD\nREST API and also Containers,\nVideo,\nNathan Rini, SchedMD\nburst_buffer/lua and slurmscriptd,\nVideo,\nMarshall Garey, SchedMD\nSlurm in the Clouds,\nVideo,\nNick Ihli, SchedMD\nSlurm 21.08 and Beyond,\nVideo,\nTim Wickberg, SchedMD\nPresentations from SC20, November 2020\nBOF: Slurm Birds of a Feather,\nTim Wickberg, SchedMD\nPresentations from Slurm User Group Meeting, September 2020\nField Notes 4: From The Frontlines of Slurm Support,\nVideo,\nJason Booth, SchedMD\nCloud and Stuff,\nVideo,\nBrian Christiansen, SchedMD\nREST API,\nVideo,\nNathan Rini, SchedMD\nSlurm 20.11 and Beyond,\nVideo,\nTim Wickberg, SchedMD\nPresentations from PEARC HPCSYSPROS Workshop, August 2020\nREST API, Nathan Rini, SchedMD\nPresentations from Slurm User Group Meeting, September 2019\n\nWelcome,\nDanny Auble, SchedMD\nTutorial: TRES and Banking,\nBrian Christiansen, SchedMD\nTechnical: GPU Scheduling and the cons_tres plugin,\nChad Vizino and Morris Jette, SchedMD\nSite Report: LANL,\nJoseph 'Joshi' Fullop, LANL\nTutorial: Cgroups and pam_slurm_adopt,\nMarshall Garey, SchedMD\nSite Report: Enabling and Scaling Diverse Work Loads Efficiently With Slurm,\nChansup Byun et al., MIT Lincoln Laboratory\nTutorial: Priority and Fair Trees,\nShawn Hoopes, SchedMD\nTechnical: Slurm: Seamless Integration With Unprivileged Containers,\nLuke Yeager et al., NVIDIA\nTechnical: REST API,\nNathan Rini, SchedMD\nTechnical: Job Container plugin for managing node local namespaces,\nAditi Gaur, NERSC\n\nTechnical: VMs and containers for a Slurm-based development cluster,\nFran\u00e7ois Daikhat\u00e9, CEA\nTechnical: High Throughput Computing,\nBroderick Gardner, SchedMD\nSite Report: Slurm on Sherlock,\nKilian Cavalotti, Stanford Research Computing Center\nTechnical: Slurm + GCP,\nBrian Christiansen (SchedMD) and Keith Binder (Google)\nSite Report: ORNL,\nMatt Ezell, ORNL\nTechnical: Monitoring Slurm with a Splunk App,\nNicole Dobson, LANL\nSite Report: NERSC,\nChris Samuel, NERSC\nTutorial: Troubleshooting,\nAlbert Gil and Jason Booth, SchedMD\nTechnical: Slurm Account Synchronization with UNIX Groups and Users,\nOle Nielsen, Technical University of Denmark (DTU)\nTechnical: A fully configurable HPC web portal for managing Slurm jobs,\nPatrice Calegari, Atos\nTechnical: Slurm 19.05,\nTim Wickberg, SchedMD\nTechnical: Slurm 20.02 and Beyond,\nTim Wickberg, SchedMD\nTechnical: Field Notes From A MadMan,\nTim Wickberg, SchedMD\nPresentations from Slurm User Group Meeting, September 2018\n\n\nTutorial: Slurm Overview,\nFelip Moll Marqu\u00e8s, SchedMD\nTechnical: Workload Management Requirements for an Interactive Computing e-Infrastructure,\nSadaf Alam (CSCS) and the ICEI team (BSC, CEA, CINECA, CSCS, J\u00fclich)\nTechnical: Slurm in a Container Only World \u2014 Are We Crazy?,\nPaul Peltz and Lowell Wofford (LANL)\nTechnical: Kraken - A stateful approach to cluster management,\nPaul Peltz and Lowell Wofford (LANL)\nTechnical: A Declarative Programming Style Job Submission Filter,\nDouglas Jacobsen, NERSC\nTechnical: Generalized Hypercube (GHC) \u2014 A Topology Plugin,\nM. Clayer and A. Faure, Atos\nTechnical: Keeping Accounts Consistent Across Clusters Using LDAP and YAML,\nChristian Cl\u00e9men\u00e7on, Ewan Roche, Ricardo Silva (EPFL)\nTechnical: Real-Time Job Monitoring Using An Extended slurmctld Generic Plugin \u2014 Introducing the plugin architecture SPACE,\nMike Arnhold, Ulf Markwardt, and Danny Rotscher (Dresden)\nTechnical: Scheduling by Trackable Resource (cons_tres),\nMorris Jette and Dominik Bartkiewicz, SchedMD\n\nTechnical: Slurm 18.08 Overview,\nBrian Christiansen, SchedMD\n\nTechnical: Layout For Checkpoint Restart on Specialized Blades,\nBill Brophy, Martin Perry, Doug Parisek, and Steve Mehlberg (Atos)\nSite Report: CEA Site Report,\nRegine Gaudin, CEA\nSite Report: Colliding High Energy Physics With HPC, Cloud, and Parallel Filesystems,\nCarolina Lindqvist, Pablo Llopis, and Nils H\u00f8imyr (CERN)\nTechnical: Slurm Simulator Improvements and Evaluation,\nMarco D'Amico, Ana Jokanovic, Julita Corbalan (BSC)\nSite Report: CETA-CIEMAT Site Report,\nAlfonso Pardo, CETA-CIEMAT\nSite Report: Tuning Slurm the CSCS Way,\nMiguel Gila, CSCS\nTechnical: Workload Scheduling and Power Management,\nMorris Jette and Alejandro Sanchez, SchedMD\nSite Report: LANL Site Report \u2014 One Year Post Migration,\nJoseph 'Joshi' Fullop, LANL\nTechnical: Field Notes Mark 2: Random Musings From Under A New Hat,\nTim Wickberg, SchedMD\nPresentations from Slurm Booth and Birds of a Feather, SC17, November 2017\nBooth: Slurm Overview,\nBrian Christiansen, Marshall Garey, Isaac Hartung (SchedMD)\nBooth: Heterogeneous Job Support,\nMorris Jette, Tim Wickberg (SchedMD)\nBooth: From Moab to Slurm: 12 HPC Systems in 2 Months,\nPaul Peltz, Los Alamost National Laboratory\nBooth: PMIx Multi-Cluster Operations,\nRalph H. Castain\nBooth: Federated Cluster Support,\nBrian Christiansen, SchedMD\nBooth: PMIx Plugin with UCX Support,\nArtem Polyakov, Mellanox\nBOF: Slurm Birds of a Feather,\nTim Wickberg, SchedMD\nPresentations from Slurm User Group Meeting, September 2017\n\nKeynote: Supernova Cosmology & Supercomputing,\nAlex Kim, Lawrence Berkeley National Laboratory\nTutorial: Introduction to Slurm,\nTim Wickberg, SchedMD\nTechnical: SLURMFS \u2014 Resource Manager File System for Slurm,\nSteven Senator, Los Alamos National Laboratory\nTechnical: Federated Cluster Support,\nBrian Christiansen and Danny Auble, SchedMD\nTechnical: Utilizing Slurm and Passive Nagios Plugins for Scalable KNL Compute Node Monitoring,\nTony Quan and Basil Lalli, NERSC/LBNL\nTechnical: Field Notes From the Frontlines of Slurm Support,\nTim Wickberg, SchedMD\n\nTechnical: Towards Modular Supercomputing with Slurm\nDorian Krause et al., JSC\nTechnical: Heterogeneous Job Support,\nMorris Jette, SchedMD\nTechnical: cli_filter \u2014 command line filtration, manipulation, and introspection of job submissions\n(PDF version),\nDouglas Jacobsen, NERSC\nTechnical: Slurm \u2014 Some Slightly Unconventional Use Cases,\nChris Hill (MIT, Rajul Kumar (Northeastern), Evan Weinberg and Naved Ansari (BU), Tim Donahue\nTechnical: Managing Diversity in Complex Workloads in a Complex Environment,\nNicholas Cardo, CSCS\n\nTechnical: SELinux policy for Slurm,\nGilles Wiber and Mathieu Blanc (CEA), M'hamed Bouaziz and Liana Bozga (Atos)\nSite Report: From Moab to Slurm: 12 HPC Systems in 2 Months,\nPeltz, Fullop, Jennings, Senator, Grunau (Los Alamost National Laboratory)\nSite Report: NERSC Site Report,\nJames Botts and Douglas Jacobsen\nTechnical: Slurm Roadmap - 17.11, 18.08, and Beyond,\nDanny Auble, Morris Jette, Tim Wickberg (SchedMD)\nTechnical: New Statistics Using TRES,\nBill Brophy, Martin Perry, Thomas Cadeau (Atos)\nTechnical: Enabling web-based interactive notebooks on geographically distributed HPC resources,\nAlexandre Beche, EPFL\nTechnical: Slurm Singularity Spank Plugin,\nMartin Perry, Steve Mehlberg, Thomas Cadeau (Atos)\nSite Report: A Slurm Odyssey: Slurm at Harvard FAS Research Computing,\nPaul Edmond\nSite Report: LLSC Adoption of Slurm for Managing Diverse Resources and Workloads,\nChansup Byun et al., MIT Lincoln Laboratory\nSite Report: Cyfronet Site Report \u2014 Improving Slurm Usability and Monitoring,\nM Pawlik, J. Budzowski, L. Flis, P Laso\u0144, M. Magry\u015b\nTechnical: When you have a hammer, everything looks like a nail \u2014 Checkpoint/restart in Slurm\nManuel Rodr\u00edguez-Pascual, J.A. Mor\u00ed\u00f1igo, and Rafael Mayo-Garc\u00eda, CIEMAT\nPresentations from Slurm Booth and Birds of a Feather, SC16, November 2016\nBooth: Process Management Interface - Exascale (PMIx),\nRalph H. Castain\nBooth: Bull Slurm Related Developments,\nJob Packs demo video,\nYiannis Georgiou, Bull AtoS\nBooth: Transition Hangout (a.k.a. how we converted to Slurm),\nRyan Cox (BYU), Bruce Pfaff (NASA)\nBooth: Expanding Serial Analysis with Slurm Arrays,\nChristopher Coffey, Northern Arizona University\nBooth: Intel HPC Orchestrator,\nTom Krueger, Intel\nBooth: Slurm Overview,\nMoe Jette, SchedMD\nBOF: Slurm State of the Union; v16.05, v17.02 and Beyond,\nTim Wickberg, SchedMD\nPresentations from Slurm User Group Meeting, September 2016\n\nKeynote: Computer-aided drug design for novel anti-cancer agents,\nDr. Zoe Cournia (Biomedical Research Foundation, Academy of Athens)\nTechnical: Overview of Slurm Version 16.05,\nDanny Auble (SchedMD), Yiannis Georgiou (Bull)\nTechnical: MCS (Multi-Category Security) Plugin,\nAline Roy, CEA\nTechnical: Slurm Burst Buffer Integration,\nDavid Paul, NERSC\nTechnical: Slurm Configuration Impact on Benchmarking,\nJos\u00e9 Mor\u00ed\u00f1igo, Manuel Rodr\u00edguez-Pascual, and Rafael Mayo-Garc\u00eda, CIEMAT\nTechnical: Real-time monitoring Slurm jobs with InfluxDB,\nCarlos Fenoy Garc\u00eda\nTechnical: Optimising HPC resource allocation through monitoring,\nAlexandre Beche, EPFL\nTechnical: Simunix, a large scale platform simulator,\nDavid Glesser and Adrien Faure, Bull AtoS\nSite Report: Swiss National Supercomputer Centre (CSCS),\nNicholas Cardo\nTechnical: Configure a Slurm cluster with Ansible,\nJohan Guldmyr, CSC\nTechnical: Checkpoint/restart in Slurm: current status and new developments,\nManuel Rodr\u00edguez-Pascual, J.A. Mor\u00ed\u00f1igo, and Rafael Mayo-Garc\u00eda, CIEMAT\nTechnical: Intel Knights Landing (KNL),\nMorris Jette and Tim Wickberg, SchedMD\nTechnical: Job Packs - A New Slurm Feature For Enhanced Support of Heterogeneuous Resources,\nAndry Razafinjatovo, Martin Perry, and Yiannis Georgiou (Bull AtoS), Matthieu Hautreux (CEA)\nTechnical: Improving system utilization under strict power budget using the layouts,\nDineshkumar Rajagopal, Yiannis Georgiou, and David Glesser, Bull AtoS\nTechnical: High definition power and energy monitoring support,\nThomas Cadeau and Yiannis Georgiou, Bull AtoS\nTechnical: Federated Cluster Scheduling,\nDominik Bartkiewicz and Brian Christiansen, SchedMD\nTechnical: Slurm Roadmap - SchedMD,\nDanny Auble, SchedMD\nTechnical: Slurm Roadmap - Bull,\nYiannis Georgiou and Andry Razafinjatovo, Bull AtoS\nSite Report: Electricit\u00e9 de France(EDF),\nC\u00e9cile Yoshikawa\nSite Report: Leibniz-Rechenzentrum (LRZ),\nJuan Pancorbo Armada\nSite Report: NERSC Site Report - One Year Of Slurm,\nDouglas Jacobsen\nSite Report: Experience using Slurm on ARIS HPC System,\nNikos Nikoloutsakos, GRNET\nPresentations from Slurm Booth and Birds of a Feather, SC15, November 2015\nBooth: PMIx - Enabling Application-driven Execution at Exascale ,\nRalph H. Castain\nBooth: NASA NCCS Site Update,\nBruce Pfaff, NASA\nBooth: Brigham Young University - Site Report,\nRyan Cox, BYU\nBooth: Slurm Overview\nBrian Christiansen and Danny Auble, SchedMD\nBooth: Never Port Your Code Again - Docker functionality with Shifter using SLURM,\nShane Canon, NERSC\nBooth: Slurm Burst Buffer Support,\nTim Wickberg, SchedMD\nBooth: Slurm Overview and Elasticsearch Plugin\nAlejandro Sanchez, SchedMD\nBooth: All Things TRES,\nBrian Christiansen, SchedMD\nBOF: Slurm Version 15.08,\nDanny Auble, SchedMD\nBOF: Improving Backfilling by using Machine Learning to Predict Running Times in SLURM,\nDavid Glesser, Bull\nPresentations from Slurm User Group Meeting, September 2015\n\nKeynote: 10-years of Computing and Atmospheric Research at NASA: 1 day per day,\nBill Putnam, NASA\nTechnical: Overview of Slurm Version 15.08,\nMorris Jette and Danny Auble (SchedMD), Yiannis Georgiou (Bull)\nTechnical: Trackable Resources (TRES),\nBrian Christiansen and Danny Auble, SchedMD\nTechnical: Message Aggregation,\nDanny Auble (SchedMD), Yiannis Georgiou and Martin Perry (Bull)\nTechnical:  Slurm Burst Buffer Support,\nMorris Jette (SchedMD), Tim Wickberg (GW)\nTechnical: Partition QOS,\nDanny Auble, SchedMD\nTechnical: Slurm Power Management Support,\nMorris Jette, SchedMD\nTechnical: Slurm Layouts Framework,\nMatthieu Hautreux, CEA\nTechnical: Power Adaptive Scheduling,\nYiannis Georgiou and David Glesser (Bull),\nMatthieu Hautreux (CEA),\nDenis Trystram (LIG)\nTechnical: Never Port Your Code Again - Docker functionality with Shifter using SLURM,\nDouglas Jacobsen, James Botts, and Shane Canon, NERSC\nTechnical: Increasing cluster throughput with Slurm and rCUDA,\nFederico Silla, Technical University of Valencia Spain\nTechnical: Running Virtual Machines in a Slurm Batch System,\nUlf Markwardt, Technische Universit\u00e4t Dresden\nTechnical: Supporting SR-IOV and IVSHMEM in MVAPICH2 on Slurm,\nXiaoyi Lu, Jie Zhang, et. al., The Ohio State University \nTechnical: Heterogeneous Resources and MPMD (aka Job Pack),\nRod Schultz and Martin Perry (Atos), Matthieu Hautreaux (CEA), Yiannis Georgiou (Atos)\nTechnical: Towards multi-objective resource selection,\nDineshkumar Rajagopal, David Glesser, Yiannis Georgiou, BULL\nTechnical: Enhancing Startup Performance of Parallel Applications with SLURM,\nSourav Chakraborty, et. al., OSU / LLNL\nTechnical: Adaptable Profile-Driven TestBed (\"Apt\"),\nBrian Haymore, The University of Utah\nTechnical: Using and Modifying the BSC Slurm Workload Simulator,\nStephen Trofinoff and Massimo Benini, CSCS\nTechnical: Improving Job Scheduling by using Machine Learning,\nDavid Glesser, Yiannis Georgiou (BULL) and Denis Trystram (LIG)\nTechnical: Federated Cluster Scheduling,\nBrian Christiansen and Danny Auble, SchedMD\nTechnical: Native SLURM on the XC30,\nDoug Jacobsen, James Botts, NERSC\nTechnical: Slurm Roadmap - Versions 16.05 and beyond,\nMorris Jette and Danny Auble (SchedMD), Yiannis Georgiou (Bull)\nTechnical: Exascale Process Management Interface,\nRalph Castain (Intel), Joshua Ladd (Mellanox), Artem Polyakov (Mellanox), David Bigagli (SchedMD), Gary Brown (Adaptive Computing)\nSite Report: Brigham Young University,\nRyan Cox, BYU\nSite Report: University of South Florida,\nJohn DeSantis, USF\nSite Report: NASA Center for Climate Simulation,\nBruce Pfaff, NASA\nSite Report: J\u00fclich Supercomputing Centre,\nDorian Krause, JSC\nSite Report: The George Washington University,\nTim Wickberg, GW\nPresentations from Slurm Birds of a Feather and the Slurm booth, SC14, November 2014\nSlurm Overview,\nDanny Auble and Brian Christiansen, SchedMD\nSlurm Version 14.11,\nJacob Jenson, SchedMD\nSlurm Version 15.08 Roadmap,\nJacob Jenson, SchedMD\nSlurm on Cray systems,\nDavid Wallace, Cray\nFair Tree: Fairshare Algorithm for Slurm\nRyan Cox and Levi Morrison (Brigham Young University)\nVLSCI Site Report,\nChris Samuel (VLSCI)\nPresentations from Slurm User Group Meeting, September 2014\nGroup photo\nPaul Hsi (MIT Kavli Institute for Astrophysics and Space Research)\nWelcoming Address\nColin McMurtrie (Swiss National Supercomputing Centre, CSCS)\n\nOverview of Slurm Versions 14.03 and 14.11\nJacob Jenson (SchedMD) and Yiannis Georgiou (Bull)\nWarewulf Node Health Check\nJacqueline Scoggins and Michael Jennings (Lawrence Berkeley National Lab)\nSlurm Process Isolation\nBill Brophy, Martin Perry and Yiannis Georgiou (Bull),\nMorris Jette (SchedMD),\nMatthieu Hautreux (CEA)\nImproving message forwarding logic in Slurm\nRod Schultz, Martin Perry and Yiannis Georgiou (Bull),\nMatthieu Hautreux (CEA),\nDanny Auble and Morris Jette (SchedMD)\nTuning Slurm Scheduling for Optimal Responsiveness and Utilization\nMorris Jette (SchedMD)\nImproving HPC applications scheduling with predictions\nbased on automatically-collected historical data\nCarlos Fenoy Garc\u00eda (Barcelona  Supercomputing Centre)\nOStrich: Fair Scheduler for Burst Submissions of Parallel Job\nKrzysztof Rzadca (University of Warsaw) and Filip Skalski ((University of Warsaw / Google)\nAdaptive Resource and Job Management for limited power consumption\nYiannis Georgiou and David Glesser (Bull),\nMatthieu Hautreux (CEA),\nDenis Trystram (University Grenoble-Alpes)\nIntroducing Energy based fair-share scheduling\nYiannis Georgiou and David Glesser (Bull),\nKrzysztof Rzadca (University of Warsaw),\nDenis Trystram (University Grenoble-Alpes)\nHigh Performance Data movement between Lustre and Enterprise storage systems\nAamir Rashid (Terascala)\nExtending Slurm with Support for Remote GPU Virtualization\nSergio Iserte, Adri\u00e1n Castell\u00f3, Rafael Mayo,\nEnrique S. Quintana-Ortl\u00ed, Federico Silla, Jose Duato\n(Universitat Jaume and Universitat Polit\u00e8cnica de Val\u00e8ncia)\nSLURM Migration Experience\nJacqueline Scoggins (Lawrence Berkeley National Lab)\nBudget Checking Plugin for SLURM\nHuub Stoffers (SURF sara)\nFair Tree: Fairshare Algorithm for Slurm\nRyan Cox and Levi Morrison (Brigham Young University)\nIntegrating Layouts Framework in Slurm\nThomas Cadeau and Yiannis Georgiou (Bull),\nMatthieu Hautreux (CEA)\nTopology-Aware Resource Selectiont\nEmmanuel Jeannot, Guillaume Mercier, and Ad\u00e8le Villiermet (Inria)\nSlurm Inter-Cluster Project (presentation),\n(paper)\nStephen Trofinoff (CSCS)\nSlurm Native Workload Management on Cray Systems\nMorris Jette (SchedMD)\nSlurm on Cray Systems\nJason Coverston (Cray)\nSLURM Roadmap\nYiannis Georgiou (Bull), Morris Jette and Jacob Jenson (SchedMD)\nPrivate /tmp for each job using SPANK\nMagnus Jonsson (Ume\u00e5 Universitet)\nICM Warsaw University Site Report\nDominik Bartkiewicz and Marcin Stolarek (ICM Warsaw University)\niVEC Site Report\nAndrew Elwell (iVEC)\nCEA Site Report\nMatthieu Hautreux (CEA)\nSwiss National Supercomputing Centre site report\nMassimo Benini (Swiss National Supercomputing Centre, CSCS)\nAalto University Site Report\nJanne Blomqvist, Ivan Degtyarenko and Mikko Hakala (Aalto University)\nThe George Washington University site report\nTim Wickberg (George Washington University)\nPresentations from Slurm Birds of a Feather, SC13, November 2013\nSlurm Workload Manager Project Report,\nMorris Jette and Danny Auble, SchedMD\nBull's Slurm Roadmapt,\nEric Monchalin, Bull\nNative Slurm on Cray XC30,\nDavid Wallace, Cray\nPresentations from Slurm User Group Meeting, September 2013\nGroup photo\nWelcome: Welcome\nMorris Jette (SchedMD)\nKeynote: Future Outlook for Advanced Computing\nDona Crawford (LLNL)\nTechnical: Overview of Slurm version 2.6,\nMorris Jette and Danny Auble (SchedMD), Yiannis Georgiou (Bull) \nTutorial: Energy Accounting and External Sensor Plugins,\nYiannis Georgiou, Martin Perry, Thomas Cadeau (Bull), Danny Auble (SchedMD)\n\nTechnical: Debugging Large Machines,\nMatthieu Hautreux (CEA)\nTechnical: Creating easy to use HPC portals with NICE EnginFrame and Slurm,\nAlberto Falzone, Paolo Maggi (Nice Software)\n\nTutorial: Usage of new profiling functionalities,\n Rod Schultz, Yiannis Georgiou (Bull) Danny Auble (SchedMD)\n\nTechnical: Fault Tolerant Workload Management,\nDavid Bigagli, Morris Jette (SchedMD)\nTechnical: Slurm Layouts Framework,\nYiannis Georgiou (Bull) Matthieu Hautreux (CEA)\nTechnical: License Management,\nBill Brophy (Bull)\nTechnical: Multi-Cluster Management,\nJuan Pancorbo Armada (IRZ)\n\nTechnical: \nDepth Oblivious Hierarchical Fairshare Priority Factor,\nFrancois Daikhate, Matthieu Hautreux (CEA)\nTechnical: Refactoring ALPS,\nDave Wallace (Cray)\nSite Report: CEA,\nFrancois Diakhate, Francis Belot, Matthieu Hautreux (CEA)\nSite Report: George Washington University,\nTim Wickberg (George Washington University)\nSite Report: Brigham Young University,\nRyan Cox (BYU)\n\nSite Report: Technische Universitat Dresden,\nDr. Ulf Markwardt (Technische Universit\u00e4t Dresden)\nTechnical: Slurm Roadmap,\nMorris Jette, Danny Auble (SchedMD), Yiannis Georgiou (Bull)\nPresentations from Slurm Birds of a Feather, SC12, November 2012\nSlurm Workload Manager Project Report,\nMorris Jette and Danny Auble, SchedMD\nUsing Slurm for Data Aware Scheduling in the Cloud,\nMartijn de Vries, BrightComputing\nSlurm Roadmap,\nEric Monchalin, Bull\nMapReduce Support in Slurm: Releasing the Elephant,\nRalph H. Castain, Wangda Tan, Jimmy Cao and Michael Lv, Greenplum/EMC\nSlurm at Rensselaer,\nTim Wickberg, Rensselaer Polytechnic Institute\nPresentations from Slurm User Group Meeting, October 2012\nGroup photo\nKeynote: The OmSs Programming Model and its links to resource managers,\nJesus Labarta, BSC\nSlurm Status Report,\nMorris Jette and Danny Auble, SchedMD\nSite Report: BSC/RES,\nAlejandro Lucero and Carles Fenoy, BSC\nSite Report: CSCS,\nStephen Trofinoff, CSCS\nSite Report: CEA,\nMatthieu Hautreux, CEA\nSite Report: CETA/CIEMAT,\nAlfonso Pardo Diaz, CIEMAT\nPorting Slurm to Bluegene/Q,\nDon Lipari, LLNL\nTutorial: Slurm Database Use, Accounting and Limits,\nDanny Auble, SchedMD\nTutorial: The Slurm Scheduler Design,\nDon Lipari, LLNL\nTutorial: Cgroup Support on Slurm,\nMartin Perry and Yiannis Georgiou (Bull), Matthieu Hautreux (CEA)\nTutorial: Kerberos and Slurm using Auks,\nMatthieu Hautreux, CEA\n\nKeynote: Challenges in Evaluating Parallel Job Schedulers,\nDror Feitelson, Hebrew University\nIntegration of Slurm with IBM's Parallel Environment,\nMorris Jette and Danny Auble, SchedMD\nSlurm Bank,\nJimmy Tang and Paddy Doyle, Trinity College, Dublin\nUsing Slurm for Data Aware Scheduling in the Cloud,\nMartijn de Vries, Bright Computing\nEnhancing Slurm with Energy Consumption Monitoring and Control Features,\nYiannis Georgiou, Bull\nMapReduce Support in SLURM:\nReleasing the Elephant,\nRalph H. Castain, et. al., Greenplum/EMC\nUsing Slurm via Python,\nMark Roberts (AWE) and Stephan Gorget (EDF)\nHigh Throughput Computing with Slurm,\nMorris Jette and Danny Auble, SchedMD\nEvaluating Scalability and Efficiency of the Resource and Job Management System on large HPC clusters,\nYiannis Georgiou (Bull) and Matthieu Hautreux (CEA)\nInteger Programming Based Herogeneous CPU-GPU Clusters,\nSeren Soner, Bogazici University\nJob Resource Utilization as a Metric for Clusters Comparison and Optimization,\nJoseph Emeras, INRIA/LIG\nPresentations from the Sixth Linux Collaboration Summit, April 2012\nResource Management with Linux Control Groups in HPC Clusters\nYiannis Georgiou, Bull\nPresentations from Slurm Birds Of a Feather, SC11, November 2011\nSlurm Version 2.3 and Beyond\nMorris Jette, SchedMD LLC\nBull's Slurm Roadmap\nEric Monchalin, Bull\nCloud Bursting with Slurm and Bright Cluster Manager\nMartijn de Vries, Bright Computing\nPresentations from Slurm User Group Meeting, September 2011\nGroup photo\nBasic Configuration and Usage,\nRod Schultz, Groupe Bull\nSLURM: Advanced Usage,\nRod Schultz, Groupe Bull\nCPU Management Allocation and Binding,\nMartin Perry, Groupe Bull\nConfiguring Slurm for HA,\nDavid Egolf and Bill Brophy, Groupe Bull\nSlurm Resources isolation through cgroups,\nYiannis Georgiou (Groupe Bull), Matthieu Hautreux (CEA)\nSlurm Operation on Cray XT and XE,\nMoe Jette, SchedMD LLC\n\nChallenges and Opportunities for Exascale\nResource Management and How Today's Petascale Systems are Guiding the Way,\nWilliam Kramer, NCSA\nCEA Site report,\nMatthieu Hautreux, CEA\nLLNL Site Report,\nDon Lipari, LLNL\nSlurm Version 2.3 and Beyond,\nMoe Jette, SchedMD LLC\nSlurm Simulator,\nAlejandro Lucero, BSC\nProposed Design for Enhanced Enterprise-wide Scheduling,\nDon Lipari, LLNL\nBright Cluster Manager & SLURM,\nRobert Stober, Bright Computing\nJob Step Management in User Space,\nMoe Jette, SchedMD LLC\nSlurm Operation IBM BlueGene/Q,\nDanny Auble, SchedMD LLC\nPresentations from Slurm Birds Of a Feather, SC10, November 2010\n\nSlurm Version 2.2: Features and Release Plans,\nMorris Jette, Danny Auble and Donald Lipari, Lawrence Livermore National Laboratory\nPresentations from Slurm User Group Meeting, October 2010\nGroup photo\n\nSlurm: Resource Management from the Simple to the Sophisticated,\nMorris Jette and Danny Auble, Lawrence Livermore National Laboratory\n\nSlurm at CEA,\nMatthieu Hautreux, CEA/DAM/DIF\n\nSlurm Support for Linux Control Groups,\nMartin Perry, Bull Information Systems\n\nSlurm at BSC,\nCarles Fenoy and Alejandro Lucero, Barcelona Supercomputing Center\n\nPorting Slurm to the Cray XT and XE,\nNeil Stringfellow and Gerrit Renker, Swiss National Supercomputer Centre\n\nReal Scale Experimentations of Slurm Resource and Job Management System,\nYiannis Georgiou, Bull Information Systems\n\nSlurm Version 2.2: Features and Release Plans,\nMorris Jette and Danny Auble, Lawrence Livermore National Laboratory\nPresentations from Slurm Birds of a Feather, SC09, November 2009\n\nSlurm Community Meeting,\nMorris Jette, Danny Auble and Don Lipari, Lawrence Livermore National Laboratory\nPresentations from Slurm Birds of a Feather, SC08, November 2008\n\nHigh Scalability Resource Management with SLURM,\nMorris Jette, Lawrence Livermore National Laboratory\n\nSlurm Status Report,\nMorris Jette and Danny Auble, Lawrence Livermore National Laboratory\nOther Presentations\n\nSlurm Version 1.3,\nMorris Jette and Danny Auble, Lawrence Livermore National Laboratory\n(May 2008)\n\nManaging Clusters with Moab and Slurm,\nMorris Jette and Donald Lipari, Lawrence Livermore National Laboratory\n(May 2008)\n\nResource Management at LLNL, Slurm Version 1.2,\nMorris Jette, Danny Auble and Chris Morrone, Lawrence Livermore National Laboratory\n(April 2007)\n\nResource Management Using Slurm,\nMorris Jette, Lawrence Livermore National Laboratory\n(Tutorial, The 7th International Conference on Linux Clusters, May 2006)\nPublications\nEnergy Accounting and Control with Slurm Resource and Job Management System,\nYiannis Georgiou, et. al.\n(ICDCN 2014, January 2014)\n\nEvaluating scalability and efficiency of the Resource and Job Management System on large HPC Clusters,\nYiannis Georgiou (BULL S.A.S, France); Matthieu Hautreux (CEA-DAM, France)\n(16th Workshop on Job Scheduling Strategies for Parallel Processing, May 2012)\nGreenSlot: Scheduling Energy Consumption in Green Datacenters,\nInigo Goiri, et. al.\n(SuperComputing 2011, November 2011)\n\nContributions for Resource and Job Management in High Performance Computing,\nYiannis Georgiou, Universite Joseph Fourier\n(Thesis, December 2010)\n\nCaos NSA and Perceus: All-in-one Cluster Software Stack,\nJeffrey B. Layton,\nLinux Magazine,\n5 February 2009.\nEnhancing an Open Source Resource Manager with Multi-Core/Multi-threaded Support,\nS. M. Balle and D. Palermo,\nJob Scheduling Strategies for Parallel Processing,\n2007.\n\n\nSlurm: Simple Linux Utility for Resource Management [PDF],\nM. Jette and M. Grondona,\nProceedings of ClusterWorld Conference and Expo,\nSan Jose, California, June 2003.\nSlurm: Simple Linux Utility for Resource Management,\nA. Yoo, M. Jette, and M. Grondona,\nJob Scheduling Strategies for Parallel Processing,\nvolume 2862 of Lecture Notes in Computer Science,\npages 44-60,\nSpringer-Verlag, 2003.\nInterview\nRCE 10: Slurm (podcast):\nBrock Palen and Jeff Squyres speak with Morris Jette and\nDanny Auble of LLNL about Slurm.Other Resources\nLearning Chef: Compute Cluter with Slurm\nA Slurm Cookbook by Adam DeConinckLast modified 07 May 2024"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        }
    ]
}