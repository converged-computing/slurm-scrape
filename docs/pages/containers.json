{
    "url": "https://slurm.schedmd.com/containers.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Containers Guide",
            "content": "Contents\nOverview\nKnown limitations\nPrerequisites\nRequired software\nExample configurations for various OCI Runtimes\nTesting OCI runtime outside of Slurm\nRequesting container jobs or steps\nIntegration with Rootless Docker\nIntegration with Podman\nOCI Container bundle\nExample OpenMPI v5 + PMIx v4 container\nContainer support via Plugin\n\nShifter\nENROOT and Pyxis\nSarus\n\nOverviewContainers are being adopted in HPC workloads.\nContainers rely on existing kernel features to allow greater user control over\nwhat applications see and can interact with at any given time. For HPC\nWorkloads, these are usually restricted to the\nmount namespace.\nSlurm natively supports the requesting of unprivileged OCI Containers for jobs\nand steps.Setting up containers requires several steps:\n\nSet up the kernel and a\n    container runtime.\nDeploy a suitable oci.conf file accessible to\n    the compute nodes (examples below).\nRestart or reconfigure slurmd on the compute nodes.\nGenerate OCI bundles for containers that are needed\n    and place them on the compute nodes.\nVerify that you can run containers directly through\n    the chosen OCI runtime.\nVerify that you can request a container through\n    Slurm.\n\nKnown limitations\n\nThe following is a list of known limitations of the Slurm OCI container\nimplementation.\nAll containers must run under unprivileged (i.e. rootless) invocation.\nAll commands are called by Slurm as the user with no special\npermissions.\nCustom container networks are not supported. All containers should work\nwith the \"host\"\nnetwork.\nSlurm will not transfer the OCI container bundle to the execution\nnodes. The bundle must already exist on the requested path on the\nexecution node.\nContainers are limited by the OCI runtime used. If the runtime does not\nsupport a certain feature, then that feature will not work for any job\nusing a container.\noci.conf must be configured on the execution node for the job, otherwise the\nrequested container will be ignored by Slurm (but can be used by the\njob or any given plugin).\nPrerequisitesThe host kernel must be configured to allow user land containers:$ sudo sysctl -w kernel.unprivileged_userns_clone=1Docker also provides a tool to verify the kernel configuration:\n$ dockerd-rootless-setuptool.sh check --force\n[INFO] Requirements are satisfied\nRequired software:\n\n\nFully functional\n\nOCI runtime. It needs to be able to run outside of Slurm first.\nFully functional OCI bundle generation tools. Slurm requires OCI\nContainer compliant bundles for jobs.\nExample configurations for various OCI Runtimes\n\n\nThe OCI Runtime\nSpecification provides requirements for all compliant runtimes but\ndoes not expressly provide requirements on how runtimes will use\narguments. In order to support as many runtimes as possible, Slurm provides\npattern replacement for commands issued for each OCI runtime operation.\nThis will allow a site to edit how the OCI runtimes are called as needed to\nensure compatibility.\n\nFor runc and crun, there are two sets of examples provided.\nThe OCI runtime specification only provides the start and create\noperations sequence, but these runtimes provides a much more efficient run\noperation. Sites are strongly encouraged to use the run operation\n(if provided) as the start and create operations require that\nSlurm poll the OCI runtime to know when the containers have completed execution.\nWhile Slurm attempts to be as efficient as possible with polling, it will\nresult in a thread using CPU time inside of the job and slower response of\nSlurm to catch when container execution is complete.\n\nThe examples provided have been tested to work but are only suggestions. Sites\nare expected to ensure that the resultant root directory used will be secure\nfrom cross user viewing and modifications. The examples provided point to\n\"/run/user/%U\" where %U will be replaced with the numeric user id. Systemd\nmanages \"/run/user/\" (independently of Slurm) and will likely need additional\nconfiguration to ensure the directories exist on compute nodes when the users\nwill not log in to the nodes directly. This configuration is generally achieved\nby calling\n\nloginctl to enable lingering sessions. Be aware that the directory in this\nexample will be cleaned up by systemd once the user session ends on the node.\noci.conf example for runc using create/start:\n\n\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeQuery=\"runc --rootless=true --root=/run/user/%U/ state %n.%u.%j.%s.%t\"\nRunTimeCreate=\"runc --rootless=true --root=/run/user/%U/ create %n.%u.%j.%s.%t -b %b\"\nRunTimeStart=\"runc --rootless=true --root=/run/user/%U/ start %n.%u.%j.%s.%t\"\nRunTimeKill=\"runc --rootless=true --root=/run/user/%U/ kill -a %n.%u.%j.%s.%t\"\nRunTimeDelete=\"runc --rootless=true --root=/run/user/%U/ delete --force %n.%u.%j.%s.%t\"\n\noci.conf example for runc using run (recommended over using\ncreate/start):\n\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeQuery=\"runc --rootless=true --root=/run/user/%U/ state %n.%u.%j.%s.%t\"\nRunTimeKill=\"runc --rootless=true --root=/run/user/%U/ kill -a %n.%u.%j.%s.%t\"\nRunTimeDelete=\"runc --rootless=true --root=/run/user/%U/ delete --force %n.%u.%j.%s.%t\"\nRunTimeRun=\"runc --rootless=true --root=/run/user/%U/ run %n.%u.%j.%s.%t -b %b\"\n\noci.conf example for crun using create/start:\n\n\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeQuery=\"crun --rootless=true --root=/run/user/%U/ state %n.%u.%j.%s.%t\"\nRunTimeKill=\"crun --rootless=true --root=/run/user/%U/ kill -a %n.%u.%j.%s.%t\"\nRunTimeDelete=\"crun --rootless=true --root=/run/user/%U/ delete --force %n.%u.%j.%s.%t\"\nRunTimeCreate=\"crun --rootless=true --root=/run/user/%U/ create --bundle %b %n.%u.%j.%s.%t\"\nRunTimeStart=\"crun --rootless=true --root=/run/user/%U/ start %n.%u.%j.%s.%t\"\n\noci.conf example for crun using run (recommended over using\ncreate/start):\n\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeQuery=\"crun --rootless=true --root=/run/user/%U/ state %n.%u.%j.%s.%t\"\nRunTimeKill=\"crun --rootless=true --root=/run/user/%U/ kill -a %n.%u.%j.%s.%t\"\nRunTimeDelete=\"crun --rootless=true --root=/run/user/%U/ delete --force %n.%u.%j.%s.%t\"\nRunTimeRun=\"crun --rootless=true --root=/run/user/%U/ run --bundle %b %n.%u.%j.%s.%t\"\n\n\noci.conf example for nvidia-container-runtime using create/start:\n\n\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeQuery=\"nvidia-container-runtime --rootless=true --root=/run/user/%U/ state %n.%u.%j.%s.%t\"\nRunTimeCreate=\"nvidia-container-runtime --rootless=true --root=/run/user/%U/ create %n.%u.%j.%s.%t -b %b\"\nRunTimeStart=\"nvidia-container-runtime --rootless=true --root=/run/user/%U/ start %n.%u.%j.%s.%t\"\nRunTimeKill=\"nvidia-container-runtime --rootless=true --root=/run/user/%U/ kill -a %n.%u.%j.%s.%t\"\nRunTimeDelete=\"nvidia-container-runtime --rootless=true --root=/run/user/%U/ delete --force %n.%u.%j.%s.%t\"\n\n\noci.conf example for nvidia-container-runtime using run (recommended over using\ncreate/start):\n\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeQuery=\"nvidia-container-runtime --rootless=true --root=/run/user/%U/ state %n.%u.%j.%s.%t\"\nRunTimeKill=\"nvidia-container-runtime --rootless=true --root=/run/user/%U/ kill -a %n.%u.%j.%s.%t\"\nRunTimeDelete=\"nvidia-container-runtime --rootless=true --root=/run/user/%U/ delete --force %n.%u.%j.%s.%t\"\nRunTimeRun=\"nvidia-container-runtime --rootless=true --root=/run/user/%U/ run %n.%u.%j.%s.%t -b %b\"\n\noci.conf example for\n\nSingularity v4.1.3 using native runtime:\n\n\nIgnoreFileConfigJson=true\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeRun=\"singularity exec --userns %r %@\"\nRunTimeKill=\"kill -s SIGTERM %p\"\nRunTimeDelete=\"kill -s SIGKILL %p\"\n\noci.conf example for\n\nSingularity v4.0.2 in OCI mode:\n\nSingularity v4.x requires setuid mode for OCI support.\n\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeQuery=\"sudo singularity oci state %n.%u.%j.%s.%t\"\nRunTimeRun=\"sudo singularity oci run --bundle %b %n.%u.%j.%s.%t\"\nRunTimeKill=\"sudo singularity oci kill %n.%u.%j.%s.%t\"\nRunTimeDelete=\"sudo singularity oci delete %n.%u.%j.%s.%t\"\n\nWARNING: Singularity (v4.0.2) requires sudo or setuid binaries\nfor OCI support, which is a security risk since the user is able to modify\nthese calls. This example is only provided for testing purposes.WARNING:\n\nUpstream singularity development of the OCI interface appears to have\nceased and sites should use the user\nnamespace support instead.oci.conf example for hpcng Singularity v3.8.0:\n\n\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nOCIRunTimeQuery=\"sudo singularity oci state %n.%u.%j.%s.%t\"\nOCIRunTimeCreate=\"sudo singularity oci create --bundle %b %n.%u.%j.%s.%t\"\nOCIRunTimeStart=\"sudo singularity oci start %n.%u.%j.%s.%t\"\nOCIRunTimeKill=\"sudo singularity oci kill %n.%u.%j.%s.%t\"\nOCIRunTimeDelete=\"sudo singularity oci delete %n.%u.%j.%s.%t\n\nWARNING: Singularity (v3.8.0) requires sudo or setuid binaries\nfor OCI support, which is a security risk since the user is able to modify\nthese calls. This example is only provided for testing purposes.WARNING:\n\nUpstream singularity development of the OCI interface appears to have\nceased and sites should use the user\nnamespace support instead.oci.conf example for\nCharliecloud (v0.30)\n\n\nIgnoreFileConfigJson=true\nCreateEnvFile=newline\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeRun=\"env -i PATH=/usr/local/bin:/usr/local/sbin:/usr/bin:/usr/sbin:/bin/:/sbin/ USER=$(whoami) HOME=/home/$(whoami)/ ch-run -w --bind /etc/group:/etc/group --bind /etc/passwd:/etc/passwd --bind /etc/slurm:/etc/slurm --bind %m:/var/run/slurm/ --bind /var/run/munge/:/var/run/munge/ --set-env=%e --no-passwd %r -- %@\"\nRunTimeKill=\"kill -s SIGTERM %p\"\nRunTimeDelete=\"kill -s SIGKILL %p\"\n\noci.conf example for\nEnroot (3.3.0)\n\n\nIgnoreFileConfigJson=true\nCreateEnvFile=newline\nEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeEnvExclude=\"^(SLURM_CONF|SLURM_CONF_SERVER)=\"\nRunTimeRun=\"/usr/local/bin/enroot-start-wrapper %b %m %e -- %@\"\nRunTimeKill=\"kill -s SIGINT %p\"\nRunTimeDelete=\"kill -s SIGTERM %p\"\n\n/usr/local/bin/enroot-start-wrapper:\n\n#!/bin/bash\nBUNDLE=\"$1\"\nSPOOLDIR=\"$2\"\nENVFILE=\"$3\"\nshift 4\nIMAGE=\n\nexport USER=$(whoami)\nexport HOME=\"$BUNDLE/\"\nexport TERM\nexport ENROOT_SQUASH_OPTIONS='-comp gzip -noD'\nexport ENROOT_ALLOW_SUPERUSER=n\nexport ENROOT_MOUNT_HOME=y\nexport ENROOT_REMAP_ROOT=y\nexport ENROOT_ROOTFS_WRITABLE=y\nexport ENROOT_LOGIN_SHELL=n\nexport ENROOT_TRANSFER_RETRIES=2\nexport ENROOT_CACHE_PATH=\"$SPOOLDIR/\"\nexport ENROOT_DATA_PATH=\"$SPOOLDIR/\"\nexport ENROOT_TEMP_PATH=\"$SPOOLDIR/\"\nexport ENROOT_ENVIRON=\"$ENVFILE\"\n\nif [ ! -f \"$BUNDLE\" ]\nthen\n        IMAGE=\"$SPOOLDIR/container.sqsh\"\n        enroot import -o \"$IMAGE\" -- \"$BUNDLE\" && \\\n        enroot create \"$IMAGE\"\n        CONTAINER=\"container\"\nelse\n        CONTAINER=\"$BUNDLE\"\nfi\n\nenroot start -- \"$CONTAINER\" \"$@\"\nrc=$?\n\n[ $IMAGE ] && unlink $IMAGE\n\nexit $rc\n\nTesting OCI runtime outside of Slurm\n\nSlurm calls the OCI runtime directly in the job step. If it fails,\nthen the job will also fail.\nGo to the directory containing the OCI Container bundle:\ncd $ABS_PATH_TO_BUNDLE\nExecute OCI Container runtime (You can find a few examples on how to build\na bundle below):\n$OCIRunTime $ARGS create test --bundle $PATH_TO_BUNDLE\n$OCIRunTime $ARGS start test\n$OCIRunTime $ARGS kill test\n$OCIRunTime $ARGS delete test\nIf these commands succeed, then the OCI runtime is correctly\nconfigured and can be tested in Slurm.\n\nRequesting container jobs or steps\n\n\nsalloc, srun and sbatch (in Slurm 21.08+) have the\n'--container' argument, which can be used to request container runtime\nexecution. The requested job container will not be inherited by the steps\ncalled, excluding the batch and interactive steps.\n\nBatch step inside of container:\nsbatch --container $ABS_PATH_TO_BUNDLE --wrap 'bash -c \"cat /etc/*rel*\"'\n\nBatch job with step 0 inside of container:\n\nsbatch --wrap 'srun bash -c \"--container $ABS_PATH_TO_BUNDLE cat /etc/*rel*\"'\n\nInteractive step inside of container:\nsalloc --container $ABS_PATH_TO_BUNDLE bash -c \"cat /etc/*rel*\"\nInteractive job step 0 inside of container:\nsalloc srun --container $ABS_PATH_TO_BUNDLE bash -c \"cat /etc/*rel*\"\n\nJob with step 0 inside of container:\nsrun --container $ABS_PATH_TO_BUNDLE bash -c \"cat /etc/*rel*\"\nJob with step 1 inside of container:\nsrun srun --container $ABS_PATH_TO_BUNDLE bash -c \"cat /etc/*rel*\"\n\nIntegration with Rootless Docker (Docker Engine v20.10+ & Slurm-23.02+)\n\nSlurm's scrun can be directly integrated with Rootless Docker to\nrun containers as jobs. No special user permissions are required and should\nnot be granted to use this functionality.Prerequisites\nslurm.conf must be configured to use Munge\nauthentication.AuthType=auth/munge\nscrun.lua\nmust be configured for site storage configuration.\n\n\tConfigure kernel to allow pings\n\n\tConfigure rootless dockerd to allow listening on privileged ports\n\t\n\n\tscrun.lua must be present on any node where scrun may be run. The\n\texample should be sufficent for most environments but paths should be\n\tmodified to match available local storage.\noci.conf must be present on any node where any\n\tcontainer job may be run. Example configurations for\n\t\n\tknown OCI runtimes are provided above. Examples may require\n\tpaths to be correct to installation locations.\nLimitations\nJWT authentication is not supported.\nDocker container building is not currently functional pending merge of\n Docker pull request.\nDocker does not expose configuration options to disable security\noptions needed to run jobs. This requires that all calls to docker provide the\nfollowing command line arguments.  This can be done via shell variable, an\nalias, wrapper function, or wrapper script:\n--security-opt label:disable --security-opt seccomp=unconfined --security-opt apparmor=unconfined --net=none\nDocker's builtin security functionality is not required (or wanted) for\ncontainers being run by Slurm.  Docker is only acting as a container image\nlifecycle manager. The containers will be executed remotely via Slurm following\nthe existing security configuration in Slurm outside of unprivileged user\ncontrol.\nAll containers must use the\n\"none\" networking driver\n. Attempting to use bridge, overlay, host, ipvlan, or macvlan can result in\nscrun being isolated from the network and not being able to communicate with\nthe Slurm controller. The container is run by Slurm on the compute nodes which\nmakes having Docker setup a network isolation layer ineffective for the\ncontainer.\ndocker exec command is not supported.\ndocker compose command is not supported.\ndocker pause command is not supported.\ndocker unpause command is not supported.\ndocker swarm command is not supported.\nSetup procedure\n Install and\nconfigure Rootless Docker Rootless Docker must be fully operational and\nable to run containers before continuing.\n\nSetup environment for all docker calls:\nexport DOCKER_HOST=unix://$XDG_RUNTIME_DIR/docker.sock\nAll commands following this will expect this environment variable to be set.\nStop rootless docker: systemctl --user stop docker\nConfigure Docker to call scrun instead of the default OCI runtime.\n\n\nTo configure for all users: /etc/docker/daemon.json\nTo configure per user: ~/.config/docker/daemon.json\n\nSet the following fields to configure Docker:\n{\n    \"experimental\": true,\n    \"iptables\": false,\n    \"bridge\": \"none\",\n    \"no-new-privileges\": true,\n    \"rootless\": true,\n    \"selinux-enabled\": false,\n    \"default-runtime\": \"slurm\",\n    \"runtimes\": {\n        \"slurm\": {\n            \"path\": \"/usr/local/bin/scrun\"\n        }\n    },\n    \"data-root\": \"/run/user/${USER_ID}/docker/\",\n    \"exec-root\": \"/run/user/${USER_ID}/docker-exec/\"\n}\nCorrect path to scrun as if installation prefix was configured. Replace\n${USER_ID} with numeric user id or target a different directory with global\nwrite permissions and sticky bit. Rootless docker requires a different root\ndirectory than the system's default to avoid permission errors.\nIt is strongly suggested that sites consider using inter-node shared\nfilesystems to store Docker's containers. While it is possible to have a\nscrun.lua script to push and pull images for each deployment, there can be a\nmassive performance penalty.  Using a shared filesystem will avoid moving these\nfiles around.Possible configuration additions to daemon.json to use a\nshared filesystem with  vfs storage\ndriver:\n{\n  \"storage-driver\": \"vfs\",\n  \"data-root\": \"/path/to/shared/filesystem/user_name/data/\",\n  \"exec-root\": \"/path/to/shared/filesystem/user_name/exec/\",\n}\nAny node expected to be able to run containers from Docker must have ability to\natleast read the filesystem used. Full write privileges are suggested and will\nbe required if changes to the container filesystem are desired.\nConfigure dockerd to not setup network namespace, which will break scrun's\n\tability to talk to the Slurm controller.\n\n\nTo configure for all users:\n/etc/systemd/user/docker.service.d/override.conf\nTo configure per user:\n~/.config/systemd/user/docker.service.d/override.conf\n\n\n[Service]\nEnvironment=\"DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER=none\"\nEnvironment=\"DOCKERD_ROOTLESS_ROOTLESSKIT_NET=host\"\n\n\nReload docker's service unit in systemd:\nsystemctl --user daemon-reload\nStart rootless docker: systemctl --user start docker\nVerify Docker is using scrun:\nexport DOCKER_SECURITY=\"--security-opt label=disable --security-opt seccomp=unconfined  --security-opt apparmor=unconfined --net=none\"\ndocker run $DOCKER_SECURITY hello-world\ndocker run $DOCKER_SECURITY alpine /bin/printenv SLURM_JOB_ID\ndocker run $DOCKER_SECURITY alpine /bin/hostname\ndocker run $DOCKER_SECURITY -e SCRUN_JOB_NUM_NODES=10 alpine /bin/hostname\n\nIntegration with Podman (Slurm-23.02+)\n\n\nSlurm's scrun can be directly integrated with\nPodman\nto run containers as jobs. No special user permissions are required and\nshould not be granted to use this functionality.\nPrerequisites\nSlurm must be fully configured and running on host running podman.\nslurm.conf must be configured to use Munge\nauthentication.AuthType=auth/munge\nscrun.lua must be configured for site storage\nconfiguration.\n\n\tscrun.lua must be present on any node where scrun may be run. The\n\texample should be sufficent for most environments but paths should be\n\tmodified to match available local storage.\noci.conf\n\tmust be present on any node where any container job may be run.\n\tExample configurations for\n\t\n\tknown OCI runtimes are provided above. Examples may require\n\tpaths to be correct to installation locations.\nLimitations\nJWT authentication is not supported.\nAll containers must use\n\nhost networking\npodman exec command is not supported.\npodman kube command is not supported.\npodman pod command is not supported.\nSetup procedure\nInstall Podman\n\nConfigure rootless Podman\nVerify rootless podman is configured\n\t$ podman info --format '{{.Host.Security.Rootless}}'\ntrue\nVerify rootless Podman is fully functional before adding Slurm support:\n\nThe value printed by the following commands should be the same:\n\t$ id\n$ podman run --userns keep-id alpine id\n$ sudo id\n$ podman run --userns nomap alpine id\n\n\nConfigure Podman to call scrun instead of the  default OCI runtime.\nSee \nupstream documentation for details on configuration locations and loading\norder for containers.conf.\n\nTo configure for all users:\n/etc/containers/containers.conf\nTo configure per user:\n$XDG_CONFIG_HOME/containers/containers.conf\nor\n~/.config/containers/containers.conf\n(if $XDG_CONFIG_HOME is not defined).\n\nSet the following configuration parameters to configure Podman's containers.conf:\n[containers]\napparmor_profile = \"unconfined\"\ncgroupns = \"host\"\ncgroups = \"enabled\"\ndefault_sysctls = []\nlabel = false\nnetns = \"host\"\nno_hosts = true\npidns = \"host\"\nutsns = \"host\"\nuserns = \"host\"\nlog_driver = \"journald\"\n\n[engine]\ncgroup_manager = \"systemd\"\nruntime = \"slurm\"\nremote = false\n\n[engine.runtimes]\nslurm = [\n\t\"/usr/local/bin/scrun\",\n\t\"/usr/bin/scrun\"\n]\nCorrect path to scrun as if installation prefix was configured.\nThe \"cgroup_manager\" field will need to be swapped to \"cgroupfs\" on systems\nnot running systemd.\nIt is strongly suggested that sites consider using inter-node shared\nfilesystems to store Podman's containers. While it is possible to have a\nscrun.lua script to push and pull images for each deployment, there can be a\nmassive performance penalty. Using a shared filesystem will avoid moving these\nfiles around.\n\nTo configure for all users: /etc/containers/storage.conf\nTo configure per user: $XDG_CONFIG_HOME/containers/storage.conf\n\nPossible configuration additions to storage.conf to use a shared filesystem with\n\nvfs storage driver:\n[storage]\ndriver = \"vfs\"\nrunroot = \"$HOME/containers\"\ngraphroot = \"$HOME/containers\"\n\n[storage.options]\npull_options = {use_hard_links = \"true\", enable_partial_images = \"true\"}\n\n\n[storage.options.vfs]\nignore_chown_errors = \"true\"\nAny node expected to be able to run containers from Podman must have ability to\natleast read the filesystem used. Full write privileges are suggested and will\nbe required if changes to the container filesystem are desired.\n Verify Podman is using scrun:\npodman run hello-world\npodman run alpine printenv SLURM_JOB_ID\npodman run alpine hostname\npodman run alpine -e SCRUN_JOB_NUM_NODES=10 hostname\nsalloc podman run --env-host=true alpine hostname\nsalloc sh -c 'podman run -e SLURM_JOB_ID=$SLURM_JOB_ID alpine hostname'\n\nOptional: Create alias for Docker:\n\talias docker=podman or\n\talias docker='podman --config=/some/path \"$@\"'\n\nTroubleshooting\nPodman runs out of locks:\n$ podman run alpine uptime\nError: allocating lock for new container: allocation failed; exceeded num_locks (2048)\n\n\nTry renumbering:podman system renumber\nTry reseting all storage:podman system reset\n\n\nOCI Container bundle\n\nThere are multiple ways to generate an OCI Container bundle. The\ninstructions below are the method we found the easiest. The OCI standard\nprovides the requirements for any given bundle:\n\nFilesystem Bundle\nHere are instructions on how to generate a container using a few\nalternative container solutions:\nCreate an image and prepare it for use with runc:\n    \n\n\tUse an existing tool to create a filesystem image in /image/rootfs:\n\t\n\n\t\tdebootstrap:\n\t\tsudo debootstrap stable /image/rootfs http://deb.debian.org/debian/\n\n\n\t\tyum:\n\t\tsudo yum --config /etc/yum.conf --installroot=/image/rootfs/ --nogpgcheck --releasever=${CENTOS_RELEASE} -y\n\n\n\t\tdocker:\n\t\t\nmkdir -p ~/oci_images/alpine/rootfs\ncd ~/oci_images/\ndocker pull alpine\ndocker create --name alpine alpine\ndocker export alpine | tar -C ~/oci_images/alpine/rootfs -xf -\ndocker rm alpine\n\n\n\n\tConfigure a bundle for runtime to execute:\n\t\nUse runc\n\t    to generate a config.json:\n\t    \ncd ~/oci_images/alpine\nrunc --rootless=true spec --rootless\n\nTest running image:\n\nsrun --container ~/oci_images/alpine/ uptime\n\nUse umoci\n    and skopeo to generate a full image:\n    \nmkdir -p ~/oci_images/\ncd ~/oci_images/\nskopeo copy docker://alpine:latest oci:alpine:latest\numoci unpack --rootless --image alpine ~/oci_images/alpine\nsrun --container ~/oci_images/alpine uptime\n\n    Use \n    singularity to generate a full image:\n    \nmkdir -p ~/oci_images/alpine/\ncd ~/oci_images/alpine/\nsingularity pull alpine\nsudo singularity oci mount ~/oci_images/alpine/alpine_latest.sif ~/oci_images/alpine\nmv config.json singularity_config.json\nrunc spec --rootless\nsrun --container ~/oci_images/alpine/ uptime\nExample OpenMPI v5 + PMIx v4 container\n\nDockerfile\nFROM almalinux:latest\nRUN dnf -y update && dnf -y upgrade && dnf install -y yum-utils && dnf config-manager --set-enabled powertools\nRUN dnf -y install make automake gcc gcc-c++ kernel-devel bzip2 python3 wget libevent-devel hwloc-devel munge-devel\n\nWORKDIR /usr/local/src/\nRUN wget 'https://github.com/openpmix/openpmix/releases/download/v4.2.2/pmix-4.2.2.tar.bz2' -O - | tar -xvjf -\nWORKDIR /usr/local/src/pmix-4.2.2/\nRUN ./configure && make -j && make install\n\nWORKDIR /usr/local/src/\nRUN wget --inet4-only 'https://download.open-mpi.org/release/open-mpi/v5.0/openmpi-5.0.0rc9.tar.gz' -O - | tar -xvzf -\nWORKDIR /usr/local/src/openmpi-5.0.0rc9\nRUN ./configure --disable-pty-support --enable-ipv6 --without-slurm --with-pmix --enable-debug && make -j && make install\n\nWORKDIR /usr/local/src/openmpi-5.0.0rc9/examples\nRUN make && cp -v hello_c ring_c connectivity_c spc_example /usr/local/bin\nContainer support via Plugin\nSlurm allows container developers to create SPANK\nPlugins that can be called at various points of job execution to support\ncontainers. Any site using one of these plugins to start containers should\nnot have an \"oci.conf\" configuration file. The \"oci.conf\" file activates the\nbuiltin container functionality which may conflict with the SPANK based plugin\nfunctionality.The following projects are third party container solutions that have been\ndesigned to work with Slurm, but they have not been tested or validated by\nSchedMD.ShifterShifter is a container\nproject out of NERSC\nto provide HPC containers with full scheduler integration.\n\n\nShifter provides full\n\t\t\n\t\t\tinstructions to integrate with Slurm.\n\t\nPresentations about Shifter and Slurm:\n\t\t\n \n\t\t\t\tNever Port Your Code Again - Docker functionality with Shifter using SLURM\n\t\t\t \n \n\t\t\t\tShifter: Containers in HPC Environments\n\t\t\t \n\n\n\nENROOT and PyxisEnroot is a user namespace\ncontainer system sponsored by NVIDIA\nthat supports:\n\nSlurm integration via\n\t\tpyxis\n\nNative support for Nvidia GPUs\nFaster Docker image imports\n\nSarusSarus is a privileged\ncontainer system sponsored by ETH Zurich\nCSCS that supports:\n\n\n\n\t\t\tSlurm image synchronization via OCI hook\n\nNative OCI Image support\nNVIDIA GPU Support\nSimilar design to Shifter\n\nOverview slides of Sarus are\n\n\there.\nLast modified 08 October 2024"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        }
    ]
}