{
    "url": "https://slurm.schedmd.com/mpi_guide.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "MPI Users Guide",
            "content": "MPI use depends upon the type of MPI being used.\nThere are three fundamentally different modes of operation used\nby these various MPI implementations.\n\nSlurm directly launches the tasks and performs initialization of\ncommunications through the PMI-1, PMI-2 or PMIx APIs. (Supported by most\nmodern MPI implementations.)\nSlurm creates a resource allocation for the job and then\nmpirun launches tasks using Slurm's infrastructure (srun).\nSlurm creates a resource allocation for the job and then\nmpirun launches tasks using some mechanism other than Slurm,\nsuch as SSH or RSH.\nThese tasks are initiated outside of Slurm's monitoring\nor control and require access to the nodes from the batch node (e.g. SSH).\nSlurm's epilog should be configured to purge\nthese tasks when the job's allocation is relinquished. The\nuse of pam_slurm_adopt is strongly recommended.\n\nNOTE: Slurm is not directly launching the user application in case 3,\nwhich may prevent the desired behavior of binding tasks to CPUs and/or\naccounting and is not a recommended way.\nTwo Slurm parameters control which PMI (Process Management Interface)\nimplementation will be supported. Proper configuration is essential for Slurm to\nestablish the proper environment for the MPI job, such as setting the\nappropriate environment variables. The MpiDefault configuration parameter\nin slurm.conf establishes the system's default PMI to be used.\nThe srun option --mpi= (or the equivalent environment\nvariable SLURM_MPI_TYPE) can be used to specify when a\ndifferent PMI implementation is to be used for an individual job.\nThere are parameters that can be set in the\nmpi.conf file that allow you to modify\nthe behavior of the PMI plugins.\nNOTE: Use of an MPI implementation without the appropriate Slurm\nplugin may result in application failure. If multiple MPI implementations\nare used on a system then some users may be required to explicitly specify\na suitable Slurm MPI plugin.\nNOTE: If installing Slurm with RPMs, the slurm-libpmi\npackage will conflict with the pmix-libpmi package if it is\ninstalled. If policies at your site allow you to install from source, this\nwill allow you to install these packages to different locations, so you can\nchoose which libraries to use.\nNOTE: If you build any MPI stack component with hwloc, note that\nversions 2.5.0 through 2.7.0 (inclusive) of hwloc have a bug that pushes an\nuntouchable value into the environ array, causing a segfault when accessing it.\nIt is advisable to build with hwloc version 2.7.1 or later.\nLinks to instructions for using several varieties of MPI/PMI\nwith Slurm are provided below.\n\nPMIx\nOpen MPI\nIntel-MPI\nMPICH\nMVAPICH2\nHPE Cray PMI Support\n\n\nPMIx\n\n\nBuilding PMIx\n\n\nBefore building PMIx, it is advisable to read these\nHow-To Guides. They\nprovide some details on\n\nbuilding dependencies and installation steps as well as some relevant notes\nwith regards to\nSlurm Support\n.\nThis section is intended to complement the PMIx FAQ with some notes on how to\nprepare Slurm and PMIx to work together. PMIx can be obtained from the official\nPMIx GitHub repository,\neither by cloning the repository or by downloading a packaged release.\nSlurm support for PMIx was first included in Slurm 16.05 based on the PMIx\nv1.2 release. It has since been updated to support up to version 5.x of the\nPMIx series, as per the following table:\n\nSlurm 20.11+ supports PMIx v1.2+, v2.x and v3.x.\nSlurm 22.05+ supports PMIx v2.x, v3.x., v4.x. and v5.x.\n\nIf running PMIx v1, it is recommended to run at least 1.2.5 since older\nversions may have some compatibility issues with support of pmi and pmi2 APIs.\n\nNote also that Intel MPI doesn't officially support PMIx. It may work since PMIx\noffers some compatibility with PMI-2, but there is no guarantee that it will.\n\nAdditional PMIx notes can be found in the SchedMD\nPublications and Presentations page.\nBuilding Slurm with PMIx support\n\n\nAt configure time, Slurm won't build with PMIx unless --with-pmix is\nset. Then it will look by default for a PMIx installation under:\n\n/usr\n/usr/local\n\nIf PMIx isn't installed in any of the previous locations, the Slurm configure\nscript can be requested to point to the non default location. Here's an example\nassuming the installation dir is /home/user/pmix/v4.1.2/:\n\n\nuser@testbox:~/slurm/22.05/build$ ../src/configure \\\n> --prefix=/home/user/slurm/22.05/inst \\\n> --with-pmix=/home/user/pmix/4.1.2\n\nOr the analogous with RPM based building:\n\nuser@testbox:~/slurm_rpm$ rpmbuild \\\n> --define '_prefix /home/user/slurm/22.05/inst' \\\n> --define '_slurm_sysconfdir /home/user/slurm/22.05/inst/etc' \\\n> --define '_with_pmix --with-pmix=/home/user/pmix/4.1.2' \\\n> -ta slurm-22.05.2.1.tar.bz2\n\nNOTE: It is also possible to build against multiple PMIx versions\nwith a ':' separator. For instance to build against 3.2 and 4.1:\n\n...\n> --with-pmix=/path/to/pmix/3.2.3:/path/to/pmix/4.1.2 \\\n...\n\nThen, when submitting a job, the desired version can then be selected\nusing any of the available from --mpi=list. The default for pmix will be the\nhighest version of the library:\n\n$ srun --mpi=list\nMPI plugin types are...\n\tcray_shasta\n\tnone\n\tpmi2\n\tpmix\nspecific pmix plugin versions available: pmix_v3,pmix_v4\n\nContinuing with the configuration, if Slurm is unable to locate the PMIx\ninstallation and/or finds it but considers it not usable, the configure output\nshould log something like this:\n\nchecking for pmix installation...\nconfigure: WARNING: unable to locate pmix installation\n\nInspecting the generated config.log in the Slurm build directory might\nprovide more detail for troubleshooting purposes. After configuration,\nwe can proceed to install Slurm (using make or rpm accordingly):\n\nuser@testbox:~/slurm/22.05/build$ make -j install\nuser@testbox:~/slurm/22.05/build$ cd /home/user/slurm/22.05/inst/lib/slurm/\nuser@testbox:~/slurm/22.05/inst/lib/slurm$ ls -l *pmix*\nlrwxrwxrwx 1 user user      16 jul  6 17:17 mpi_pmix.so -> ./mpi_pmix_v4.so\n-rw-r--r-- 1 user user 9387254 jul  6 17:17 mpi_pmix_v3.a\n-rwxr-xr-x 1 user user    1065 jul  6 17:17 mpi_pmix_v3.la\n-rwxr-xr-x 1 user user 1265840 jul  6 17:17 mpi_pmix_v3.so\n-rw-r--r-- 1 user user 9935358 jul  6 17:17 mpi_pmix_v4.a\n-rwxr-xr-x 1 user user    1059 jul  6 17:17 mpi_pmix_v4.la\n-rwxr-xr-x 1 user user 1286936 jul  6 17:17 mpi_pmix_v4.so\n\nIf support for PMI-1 or PMI-2 version is also needed, it can also be\ninstalled from the contribs directory:\n\nuser@testbox:~/slurm/22.05/build/$ cd contribs/pmi1\nuser@testbox:~/slurm/22.05/build/contribs/pmi1$ make -j install\n\nuser@testbox:~/slurm/22.05/build/$ cd contribs/pmi2\nuser@testbox:~/slurm/22.05/build/contribs/pmi2$ make -j install\n\nuser@testbox:~/$ ls -l /home/user/slurm/22.05/inst/lib/*pmi*\n-rw-r--r-- 1 user user 493024 jul  6 17:27 libpmi2.a\n-rwxr-xr-x 1 user user    987 jul  6 17:27 libpmi2.la\nlrwxrwxrwx 1 user user     16 jul  6 17:27 libpmi2.so -> libpmi2.so.0.0.0\nlrwxrwxrwx 1 user user     16 jul  6 17:27 libpmi2.so.0 -> libpmi2.so.0.0.0\n-rwxr-xr-x 1 user user 219712 jul  6 17:27 libpmi2.so.0.0.0\n-rw-r--r-- 1 user user 427768 jul  6 17:27 libpmi.a\n-rwxr-xr-x 1 user user   1039 jul  6 17:27 libpmi.la\nlrwxrwxrwx 1 user user     15 jul  6 17:27 libpmi.so -> libpmi.so.0.0.0\nlrwxrwxrwx 1 user user     15 jul  6 17:27 libpmi.so.0 -> libpmi.so.0.0.0\n-rwxr-xr-x 1 user user 241640 jul  6 17:27 libpmi.so.0.0.0\n\nNOTE: Since Slurm and PMIx lower than 4.x both provide libpmi[2].so\nlibraries, we recommend you install both pieces of software in\ndifferent locations. Otherwise, these same libraries might end up being\ninstalled under standard locations like /usr/lib64 and the\npackage manager would error out, reporting the conflict.\nNOTE: Any application compiled against PMIx should use the same PMIx\nor at least a PMIx with the same security domain than the one Slurm is using,\notherwise there could be authentication issues. E.g. one PMIx compiled\n--with-munge while another compiled --without-munge (the default since PMIx\n4.2.4). A workaround which might work is to specify the desired security method\nadding \"--mca psec native\" to the cli or exporting PMIX_MCA_psec=native\nenvironment variable.\n\nNOTE: If you are setting up a test environment using multiple-slurmd,\nthe TmpFS option in your slurm.conf needs to be specified and the number of\ndirectory paths created needs to equal the number of nodes. These directories\nare used by the Slurm PMIx plugin to create temporal files and/or UNIX sockets.\nHere is an example setup for two nodes named compute[1-2]:\n\nslurm.conf:\nTmpFS=/home/user/slurm/22.05/inst/tmp/slurmd-tmpfs-%n\n\n$ mkdir /home/user/slurm/22.05/inst/tmp/slurmd-tmpfs-compute1\n$ mkdir /home/user/slurm/22.05/inst/tmp/slurmd-tmpfs-compute2\n\nTesting Slurm and PMIx\n\n\nIt is possible to directly test Slurm and PMIx without needing to have an\nMPI implementation installed. Here is an example demonstrating that\nboth components work properly:\n\n$ srun --mpi=list\nMPI plugin types are...\n\tcray_shasta\n\tnone\n\tpmi2\n\tpmix\nspecific pmix plugin versions available: pmix_v3,pmix_v4\n\n$ srun --mpi=pmix_v4 -n2 -N2 \\\n> /home/user/git/pmix/test/pmix_client -n 2 --job-fence -c\n==141756== OK\n==141774== OK\n\n\nOpenMPI\n\n\nThe current versions of Slurm and Open MPI support task launch using the\nsrun command.\nIf OpenMPI is configured with --with-pmi= pointing to either Slurm's\nPMI-1 libpmi.so or PMI-2 libpmi2.so libraries, the OMPI jobs can then be\nlaunched directly using the srun command. This is the preferred mode of\noperation since accounting features and affinity done by Slurm will become\navailable. If pmi2 support is enabled, the option '--mpi=pmi2' must be\nspecified on the srun command line.\nAlternately configure 'MpiDefault=pmi' or 'MpiDefault=pmi2' in slurm.conf.\nStarting with Open MPI version 3.1, PMIx is natively supported. To launch\nOpen MPI applications using PMIx the '--mpi=pmix' option must be specified on\nthe srun command line or 'MpiDefault=pmix' must be configured in slurm.conf.\nIt is also possible to build OpenMPI using an external PMIx installation.\nRefer to the OpenMPI documentation for a detailed procedure but it basically\nconsists of specifying --with-pmix=PATH when configuring OpenMPI.\nNote that if building OpenMPI using an external PMIx installation, both OpenMPI\nand PMIx need to be built against the same libevent/hwloc installations.\nOpenMPI configure script provides the options\n--with-libevent=PATH  and/or --with-hwloc=PATH to make OpenMPI\nmatch what PMIx was built against.\nA set of parameters are available to control the behavior of the\nSlurm PMIx plugin, read mpi.conf for more\ninformation.\nNOTE: OpenMPI has a limitation that does not support calls to\nMPI_Comm_spawn() from within a Slurm allocation. If you need to\nuse the MPI_Comm_spawn() function you will need to use another MPI\nimplementation combined with PMI-2 since PMIx doesn't support it either.\nNOTE: Some kernels and system configurations have resulted in a locked\nmemory too small for proper OpenMPI functionality, resulting in application\nfailure with a segmentation fault. This may be fixed by configuring the slurmd\ndaemon to execute with a larger limit. For example, add \"LimitMEMLOCK=infinity\"\nto your slurmd.service file.\n\nIntel MPI\n\n\nIntel\u00ae MPI Library for Linux OS supports the following methods of\nlaunching the MPI jobs under the control of the Slurm job manager:\n\nThe mpirun command over the Hydra PM\n\nThe srun command (Slurm, recommended)\n\n\nThis description provides detailed information on these two methods.\nThe mpirun Command over the Hydra Process Manager\n\n\nSlurm is supported by the mpirun command of the Intel\u00ae MPI Library\nthrough the Hydra Process Manager by default. When launched within an allocation\nthe mpirun command will automatically read the environment variables set\nby Slurm such as nodes, cpus, tasks, etc, in order to start the required\nhydra daemons on every node. These daemons will be started using srun and\nwill subsequently start the user application. Since Intel\u00ae MPI supports\nonly PMI-1 and PMI-2 (not PMIx), it is highly recommended to configure this mpi\nimplementation to use Slurm's PMI-2, which offers better scalability than PMI-1.\nPMI-1 is not recommended and should be deprecated soon.\nBelow is an example of how a user app can be launched within an exclusive\nallocation of 10 nodes using Slurm's PMI-2 library installed from contribs:\n\n$ salloc -N10 --exclusive\n$ export I_MPI_PMI_LIBRARY=/path/to/slurm/lib/libpmi2.so\n$ mpirun -np <num_procs> user_app.bin\n\nNote that by default Slurm will inject two environment variables to ensure\nmpirun or mpiexec will use the Slurm bootstrap mechanism (srun) to launch Hydra.\nWith these, Hydra will also pass the argument '--external-launcher' to srun in\norder to not consider these hydra processes as regular steps. This automatic\nvariable injection mechanism can be disabled by setting\n\"MpiParams=disable_slurm_hydra_bootstrap\" in slurm.conf, with the exception\nof the cases where \"slurm\" is already explicitly set as a bootstrap.\nIt is possible to run Intel MPI using a different bootstrap mechanism. To do\nso, explicitly set the following environment variable prior to submitting the\njob with sbatch or salloc:\n\n$ export I_MPI_HYDRA_BOOTSTRAP=ssh\n$ salloc -N10\n$ mpirun -np <num_procs> user_app.bin\n\nThe srun Command (Slurm, recommended)\n\n\nThis method is also supported by the Intel\u00ae MPI Library.\nThis method is the best integrated with Slurm and supports process tracking,\naccounting, task affinity, suspend/resume and other features.\nAs in the previous case, we show an example of how a user app can be\nlaunched within an exclusive allocation of 10 nodes using Slurm's PMI-2 library\ninstalled from contribs, allowing it to take advantage of of all the Slurm\nfeatures. This can be done with sbatch or salloc commands:\n\n$ salloc -N10 --exclusive\n$ export I_MPI_PMI_LIBRARY=/path/to/slurm/lib/libpmi2.so\n$ srun user_app.bin\n\nNOTE: The reason we're pointing manually to Slurm's PMI-1 or PMI-2\nlibrary is for licensing reasons. IMPI doesn't link directly to any external\nPMI implementations so, unlike other stacks (OMPI, MPICH, MVAPICH...), Intel is\nnot built against Slurm libs. Pointing to this library will cause Intel to\ndlopen and use this PMI library.\nNOTE: There is no official support provided by Intel against PMIx\nlibraries. Since IMPI is based on MPICH, using PMIx with Intel may work due to\nPMIx maintaining compatibility with pmi2 (which are the libraries used in MPICH)\nbut it is not guaranteed to run in all cases and PMIx could break this\ncompatibility in future versions.\n For more information see:\nIntel MPI Library\n.\n\nMPICH\n\n\nMPICH was formerly known as MPICH2.\nMPICH jobs can be launched using srun or mpiexec.\nBoth modes of operation are described below. The MPICH implementation supports\nPMI-1, PMI-2 and PMIx (starting with MPICH v4).\n\nNote that by default Slurm will inject two environment variables to ensure\nmpirun or mpiexec will use the Slurm bootstrap mechanism (srun) to launch Hydra.\nWith these, Hydra will also pass the argument '--external-launcher' to srun in\norder to not consider these hydra processes as regular steps. This automatic\nvariable injection mechanism can be disabled by setting\n\"MpiParams=disable_slurm_hydra_bootstrap\" in slurm.conf, with the exception\nof the cases where \"slurm\" is already explicitly set as a bootstrap.\nIt is possible to run MPICH using a different bootstrap mechanism. To do\nso, explicitly set the following environment variable prior to submitting the\njob with sbatch or salloc:\n\n$ export HYDRA_BOOTSTRAP=ssh\n$ salloc -N10\n$ mpirun -np <num_procs> user_app.bin\n\n\nMPICH with srun and linked with Slurm's PMI-1 or PMI-2 libs\n\n\nMPICH can be built specifically for use with Slurm and its PMI-1 or PMI-2\nlibraries using a configure line similar to that shown below. Building this way\nwill force the use of this library on every execution. Note that the\nLD_LIBRARY_PATH may not be necessary depending on your Slurm installation path:\n\n For PMI-2:\n\nuser@testbox:~/mpich-4.0.2/build$ LD_LIBRARY_PATH=~/slurm/22.05/inst/lib/ \\\n> ../configure --prefix=/home/user/bin/mpich/ --with-pmilib=slurm \\\n> --with-pmi=pmi2 --with-slurm=/home/lipi/slurm/master/inst\n\n\nor for PMI-1:\n\n\nuser@testbox:~/mpich-4.0.2/build$ LD_LIBRARY_PATH=~/slurm/22.05/inst/lib/ \\\n> ../configure --prefix=/home/user/bin/mpich/ --with-pmilib=slurm \\\n> --with-slurm=/home/user/slurm/22.05/inst\n\n These configure lines will detect the Slurm's installed PMI libraries and\nlink against them, but will not install the mpiexec commands. Since PMI-1\nis already old and doesn't scale well we don't recommend you link against it.\nIt is preferable to use PMI-2. You can follow this example to run a job with\nPMI-2:\n\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmi2 ./hello_world\n\nA Slurm upgrade will not affect this MPICH installation. There is only one\nunlikely scenario where a recompile of the MPI stack would be needed after an\nupgrade, which is when we forcibly link against Slurm's PMI-1 and/or PMI-2\nlibraries and if their APIs ever changed. These should not change often but\nif it were to happen, it would be noted in Slurm's RELEASE_NOTES file.\nMPICH with PMIx and integrated with Slurm\n\n\n You can also build MPICH using an external PMIx library which should be the\nsame one used when building Slurm:\n\n$ LD_LIBRARY_PATH=~/slurm/22.05/inst/lib/ ../configure \\\n> --prefix=/home/user/bin/mpich/ \\\n> --with-pmix=/home/user/bin/pmix_4.1.2/ \\\n> --with-pmi=pmix \\\n> --with-slurm=/home/user/slurm/master/inst\n\nAfter building this way, any execution must be made with Slurm (srun) since\nthe Hydra process manager is not installed, as it was in previous examples.\nCompile and run a process with:\n\n\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmix ./hello_world\n\nMPICH with its internal PMI and integrated with Slurm\n\n\nAnother option is to just compile MPICH but not set --with-pmilib,\n--with-pmix or --with-pmi, and only keep --with-slurm.\nIn that case, MPICH will not forcibly link against any PMI libraries and it will\ninstall the mpiexec.hydra command by default. This will cause it to use its\ninternal PMI implementation (based on PMI-1) and Slurm API functions to detect\nthe job environment and launch processes accordingly:\n\n\nuser@testbox:~/mpich-4.0.2/build$ ../configure \\\n> --prefix=/home/user/bin/mpich/ \\\n> --with-slurm=/home/user/slurm/22.05/inst\n\nThen the app can be run with srun or mpiexec:\n\n$ mpicc -o hello_world hello_world.c\n$ srun ./hello_world\n\nor\n\n$ mpiexec.hydra ./hello_world\n\nmpiexec.hydra will spawn its daemons using Slurm steps launched with srun and\nwill use its internal PMI implementation.\nNOTE: In this case, compiling with the --with-slurm option\ncreated the Hydra bootstrap commands (mpiexec.hydra and others) and linked them\nagainst the versioned Slurm's main public API (libslurm.so.X.0.0). That is\nbecause these commands use some Slurm functions to detect the job environment.\nBe aware then that upgrading Slurm would need a recompile of the MPICH stack.\nIt is usually enough to symlink the name of the linked library to the new one,\nbut this is not guaranteed to work.\n\nMPICH without Slurm integration\n\n\nFinally, it is possible to compile MPICH without integrating it with Slurm.\nIn that case it will not identify the job and will just run the processes as if\nit were on a local machine. We recommend reading MPICH documentation and the\nconfigure scripts for more information on the existing possibilities.\n\n\n\nMVAPICH2\n\n\n\nMVAPICH2 has support for Slurm. To enable it you need to build MVAPICH2\nwith a command similar to this:\n\n$ ./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/22.05/inst/\n\nNOTE:In certain MVAPICH2 versions and when building with GCC > 10.x,\nit is possible that these flags must be prepended to the configure line:\n\nFFLAGS=\"-std=legacy\" FCFLAGS=\"-std=legacy\" ./configure ...\n\nWhen MVAPICH2 is built with Slurm support it will detect that it is\nwithin a Slurm allocation, and will use the 'srun' command to spawn its hydra\ndaemons. It does not link to the Slurm API, which means that during an upgrade\nof Slurm there is no need to recompile MVAPICH2. By default it will use the\ninternal PMI implementation.\nNote that by default Slurm will inject two environment variables to ensure\nmpirun or mpiexec will use the Slurm bootstrap mechanism (srun) to launch Hydra.\nWith these, Hydra will also pass the argument '--external-launcher' to srun in\norder to not consider these hydra processes as regular steps. This automatic\nvariable injection mechanism can be disabled by setting\n\"MpiParams=disable_slurm_hydra_bootstrap\" in slurm.conf, with the exception\nof the cases where \"slurm\" is already explicitly set as a bootstrap.\nIt is possible to run MVAPICH2 using a different bootstrap mechanism. To do\nso, explicitly set the following environment variable prior to submitting the\njob with sbatch or salloc:\n\n$ export HYDRA_BOOTSTRAP=ssh\n$ salloc -N10\n$ mpirun -np <num_procs> user_app.bin\n\n\nMVAPICH2 with srun and linked with Slurm's PMI-1 or PMI-2 libs\n\n\nIt is possible to force MVAPICH2 to use one of the Slurm's PMI-1\n(libpmi.so.0.0.0)or PMI-2 (libpmi2.so.0.0.0) libraries. Building with this mode\nwill cause all the executions to use Slurm and its PMI libraries.\nThe Hydra process manager binaries (mpiexec) won't be installed. In fact the\nmpiexec command will exist as a symbolic link to Slurm's srun command. It is\nrecommended not to use PMI-1, but to use at least PMI-2 libs.\nSee below for an example of how to configure and usage:\n\nFor PMI-2:\n\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/  \\\n> --with-pm=slurm --with-pmi=pmi2\n\nand for PMI-1:\n\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/ \\\n> --with-pm=slurm --with-pmi=pmi1\n\nTo compile and run a user application in Slurm:\n\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmi2 ./hello_world\n\nFor more information, please see the MVAPICH2 documentation on their\nwebpage\n\nMVAPICH2 with Slurm support and linked with external PMIx\n\n\nIt is possible to use PMIx within MVAPICH2 and integrated with Slurm. This\nway no Hydra Process Manager will be installed and the user apps will need to\nrun with srun, assuming Slurm has been compiled against the same or a compatible\nPMIx version as the one used when building MVAPICH2.\nTo build MVAPICH2 to use PMIx and integrated with Slurm, a configuration line\nsimilar to this is needed:\n\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/ \\\n> --with-pm=slurm \\\n> --with-pmix=/home/user/bin/pmix_4.1.2/ \\\n> --with-pmi=pmix\n\nRunning a job looks similar to previous examples:\n\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmix ./hello_world\n\nNOTE: In the MVAPICH2 case, compiling with integration with Slurm\n(--with-slurm) doesn't add any dependency to commands or libraries, so\nupgrading Slurm should be safe without any need to recompile MVAPICH2. There\nis only one unlikely scenario where a recompile of the MPI stack would be\nneeded after an upgrade, which is when we forcibly link against Slurm's PMI-1\nand/or PMI-2 libraries and if their APIs ever changed. These should not change\noften but if it were to happen, it would be noted in Slurm's RELEASE_NOTES\nfile.\n\nHPE Cray PMI support\n\n\nSlurm comes by default with a Cray PMI vendor-specific plugin which provides\ncompatibility with the HPE Cray Programming Environment's PMI. It is intended to\nbe used in applications built with this environment on HPE Cray machines.\nThe plugin is named cray_shasta (Shasta was the first Cray\narchitecture this plugin supported) and built by default in all Slurm\ninstallations. Its availability is shown by running the following command:\n\n$ srun --mpi=list\nMPI plugin types are...\n\tcray_shasta\n\tnone\n\nThe Cray PMI plugin will use some reserved ports for its communication. These\nports are configurable by using --resv-ports option on the command line\nwith srun, or by setting MpiParams=ports=[port_range]\nin your slurm.conf. The first port listed in this option will be used as the\nPMI control port, defined by Cray as the PMI_CONTROL_PORT environment\nvariable. There cannot be more than one application launched in the same node\nusing the same PMI_CONTROL_PORT.\nThis plugin does not support MPMD/heterogeneous jobs and it requires\nlibpals >= 0.2.8.\nLast modified 21 June 2024\n"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        },
        {
            "title": "PMIx\n\n",
            "content": "Building PMIx\n\nBefore building PMIx, it is advisable to read these\nHow-To Guides. They\nprovide some details on\n\nbuilding dependencies and installation steps as well as some relevant notes\nwith regards to\nSlurm Support\n.This section is intended to complement the PMIx FAQ with some notes on how to\nprepare Slurm and PMIx to work together. PMIx can be obtained from the official\nPMIx GitHub repository,\neither by cloning the repository or by downloading a packaged release.Slurm support for PMIx was first included in Slurm 16.05 based on the PMIx\nv1.2 release. It has since been updated to support up to version 5.x of the\nPMIx series, as per the following table:\n\nSlurm 20.11+ supports PMIx v1.2+, v2.x and v3.x.\nSlurm 22.05+ supports PMIx v2.x, v3.x., v4.x. and v5.x.\n\nIf running PMIx v1, it is recommended to run at least 1.2.5 since older\nversions may have some compatibility issues with support of pmi and pmi2 APIs.\n\nNote also that Intel MPI doesn't officially support PMIx. It may work since PMIx\noffers some compatibility with PMI-2, but there is no guarantee that it will.\nAdditional PMIx notes can be found in the SchedMD\nPublications and Presentations page.Building Slurm with PMIx support\n\nAt configure time, Slurm won't build with PMIx unless --with-pmix is\nset. Then it will look by default for a PMIx installation under:\n/usr\n/usr/local\nIf PMIx isn't installed in any of the previous locations, the Slurm configure\nscript can be requested to point to the non default location. Here's an example\nassuming the installation dir is /home/user/pmix/v4.1.2/:\n\nuser@testbox:~/slurm/22.05/build$ ../src/configure \\\n> --prefix=/home/user/slurm/22.05/inst \\\n> --with-pmix=/home/user/pmix/4.1.2\nOr the analogous with RPM based building:\nuser@testbox:~/slurm_rpm$ rpmbuild \\\n> --define '_prefix /home/user/slurm/22.05/inst' \\\n> --define '_slurm_sysconfdir /home/user/slurm/22.05/inst/etc' \\\n> --define '_with_pmix --with-pmix=/home/user/pmix/4.1.2' \\\n> -ta slurm-22.05.2.1.tar.bz2\nNOTE: It is also possible to build against multiple PMIx versions\nwith a ':' separator. For instance to build against 3.2 and 4.1:\n...\n> --with-pmix=/path/to/pmix/3.2.3:/path/to/pmix/4.1.2 \\\n...\nThen, when submitting a job, the desired version can then be selected\nusing any of the available from --mpi=list. The default for pmix will be the\nhighest version of the library:\n$ srun --mpi=list\nMPI plugin types are...\n\tcray_shasta\n\tnone\n\tpmi2\n\tpmix\nspecific pmix plugin versions available: pmix_v3,pmix_v4\nContinuing with the configuration, if Slurm is unable to locate the PMIx\ninstallation and/or finds it but considers it not usable, the configure output\nshould log something like this:\nchecking for pmix installation...\nconfigure: WARNING: unable to locate pmix installation\nInspecting the generated config.log in the Slurm build directory might\nprovide more detail for troubleshooting purposes. After configuration,\nwe can proceed to install Slurm (using make or rpm accordingly):\nuser@testbox:~/slurm/22.05/build$ make -j install\nuser@testbox:~/slurm/22.05/build$ cd /home/user/slurm/22.05/inst/lib/slurm/\nuser@testbox:~/slurm/22.05/inst/lib/slurm$ ls -l *pmix*\nlrwxrwxrwx 1 user user      16 jul  6 17:17 mpi_pmix.so -> ./mpi_pmix_v4.so\n-rw-r--r-- 1 user user 9387254 jul  6 17:17 mpi_pmix_v3.a\n-rwxr-xr-x 1 user user    1065 jul  6 17:17 mpi_pmix_v3.la\n-rwxr-xr-x 1 user user 1265840 jul  6 17:17 mpi_pmix_v3.so\n-rw-r--r-- 1 user user 9935358 jul  6 17:17 mpi_pmix_v4.a\n-rwxr-xr-x 1 user user    1059 jul  6 17:17 mpi_pmix_v4.la\n-rwxr-xr-x 1 user user 1286936 jul  6 17:17 mpi_pmix_v4.so\nIf support for PMI-1 or PMI-2 version is also needed, it can also be\ninstalled from the contribs directory:\nuser@testbox:~/slurm/22.05/build/$ cd contribs/pmi1\nuser@testbox:~/slurm/22.05/build/contribs/pmi1$ make -j install\n\nuser@testbox:~/slurm/22.05/build/$ cd contribs/pmi2\nuser@testbox:~/slurm/22.05/build/contribs/pmi2$ make -j install\n\nuser@testbox:~/$ ls -l /home/user/slurm/22.05/inst/lib/*pmi*\n-rw-r--r-- 1 user user 493024 jul  6 17:27 libpmi2.a\n-rwxr-xr-x 1 user user    987 jul  6 17:27 libpmi2.la\nlrwxrwxrwx 1 user user     16 jul  6 17:27 libpmi2.so -> libpmi2.so.0.0.0\nlrwxrwxrwx 1 user user     16 jul  6 17:27 libpmi2.so.0 -> libpmi2.so.0.0.0\n-rwxr-xr-x 1 user user 219712 jul  6 17:27 libpmi2.so.0.0.0\n-rw-r--r-- 1 user user 427768 jul  6 17:27 libpmi.a\n-rwxr-xr-x 1 user user   1039 jul  6 17:27 libpmi.la\nlrwxrwxrwx 1 user user     15 jul  6 17:27 libpmi.so -> libpmi.so.0.0.0\nlrwxrwxrwx 1 user user     15 jul  6 17:27 libpmi.so.0 -> libpmi.so.0.0.0\n-rwxr-xr-x 1 user user 241640 jul  6 17:27 libpmi.so.0.0.0\nNOTE: Since Slurm and PMIx lower than 4.x both provide libpmi[2].so\nlibraries, we recommend you install both pieces of software in\ndifferent locations. Otherwise, these same libraries might end up being\ninstalled under standard locations like /usr/lib64 and the\npackage manager would error out, reporting the conflict.NOTE: Any application compiled against PMIx should use the same PMIx\nor at least a PMIx with the same security domain than the one Slurm is using,\notherwise there could be authentication issues. E.g. one PMIx compiled\n--with-munge while another compiled --without-munge (the default since PMIx\n4.2.4). A workaround which might work is to specify the desired security method\nadding \"--mca psec native\" to the cli or exporting PMIX_MCA_psec=native\nenvironment variable.\nNOTE: If you are setting up a test environment using multiple-slurmd,\nthe TmpFS option in your slurm.conf needs to be specified and the number of\ndirectory paths created needs to equal the number of nodes. These directories\nare used by the Slurm PMIx plugin to create temporal files and/or UNIX sockets.\nHere is an example setup for two nodes named compute[1-2]:\nslurm.conf:\nTmpFS=/home/user/slurm/22.05/inst/tmp/slurmd-tmpfs-%n\n\n$ mkdir /home/user/slurm/22.05/inst/tmp/slurmd-tmpfs-compute1\n$ mkdir /home/user/slurm/22.05/inst/tmp/slurmd-tmpfs-compute2\nTesting Slurm and PMIx\n\nIt is possible to directly test Slurm and PMIx without needing to have an\nMPI implementation installed. Here is an example demonstrating that\nboth components work properly:\n$ srun --mpi=list\nMPI plugin types are...\n\tcray_shasta\n\tnone\n\tpmi2\n\tpmix\nspecific pmix plugin versions available: pmix_v3,pmix_v4\n\n$ srun --mpi=pmix_v4 -n2 -N2 \\\n> /home/user/git/pmix/test/pmix_client -n 2 --job-fence -c\n==141756== OK\n==141774== OK\n\nOpenMPI\n\nThe current versions of Slurm and Open MPI support task launch using the\nsrun command.If OpenMPI is configured with --with-pmi= pointing to either Slurm's\nPMI-1 libpmi.so or PMI-2 libpmi2.so libraries, the OMPI jobs can then be\nlaunched directly using the srun command. This is the preferred mode of\noperation since accounting features and affinity done by Slurm will become\navailable. If pmi2 support is enabled, the option '--mpi=pmi2' must be\nspecified on the srun command line.\nAlternately configure 'MpiDefault=pmi' or 'MpiDefault=pmi2' in slurm.conf.Starting with Open MPI version 3.1, PMIx is natively supported. To launch\nOpen MPI applications using PMIx the '--mpi=pmix' option must be specified on\nthe srun command line or 'MpiDefault=pmix' must be configured in slurm.conf.It is also possible to build OpenMPI using an external PMIx installation.\nRefer to the OpenMPI documentation for a detailed procedure but it basically\nconsists of specifying --with-pmix=PATH when configuring OpenMPI.\nNote that if building OpenMPI using an external PMIx installation, both OpenMPI\nand PMIx need to be built against the same libevent/hwloc installations.\nOpenMPI configure script provides the options\n--with-libevent=PATH  and/or --with-hwloc=PATH to make OpenMPI\nmatch what PMIx was built against.A set of parameters are available to control the behavior of the\nSlurm PMIx plugin, read mpi.conf for more\ninformation.NOTE: OpenMPI has a limitation that does not support calls to\nMPI_Comm_spawn() from within a Slurm allocation. If you need to\nuse the MPI_Comm_spawn() function you will need to use another MPI\nimplementation combined with PMI-2 since PMIx doesn't support it either.NOTE: Some kernels and system configurations have resulted in a locked\nmemory too small for proper OpenMPI functionality, resulting in application\nfailure with a segmentation fault. This may be fixed by configuring the slurmd\ndaemon to execute with a larger limit. For example, add \"LimitMEMLOCK=infinity\"\nto your slurmd.service file.Intel MPI\n\nIntel\u00ae MPI Library for Linux OS supports the following methods of\nlaunching the MPI jobs under the control of the Slurm job manager:\n\nThe mpirun command over the Hydra PM\n\nThe srun command (Slurm, recommended)\n\nThis description provides detailed information on these two methods.The mpirun Command over the Hydra Process Manager\n\nSlurm is supported by the mpirun command of the Intel\u00ae MPI Library\nthrough the Hydra Process Manager by default. When launched within an allocation\nthe mpirun command will automatically read the environment variables set\nby Slurm such as nodes, cpus, tasks, etc, in order to start the required\nhydra daemons on every node. These daemons will be started using srun and\nwill subsequently start the user application. Since Intel\u00ae MPI supports\nonly PMI-1 and PMI-2 (not PMIx), it is highly recommended to configure this mpi\nimplementation to use Slurm's PMI-2, which offers better scalability than PMI-1.\nPMI-1 is not recommended and should be deprecated soon.Below is an example of how a user app can be launched within an exclusive\nallocation of 10 nodes using Slurm's PMI-2 library installed from contribs:\n$ salloc -N10 --exclusive\n$ export I_MPI_PMI_LIBRARY=/path/to/slurm/lib/libpmi2.so\n$ mpirun -np <num_procs> user_app.bin\nNote that by default Slurm will inject two environment variables to ensure\nmpirun or mpiexec will use the Slurm bootstrap mechanism (srun) to launch Hydra.\nWith these, Hydra will also pass the argument '--external-launcher' to srun in\norder to not consider these hydra processes as regular steps. This automatic\nvariable injection mechanism can be disabled by setting\n\"MpiParams=disable_slurm_hydra_bootstrap\" in slurm.conf, with the exception\nof the cases where \"slurm\" is already explicitly set as a bootstrap.It is possible to run Intel MPI using a different bootstrap mechanism. To do\nso, explicitly set the following environment variable prior to submitting the\njob with sbatch or salloc:\n$ export I_MPI_HYDRA_BOOTSTRAP=ssh\n$ salloc -N10\n$ mpirun -np <num_procs> user_app.bin\nThe srun Command (Slurm, recommended)\n\nThis method is also supported by the Intel\u00ae MPI Library.\nThis method is the best integrated with Slurm and supports process tracking,\naccounting, task affinity, suspend/resume and other features.\nAs in the previous case, we show an example of how a user app can be\nlaunched within an exclusive allocation of 10 nodes using Slurm's PMI-2 library\ninstalled from contribs, allowing it to take advantage of of all the Slurm\nfeatures. This can be done with sbatch or salloc commands:\n$ salloc -N10 --exclusive\n$ export I_MPI_PMI_LIBRARY=/path/to/slurm/lib/libpmi2.so\n$ srun user_app.bin\nNOTE: The reason we're pointing manually to Slurm's PMI-1 or PMI-2\nlibrary is for licensing reasons. IMPI doesn't link directly to any external\nPMI implementations so, unlike other stacks (OMPI, MPICH, MVAPICH...), Intel is\nnot built against Slurm libs. Pointing to this library will cause Intel to\ndlopen and use this PMI library.NOTE: There is no official support provided by Intel against PMIx\nlibraries. Since IMPI is based on MPICH, using PMIx with Intel may work due to\nPMIx maintaining compatibility with pmi2 (which are the libraries used in MPICH)\nbut it is not guaranteed to run in all cases and PMIx could break this\ncompatibility in future versions. For more information see:\nIntel MPI Library\n.\nMPICH\n\nMPICH was formerly known as MPICH2.MPICH jobs can be launched using srun or mpiexec.\nBoth modes of operation are described below. The MPICH implementation supports\nPMI-1, PMI-2 and PMIx (starting with MPICH v4).\nNote that by default Slurm will inject two environment variables to ensure\nmpirun or mpiexec will use the Slurm bootstrap mechanism (srun) to launch Hydra.\nWith these, Hydra will also pass the argument '--external-launcher' to srun in\norder to not consider these hydra processes as regular steps. This automatic\nvariable injection mechanism can be disabled by setting\n\"MpiParams=disable_slurm_hydra_bootstrap\" in slurm.conf, with the exception\nof the cases where \"slurm\" is already explicitly set as a bootstrap.It is possible to run MPICH using a different bootstrap mechanism. To do\nso, explicitly set the following environment variable prior to submitting the\njob with sbatch or salloc:\n$ export HYDRA_BOOTSTRAP=ssh\n$ salloc -N10\n$ mpirun -np <num_procs> user_app.bin\n\nMPICH with srun and linked with Slurm's PMI-1 or PMI-2 libs\n\nMPICH can be built specifically for use with Slurm and its PMI-1 or PMI-2\nlibraries using a configure line similar to that shown below. Building this way\nwill force the use of this library on every execution. Note that the\nLD_LIBRARY_PATH may not be necessary depending on your Slurm installation path:\n For PMI-2:\nuser@testbox:~/mpich-4.0.2/build$ LD_LIBRARY_PATH=~/slurm/22.05/inst/lib/ \\\n> ../configure --prefix=/home/user/bin/mpich/ --with-pmilib=slurm \\\n> --with-pmi=pmi2 --with-slurm=/home/lipi/slurm/master/inst\n\nor for PMI-1:\n\nuser@testbox:~/mpich-4.0.2/build$ LD_LIBRARY_PATH=~/slurm/22.05/inst/lib/ \\\n> ../configure --prefix=/home/user/bin/mpich/ --with-pmilib=slurm \\\n> --with-slurm=/home/user/slurm/22.05/inst\n These configure lines will detect the Slurm's installed PMI libraries and\nlink against them, but will not install the mpiexec commands. Since PMI-1\nis already old and doesn't scale well we don't recommend you link against it.\nIt is preferable to use PMI-2. You can follow this example to run a job with\nPMI-2:\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmi2 ./hello_world\nA Slurm upgrade will not affect this MPICH installation. There is only one\nunlikely scenario where a recompile of the MPI stack would be needed after an\nupgrade, which is when we forcibly link against Slurm's PMI-1 and/or PMI-2\nlibraries and if their APIs ever changed. These should not change often but\nif it were to happen, it would be noted in Slurm's RELEASE_NOTES file.MPICH with PMIx and integrated with Slurm\n\n You can also build MPICH using an external PMIx library which should be the\nsame one used when building Slurm:\n$ LD_LIBRARY_PATH=~/slurm/22.05/inst/lib/ ../configure \\\n> --prefix=/home/user/bin/mpich/ \\\n> --with-pmix=/home/user/bin/pmix_4.1.2/ \\\n> --with-pmi=pmix \\\n> --with-slurm=/home/user/slurm/master/inst\nAfter building this way, any execution must be made with Slurm (srun) since\nthe Hydra process manager is not installed, as it was in previous examples.\nCompile and run a process with:\n\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmix ./hello_world\nMPICH with its internal PMI and integrated with Slurm\n\nAnother option is to just compile MPICH but not set --with-pmilib,\n--with-pmix or --with-pmi, and only keep --with-slurm.\nIn that case, MPICH will not forcibly link against any PMI libraries and it will\ninstall the mpiexec.hydra command by default. This will cause it to use its\ninternal PMI implementation (based on PMI-1) and Slurm API functions to detect\nthe job environment and launch processes accordingly:\n\nuser@testbox:~/mpich-4.0.2/build$ ../configure \\\n> --prefix=/home/user/bin/mpich/ \\\n> --with-slurm=/home/user/slurm/22.05/inst\nThen the app can be run with srun or mpiexec:\n$ mpicc -o hello_world hello_world.c\n$ srun ./hello_world\nor\n$ mpiexec.hydra ./hello_world\nmpiexec.hydra will spawn its daemons using Slurm steps launched with srun and\nwill use its internal PMI implementation.\nNOTE: In this case, compiling with the --with-slurm option\ncreated the Hydra bootstrap commands (mpiexec.hydra and others) and linked them\nagainst the versioned Slurm's main public API (libslurm.so.X.0.0). That is\nbecause these commands use some Slurm functions to detect the job environment.\nBe aware then that upgrading Slurm would need a recompile of the MPICH stack.\nIt is usually enough to symlink the name of the linked library to the new one,\nbut this is not guaranteed to work.\n\nMPICH without Slurm integration\n\n\nFinally, it is possible to compile MPICH without integrating it with Slurm.\nIn that case it will not identify the job and will just run the processes as if\nit were on a local machine. We recommend reading MPICH documentation and the\nconfigure scripts for more information on the existing possibilities.\n\n\n\nMVAPICH2\n\n\n\nMVAPICH2 has support for Slurm. To enable it you need to build MVAPICH2\nwith a command similar to this:\n\n$ ./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/22.05/inst/\n\nNOTE:In certain MVAPICH2 versions and when building with GCC > 10.x,\nit is possible that these flags must be prepended to the configure line:\n\nFFLAGS=\"-std=legacy\" FCFLAGS=\"-std=legacy\" ./configure ...\n\nWhen MVAPICH2 is built with Slurm support it will detect that it is\nwithin a Slurm allocation, and will use the 'srun' command to spawn its hydra\ndaemons. It does not link to the Slurm API, which means that during an upgrade\nof Slurm there is no need to recompile MVAPICH2. By default it will use the\ninternal PMI implementation.\nNote that by default Slurm will inject two environment variables to ensure\nmpirun or mpiexec will use the Slurm bootstrap mechanism (srun) to launch Hydra.\nWith these, Hydra will also pass the argument '--external-launcher' to srun in\norder to not consider these hydra processes as regular steps. This automatic\nvariable injection mechanism can be disabled by setting\n\"MpiParams=disable_slurm_hydra_bootstrap\" in slurm.conf, with the exception\nof the cases where \"slurm\" is already explicitly set as a bootstrap.\nIt is possible to run MVAPICH2 using a different bootstrap mechanism. To do\nso, explicitly set the following environment variable prior to submitting the\njob with sbatch or salloc:\n\n$ export HYDRA_BOOTSTRAP=ssh\n$ salloc -N10\n$ mpirun -np <num_procs> user_app.bin\n\n\nMVAPICH2 with srun and linked with Slurm's PMI-1 or PMI-2 libs\n\n\nIt is possible to force MVAPICH2 to use one of the Slurm's PMI-1\n(libpmi.so.0.0.0)or PMI-2 (libpmi2.so.0.0.0) libraries. Building with this mode\nwill cause all the executions to use Slurm and its PMI libraries.\nThe Hydra process manager binaries (mpiexec) won't be installed. In fact the\nmpiexec command will exist as a symbolic link to Slurm's srun command. It is\nrecommended not to use PMI-1, but to use at least PMI-2 libs.\nSee below for an example of how to configure and usage:\n\nFor PMI-2:\n\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/  \\\n> --with-pm=slurm --with-pmi=pmi2\n\nand for PMI-1:\n\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/ \\\n> --with-pm=slurm --with-pmi=pmi1\n\nTo compile and run a user application in Slurm:\n\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmi2 ./hello_world\n\nFor more information, please see the MVAPICH2 documentation on their\nwebpage\n\nMVAPICH2 with Slurm support and linked with external PMIx\n\n\nIt is possible to use PMIx within MVAPICH2 and integrated with Slurm. This\nway no Hydra Process Manager will be installed and the user apps will need to\nrun with srun, assuming Slurm has been compiled against the same or a compatible\nPMIx version as the one used when building MVAPICH2.\nTo build MVAPICH2 to use PMIx and integrated with Slurm, a configuration line\nsimilar to this is needed:\n\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/ \\\n> --with-pm=slurm \\\n> --with-pmix=/home/user/bin/pmix_4.1.2/ \\\n> --with-pmi=pmix\n\nRunning a job looks similar to previous examples:\n\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmix ./hello_world\n\nNOTE: In the MVAPICH2 case, compiling with integration with Slurm\n(--with-slurm) doesn't add any dependency to commands or libraries, so\nupgrading Slurm should be safe without any need to recompile MVAPICH2. There\nis only one unlikely scenario where a recompile of the MPI stack would be\nneeded after an upgrade, which is when we forcibly link against Slurm's PMI-1\nand/or PMI-2 libraries and if their APIs ever changed. These should not change\noften but if it were to happen, it would be noted in Slurm's RELEASE_NOTES\nfile.\n\nHPE Cray PMI support\n\n\nSlurm comes by default with a Cray PMI vendor-specific plugin which provides\ncompatibility with the HPE Cray Programming Environment's PMI. It is intended to\nbe used in applications built with this environment on HPE Cray machines.\nThe plugin is named cray_shasta (Shasta was the first Cray\narchitecture this plugin supported) and built by default in all Slurm\ninstallations. Its availability is shown by running the following command:\n\n$ srun --mpi=list\nMPI plugin types are...\n\tcray_shasta\n\tnone\n\nThe Cray PMI plugin will use some reserved ports for its communication. These\nports are configurable by using --resv-ports option on the command line\nwith srun, or by setting MpiParams=ports=[port_range]\nin your slurm.conf. The first port listed in this option will be used as the\nPMI control port, defined by Cray as the PMI_CONTROL_PORT environment\nvariable. There cannot be more than one application launched in the same node\nusing the same PMI_CONTROL_PORT.\nThis plugin does not support MPMD/heterogeneous jobs and it requires\nlibpals >= 0.2.8.\nLast modified 21 June 2024\n"
        },
        {
            "title": "\n\nMVAPICH2\n\n\n",
            "content": "MVAPICH2 has support for Slurm. To enable it you need to build MVAPICH2\nwith a command similar to this:\n$ ./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/22.05/inst/\nNOTE:In certain MVAPICH2 versions and when building with GCC > 10.x,\nit is possible that these flags must be prepended to the configure line:\nFFLAGS=\"-std=legacy\" FCFLAGS=\"-std=legacy\" ./configure ...\nWhen MVAPICH2 is built with Slurm support it will detect that it is\nwithin a Slurm allocation, and will use the 'srun' command to spawn its hydra\ndaemons. It does not link to the Slurm API, which means that during an upgrade\nof Slurm there is no need to recompile MVAPICH2. By default it will use the\ninternal PMI implementation.Note that by default Slurm will inject two environment variables to ensure\nmpirun or mpiexec will use the Slurm bootstrap mechanism (srun) to launch Hydra.\nWith these, Hydra will also pass the argument '--external-launcher' to srun in\norder to not consider these hydra processes as regular steps. This automatic\nvariable injection mechanism can be disabled by setting\n\"MpiParams=disable_slurm_hydra_bootstrap\" in slurm.conf, with the exception\nof the cases where \"slurm\" is already explicitly set as a bootstrap.It is possible to run MVAPICH2 using a different bootstrap mechanism. To do\nso, explicitly set the following environment variable prior to submitting the\njob with sbatch or salloc:\n$ export HYDRA_BOOTSTRAP=ssh\n$ salloc -N10\n$ mpirun -np <num_procs> user_app.bin\n\nMVAPICH2 with srun and linked with Slurm's PMI-1 or PMI-2 libs\n\nIt is possible to force MVAPICH2 to use one of the Slurm's PMI-1\n(libpmi.so.0.0.0)or PMI-2 (libpmi2.so.0.0.0) libraries. Building with this mode\nwill cause all the executions to use Slurm and its PMI libraries.\nThe Hydra process manager binaries (mpiexec) won't be installed. In fact the\nmpiexec command will exist as a symbolic link to Slurm's srun command. It is\nrecommended not to use PMI-1, but to use at least PMI-2 libs.\nSee below for an example of how to configure and usage:\nFor PMI-2:\n\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/  \\\n> --with-pm=slurm --with-pmi=pmi2\n\nand for PMI-1:\n\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/ \\\n> --with-pm=slurm --with-pmi=pmi1\nTo compile and run a user application in Slurm:\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmi2 ./hello_world\nFor more information, please see the MVAPICH2 documentation on their\nwebpage\nMVAPICH2 with Slurm support and linked with external PMIx\n\nIt is possible to use PMIx within MVAPICH2 and integrated with Slurm. This\nway no Hydra Process Manager will be installed and the user apps will need to\nrun with srun, assuming Slurm has been compiled against the same or a compatible\nPMIx version as the one used when building MVAPICH2.To build MVAPICH2 to use PMIx and integrated with Slurm, a configuration line\nsimilar to this is needed:\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/ \\\n> --with-pm=slurm \\\n> --with-pmix=/home/user/bin/pmix_4.1.2/ \\\n> --with-pmi=pmix\nRunning a job looks similar to previous examples:\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmix ./hello_world\nNOTE: In the MVAPICH2 case, compiling with integration with Slurm\n(--with-slurm) doesn't add any dependency to commands or libraries, so\nupgrading Slurm should be safe without any need to recompile MVAPICH2. There\nis only one unlikely scenario where a recompile of the MPI stack would be\nneeded after an upgrade, which is when we forcibly link against Slurm's PMI-1\nand/or PMI-2 libraries and if their APIs ever changed. These should not change\noften but if it were to happen, it would be noted in Slurm's RELEASE_NOTES\nfile.\nHPE Cray PMI support\n\nSlurm comes by default with a Cray PMI vendor-specific plugin which provides\ncompatibility with the HPE Cray Programming Environment's PMI. It is intended to\nbe used in applications built with this environment on HPE Cray machines.The plugin is named cray_shasta (Shasta was the first Cray\narchitecture this plugin supported) and built by default in all Slurm\ninstallations. Its availability is shown by running the following command:\n$ srun --mpi=list\nMPI plugin types are...\n\tcray_shasta\n\tnone\nThe Cray PMI plugin will use some reserved ports for its communication. These\nports are configurable by using --resv-ports option on the command line\nwith srun, or by setting MpiParams=ports=[port_range]\nin your slurm.conf. The first port listed in this option will be used as the\nPMI control port, defined by Cray as the PMI_CONTROL_PORT environment\nvariable. There cannot be more than one application launched in the same node\nusing the same PMI_CONTROL_PORT.This plugin does not support MPMD/heterogeneous jobs and it requires\nlibpals >= 0.2.8.Last modified 21 June 2024"
        },
        {
            "title": "MPICH without Slurm integration\n\n",
            "content": "Finally, it is possible to compile MPICH without integrating it with Slurm.\nIn that case it will not identify the job and will just run the processes as if\nit were on a local machine. We recommend reading MPICH documentation and the\nconfigure scripts for more information on the existing possibilities.\n\nMVAPICH2\n\n\nMVAPICH2 has support for Slurm. To enable it you need to build MVAPICH2\nwith a command similar to this:\n$ ./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/22.05/inst/\nNOTE:In certain MVAPICH2 versions and when building with GCC > 10.x,\nit is possible that these flags must be prepended to the configure line:\nFFLAGS=\"-std=legacy\" FCFLAGS=\"-std=legacy\" ./configure ...\nWhen MVAPICH2 is built with Slurm support it will detect that it is\nwithin a Slurm allocation, and will use the 'srun' command to spawn its hydra\ndaemons. It does not link to the Slurm API, which means that during an upgrade\nof Slurm there is no need to recompile MVAPICH2. By default it will use the\ninternal PMI implementation.Note that by default Slurm will inject two environment variables to ensure\nmpirun or mpiexec will use the Slurm bootstrap mechanism (srun) to launch Hydra.\nWith these, Hydra will also pass the argument '--external-launcher' to srun in\norder to not consider these hydra processes as regular steps. This automatic\nvariable injection mechanism can be disabled by setting\n\"MpiParams=disable_slurm_hydra_bootstrap\" in slurm.conf, with the exception\nof the cases where \"slurm\" is already explicitly set as a bootstrap.It is possible to run MVAPICH2 using a different bootstrap mechanism. To do\nso, explicitly set the following environment variable prior to submitting the\njob with sbatch or salloc:\n$ export HYDRA_BOOTSTRAP=ssh\n$ salloc -N10\n$ mpirun -np <num_procs> user_app.bin\n\nMVAPICH2 with srun and linked with Slurm's PMI-1 or PMI-2 libs\n\nIt is possible to force MVAPICH2 to use one of the Slurm's PMI-1\n(libpmi.so.0.0.0)or PMI-2 (libpmi2.so.0.0.0) libraries. Building with this mode\nwill cause all the executions to use Slurm and its PMI libraries.\nThe Hydra process manager binaries (mpiexec) won't be installed. In fact the\nmpiexec command will exist as a symbolic link to Slurm's srun command. It is\nrecommended not to use PMI-1, but to use at least PMI-2 libs.\nSee below for an example of how to configure and usage:\nFor PMI-2:\n\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/  \\\n> --with-pm=slurm --with-pmi=pmi2\n\nand for PMI-1:\n\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/ \\\n> --with-pm=slurm --with-pmi=pmi1\nTo compile and run a user application in Slurm:\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmi2 ./hello_world\nFor more information, please see the MVAPICH2 documentation on their\nwebpage\nMVAPICH2 with Slurm support and linked with external PMIx\n\nIt is possible to use PMIx within MVAPICH2 and integrated with Slurm. This\nway no Hydra Process Manager will be installed and the user apps will need to\nrun with srun, assuming Slurm has been compiled against the same or a compatible\nPMIx version as the one used when building MVAPICH2.To build MVAPICH2 to use PMIx and integrated with Slurm, a configuration line\nsimilar to this is needed:\n./configure --prefix=/home/user/bin/mvapich2 \\\n> --with-slurm=/home/user/slurm/master/inst/ \\\n> --with-pm=slurm \\\n> --with-pmix=/home/user/bin/pmix_4.1.2/ \\\n> --with-pmi=pmix\nRunning a job looks similar to previous examples:\n$ mpicc -o hello_world hello_world.c\n$ srun --mpi=pmix ./hello_world\nNOTE: In the MVAPICH2 case, compiling with integration with Slurm\n(--with-slurm) doesn't add any dependency to commands or libraries, so\nupgrading Slurm should be safe without any need to recompile MVAPICH2. There\nis only one unlikely scenario where a recompile of the MPI stack would be\nneeded after an upgrade, which is when we forcibly link against Slurm's PMI-1\nand/or PMI-2 libraries and if their APIs ever changed. These should not change\noften but if it were to happen, it would be noted in Slurm's RELEASE_NOTES\nfile.\nHPE Cray PMI support\n\nSlurm comes by default with a Cray PMI vendor-specific plugin which provides\ncompatibility with the HPE Cray Programming Environment's PMI. It is intended to\nbe used in applications built with this environment on HPE Cray machines.The plugin is named cray_shasta (Shasta was the first Cray\narchitecture this plugin supported) and built by default in all Slurm\ninstallations. Its availability is shown by running the following command:\n$ srun --mpi=list\nMPI plugin types are...\n\tcray_shasta\n\tnone\nThe Cray PMI plugin will use some reserved ports for its communication. These\nports are configurable by using --resv-ports option on the command line\nwith srun, or by setting MpiParams=ports=[port_range]\nin your slurm.conf. The first port listed in this option will be used as the\nPMI control port, defined by Cray as the PMI_CONTROL_PORT environment\nvariable. There cannot be more than one application launched in the same node\nusing the same PMI_CONTROL_PORT.This plugin does not support MPMD/heterogeneous jobs and it requires\nlibpals >= 0.2.8.Last modified 21 June 2024"
        }
    ]
}