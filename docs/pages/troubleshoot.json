{
    "url": "https://slurm.schedmd.com/troubleshoot.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Slurm Troubleshooting Guide",
            "content": "This guide is meant as a tool to help system administrators\nor operators troubleshoot Slurm failures and restore services.\nThe Frequently Asked Questions document\nmay also prove useful.\nSlurm is not responding\nJobs are not getting scheduled\nJobs and nodes are stuck in COMPLETING state\nNodes are getting set to a DOWN state\nNetworking and configuration problems\nSlurm is not responding\n\n\nExecute \"scontrol ping\" to determine if the primary\nand backup controllers are responding.\n\nIf it responds for you, this could be a networking\nor configuration problem specific to some user or node in the\ncluster.\nIf not responding, directly login to the machine and try again\nto rule out network and configuration problems.\nIf still not responding, check if there is an active slurmctld\ndaemon by executing \"ps -el | grep slurmctld\".\nIf slurmctld is not running, restart it (typically as user root\nusing the command \"/etc/init.d/slurm start\").\nYou should check the log file (SlurmctldLog in the\nslurm.conf file) for an indication of why it failed.\nIf slurmctld is running but not responding (a very rare situation),\nthen kill and restart it (typically as user root using the commands\n\"/etc/init.d/slurm stop\" and then \"/etc/init.d/slurm start\").\nIf it hangs again, increase the verbosity of debug messages\n(increase SlurmctldDebug in the slurm.conf file)\nand restart.\nAgain check the log file for an indication of why it failed.\nIf it continues to fail without an indication as to the failure\nmode, restart without preserving state (typically as user root\nusing the commands \"/etc/init.d/slurm stop\"\nand then \"/etc/init.d/slurm startclean\").\nNote: All running jobs and other state information will be lost.\nJobs are not getting scheduled\n\nThis is dependent upon the scheduler used by Slurm.\nExecuting the command \"scontrol show config | grep SchedulerType\"\nto determine this.\nFor any scheduler, you can check priorities of jobs using the\ncommand \"scontrol show job\".\nIf the scheduler type is builtin, then jobs will be executed\nin the order of submission for a given partition.\nEven if resources are available to initiate jobs immediately,\nit will be deferred until no previously submitted job is pending.\nIf the scheduler type is backfill, then jobs will generally\nbe executed in the order of submission for a given partition with one\nexception: later submitted jobs will be initiated early if doing so\ndoes not delay the expected execution time of an earlier submitted job.\nIn order for backfill scheduling to be effective, users jobs should\nspecify reasonable time limits.\nIf jobs do not specify time limits, then all jobs will receive the\nsame time limit (that associated with the partition), and the ability\nto backfill schedule jobs will be limited.\nThe backfill scheduler does not alter job specifications of required\nor excluded nodes, so jobs which specify nodes will substantially\nreduce the effectiveness of backfill scheduling.\nSee the backfill documentation\nfor more details.\nJobs and nodes are stuck in COMPLETING state\n\nThis is typically due to non-killable processes associated with the job.\nSlurm will continue to attempt terminating the processes with SIGKILL, but\nsome jobs may be stuck performing I/O and non-killable.\nThis is typically due to a file system problem and may be addressed in\na couple of ways.\nFix the file system and/or reboot the node. -OR-\nSet the node to a DOWN state and then return it to service\n(\"scontrol update NodeName=<node> State=down Reason=hung_proc\"\nand \"scontrol update NodeName=<node> State=resume\").\nThis permits other jobs to use the node, but leaves the non-killable\nprocess in place.\nIf the process should ever complete the I/O, the pending SIGKILL\nshould terminate it immediately. -OR-\nUse the UnkillableStepProgram and UnkillableStepTimeout\nconfiguration parameters to automatically respond to processes which can not\nbe killed, by sending email or rebooting the node. For more information,\nsee the slurm.conf documentation.\n\nIf it doesn't look like your job is stuck because of filesystem problems\nit may take some debugging to find the cause.  If you can reproduce\nthe behavior you can set the SlurmdDebug level to 'debug' and restart\nslurmd on a node you'll use to reproduce the problem.  The slurmd.log\nfile should then have more information to help troubleshoot the issue.\n\nLooking at slurmctld.log may also provide clues. If nodes stop responding,\nyou may want to look into why since they may prevent job cleanup and\ncause jobs to remain in a COMPLETING state. When looking for connectivity\nproblems, the relevant log entries should look something like this:\n\nerror: Nodes node[00,03,25] not responding\nNode node00 now responding\n\nNodes are getting set to a DOWN state\n\n\nCheck the reason why the node is down using the command\n\"scontrol show node <name>\".\nThis will show the reason why the node was set down and the\ntime when it happened.\nIf there is insufficient disk space, memory space, etc. compared\nto the parameters specified in the slurm.conf file then\neither fix the node or change slurm.conf.\nIf the reason is \"Not responding\", then check communications\nbetween the control machine and the DOWN node using the command\n\"ping <address>\" being sure to specify the\nNodeAddr values configured in slurm.conf.\nIf ping fails, then fix the network or addresses in slurm.conf.\nNext, login to a node that Slurm considers to be in a DOWN\nstate and check if the slurmd daemon is running with the command\n\"ps -el | grep slurmd\".\nIf slurmd is not running, restart it (typically as user root\nusing the command \"/etc/init.d/slurm start\").\nYou should check the log file (SlurmdLog in the\nslurm.conf file) for an indication of why it failed.\nYou can get the status of the running slurmd daemon by\nexecuting the command \"scontrol show slurmd\" on\nthe node of interest.\nCheck the value of \"Last slurmctld msg time\" to determine\nif the slurmctld is able to communicate with the slurmd.\nIf slurmd is running but not responding (a very rare situation),\nthen kill and restart it (typically as user root using the commands\n\"/etc/init.d/slurm stop\" and then \"/etc/init.d/slurm start\").\nIf still not responding, try again to rule out\nnetwork and configuration problems.\nIf still not responding, increase the verbosity of debug messages\n(increase SlurmdDebug in the slurm.conf file)\nand restart.\nAgain check the log file for an indication of why it failed.\nIf still not responding without an indication as to the failure\nmode, restart without preserving state (typically as user root\nusing the commands \"/etc/init.d/slurm stop\"\nand then \"/etc/init.d/slurm startclean\").\nNote: All jobs and other state information on that node will be lost.\nNetworking and configuration problems\n\n\nCheck the controller and/or slurmd log files (SlurmctldLog\nand SlurmdLog in the slurm.conf file) for an indication\nof why it is failing.\nCheck for consistent slurm.conf and credential files on\nthe node(s) experiencing problems.\nIf this is user-specific problem, check that the user is\nconfigured on the controller computer(s) as well as the\ncompute nodes.\nThe user doesn't need to be able to login, but his user ID\nmust exist.\nCheck that compatible versions of Slurm exists on all of\nthe nodes (execute \"sinfo -V\" or \"rpm -qa | grep slurm\").\nThe Slurm version number contains three period-separated numbers\nthat represent both the major Slurm release and maintenance release level.\nThe first two parts combine together to represent the major release, and match\nthe year and month of that major release. The third number in the version\ndesignates a specific maintenance level:\nyear.month.maintenance-release (e.g. 17.11.5 is major Slurm release 17.11, and\nmaintenance version 5).\nThus version 17.11.x was initially released in November 2017.\nSlurm daemons will support RPCs and state files from the two previous major\nreleases (e.g. a version 17.11.x SlurmDBD will support slurmctld daemons and\ncommands with a version of 17.11.x, 17.02.x or 16.05.x).\nLast modified 28 May 2020"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        }
    ]
}