{
    "url": "https://slurm.schedmd.com/programmer_guide.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Slurm Programmer's Guide",
            "content": "Contents\nOverview\nPlugins\nDirectory Structure\nDocumentation\nSource Code\nSource Code Management\nAdding New Modules\nCompiling\nConfiguration\nTest Suite\nAdding Files and Directories\nTricks of the Trade\nOverviewSlurm is an open source, fault-tolerant,\nand highly scalable cluster management and job scheduling system for large and\nsmall Linux clusters. Components include machine status, partition management,\njob management, scheduling, and stream copy modules. Slurm requires no kernel\nmodifications for it operation and is relatively self-contained.\n\nSlurm is written in the C language and uses a GNU autoconf configuration\nengine. While initially written for Linux, other UNIX-like operating systems should\nbe easy porting targets. Code should adhere to the \nLinux kernel coding style. (Some components of Slurm have been taken from\nvarious sources. Some of these components do not conform to the Linux kernel\ncoding style. However, new code written for Slurm should follow these standards.)\nPlugins\nTo make the use of different infrastructures possible, Slurm uses a general\npurpose plugin mechanism. A Slurm plugin is a dynamically linked code object that\nis loaded explicitly at run time by the Slurm libraries. It provides a customized\nimplementation of a well-defined API connected to tasks such as authentication,\ninterconnect fabric, task scheduling, etc. A set of functions is defined for use\nby all of the different infrastructures of a particular variety. When a Slurm\ndaemon is initiated, it reads the configuration file to determine which of the\navailable plugins should be used. A plugin developer's\nguide is available with general information about plugins.\n\n\nDirectory Structure\n\n\nThe contents of the Slurm directory structure will be described below in increasing\ndetail as the structure is descended. The top level directory contains the scripts\nand tools required to build the entire Slurm system. It also contains a variety\nof subdirectories for each type of file.\nGeneral build tools/files include: acinclude.m4,\nconfigure.ac, Makefile.am, Make-rpm.mk, META, README,\nslurm.spec.in, and the contents of the auxdir directory. autoconf\nand make commands are used to build and install\nSlurm in an automated fashion.\nNOTE: autoconf\nversion 2.52 or higher is required to build Slurm. Execute\nautoconf -V to check your version number.\nThe build process is described in the README file.\n\nCopyright and disclaimer information are in the files COPYING and DISCLAIMER.\nAll of the top-level subdirectories are described below.\nauxdir \u2014 Used for building Slurm.\ncontribs \u2014 Various contributed tools.\ndoc \u2014 Documentation including man pages. \netc \u2014 Sample configuration files.\nslurm \u2014 Header files for API use. These files must be installed. Placing\nthese header files in this location makes for better code portability.\nsrc \u2014 Contains all source code and header files not in the \"slurm\" subdirectory\ndescribed above.\ntestsuite \u2014 Check, Expect and Pytest tests are here.\nDocumentation\n\n\nAll of the documentation is in the subdirectory doc.\nTwo directories are of particular interest:\n\ndoc/man \u2014 contains the man pages for the APIs,\nconfiguration file, commands, and daemons.\ndoc/html \u2014 contains the web pages.\nSource Code\n\n\nFunctions are divided into several categories, each in its own subdirectory.\nThe details of each directory's contents are provided below. The directories are\nas follows: \n\napi \u2014 Application Program Interfaces into\nthe Slurm code. Used to send and get Slurm information from the central manager.\nThese are the functions user applications might utilize.\ncommon \u2014 General purpose functions for widespread use throughout\nSlurm.\ndatabase \u2014 Various database files that support the accounting\n storage plugin.\nplugins \u2014 Plugin functions for various infrastructures or optional\nbehavior. A separate subdirectory is used for each plugin class:\n\naccounting_storage for specifying the type of storage for accounting,\nauth for user authentication,\ncred for job credential functions,\njobacct_gather for job accounting,\njobcomp for job completion logging,\nmpi for MPI support,\npriority calculates job priority based on a number of factors\nincluding fair-share,\nproctrack for process tracking,\nsched for job scheduler,\nselect for a job's node selection,\nswitch for switch (interconnect) specific functions,\ntask for task affinity to processors,\ntopology methods for assigning nodes to jobs based on node\ntopology.\n\n\nsacct \u2014 User command to view accounting information about jobs.\nsacctmgr \u2014 User and administrator tool to manage accounting.\nsalloc \u2014 User command to allocate resources for a job.\nsattach \u2014 User command to attach standard input, output and error\nfiles to a running job or job step.\nsbatch \u2014 User command to submit a batch job (script for later execution).\nsbcast \u2014 User command to broadcast a file to all nodes associated\nwith an existing Slurm job.\nscancel \u2014 User command to cancel (or signal) a job or job step.\nscontrol \u2014 Administrator tool to manage Slurm.\nsinfo \u2014 User command to get information on Slurm nodes and partitions.\nslurmctld \u2014 Slurm central manager daemon code.\nslurmd \u2014 Slurm daemon code to manage the compute server nodes including\nthe execution of user applications.\nslurmdbd \u2014 Slurm database daemon managing access to the accounting\nstorage database.\nsprio \u2014 User command to see the breakdown of a job's priority\ncalculation when the Multifactor Job Priority plugin is installed.\nsqueue \u2014 User command to get information on Slurm jobs and job steps.\nsreport \u2014 User command to view various reports about past\nusage across the enterprise.\nsrun \u2014 User command to submit a job, get an allocation, and/or\ninitiation a parallel job step.\nsshare \u2014 User command to view shares and usage when the Multifactor\nJob Priority plugin is installed.\nsstat \u2014 User command to view detailed statistics about running\njobs when a Job Accounting Gather plugin is installed.\nstrigger \u2014 User and administrator tool to manage event triggers.\nsview \u2014 User command to view and update node, partition, and\njob state information.\nSource Code Management\n\n\nThe latest code is in github:\nhttps://github.com/SchedMD/slurm.\nCreating your own branch will make it easier to keep it synchronized\nwith our work.\nAdding New Modules\n\n\nAdd the new file name to the Makefile.am file in the appropriate directory.\nThen execute autoreconf (at the top level of the Slurm source directory).\nNote that a relatively current version of automake is required.\nThe autoreconf program will build Makefile.in files from the Makefile.am files.\nIf any new files need to be installed, update the slurm.spec file to identify\nthe RPM in which the new files should be placed\nIf new directories need to be added, add to the configure.ac file the path\nto the Makefile to be built in the new directory. In summary:\nautoreconf translates .am files into .in files\nconfigure translates .in files, adding paths and version numbers.\nCompiling\nSending the standard output of \"make\" to a file makes it easier to see any\nwarning or error messages:\n\"make -j install >make.out\"\nConfiguration\n\n\nSample configuration files are included in the etc subdirectory.\nThe slurm.conf can be built using a configuration tool.\nSee doc/man/man5/slurm.conf.5 and the man pages for other configuration files\nfor more details.\ninit.d.slurm is a script that determines which\nSlurm daemon(s) should execute on any node based upon the configuration file contents.\nIt will also manage these daemons: starting, signaling, restarting, and stopping them.\nTest Suite\nThe testsuite files use Check, Expect and Pytest for testing Slurm in\ndifferents ways.\nThe Check tests are designed to unit test C code. Only with\nmake check, without Slurm installed, they will validate that key C\nfunctions work correctly.\nWe also have a set of Expect Slurm tests available under the testsuite/expect\ndirectory.  These tests are executed after Slurm has been installed\nand the daemons initiated. These tests exercise all Slurm commands\nand options including stress tests.  The file testsuite/expect/globals\ncontains the Expect test framewrok for all of the individual tests.  At\nthe very least, you will need to set the slurm_dir variable to the correct\nvalue.  To avoid conflicts with other developers, you can override variable settings\nin a separate file named testsuite/expect/globals.local.\nSet your working directory to testsuite/expect before\nstarting these tests.  Tests may be executed individually by name\n(e.g.  test1.1)\nor the full test suite may be executed with the single command\nregression.py.\nSee testsuite/expect/README for more information.\nSlurm also has a Pytest environment that can work like the Expect one, but it\nalso works together with an external QA framework to improve the overall QA\nof Slurm.\nAdding Files and Directories\n\n\nIf you are adding files and directories to Slurm, it will be necessary to\nre-build configuration files before executing the configure command.\nUpdate Makefile.am files as needed then execute\nautoreconf before executing configure.\n\nTricks of the Trade\n\n\nHAVE_FRONT_END\n\n\nYou can make a single node appear to Slurm as a Linux cluster by running\nconfigure with the --enable-front-end option. This\ndefines HAVE_FRONT_END with a non-zero value in the file config.h.\nAll (fake) nodes should be defined in the slurm.conf file.\nThese nodes should be configured with a single NodeAddr value\nindicating the node on which single slurmd daemon\nexecutes.  Initiate one slurmd and one\nslurmctld daemon. Do not initiate too many\nsimultaneous job steps to avoid overloading the\nslurmd daemon executing them all.\nMultiple slurmd support\n\n\nIt is possible to run multiple slurmd daemons on a single node, each using\na different port number and NodeName alias.  This is very useful for testing\nnetworking and protocol changes, or anytime you want to simulate a larger\ncluster than you really have.  The author uses this on his desktop to simulate\nmultiple nodes.  However, it is important to note that not all slurm functions\nwill work with multiple slurmd support enabled (e.g. many switch plugins will\nnot work, it is best not to use any).\nMultiple support is enabled at configure-time with the\n\"--enable-multiple-slurmd\" parameter.  This enables a new parameter in the\nslurm.conf file on the NodeName line, \"Port=\", and adds a new\ncommand line parameter to slurmd, \"-N\".\nEach slurmd needs to have its own NodeName, and its own TCP port number. Here\nis an example of the NodeName lines for running three slurmd daemons on each\nof ten nodes:\n\nNodeName=foo[1-10] NodeHostname=host[1-10]  Port=17001\nNodeName=foo[11-20] NodeHostname=host[1-10] Port=17002\nNodeName=foo[21-30] NodeHostname=host[1-10] Port=17003\n\n\nIt is likely that you will also want to use the \"%n\" symbol in any slurmd\nrelated paths in the slurm.conf file, for instance SlurmdLogFile,\nSlurmdPidFile, and especially SlurmdSpoolDir.  Each slurmd replaces the \"%n\"\nwith its own NodeName.  Here is an example:\n\nSlurmdLogFile=/var/log/slurm/slurmd.%n.log\nSlurmdPidFile=/var/run/slurmd.%n.pid\nSlurmdSpoolDir=/var/spool/slurmd.%n\n\n\nYou can manually start each slurmd daemon with the proper NodeName.\nFor example, to start the slurmd daemons for host1 from the\nabove slurm.conf example:\n\nhost1> slurmd -N foo1\nhost1> slurmd -N foo11\nhost1> slurmd -N foo21\n\n\nIf you have SysV init scripts, slurmd daemons will automatically be started by\nwhenever MULTIPLE_SLURMD is set to yes in /etc/sysconfig/slurm.\nIf your distribution uses systemd, you may want to use templating feature to\ndefine one slurmd.service file and registering each of your virtual nodes\nwithin it, for example:\n\n[Unit]\nDescription=Slurm node daemon\nAfter=network.target munge.service remote-fs.target\nConditionPathExists=/etc/slurm.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmd\nExecStart=/usr/sbin/slurmd -N%i $SLURMD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurmd-%i.pid\nKillMode=process\nLimitNOFILE=51200\nLimitMEMLOCK=infinity\nLimitSTACK=infinity\nDelegate=yes\n\n[Install]\nWantedBy=multi-user.target\n\nThen, enabling/managing a service like this (what is %i in the file\nwill be replaced by what is after the @ in the command line):\n\nsystemctl enable slurmd@nodeXYZ\nsystemctl start/stop/restart slurmd@nodeXYZ\n\n\nLast modified 6 August 2021\n"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        },
        {
            "title": "Plugins",
            "content": "To make the use of different infrastructures possible, Slurm uses a general\npurpose plugin mechanism. A Slurm plugin is a dynamically linked code object that\nis loaded explicitly at run time by the Slurm libraries. It provides a customized\nimplementation of a well-defined API connected to tasks such as authentication,\ninterconnect fabric, task scheduling, etc. A set of functions is defined for use\nby all of the different infrastructures of a particular variety. When a Slurm\ndaemon is initiated, it reads the configuration file to determine which of the\navailable plugins should be used. A plugin developer's\nguide is available with general information about plugins.\n\n\nDirectory Structure\n\n\nThe contents of the Slurm directory structure will be described below in increasing\ndetail as the structure is descended. The top level directory contains the scripts\nand tools required to build the entire Slurm system. It also contains a variety\nof subdirectories for each type of file.\nGeneral build tools/files include: acinclude.m4,\nconfigure.ac, Makefile.am, Make-rpm.mk, META, README,\nslurm.spec.in, and the contents of the auxdir directory. autoconf\nand make commands are used to build and install\nSlurm in an automated fashion.\nNOTE: autoconf\nversion 2.52 or higher is required to build Slurm. Execute\nautoconf -V to check your version number.\nThe build process is described in the README file.\n\nCopyright and disclaimer information are in the files COPYING and DISCLAIMER.\nAll of the top-level subdirectories are described below.\nauxdir \u2014 Used for building Slurm.\ncontribs \u2014 Various contributed tools.\ndoc \u2014 Documentation including man pages. \netc \u2014 Sample configuration files.\nslurm \u2014 Header files for API use. These files must be installed. Placing\nthese header files in this location makes for better code portability.\nsrc \u2014 Contains all source code and header files not in the \"slurm\" subdirectory\ndescribed above.\ntestsuite \u2014 Check, Expect and Pytest tests are here.\nDocumentation\n\n\nAll of the documentation is in the subdirectory doc.\nTwo directories are of particular interest:\n\ndoc/man \u2014 contains the man pages for the APIs,\nconfiguration file, commands, and daemons.\ndoc/html \u2014 contains the web pages.\nSource Code\n\n\nFunctions are divided into several categories, each in its own subdirectory.\nThe details of each directory's contents are provided below. The directories are\nas follows: \n\napi \u2014 Application Program Interfaces into\nthe Slurm code. Used to send and get Slurm information from the central manager.\nThese are the functions user applications might utilize.\ncommon \u2014 General purpose functions for widespread use throughout\nSlurm.\ndatabase \u2014 Various database files that support the accounting\n storage plugin.\nplugins \u2014 Plugin functions for various infrastructures or optional\nbehavior. A separate subdirectory is used for each plugin class:\n\naccounting_storage for specifying the type of storage for accounting,\nauth for user authentication,\ncred for job credential functions,\njobacct_gather for job accounting,\njobcomp for job completion logging,\nmpi for MPI support,\npriority calculates job priority based on a number of factors\nincluding fair-share,\nproctrack for process tracking,\nsched for job scheduler,\nselect for a job's node selection,\nswitch for switch (interconnect) specific functions,\ntask for task affinity to processors,\ntopology methods for assigning nodes to jobs based on node\ntopology.\n\n\nsacct \u2014 User command to view accounting information about jobs.\nsacctmgr \u2014 User and administrator tool to manage accounting.\nsalloc \u2014 User command to allocate resources for a job.\nsattach \u2014 User command to attach standard input, output and error\nfiles to a running job or job step.\nsbatch \u2014 User command to submit a batch job (script for later execution).\nsbcast \u2014 User command to broadcast a file to all nodes associated\nwith an existing Slurm job.\nscancel \u2014 User command to cancel (or signal) a job or job step.\nscontrol \u2014 Administrator tool to manage Slurm.\nsinfo \u2014 User command to get information on Slurm nodes and partitions.\nslurmctld \u2014 Slurm central manager daemon code.\nslurmd \u2014 Slurm daemon code to manage the compute server nodes including\nthe execution of user applications.\nslurmdbd \u2014 Slurm database daemon managing access to the accounting\nstorage database.\nsprio \u2014 User command to see the breakdown of a job's priority\ncalculation when the Multifactor Job Priority plugin is installed.\nsqueue \u2014 User command to get information on Slurm jobs and job steps.\nsreport \u2014 User command to view various reports about past\nusage across the enterprise.\nsrun \u2014 User command to submit a job, get an allocation, and/or\ninitiation a parallel job step.\nsshare \u2014 User command to view shares and usage when the Multifactor\nJob Priority plugin is installed.\nsstat \u2014 User command to view detailed statistics about running\njobs when a Job Accounting Gather plugin is installed.\nstrigger \u2014 User and administrator tool to manage event triggers.\nsview \u2014 User command to view and update node, partition, and\njob state information.\nSource Code Management\n\n\nThe latest code is in github:\nhttps://github.com/SchedMD/slurm.\nCreating your own branch will make it easier to keep it synchronized\nwith our work.\nAdding New Modules\n\n\nAdd the new file name to the Makefile.am file in the appropriate directory.\nThen execute autoreconf (at the top level of the Slurm source directory).\nNote that a relatively current version of automake is required.\nThe autoreconf program will build Makefile.in files from the Makefile.am files.\nIf any new files need to be installed, update the slurm.spec file to identify\nthe RPM in which the new files should be placed\nIf new directories need to be added, add to the configure.ac file the path\nto the Makefile to be built in the new directory. In summary:\nautoreconf translates .am files into .in files\nconfigure translates .in files, adding paths and version numbers.\nCompiling\nSending the standard output of \"make\" to a file makes it easier to see any\nwarning or error messages:\n\"make -j install >make.out\"\nConfiguration\n\n\nSample configuration files are included in the etc subdirectory.\nThe slurm.conf can be built using a configuration tool.\nSee doc/man/man5/slurm.conf.5 and the man pages for other configuration files\nfor more details.\ninit.d.slurm is a script that determines which\nSlurm daemon(s) should execute on any node based upon the configuration file contents.\nIt will also manage these daemons: starting, signaling, restarting, and stopping them.\nTest Suite\nThe testsuite files use Check, Expect and Pytest for testing Slurm in\ndifferents ways.\nThe Check tests are designed to unit test C code. Only with\nmake check, without Slurm installed, they will validate that key C\nfunctions work correctly.\nWe also have a set of Expect Slurm tests available under the testsuite/expect\ndirectory.  These tests are executed after Slurm has been installed\nand the daemons initiated. These tests exercise all Slurm commands\nand options including stress tests.  The file testsuite/expect/globals\ncontains the Expect test framewrok for all of the individual tests.  At\nthe very least, you will need to set the slurm_dir variable to the correct\nvalue.  To avoid conflicts with other developers, you can override variable settings\nin a separate file named testsuite/expect/globals.local.\nSet your working directory to testsuite/expect before\nstarting these tests.  Tests may be executed individually by name\n(e.g.  test1.1)\nor the full test suite may be executed with the single command\nregression.py.\nSee testsuite/expect/README for more information.\nSlurm also has a Pytest environment that can work like the Expect one, but it\nalso works together with an external QA framework to improve the overall QA\nof Slurm.\nAdding Files and Directories\n\n\nIf you are adding files and directories to Slurm, it will be necessary to\nre-build configuration files before executing the configure command.\nUpdate Makefile.am files as needed then execute\nautoreconf before executing configure.\n\nTricks of the Trade\n\n\nHAVE_FRONT_END\n\n\nYou can make a single node appear to Slurm as a Linux cluster by running\nconfigure with the --enable-front-end option. This\ndefines HAVE_FRONT_END with a non-zero value in the file config.h.\nAll (fake) nodes should be defined in the slurm.conf file.\nThese nodes should be configured with a single NodeAddr value\nindicating the node on which single slurmd daemon\nexecutes.  Initiate one slurmd and one\nslurmctld daemon. Do not initiate too many\nsimultaneous job steps to avoid overloading the\nslurmd daemon executing them all.\nMultiple slurmd support\n\n\nIt is possible to run multiple slurmd daemons on a single node, each using\na different port number and NodeName alias.  This is very useful for testing\nnetworking and protocol changes, or anytime you want to simulate a larger\ncluster than you really have.  The author uses this on his desktop to simulate\nmultiple nodes.  However, it is important to note that not all slurm functions\nwill work with multiple slurmd support enabled (e.g. many switch plugins will\nnot work, it is best not to use any).\nMultiple support is enabled at configure-time with the\n\"--enable-multiple-slurmd\" parameter.  This enables a new parameter in the\nslurm.conf file on the NodeName line, \"Port=\", and adds a new\ncommand line parameter to slurmd, \"-N\".\nEach slurmd needs to have its own NodeName, and its own TCP port number. Here\nis an example of the NodeName lines for running three slurmd daemons on each\nof ten nodes:\n\nNodeName=foo[1-10] NodeHostname=host[1-10]  Port=17001\nNodeName=foo[11-20] NodeHostname=host[1-10] Port=17002\nNodeName=foo[21-30] NodeHostname=host[1-10] Port=17003\n\n\nIt is likely that you will also want to use the \"%n\" symbol in any slurmd\nrelated paths in the slurm.conf file, for instance SlurmdLogFile,\nSlurmdPidFile, and especially SlurmdSpoolDir.  Each slurmd replaces the \"%n\"\nwith its own NodeName.  Here is an example:\n\nSlurmdLogFile=/var/log/slurm/slurmd.%n.log\nSlurmdPidFile=/var/run/slurmd.%n.pid\nSlurmdSpoolDir=/var/spool/slurmd.%n\n\n\nYou can manually start each slurmd daemon with the proper NodeName.\nFor example, to start the slurmd daemons for host1 from the\nabove slurm.conf example:\n\nhost1> slurmd -N foo1\nhost1> slurmd -N foo11\nhost1> slurmd -N foo21\n\n\nIf you have SysV init scripts, slurmd daemons will automatically be started by\nwhenever MULTIPLE_SLURMD is set to yes in /etc/sysconfig/slurm.\nIf your distribution uses systemd, you may want to use templating feature to\ndefine one slurmd.service file and registering each of your virtual nodes\nwithin it, for example:\n\n[Unit]\nDescription=Slurm node daemon\nAfter=network.target munge.service remote-fs.target\nConditionPathExists=/etc/slurm.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmd\nExecStart=/usr/sbin/slurmd -N%i $SLURMD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurmd-%i.pid\nKillMode=process\nLimitNOFILE=51200\nLimitMEMLOCK=infinity\nLimitSTACK=infinity\nDelegate=yes\n\n[Install]\nWantedBy=multi-user.target\n\nThen, enabling/managing a service like this (what is %i in the file\nwill be replaced by what is after the @ in the command line):\n\nsystemctl enable slurmd@nodeXYZ\nsystemctl start/stop/restart slurmd@nodeXYZ\n\n\nLast modified 6 August 2021\n"
        },
        {
            "title": "Directory Structure\n\n",
            "content": "The contents of the Slurm directory structure will be described below in increasing\ndetail as the structure is descended. The top level directory contains the scripts\nand tools required to build the entire Slurm system. It also contains a variety\nof subdirectories for each type of file.General build tools/files include: acinclude.m4,\nconfigure.ac, Makefile.am, Make-rpm.mk, META, README,\nslurm.spec.in, and the contents of the auxdir directory. autoconf\nand make commands are used to build and install\nSlurm in an automated fashion.\nNOTE: autoconf\nversion 2.52 or higher is required to build Slurm. Execute\nautoconf -V to check your version number.\nThe build process is described in the README file.\n\nCopyright and disclaimer information are in the files COPYING and DISCLAIMER.\nAll of the top-level subdirectories are described below.\nauxdir \u2014 Used for building Slurm.\ncontribs \u2014 Various contributed tools.\ndoc \u2014 Documentation including man pages. \netc \u2014 Sample configuration files.\nslurm \u2014 Header files for API use. These files must be installed. Placing\nthese header files in this location makes for better code portability.\nsrc \u2014 Contains all source code and header files not in the \"slurm\" subdirectory\ndescribed above.\ntestsuite \u2014 Check, Expect and Pytest tests are here.\nDocumentation\n\n\nAll of the documentation is in the subdirectory doc.\nTwo directories are of particular interest:\n\ndoc/man \u2014 contains the man pages for the APIs,\nconfiguration file, commands, and daemons.\ndoc/html \u2014 contains the web pages.\nSource Code\n\n\nFunctions are divided into several categories, each in its own subdirectory.\nThe details of each directory's contents are provided below. The directories are\nas follows: \n\napi \u2014 Application Program Interfaces into\nthe Slurm code. Used to send and get Slurm information from the central manager.\nThese are the functions user applications might utilize.\ncommon \u2014 General purpose functions for widespread use throughout\nSlurm.\ndatabase \u2014 Various database files that support the accounting\n storage plugin.\nplugins \u2014 Plugin functions for various infrastructures or optional\nbehavior. A separate subdirectory is used for each plugin class:\n\naccounting_storage for specifying the type of storage for accounting,\nauth for user authentication,\ncred for job credential functions,\njobacct_gather for job accounting,\njobcomp for job completion logging,\nmpi for MPI support,\npriority calculates job priority based on a number of factors\nincluding fair-share,\nproctrack for process tracking,\nsched for job scheduler,\nselect for a job's node selection,\nswitch for switch (interconnect) specific functions,\ntask for task affinity to processors,\ntopology methods for assigning nodes to jobs based on node\ntopology.\n\n\nsacct \u2014 User command to view accounting information about jobs.\nsacctmgr \u2014 User and administrator tool to manage accounting.\nsalloc \u2014 User command to allocate resources for a job.\nsattach \u2014 User command to attach standard input, output and error\nfiles to a running job or job step.\nsbatch \u2014 User command to submit a batch job (script for later execution).\nsbcast \u2014 User command to broadcast a file to all nodes associated\nwith an existing Slurm job.\nscancel \u2014 User command to cancel (or signal) a job or job step.\nscontrol \u2014 Administrator tool to manage Slurm.\nsinfo \u2014 User command to get information on Slurm nodes and partitions.\nslurmctld \u2014 Slurm central manager daemon code.\nslurmd \u2014 Slurm daemon code to manage the compute server nodes including\nthe execution of user applications.\nslurmdbd \u2014 Slurm database daemon managing access to the accounting\nstorage database.\nsprio \u2014 User command to see the breakdown of a job's priority\ncalculation when the Multifactor Job Priority plugin is installed.\nsqueue \u2014 User command to get information on Slurm jobs and job steps.\nsreport \u2014 User command to view various reports about past\nusage across the enterprise.\nsrun \u2014 User command to submit a job, get an allocation, and/or\ninitiation a parallel job step.\nsshare \u2014 User command to view shares and usage when the Multifactor\nJob Priority plugin is installed.\nsstat \u2014 User command to view detailed statistics about running\njobs when a Job Accounting Gather plugin is installed.\nstrigger \u2014 User and administrator tool to manage event triggers.\nsview \u2014 User command to view and update node, partition, and\njob state information.\nSource Code Management\n\n\nThe latest code is in github:\nhttps://github.com/SchedMD/slurm.\nCreating your own branch will make it easier to keep it synchronized\nwith our work.\nAdding New Modules\n\n\nAdd the new file name to the Makefile.am file in the appropriate directory.\nThen execute autoreconf (at the top level of the Slurm source directory).\nNote that a relatively current version of automake is required.\nThe autoreconf program will build Makefile.in files from the Makefile.am files.\nIf any new files need to be installed, update the slurm.spec file to identify\nthe RPM in which the new files should be placed\nIf new directories need to be added, add to the configure.ac file the path\nto the Makefile to be built in the new directory. In summary:\nautoreconf translates .am files into .in files\nconfigure translates .in files, adding paths and version numbers.\nCompiling\nSending the standard output of \"make\" to a file makes it easier to see any\nwarning or error messages:\n\"make -j install >make.out\"\nConfiguration\n\n\nSample configuration files are included in the etc subdirectory.\nThe slurm.conf can be built using a configuration tool.\nSee doc/man/man5/slurm.conf.5 and the man pages for other configuration files\nfor more details.\ninit.d.slurm is a script that determines which\nSlurm daemon(s) should execute on any node based upon the configuration file contents.\nIt will also manage these daemons: starting, signaling, restarting, and stopping them.\nTest Suite\nThe testsuite files use Check, Expect and Pytest for testing Slurm in\ndifferents ways.\nThe Check tests are designed to unit test C code. Only with\nmake check, without Slurm installed, they will validate that key C\nfunctions work correctly.\nWe also have a set of Expect Slurm tests available under the testsuite/expect\ndirectory.  These tests are executed after Slurm has been installed\nand the daemons initiated. These tests exercise all Slurm commands\nand options including stress tests.  The file testsuite/expect/globals\ncontains the Expect test framewrok for all of the individual tests.  At\nthe very least, you will need to set the slurm_dir variable to the correct\nvalue.  To avoid conflicts with other developers, you can override variable settings\nin a separate file named testsuite/expect/globals.local.\nSet your working directory to testsuite/expect before\nstarting these tests.  Tests may be executed individually by name\n(e.g.  test1.1)\nor the full test suite may be executed with the single command\nregression.py.\nSee testsuite/expect/README for more information.\nSlurm also has a Pytest environment that can work like the Expect one, but it\nalso works together with an external QA framework to improve the overall QA\nof Slurm.\nAdding Files and Directories\n\n\nIf you are adding files and directories to Slurm, it will be necessary to\nre-build configuration files before executing the configure command.\nUpdate Makefile.am files as needed then execute\nautoreconf before executing configure.\n\nTricks of the Trade\n\n\nHAVE_FRONT_END\n\n\nYou can make a single node appear to Slurm as a Linux cluster by running\nconfigure with the --enable-front-end option. This\ndefines HAVE_FRONT_END with a non-zero value in the file config.h.\nAll (fake) nodes should be defined in the slurm.conf file.\nThese nodes should be configured with a single NodeAddr value\nindicating the node on which single slurmd daemon\nexecutes.  Initiate one slurmd and one\nslurmctld daemon. Do not initiate too many\nsimultaneous job steps to avoid overloading the\nslurmd daemon executing them all.\nMultiple slurmd support\n\n\nIt is possible to run multiple slurmd daemons on a single node, each using\na different port number and NodeName alias.  This is very useful for testing\nnetworking and protocol changes, or anytime you want to simulate a larger\ncluster than you really have.  The author uses this on his desktop to simulate\nmultiple nodes.  However, it is important to note that not all slurm functions\nwill work with multiple slurmd support enabled (e.g. many switch plugins will\nnot work, it is best not to use any).\nMultiple support is enabled at configure-time with the\n\"--enable-multiple-slurmd\" parameter.  This enables a new parameter in the\nslurm.conf file on the NodeName line, \"Port=\", and adds a new\ncommand line parameter to slurmd, \"-N\".\nEach slurmd needs to have its own NodeName, and its own TCP port number. Here\nis an example of the NodeName lines for running three slurmd daemons on each\nof ten nodes:\n\nNodeName=foo[1-10] NodeHostname=host[1-10]  Port=17001\nNodeName=foo[11-20] NodeHostname=host[1-10] Port=17002\nNodeName=foo[21-30] NodeHostname=host[1-10] Port=17003\n\n\nIt is likely that you will also want to use the \"%n\" symbol in any slurmd\nrelated paths in the slurm.conf file, for instance SlurmdLogFile,\nSlurmdPidFile, and especially SlurmdSpoolDir.  Each slurmd replaces the \"%n\"\nwith its own NodeName.  Here is an example:\n\nSlurmdLogFile=/var/log/slurm/slurmd.%n.log\nSlurmdPidFile=/var/run/slurmd.%n.pid\nSlurmdSpoolDir=/var/spool/slurmd.%n\n\n\nYou can manually start each slurmd daemon with the proper NodeName.\nFor example, to start the slurmd daemons for host1 from the\nabove slurm.conf example:\n\nhost1> slurmd -N foo1\nhost1> slurmd -N foo11\nhost1> slurmd -N foo21\n\n\nIf you have SysV init scripts, slurmd daemons will automatically be started by\nwhenever MULTIPLE_SLURMD is set to yes in /etc/sysconfig/slurm.\nIf your distribution uses systemd, you may want to use templating feature to\ndefine one slurmd.service file and registering each of your virtual nodes\nwithin it, for example:\n\n[Unit]\nDescription=Slurm node daemon\nAfter=network.target munge.service remote-fs.target\nConditionPathExists=/etc/slurm.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmd\nExecStart=/usr/sbin/slurmd -N%i $SLURMD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurmd-%i.pid\nKillMode=process\nLimitNOFILE=51200\nLimitMEMLOCK=infinity\nLimitSTACK=infinity\nDelegate=yes\n\n[Install]\nWantedBy=multi-user.target\n\nThen, enabling/managing a service like this (what is %i in the file\nwill be replaced by what is after the @ in the command line):\n\nsystemctl enable slurmd@nodeXYZ\nsystemctl start/stop/restart slurmd@nodeXYZ\n\n\nLast modified 6 August 2021\n"
        },
        {
            "title": "Documentation\n\n",
            "content": "All of the documentation is in the subdirectory doc.\nTwo directories are of particular interest:\ndoc/man \u2014 contains the man pages for the APIs,\nconfiguration file, commands, and daemons.\ndoc/html \u2014 contains the web pages.Source Code\n\nFunctions are divided into several categories, each in its own subdirectory.\nThe details of each directory's contents are provided below. The directories are\nas follows: \napi \u2014 Application Program Interfaces into\nthe Slurm code. Used to send and get Slurm information from the central manager.\nThese are the functions user applications might utilize.\ncommon \u2014 General purpose functions for widespread use throughout\nSlurm.\ndatabase \u2014 Various database files that support the accounting\n storage plugin.\nplugins \u2014 Plugin functions for various infrastructures or optional\nbehavior. A separate subdirectory is used for each plugin class:\n\naccounting_storage for specifying the type of storage for accounting,\nauth for user authentication,\ncred for job credential functions,\njobacct_gather for job accounting,\njobcomp for job completion logging,\nmpi for MPI support,\npriority calculates job priority based on a number of factors\nincluding fair-share,\nproctrack for process tracking,\nsched for job scheduler,\nselect for a job's node selection,\nswitch for switch (interconnect) specific functions,\ntask for task affinity to processors,\ntopology methods for assigning nodes to jobs based on node\ntopology.\n\n\nsacct \u2014 User command to view accounting information about jobs.\nsacctmgr \u2014 User and administrator tool to manage accounting.\nsalloc \u2014 User command to allocate resources for a job.\nsattach \u2014 User command to attach standard input, output and error\nfiles to a running job or job step.\nsbatch \u2014 User command to submit a batch job (script for later execution).\nsbcast \u2014 User command to broadcast a file to all nodes associated\nwith an existing Slurm job.\nscancel \u2014 User command to cancel (or signal) a job or job step.\nscontrol \u2014 Administrator tool to manage Slurm.\nsinfo \u2014 User command to get information on Slurm nodes and partitions.\nslurmctld \u2014 Slurm central manager daemon code.\nslurmd \u2014 Slurm daemon code to manage the compute server nodes including\nthe execution of user applications.\nslurmdbd \u2014 Slurm database daemon managing access to the accounting\nstorage database.\nsprio \u2014 User command to see the breakdown of a job's priority\ncalculation when the Multifactor Job Priority plugin is installed.\nsqueue \u2014 User command to get information on Slurm jobs and job steps.\nsreport \u2014 User command to view various reports about past\nusage across the enterprise.\nsrun \u2014 User command to submit a job, get an allocation, and/or\ninitiation a parallel job step.\nsshare \u2014 User command to view shares and usage when the Multifactor\nJob Priority plugin is installed.\nsstat \u2014 User command to view detailed statistics about running\njobs when a Job Accounting Gather plugin is installed.\nstrigger \u2014 User and administrator tool to manage event triggers.\nsview \u2014 User command to view and update node, partition, and\njob state information.\nSource Code Management\n\n\nThe latest code is in github:\nhttps://github.com/SchedMD/slurm.\nCreating your own branch will make it easier to keep it synchronized\nwith our work.\nAdding New Modules\n\n\nAdd the new file name to the Makefile.am file in the appropriate directory.\nThen execute autoreconf (at the top level of the Slurm source directory).\nNote that a relatively current version of automake is required.\nThe autoreconf program will build Makefile.in files from the Makefile.am files.\nIf any new files need to be installed, update the slurm.spec file to identify\nthe RPM in which the new files should be placed\nIf new directories need to be added, add to the configure.ac file the path\nto the Makefile to be built in the new directory. In summary:\nautoreconf translates .am files into .in files\nconfigure translates .in files, adding paths and version numbers.\nCompiling\nSending the standard output of \"make\" to a file makes it easier to see any\nwarning or error messages:\n\"make -j install >make.out\"\nConfiguration\n\n\nSample configuration files are included in the etc subdirectory.\nThe slurm.conf can be built using a configuration tool.\nSee doc/man/man5/slurm.conf.5 and the man pages for other configuration files\nfor more details.\ninit.d.slurm is a script that determines which\nSlurm daemon(s) should execute on any node based upon the configuration file contents.\nIt will also manage these daemons: starting, signaling, restarting, and stopping them.\nTest Suite\nThe testsuite files use Check, Expect and Pytest for testing Slurm in\ndifferents ways.\nThe Check tests are designed to unit test C code. Only with\nmake check, without Slurm installed, they will validate that key C\nfunctions work correctly.\nWe also have a set of Expect Slurm tests available under the testsuite/expect\ndirectory.  These tests are executed after Slurm has been installed\nand the daemons initiated. These tests exercise all Slurm commands\nand options including stress tests.  The file testsuite/expect/globals\ncontains the Expect test framewrok for all of the individual tests.  At\nthe very least, you will need to set the slurm_dir variable to the correct\nvalue.  To avoid conflicts with other developers, you can override variable settings\nin a separate file named testsuite/expect/globals.local.\nSet your working directory to testsuite/expect before\nstarting these tests.  Tests may be executed individually by name\n(e.g.  test1.1)\nor the full test suite may be executed with the single command\nregression.py.\nSee testsuite/expect/README for more information.\nSlurm also has a Pytest environment that can work like the Expect one, but it\nalso works together with an external QA framework to improve the overall QA\nof Slurm.\nAdding Files and Directories\n\n\nIf you are adding files and directories to Slurm, it will be necessary to\nre-build configuration files before executing the configure command.\nUpdate Makefile.am files as needed then execute\nautoreconf before executing configure.\n\nTricks of the Trade\n\n\nHAVE_FRONT_END\n\n\nYou can make a single node appear to Slurm as a Linux cluster by running\nconfigure with the --enable-front-end option. This\ndefines HAVE_FRONT_END with a non-zero value in the file config.h.\nAll (fake) nodes should be defined in the slurm.conf file.\nThese nodes should be configured with a single NodeAddr value\nindicating the node on which single slurmd daemon\nexecutes.  Initiate one slurmd and one\nslurmctld daemon. Do not initiate too many\nsimultaneous job steps to avoid overloading the\nslurmd daemon executing them all.\nMultiple slurmd support\n\n\nIt is possible to run multiple slurmd daemons on a single node, each using\na different port number and NodeName alias.  This is very useful for testing\nnetworking and protocol changes, or anytime you want to simulate a larger\ncluster than you really have.  The author uses this on his desktop to simulate\nmultiple nodes.  However, it is important to note that not all slurm functions\nwill work with multiple slurmd support enabled (e.g. many switch plugins will\nnot work, it is best not to use any).\nMultiple support is enabled at configure-time with the\n\"--enable-multiple-slurmd\" parameter.  This enables a new parameter in the\nslurm.conf file on the NodeName line, \"Port=\", and adds a new\ncommand line parameter to slurmd, \"-N\".\nEach slurmd needs to have its own NodeName, and its own TCP port number. Here\nis an example of the NodeName lines for running three slurmd daemons on each\nof ten nodes:\n\nNodeName=foo[1-10] NodeHostname=host[1-10]  Port=17001\nNodeName=foo[11-20] NodeHostname=host[1-10] Port=17002\nNodeName=foo[21-30] NodeHostname=host[1-10] Port=17003\n\n\nIt is likely that you will also want to use the \"%n\" symbol in any slurmd\nrelated paths in the slurm.conf file, for instance SlurmdLogFile,\nSlurmdPidFile, and especially SlurmdSpoolDir.  Each slurmd replaces the \"%n\"\nwith its own NodeName.  Here is an example:\n\nSlurmdLogFile=/var/log/slurm/slurmd.%n.log\nSlurmdPidFile=/var/run/slurmd.%n.pid\nSlurmdSpoolDir=/var/spool/slurmd.%n\n\n\nYou can manually start each slurmd daemon with the proper NodeName.\nFor example, to start the slurmd daemons for host1 from the\nabove slurm.conf example:\n\nhost1> slurmd -N foo1\nhost1> slurmd -N foo11\nhost1> slurmd -N foo21\n\n\nIf you have SysV init scripts, slurmd daemons will automatically be started by\nwhenever MULTIPLE_SLURMD is set to yes in /etc/sysconfig/slurm.\nIf your distribution uses systemd, you may want to use templating feature to\ndefine one slurmd.service file and registering each of your virtual nodes\nwithin it, for example:\n\n[Unit]\nDescription=Slurm node daemon\nAfter=network.target munge.service remote-fs.target\nConditionPathExists=/etc/slurm.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmd\nExecStart=/usr/sbin/slurmd -N%i $SLURMD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurmd-%i.pid\nKillMode=process\nLimitNOFILE=51200\nLimitMEMLOCK=infinity\nLimitSTACK=infinity\nDelegate=yes\n\n[Install]\nWantedBy=multi-user.target\n\nThen, enabling/managing a service like this (what is %i in the file\nwill be replaced by what is after the @ in the command line):\n\nsystemctl enable slurmd@nodeXYZ\nsystemctl start/stop/restart slurmd@nodeXYZ\n\n\nLast modified 6 August 2021\n"
        },
        {
            "title": "Source Code Management\n\n",
            "content": "The latest code is in github:\nhttps://github.com/SchedMD/slurm.\nCreating your own branch will make it easier to keep it synchronized\nwith our work.Adding New Modules\n\nAdd the new file name to the Makefile.am file in the appropriate directory.\nThen execute autoreconf (at the top level of the Slurm source directory).\nNote that a relatively current version of automake is required.\nThe autoreconf program will build Makefile.in files from the Makefile.am files.\nIf any new files need to be installed, update the slurm.spec file to identify\nthe RPM in which the new files should be placedIf new directories need to be added, add to the configure.ac file the path\nto the Makefile to be built in the new directory. In summary:\nautoreconf translates .am files into .in files\nconfigure translates .in files, adding paths and version numbers.CompilingSending the standard output of \"make\" to a file makes it easier to see any\nwarning or error messages:\n\"make -j install >make.out\"Configuration\n\nSample configuration files are included in the etc subdirectory.\nThe slurm.conf can be built using a configuration tool.\nSee doc/man/man5/slurm.conf.5 and the man pages for other configuration files\nfor more details.\ninit.d.slurm is a script that determines which\nSlurm daemon(s) should execute on any node based upon the configuration file contents.\nIt will also manage these daemons: starting, signaling, restarting, and stopping them.Test SuiteThe testsuite files use Check, Expect and Pytest for testing Slurm in\ndifferents ways.The Check tests are designed to unit test C code. Only with\nmake check, without Slurm installed, they will validate that key C\nfunctions work correctly.We also have a set of Expect Slurm tests available under the testsuite/expect\ndirectory.  These tests are executed after Slurm has been installed\nand the daemons initiated. These tests exercise all Slurm commands\nand options including stress tests.  The file testsuite/expect/globals\ncontains the Expect test framewrok for all of the individual tests.  At\nthe very least, you will need to set the slurm_dir variable to the correct\nvalue.  To avoid conflicts with other developers, you can override variable settings\nin a separate file named testsuite/expect/globals.local.Set your working directory to testsuite/expect before\nstarting these tests.  Tests may be executed individually by name\n(e.g.  test1.1)\nor the full test suite may be executed with the single command\nregression.py.\nSee testsuite/expect/README for more information.Slurm also has a Pytest environment that can work like the Expect one, but it\nalso works together with an external QA framework to improve the overall QA\nof Slurm.Adding Files and Directories\n\nIf you are adding files and directories to Slurm, it will be necessary to\nre-build configuration files before executing the configure command.\nUpdate Makefile.am files as needed then execute\nautoreconf before executing configure.\n\nTricks of the Trade\n\n\nHAVE_FRONT_END\n\n\nYou can make a single node appear to Slurm as a Linux cluster by running\nconfigure with the --enable-front-end option. This\ndefines HAVE_FRONT_END with a non-zero value in the file config.h.\nAll (fake) nodes should be defined in the slurm.conf file.\nThese nodes should be configured with a single NodeAddr value\nindicating the node on which single slurmd daemon\nexecutes.  Initiate one slurmd and one\nslurmctld daemon. Do not initiate too many\nsimultaneous job steps to avoid overloading the\nslurmd daemon executing them all.\nMultiple slurmd support\n\n\nIt is possible to run multiple slurmd daemons on a single node, each using\na different port number and NodeName alias.  This is very useful for testing\nnetworking and protocol changes, or anytime you want to simulate a larger\ncluster than you really have.  The author uses this on his desktop to simulate\nmultiple nodes.  However, it is important to note that not all slurm functions\nwill work with multiple slurmd support enabled (e.g. many switch plugins will\nnot work, it is best not to use any).\nMultiple support is enabled at configure-time with the\n\"--enable-multiple-slurmd\" parameter.  This enables a new parameter in the\nslurm.conf file on the NodeName line, \"Port=\", and adds a new\ncommand line parameter to slurmd, \"-N\".\nEach slurmd needs to have its own NodeName, and its own TCP port number. Here\nis an example of the NodeName lines for running three slurmd daemons on each\nof ten nodes:\n\nNodeName=foo[1-10] NodeHostname=host[1-10]  Port=17001\nNodeName=foo[11-20] NodeHostname=host[1-10] Port=17002\nNodeName=foo[21-30] NodeHostname=host[1-10] Port=17003\n\n\nIt is likely that you will also want to use the \"%n\" symbol in any slurmd\nrelated paths in the slurm.conf file, for instance SlurmdLogFile,\nSlurmdPidFile, and especially SlurmdSpoolDir.  Each slurmd replaces the \"%n\"\nwith its own NodeName.  Here is an example:\n\nSlurmdLogFile=/var/log/slurm/slurmd.%n.log\nSlurmdPidFile=/var/run/slurmd.%n.pid\nSlurmdSpoolDir=/var/spool/slurmd.%n\n\n\nYou can manually start each slurmd daemon with the proper NodeName.\nFor example, to start the slurmd daemons for host1 from the\nabove slurm.conf example:\n\nhost1> slurmd -N foo1\nhost1> slurmd -N foo11\nhost1> slurmd -N foo21\n\n\nIf you have SysV init scripts, slurmd daemons will automatically be started by\nwhenever MULTIPLE_SLURMD is set to yes in /etc/sysconfig/slurm.\nIf your distribution uses systemd, you may want to use templating feature to\ndefine one slurmd.service file and registering each of your virtual nodes\nwithin it, for example:\n\n[Unit]\nDescription=Slurm node daemon\nAfter=network.target munge.service remote-fs.target\nConditionPathExists=/etc/slurm.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmd\nExecStart=/usr/sbin/slurmd -N%i $SLURMD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurmd-%i.pid\nKillMode=process\nLimitNOFILE=51200\nLimitMEMLOCK=infinity\nLimitSTACK=infinity\nDelegate=yes\n\n[Install]\nWantedBy=multi-user.target\n\nThen, enabling/managing a service like this (what is %i in the file\nwill be replaced by what is after the @ in the command line):\n\nsystemctl enable slurmd@nodeXYZ\nsystemctl start/stop/restart slurmd@nodeXYZ\n\nLast modified 6 August 2021"
        },
        {
            "title": "Tricks of the Trade\n\n",
            "content": "HAVE_FRONT_END\n\nYou can make a single node appear to Slurm as a Linux cluster by running\nconfigure with the --enable-front-end option. This\ndefines HAVE_FRONT_END with a non-zero value in the file config.h.\nAll (fake) nodes should be defined in the slurm.conf file.\nThese nodes should be configured with a single NodeAddr value\nindicating the node on which single slurmd daemon\nexecutes.  Initiate one slurmd and one\nslurmctld daemon. Do not initiate too many\nsimultaneous job steps to avoid overloading the\nslurmd daemon executing them all.Multiple slurmd support\n\nIt is possible to run multiple slurmd daemons on a single node, each using\na different port number and NodeName alias.  This is very useful for testing\nnetworking and protocol changes, or anytime you want to simulate a larger\ncluster than you really have.  The author uses this on his desktop to simulate\nmultiple nodes.  However, it is important to note that not all slurm functions\nwill work with multiple slurmd support enabled (e.g. many switch plugins will\nnot work, it is best not to use any).Multiple support is enabled at configure-time with the\n\"--enable-multiple-slurmd\" parameter.  This enables a new parameter in the\nslurm.conf file on the NodeName line, \"Port=\", and adds a new\ncommand line parameter to slurmd, \"-N\".Each slurmd needs to have its own NodeName, and its own TCP port number. Here\nis an example of the NodeName lines for running three slurmd daemons on each\nof ten nodes:\nNodeName=foo[1-10] NodeHostname=host[1-10]  Port=17001\nNodeName=foo[11-20] NodeHostname=host[1-10] Port=17002\nNodeName=foo[21-30] NodeHostname=host[1-10] Port=17003\n\nIt is likely that you will also want to use the \"%n\" symbol in any slurmd\nrelated paths in the slurm.conf file, for instance SlurmdLogFile,\nSlurmdPidFile, and especially SlurmdSpoolDir.  Each slurmd replaces the \"%n\"\nwith its own NodeName.  Here is an example:\nSlurmdLogFile=/var/log/slurm/slurmd.%n.log\nSlurmdPidFile=/var/run/slurmd.%n.pid\nSlurmdSpoolDir=/var/spool/slurmd.%n\n\nYou can manually start each slurmd daemon with the proper NodeName.\nFor example, to start the slurmd daemons for host1 from the\nabove slurm.conf example:\nhost1> slurmd -N foo1\nhost1> slurmd -N foo11\nhost1> slurmd -N foo21\n\nIf you have SysV init scripts, slurmd daemons will automatically be started by\nwhenever MULTIPLE_SLURMD is set to yes in /etc/sysconfig/slurm.\nIf your distribution uses systemd, you may want to use templating feature to\ndefine one slurmd.service file and registering each of your virtual nodes\nwithin it, for example:\n[Unit]\nDescription=Slurm node daemon\nAfter=network.target munge.service remote-fs.target\nConditionPathExists=/etc/slurm.conf\n\n[Service]\nType=forking\nEnvironmentFile=-/etc/sysconfig/slurmd\nExecStart=/usr/sbin/slurmd -N%i $SLURMD_OPTIONS\nExecReload=/bin/kill -HUP $MAINPID\nPIDFile=/var/run/slurmd-%i.pid\nKillMode=process\nLimitNOFILE=51200\nLimitMEMLOCK=infinity\nLimitSTACK=infinity\nDelegate=yes\n\n[Install]\nWantedBy=multi-user.target\nThen, enabling/managing a service like this (what is %i in the file\nwill be replaced by what is after the @ in the command line):\nsystemctl enable slurmd@nodeXYZ\nsystemctl start/stop/restart slurmd@nodeXYZ\n"
        }
    ]
}