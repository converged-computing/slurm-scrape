{
    "url": "https://slurm.schedmd.com/high_throughput.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "High Throughput Computing Administration Guide",
            "content": "This document contains Slurm administrator information specifically\nfor high throughput computing, namely the execution of many short jobs.\nGetting optimal performance for high throughput computing does require\nsome tuning and this document should help you off to a good start.\nA working knowledge of Slurm should be considered a prerequisite\nfor this material.Performance Results\n\nSlurm has also been validated to execute 500 simple batch jobs per second\non a sustained basis with short bursts of activity at a much higher level.\nActual performance depends upon the jobs to be executed plus the hardware and\nconfiguration used.System configuration\n\nSeveral system configuration parameters may require modification to support a large number\nof open files and TCP connections with large bursts of messages. Changes can\nbe made using the /etc/rc.d/rc.local or /etc/sysctl.conf \nscript to preserve changes after reboot. In either case, you can write values\ndirectly into these files\n(e.g. \"echo 32832 > /proc/sys/fs/file-max\").\n/proc/sys/fs/file-max:\nThe maximum number of concurrently open files.\nWe recommend a limit of at least 32,832.\n/proc/sys/net/ipv4/tcp_max_syn_backlog:\nThe maximum number of SYN requests to keep in memory that we have yet to get\nthe third packet in a 3-way handshake from.\nThe default value is 1024 for systems with more than 128Mb of memory, and 128\nfor low memory machines. If server suffers of overload, try to increase this\nnumber.\n/proc/sys/net/ipv4/tcp_syncookies:\nUsed to send out syncookies to hosts when the kernels syn backlog queue\nfor a specific socket is overflowed.\nThe default value is 0, which disables this functionality.\nSet the value to 1.\n/proc/sys/net/ipv4/tcp_synack_retries:\nHow many times to retransmit the SYN,ACK reply to an SYN request.\nIn other words, this tells the system how many times to try to establish a\npassive TCP connection that was started by another host.\nThis variable takes an integer value, but should under no circumstances be\nlarger than 255.\nEach retransmission will take approximately 30 to 40 seconds.\nThe default value of 5, which results in a timeout of passive TCP connections\nof approximately 180 seconds and is generally satisfactory.\n/proc/sys/net/core/somaxconn:\nLimit of socket listen() backlog, known in userspace as SOMAXCONN. Defaults to\n128. The value should be raised substantially to support bursts of request.\nFor example, to support a burst of 1024 requests, set somaxconn to 1024.\n/proc/sys/net/ipv4/ip_local_port_range:\nIdentify the ephemeral ports available, which are used for many Slurm\ncommunications. The value may be raised to support a high volume of\ncommunications.\nFor example, write the value \"32768 65535\" into the ip_local_port_range file\nin order to make that range of ports available.\nThe transmit queue length (txqueuelen) may also need to be modified\nusing the ifconfig command. A value of 4096 has been found to work well for one\nsite with a very large cluster\n(e.g. \"ifconfig  txqueuelen 4096\").Munge configuration\n\nBy default the Munge daemon runs with two threads, but a higher thread count\ncan improve its throughput. We suggest starting the Munge daemon with ten\nthreads for high throughput support (e.g. \"munged --num-threads 10\").User limits\n\nThe ulimit values in effect for the slurmctld daemon should\nbe set quite high for memory size, open file count and stack size.Slurm Configuration\n\nSeveral Slurm configuration parameters should be adjusted to\nreflect the needs of high throughput computing. The changes described below\nwill not be possible in all environments, but these are the configuration\noptions that you may want to consider for higher throughput.\nAccountingStorageType:\nDisabling storing accounting records by not setting this plugin.\nTurning accounting off provides minimal improvement in performance.\nIf using the SlurmDBD increased speedup can be achieved by setting the\nCommitDelay option in the slurmdbd.conf\nJobAcctGatherType:\nDisabling the collection of job accounting information will improve job\nthroughput. Disable collection of accounting by using the\njobacct_gather/none plugin.\nJobCompType:\nDisabling recording of job completion information will improve job\nthroughput. Disable recording of job completion information by using the\njobcomp/none plugin.\nJobSubmitPlugins:\nUse of a lua job submit plugin is not recommended. slurmctld runs this\nscript while holding internal locks, and only a single copy of this script\ncan run at a time. This blocks most concurrency in slurmctld. Therefore, we\ndo not recommend using it in a high throughput environment.\nMaxJobCount:\nControls how many jobs may be in the slurmctld daemon records at any\npoint in time (pending, running, suspended or completed[temporarily]).\nThe default value is 10,000.\nMessageTimeout:\nControls how long to wait for a response to messages.\nThe default value is 10 seconds.\nWhile the slurmctld daemon is highly threaded, its responsiveness\nis load dependent. This value might need to be increased somewhat.\nMinJobAge:\nControls how soon the record of a completed job can be purged from the\nslurmctld memory and thus not visible using the squeue command.\nThe record of jobs run will be preserved in accounting records and logs.\nThe default value is 300 seconds. The value should be reduced to a few\nseconds if possible. Use of accounting records for older jobs can increase\nthe job throughput rate compared with retaining old jobs in the memory of\nthe slurmctld daemon.\nPriorityType:\nThe priority/basic is considerably faster than other options, but\nschedules jobs only on a First In First Out (FIFO) basis.\nPrologSlurmctld/EpilogSlurmctld:\nNeither of these is recommended for a high throughput environment. When they\nare enabled a separate slurmctld thread has to be created for every job start\n(or task for a job array).\nCurrent architecture requires acquisition of a job write lock in every thread,\nwhich is a costly operation that severely limits scheduler throughput.\nSchedulerParameters:\nMany scheduling parameters are available.\n\nSetting option batch_sched_delay will control how long the\nscheduling of batch jobs can be delayed. This effects only batch jobs.\nFor example, if many jobs are submitted each second, the overhead of\ntrying to schedule each one will adversely impact the rate at which jobs\ncan be submitted. The default value is 3 seconds.\nSetting option defer will avoid attempting to schedule each job\nindividually at job submit time, but defer it until a later time when\nscheduling multiple jobs simultaneously may be possible.\nThis option may improve system responsiveness when large numbers of jobs\n(many hundreds) are submitted at the same time, but it will delay the\ninitiation time of individual jobs.\nSetting the defer_batch option is similar to the defer\noption, as explained above. The difference is that defer_batch will\nallow interactive jobs to be started immediately, but jobs submitted with\nsbatch will be deferred to allow multiple jobs to accumulate and be scheduled\nat once.\nsched_min_interval is yet another configuration parameter to control\nhow frequently the scheduling logic runs. It can still be triggered on each\njob submit, job termination, or other state change which could permit a new\njob to be started. However that triggering does not cause the scheduling logic\nto be started immediately, but only within the configured sched_interval.\nFor example, if sched_min_interval=2000000 (microseconds) and 100 jobs are submitted\nwithin a 2 second time window, then the scheduling logic will be executed one time\nrather than 100 times if sched_min_interval was set to 0 (no delay).\nBesides controlling how frequently the scheduling logic is executed, the\ndefault_queue_depth configuration parameter controls how many jobs are\nconsidered to be started in each scheduler iteration. The default value of\ndefault_queue_depth is 100 (jobs), which should be fine in most cases.\nThe sched/backfill plugin has relatively high overhead if used with\nlarge numbers of job. Configuring bf_max_job_test to a modest size (say 100\njobs or less) and bf_interval to 30 seconds or more will limit the\noverhead of backfill scheduling (NOTE: the default values are fine for\nboth of these parameters). Other backfill options available for tuning backfill\nscheduling include bf_max_job_user, bf_resolution and\nbf_window. See the slurm.conf man page for details.\nA set of scheduling parameters currently used for running hundreds of jobs\nper second on a sustained basis on one cluster follows. Note that every\nenvironment is different and this set of parameters will not work well\nin every case, but it may serve as a good starting point.\n\nbatch_sched_delay=20\nbf_continue\nbf_interval=300\nbf_min_age_reserve=10800\nbf_resolution=600\nbf_yield_interval=1000000\npartition_job_depth=500\nsched_max_job_start=200\nsched_min_interval=2000000\n\n\nSchedulerType:\nIf most jobs are short lived then use of the sched/builtin plugin is\nrecommended. This manages a queue of jobs on a First-In-First-Out (FIFO) basis\nand eliminates logic used to sort the queue by priority.\nSlurmctldDebug:\nMore detailed logging will decrease system throughput. Set to error or\ninfo for regular operations with high throughput workload.\nSlurmctldPort:\nIt is desirable to configure the slurmctld daemon to accept incoming\nmessages on more than one port in order to avoid having incoming messages\ndiscarded by the operating system due to exceeding the SOMAXCONN limit\ndescribed above. Using between two and ten ports is suggested when large\nnumbers of simultaneous requests are to be supported.\nSlurmdDebug:\nMore detailed logging will decrease system throughput. Set to error or\ninfo for regular operations with high throughput workload.\nSlurmdLogFile:\nWriting to local storage is recommended.\nThe ability to do RPC rate limiting on a per-user basis is a new feature\nwith 23.02. It acts as a virtual bucket of tokens that users consume with\nRemote Procedure Calls. This allows users to submit a large number of requests\nin a short period of time, but not a sustained high rate of requests that\nwould add stress to the scheduler. You can define the maximum number of tokens\nwith rl_bucket_size, the rate at which new tokens are added with\nrl_refill_rate, the frequency with which tokens are refilled with\nrl_refill_period and the number of entities to track with\nrl_table_size. It is enabled with rl_enable.\nOther: Configure logging, accounting and other overhead to a minimum\nappropriate for your environment.\nSlurmDBD Configuration\n\nTurning accounting off provides a minimal improvement in performance.\n  If using SlurmDBD increased speedup can be achieved by setting the CommitDelay\n  option in the slurmdbd.conf to introduce a\n  delay between the time slurmdbd receives a connection from slurmctld and\n  when it commits the information to the database. This allows multiple\n  requests to be accumulated and reduces the number of commit requests\n  to the database.You might also consider setting the 'Purge*' options in your\n  slurmdbd.conf to clear out old Data.  A Typical configuration would\n  look like this...\nPurgeEventAfter=12months\nPurgeJobAfter=12months\nPurgeResvAfter=2months\nPurgeStepAfter=2months\nPurgeSuspendAfter=1month\nPurgeTXNAfter=12months\nPurgeUsageAfter=12months\nLast modified 6 December 2023"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        }
    ]
}