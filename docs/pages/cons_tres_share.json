{
    "url": "https://slurm.schedmd.com/cons_tres_share.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Sharing Consumable Resources",
            "content": "CPU Management\n\n\n(Disclaimer: In this \"CPU Management\" section, the term \"consumable resource\"\ndoes not include memory. The management of memory as a consumable resource is\ndiscussed in its own section below.)\n\nThe per-partition OverSubscribe setting applies to the entity\nbeing selected for scheduling:\n\nWhen the select/linear plugin is enabled, the\nper-partition OverSubscribe setting controls whether or not the\nnodes are shared among jobs.\n\nWhen the default select/cons_tres plugin is\nenabled, the per-partition OverSubscribe setting controls\nwhether or not the configured consumable resources are shared among jobs.\nWhen a consumable resource such as a core,\nsocket, or CPU is shared, it means that more than one job can be assigned to it.\n\n\nThe following table describes this new functionality in more detail:\n\nSelection Setting\nPer-partition OverSubscribe Setting\nResulting Behavior\n\nSelectType=select/linear\nOverSubscribe=NO\nWhole nodes are allocated to jobs. No node will run more than one job\nper partition/queue.\n\nOverSubscribe=YES\nBy default same as OverSubscribe=NO. Nodes allocated to a job may be shared with\nother jobs if each job allows sharing via the srun --oversubscribe\noption.\n\nOverSubscribe=FORCE\nEach whole node can be allocated to multiple jobs up to the count specified\nper partition/queue (default 4 jobs per node)\n\nSelectType=select/cons_tres\nPlus one of the following:\nSelectTypeParameters=CR_Core\nSelectTypeParameters=CR_Core_Memory\nOverSubscribe=NO\nCores are allocated to jobs. No core will run more than one job\nper partition/queue.\n\nOverSubscribe=YES\nBy default same as OverSubscribe=NO. Cores allocated to a job may be shared with\nother jobs if each job allows sharing via the srun --oversubscribe\noption.\n\nOverSubscribe=FORCE\nEach core can be allocated to multiple jobs up to the count specified\nper partition/queue (default 4 jobs per core).\n\nSelectType=select/cons_tres\nPlus one of the following:\nSelectTypeParameters=CR_CPU\nSelectTypeParameters=CR_CPU_Memory\nOverSubscribe=NO\nCPUs are allocated to jobs. No CPU will run more than one job\nper partition/queue.\n\nOverSubscribe=YES\nBy default same as OverSubscribe=NO. CPUs allocated to a job may be shared with\nother jobs if each job allows sharing via the srun --oversubscribe\noption.\n\nOverSubscribe=FORCE\nEach CPU can be allocated to multiple jobs up to the count specified\nper partition/queue (default 4 jobs per CPU).\n\nSelectType=select/cons_tres\nPlus one of the following:\nSelectTypeParameters=CR_Socket\nSelectTypeParameters=CR_Socket_Memory\nOverSubscribe=NO\nSockets are allocated to jobs. No Socket will run more than one job\nper partition/queue.\n\nOverSubscribe=YES\nBy default same as OverSubscribe=NO. Sockets allocated to a job may be shared with\nother jobs if each job allows sharing via the srun --oversubscribe\noption.\n\nOverSubscribe=FORCE\nEach socket can be allocated to multiple jobs up to the count specified\nper partition/queue (default 4 jobs per socket).\n\nWhen OverSubscribe=FORCE is configured, the consumable resources are\nscheduled for jobs using a least-loaded algorithm. Thus, idle\nCPUs|cores|sockets will be allocated to a job before busy ones, and\nCPUs|cores|sockets running one job will be allocated to a job before ones\nrunning two or more jobs. This is the same approach that the\nselect/linear plugin uses when allocating \"shared\" nodes.\n\nNote that the granularity of the \"least-loaded\" algorithm is what\ndistinguishes the consumable resource and linear plugins\nwhen OverSubscribe=FORCE is configured. With the\nselect/cons_tres plugin enabled,\nthe CPUs of a node are not\novercommitted until all of the rest of the CPUs are overcommitted on the\nother nodes. Thus if one job allocates half of the CPUs on a node and then a\nsecond job is submitted that requires more than half of the CPUs, the\nconsumable resource plugin will attempt to place this new job on other\nbusy nodes that have more than half of the CPUs available for use. The\nselect/linear plugin simply counts jobs on nodes, and does not\ntrack the CPU usage on each node.\n\nThe sharing functionality in the\nselect/cons_tres plugin also supports the\nnew OverSubscribe=FORCE:<num> syntax. If OverSubscribe=FORCE:3\nis configured with a consumable resource plugin and CR_Core or\nCR_Core_Memory, then the plugin will\nrun up to 3 jobs on each core of each node in the partition. If\nCR_Socket or CR_Socket_Memory is configured, then the\nplugin will run up to 3 jobs on each socket\nof each node in the partition.\nNodes in Multiple Partitions\n\n\nSlurm has supported configuring nodes in more than one partition since version\n0.7.0. The following table describes how nodes configured in two partitions with\ndifferent OverSubscribe settings will be allocated to jobs. Note that\n\"shared\" jobs are jobs that are submitted to partitions configured with\nOverSubscribe=FORCE or with OverSubscribe=YES and the job requested\nsharing with the srun --oversubscribe option. Conversely, \"non-shared\"\njobs are jobs that are submitted to partitions configured with\nOverSubscribe=NO or OverSubscribe=YES and the job did not\nrequest shareable resources.\n\n\u00a0First job \"shareable\"First job not\n\"shareable\"\nSecond job \"shareable\"Both jobs can run on the same nodes and\nmay share resourcesJobs do not run on the same nodes\nSecond job not \"shareable\"Jobs do not run on the same nodes\nJobs can run on the same nodes but will not share resources\n\nThe next table contains several scenarios with the select/cons_tres\nplugin enabled to further\nclarify how a node is used when it is configured in more than one partition and\nthe partitions have different \"OverSubscribe\" policies.\n\nSlurm configuration\nResulting Behavior\n\nTwo OverSubscribe=NO partitions assigned the same set of nodes\nJobs from either partition will be assigned to all available consumable\nresources. No consumable resource will be shared. One node could have 2 jobs\nrunning on it, and each job could be from a different partition.\n\nTwo partitions assigned the same set of nodes: one partition is\nOverSubscribe=FORCE, and the other is OverSubscribe=NO\nA node will only run jobs from one partition at a time. If a node is\nrunning jobs from the OverSubscribe=NO partition, then none of its\nconsumable resources will be shared. If a node is running jobs from the\nOverSubscribe=FORCE partition, then its consumable resources can be\nshared.\n\nTwo OverSubscribe=FORCE partitions assigned the same set of nodes\nJobs from either partition will be assigned consumable resources. All\nconsumable resources can be shared. One node could have 2 jobs running on it,\nand each job could be from a different partition.\n\nTwo partitions assigned the same set of nodes: one partition is\nOverSubscribe=FORCE:3, and the other is OverSubscribe=FORCE:5\nGenerally the same behavior as above. However no consumable resource will\never run more than 3 jobs from the first partition, and no consumable resource\nwill ever run more than 5 jobs from the second partition. A consumable resource\ncould have up to 8 jobs running on it at one time.\n\n\nNote that the \"mixed shared setting\" configuration (row #2 above) introduces the\npossibility of starvation between jobs in each partition. If a set of\nnodes are running jobs from the OverSubscribe=NO partition, then these\nnodes will continue to only be available to jobs from that partition, even if\njobs submitted to the OverSubscribe=FORCE partition have a higher\npriority. This works in reverse also, and in fact it's easier for jobs from the\nOverSubscribe=FORCE partition to hold onto the nodes longer because the\nconsumable resource \"sharing\" provides more resource availability for new jobs\nto begin running \"on top of\" the existing jobs. This happens with the\nselect/linear plugin also, so it's not specific to the\nselect/cons_tres plugin.\nMemory Management\n\n\nThe management of memory as a consumable resource remains unchanged and\ncan be used to prevent oversubscription of memory, which would result in\nhaving memory pages swapped out and severely degraded performance.\n\nSelection Setting\nResulting Behavior\n\nSelectType=select/linear\nMemory allocation is not tracked. Jobs are allocated to nodes without\nconsidering if there is enough free memory. Swapping could occur!\n\nSelectType=select/linear plus\nSelectTypeParameters=CR_Memory\nMemory allocation is tracked.  Nodes that do not have enough available\nmemory to meet the jobs memory requirement will not be allocated to the job.\n\n\nSelectType=select/cons_tres\nPlus one of the following:\nSelectTypeParameters=CR_Core\nSelectTypeParameters=CR_CPU\nSelectTypeParameters=CR_Socket\nMemory allocation is not tracked. Jobs are allocated to consumable resources\nwithout considering if there is enough free memory. Swapping could occur!\n\nSelectType=select/cons_tres\nPlus one of the following:\nSelectTypeParameters=CR_Memory\nSelectTypeParameters=CR_Core_Memory\nSelectTypeParameters=CR_CPU_Memory\nSelectTypeParameters=CR_Socket_Memory\nMemory allocation for all jobs are tracked. Nodes that do not have enough\navailable memory to meet the jobs memory requirement will not be allocated to\nthe job.\n\nUsers can specify their job's memory requirements one of two ways. The\nsrun --mem=<num> option can be used to specify the jobs\nmemory requirement on a per allocated node basis. This option is recommended\nfor use with the select/linear plugin, which allocates\nwhole nodes to jobs. The\nsrun --mem-per-cpu=<num> option can be used to specify the\njobs memory requirement on a per allocated CPU basis. This is recommended\nfor use with the select/cons_tres\nplugin, which can allocate individual CPUs to jobs.Default and maximum values for memory on a per node or per CPU basis can\nbe configured by the system administrator using the following\nslurm.conf options: DefMemPerCPU,\nDefMemPerNode, MaxMemPerCPU and\nMaxMemPerNode.\nUsers can use the --mem or --mem-per-cpu option\nat job submission time to override the default value, but they cannot exceed\nthe maximum value.\n\nEnforcement of a jobs memory allocation is performed by setting the \"maximum\ndata segment size\" and the \"maximum virtual memory size\" system limits to the\nappropriate values before launching the tasks. Enforcement is also managed by\nthe accounting plugin, which periodically gathers data about running jobs. Set\nJobAcctGather and JobAcctFrequency to\nvalues suitable for your system.\nNOTE: The --oversubscribe and --exclusive\noptions are mutually exclusive when used at job submission. If both options are\nset when submitting a job, the job submission command used will fatal.\nLast modified 30 May 2023"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        }
    ]
}