{
    "url": "https://slurm.schedmd.com/topology.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Topology Guide",
            "content": "Slurm can be configured to support topology-aware resource\nallocation to optimize job performance.\nSlurm supports several modes of operation, one to optimize performance on\nsystems with a three-dimensional torus interconnect and another for\na hierarchical interconnect.\nThe hierarchical mode of operation supports both fat-tree or dragonfly networks,\nusing slightly different algorithms.Slurm's native mode of resource selection is to consider the nodes\nas a one-dimensional array.\nJobs are allocated resources on a best-fit basis.\nFor larger jobs, this minimizes the number of sets of consecutive nodes\nallocated to the job.Three-dimension Topology\n\nSome larger computers rely upon a three-dimensional torus interconnect.\nThe Cray XT and XE systems also have three-dimensional\ntorus interconnects, but do not require that jobs execute in adjacent nodes.\nOn those systems, Slurm only needs to allocate resources to a job which\nare nearby on the network.\nSlurm accomplishes this using a\nHilbert curve\nto map the nodes from a three-dimensional space into a one-dimensional\nspace.\nSlurm's native best-fit algorithm is thus able to achieve a high degree\nof locality for jobs.\n\nHierarchical Networks\n\n\nSlurm can also be configured to allocate resources to jobs on a\nhierarchical network to minimize network contention.\nThe basic algorithm is to identify the lowest level switch in the\nhierarchy that can satisfy a job's request and then allocate resources\non its underlying leaf switches using a best-fit algorithm.\nUse of this logic requires a configuration setting of\nTopologyPlugin=topology/tree.\nNote that slurm uses a best-fit algorithm on the currently\navailable resources. This may result in an allocation with\nmore than the optimum number of switches. The user can request\na maximum number of leaf switches for the job as well as a\nmaximum time willing to wait for that number using the --switches\noption with the salloc, sbatch and srun commands. The parameters can\nalso be changed for pending jobs using the scontrol and squeue commands.\nAt some point in the future Slurm code may be provided to\ngather network topology information directly.\nNow the network topology information must be included\nin a topology.conf configuration file as shown in the\nexamples below.\nThe first example describes a three level switch in which\neach switch has two children.\nNote that the SwitchName values are arbitrary and only\nused for bookkeeping purposes, but a name must be specified on\neach line.\nThe leaf switch descriptions contain a SwitchName field\nplus a Nodes field to identify the nodes connected to the\nswitch.\nHigher-level switch descriptions contain a SwitchName field\nplus a Switches field to identify the child switches.\nSlurm's hostlist expression parser is used, so the node and switch\nnames need not be consecutive (e.g. \"Nodes=tux[0-3,12,18-20]\"\nand \"Switches=s[0-2,4-8,12]\" will parse fine).\n\nAn optional LinkSpeed option can be used to indicate the\nrelative performance of the link.\nThe units used are arbitrary and this information is currently not used.\nIt may be used in the future to optimize resource allocations.\nThe first example shows what a topology would look like for an\neight node cluster in which all switches have only two children as\nshown in the diagram (not a very realistic configuration, but\nuseful for an example).\n\n# topology.conf\n# Switch Configuration\nSwitchName=s0 Nodes=tux[0-1]\nSwitchName=s1 Nodes=tux[2-3]\nSwitchName=s2 Nodes=tux[4-5]\nSwitchName=s3 Nodes=tux[6-7]\nSwitchName=s4 Switches=s[0-1]\nSwitchName=s5 Switches=s[2-3]\nSwitchName=s6 Switches=s[4-5]\n\n\nThe next example is for a network with two levels and\neach switch has four connections.\n\n# topology.conf\n# Switch Configuration\nSwitchName=s0 Nodes=tux[0-3]   LinkSpeed=900\nSwitchName=s1 Nodes=tux[4-7]   LinkSpeed=900\nSwitchName=s2 Nodes=tux[8-11]  LinkSpeed=900\nSwitchName=s3 Nodes=tux[12-15] LinkSpeed=1800\nSwitchName=s4 Switches=s[0-3]  LinkSpeed=1800\nSwitchName=s5 Switches=s[0-3]  LinkSpeed=1800\nSwitchName=s6 Switches=s[0-3]  LinkSpeed=1800\nSwitchName=s7 Switches=s[0-3]  LinkSpeed=1800\n\n\nAs a practical matter, listing every switch connection\ndefinitely results in a slower scheduling algorithm for Slurm\nto optimize job placement.\nThe application performance may achieve little benefit from such optimization.\nListing the leaf switches with their nodes plus one top level switch\nshould result in good performance for both applications and Slurm.\nThe previous example might be configured as follows:\n\n# topology.conf\n# Switch Configuration\nSwitchName=s0 Nodes=tux[0-3]\nSwitchName=s1 Nodes=tux[4-7]\nSwitchName=s2 Nodes=tux[8-11]\nSwitchName=s3 Nodes=tux[12-15]\nSwitchName=s4 Switches=s[0-3]\n\nNote that compute nodes on switches that lack a common parent switch can\nbe used, but no job will span leaf switches without a common parent\n(unless the TopologyParam=TopoOptional option is used).\nFor example, it is legal to remove the line \"SwitchName=s4 Switches=s[0-3]\"\nfrom the above topology.conf file.\nIn that case, no job will span more than four compute nodes on any single leaf\nswitch.\nThis configuration can be useful if one wants to schedule multiple physical\nclusters as a single logical cluster under the control of a single slurmctld\ndaemon.\nIf you have nodes that are in separate networks and are associated with\nunique switches in your topology.conf file, it's possible that you\ncould get in a situation where a job isn't able to run.  If a job requests\nnodes that are in the different networks, either by requesting the nodes\ndirectly or by requesting a feature, the job will fail because the requested\nnodes can't communicate with each other.  We recommend placing nodes in\nseparate network segments in disjoint partitions.\nFor systems with a dragonfly network, configure Slurm with\nTopologyPlugin=topology/tree plus TopologyParam=dragonfly.\nIf a single job can not be entirely placed within a single network leaf\nswitch, the job will be spread across as many leaf switches as possible\nin order to optimize the job's network bandwidth.\nNOTE: When using the topology/tree plugin, Slurm identifies\nthe network switches which provide the best fit for pending jobs. If nodes\nhave a Weight defined, this will override the resource selection based\non network topology. If optimizing resource selection by node weight is more\nimportant than optimizing network topology then do NOT use the\ntopology/tree plugin.\nConfiguration Generators\n\n\nThe following independently maintained tools may be useful in generating the\ntopology.conf file for certain switch types:\n\n\nInfiniband switch - slurmibtopology\n\nhttps://github.com/OleHolmNielsen/Slurm_tools/tree/master/slurmibtopology\nOmni-Path (OPA) switch - opa2slurm\n\nhttps://gitlab.com/jtfrey/opa2slurm\nAWS Elastic Fabric Adapter (EFA) - ec2-topology\n\nhttps://github.com/aws-samples/ec2-topology-aware-for-slurm\n\nUser Options\nFor use with the topology/tree plugin, user can also specify the maximum\nnumber of leaf switches to be used for their job with the maximum time the\njob should wait for this optimized configuration. The syntax for this option\nis \"--switches=count[@time]\".\nThe system administrator can limit the maximum time that any job can\nwait for this optimized configuration using the SchedulerParameters\nconfiguration parameter with the max_switch_wait option.\nEnvironment Variables\n\n\nIf the topology/tree plugin is used, two environment variables will be set\nto describe that job's network topology. Note that these environment variables\nwill contain different data for the tasks launched on each node. Use of these\nenvironment variables is at the discretion of the user.\nSLURM_TOPOLOGY_ADDR:\nThe value will be set to the names network switches which may be involved in\nthe job's communications from the system's top level switch down to the leaf\nswitch and ending with node name. A period is used to separate each hardware\ncomponent name.\nSLURM_TOPOLOGY_ADDR_PATTERN:\nThis is set only if the system has the topology/tree plugin configured.\nThe value will be set component types listed in SLURM_TOPOLOGY_ADDR.\nEach component will be identified as either \"switch\" or \"node\".\nA period is used to separate each hardware component type.\nLast modified 01 April 2024\n"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        },
        {
            "title": "Hierarchical Networks\n\n",
            "content": "Slurm can also be configured to allocate resources to jobs on a\nhierarchical network to minimize network contention.\nThe basic algorithm is to identify the lowest level switch in the\nhierarchy that can satisfy a job's request and then allocate resources\non its underlying leaf switches using a best-fit algorithm.\nUse of this logic requires a configuration setting of\nTopologyPlugin=topology/tree.Note that slurm uses a best-fit algorithm on the currently\navailable resources. This may result in an allocation with\nmore than the optimum number of switches. The user can request\na maximum number of leaf switches for the job as well as a\nmaximum time willing to wait for that number using the --switches\noption with the salloc, sbatch and srun commands. The parameters can\nalso be changed for pending jobs using the scontrol and squeue commands.At some point in the future Slurm code may be provided to\ngather network topology information directly.\nNow the network topology information must be included\nin a topology.conf configuration file as shown in the\nexamples below.\nThe first example describes a three level switch in which\neach switch has two children.\nNote that the SwitchName values are arbitrary and only\nused for bookkeeping purposes, but a name must be specified on\neach line.\nThe leaf switch descriptions contain a SwitchName field\nplus a Nodes field to identify the nodes connected to the\nswitch.\nHigher-level switch descriptions contain a SwitchName field\nplus a Switches field to identify the child switches.\nSlurm's hostlist expression parser is used, so the node and switch\nnames need not be consecutive (e.g. \"Nodes=tux[0-3,12,18-20]\"\nand \"Switches=s[0-2,4-8,12]\" will parse fine).\nAn optional LinkSpeed option can be used to indicate the\nrelative performance of the link.\nThe units used are arbitrary and this information is currently not used.\nIt may be used in the future to optimize resource allocations.The first example shows what a topology would look like for an\neight node cluster in which all switches have only two children as\nshown in the diagram (not a very realistic configuration, but\nuseful for an example).\n# topology.conf\n# Switch Configuration\nSwitchName=s0 Nodes=tux[0-1]\nSwitchName=s1 Nodes=tux[2-3]\nSwitchName=s2 Nodes=tux[4-5]\nSwitchName=s3 Nodes=tux[6-7]\nSwitchName=s4 Switches=s[0-1]\nSwitchName=s5 Switches=s[2-3]\nSwitchName=s6 Switches=s[4-5]\nThe next example is for a network with two levels and\neach switch has four connections.\n# topology.conf\n# Switch Configuration\nSwitchName=s0 Nodes=tux[0-3]   LinkSpeed=900\nSwitchName=s1 Nodes=tux[4-7]   LinkSpeed=900\nSwitchName=s2 Nodes=tux[8-11]  LinkSpeed=900\nSwitchName=s3 Nodes=tux[12-15] LinkSpeed=1800\nSwitchName=s4 Switches=s[0-3]  LinkSpeed=1800\nSwitchName=s5 Switches=s[0-3]  LinkSpeed=1800\nSwitchName=s6 Switches=s[0-3]  LinkSpeed=1800\nSwitchName=s7 Switches=s[0-3]  LinkSpeed=1800\nAs a practical matter, listing every switch connection\ndefinitely results in a slower scheduling algorithm for Slurm\nto optimize job placement.\nThe application performance may achieve little benefit from such optimization.\nListing the leaf switches with their nodes plus one top level switch\nshould result in good performance for both applications and Slurm.\nThe previous example might be configured as follows:\n\n# topology.conf\n# Switch Configuration\nSwitchName=s0 Nodes=tux[0-3]\nSwitchName=s1 Nodes=tux[4-7]\nSwitchName=s2 Nodes=tux[8-11]\nSwitchName=s3 Nodes=tux[12-15]\nSwitchName=s4 Switches=s[0-3]\n\nNote that compute nodes on switches that lack a common parent switch can\nbe used, but no job will span leaf switches without a common parent\n(unless the TopologyParam=TopoOptional option is used).\nFor example, it is legal to remove the line \"SwitchName=s4 Switches=s[0-3]\"\nfrom the above topology.conf file.\nIn that case, no job will span more than four compute nodes on any single leaf\nswitch.\nThis configuration can be useful if one wants to schedule multiple physical\nclusters as a single logical cluster under the control of a single slurmctld\ndaemon.\nIf you have nodes that are in separate networks and are associated with\nunique switches in your topology.conf file, it's possible that you\ncould get in a situation where a job isn't able to run.  If a job requests\nnodes that are in the different networks, either by requesting the nodes\ndirectly or by requesting a feature, the job will fail because the requested\nnodes can't communicate with each other.  We recommend placing nodes in\nseparate network segments in disjoint partitions.\nFor systems with a dragonfly network, configure Slurm with\nTopologyPlugin=topology/tree plus TopologyParam=dragonfly.\nIf a single job can not be entirely placed within a single network leaf\nswitch, the job will be spread across as many leaf switches as possible\nin order to optimize the job's network bandwidth.\nNOTE: When using the topology/tree plugin, Slurm identifies\nthe network switches which provide the best fit for pending jobs. If nodes\nhave a Weight defined, this will override the resource selection based\non network topology. If optimizing resource selection by node weight is more\nimportant than optimizing network topology then do NOT use the\ntopology/tree plugin.\nConfiguration Generators\n\n\nThe following independently maintained tools may be useful in generating the\ntopology.conf file for certain switch types:\n\n\nInfiniband switch - slurmibtopology\n\nhttps://github.com/OleHolmNielsen/Slurm_tools/tree/master/slurmibtopology\nOmni-Path (OPA) switch - opa2slurm\n\nhttps://gitlab.com/jtfrey/opa2slurm\nAWS Elastic Fabric Adapter (EFA) - ec2-topology\n\nhttps://github.com/aws-samples/ec2-topology-aware-for-slurm\n\nUser Options\nFor use with the topology/tree plugin, user can also specify the maximum\nnumber of leaf switches to be used for their job with the maximum time the\njob should wait for this optimized configuration. The syntax for this option\nis \"--switches=count[@time]\".\nThe system administrator can limit the maximum time that any job can\nwait for this optimized configuration using the SchedulerParameters\nconfiguration parameter with the max_switch_wait option.\nEnvironment Variables\n\n\nIf the topology/tree plugin is used, two environment variables will be set\nto describe that job's network topology. Note that these environment variables\nwill contain different data for the tasks launched on each node. Use of these\nenvironment variables is at the discretion of the user.\nSLURM_TOPOLOGY_ADDR:\nThe value will be set to the names network switches which may be involved in\nthe job's communications from the system's top level switch down to the leaf\nswitch and ending with node name. A period is used to separate each hardware\ncomponent name.\nSLURM_TOPOLOGY_ADDR_PATTERN:\nThis is set only if the system has the topology/tree plugin configured.\nThe value will be set component types listed in SLURM_TOPOLOGY_ADDR.\nEach component will be identified as either \"switch\" or \"node\".\nA period is used to separate each hardware component type.\nLast modified 01 April 2024\n"
        },
        {
            "title": "User Options",
            "content": "For use with the topology/tree plugin, user can also specify the maximum\nnumber of leaf switches to be used for their job with the maximum time the\njob should wait for this optimized configuration. The syntax for this option\nis \"--switches=count[@time]\".\nThe system administrator can limit the maximum time that any job can\nwait for this optimized configuration using the SchedulerParameters\nconfiguration parameter with the max_switch_wait option.Environment Variables\n\nIf the topology/tree plugin is used, two environment variables will be set\nto describe that job's network topology. Note that these environment variables\nwill contain different data for the tasks launched on each node. Use of these\nenvironment variables is at the discretion of the user.SLURM_TOPOLOGY_ADDR:\nThe value will be set to the names network switches which may be involved in\nthe job's communications from the system's top level switch down to the leaf\nswitch and ending with node name. A period is used to separate each hardware\ncomponent name.SLURM_TOPOLOGY_ADDR_PATTERN:\nThis is set only if the system has the topology/tree plugin configured.\nThe value will be set component types listed in SLURM_TOPOLOGY_ADDR.\nEach component will be identified as either \"switch\" or \"node\".\nA period is used to separate each hardware component type.Last modified 01 April 2024"
        },
        {
            "title": "Configuration Generators\n",
            "content": "topology.conf\nInfiniband switch - slurmibtopology\n\nhttps://github.com/OleHolmNielsen/Slurm_tools/tree/master/slurmibtopology\nOmni-Path (OPA) switch - opa2slurm\n\nhttps://gitlab.com/jtfrey/opa2slurm\nAWS Elastic Fabric Adapter (EFA) - ec2-topology\n\nhttps://github.com/aws-samples/ec2-topology-aware-for-slurm\nUser OptionsFor use with the topology/tree plugin, user can also specify the maximum\nnumber of leaf switches to be used for their job with the maximum time the\njob should wait for this optimized configuration. The syntax for this option\nis \"--switches=count[@time]\".\nThe system administrator can limit the maximum time that any job can\nwait for this optimized configuration using the SchedulerParameters\nconfiguration parameter with the max_switch_wait option.Environment Variables\n\nIf the topology/tree plugin is used, two environment variables will be set\nto describe that job's network topology. Note that these environment variables\nwill contain different data for the tasks launched on each node. Use of these\nenvironment variables is at the discretion of the user.SLURM_TOPOLOGY_ADDR:\nThe value will be set to the names network switches which may be involved in\nthe job's communications from the system's top level switch down to the leaf\nswitch and ending with node name. A period is used to separate each hardware\ncomponent name.SLURM_TOPOLOGY_ADDR_PATTERN:\nThis is set only if the system has the topology/tree plugin configured.\nThe value will be set component types listed in SLURM_TOPOLOGY_ADDR.\nEach component will be identified as either \"switch\" or \"node\".\nA period is used to separate each hardware component type.Last modified 01 April 2024"
        }
    ]
}