{
    "url": "https://slurm.schedmd.com/gang_scheduling.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Gang Scheduling",
            "content": "\nSlurm supports timesliced gang scheduling in which two or more jobs are\nallocated to the same resources in the same partition and these jobs are\nalternately suspended to let one job at a time have dedicated access to the\nresources for a configured period of time.\n\nSlurm also supports preemptive job scheduling that allows a job in a\nhigher PriorityTier partition, or in a preempting QOS, to preempt other\njobs. Preemption is related to Gang scheduling\nbecause SUSPEND is one of the PreemptionModes, and it uses the Gang\nscheduler to resume suspended jobs.\n\nA workload manager that supports timeslicing can improve responsiveness\nand utilization by allowing more jobs to begin running sooner.\nShorter-running jobs no longer have to wait in a queue behind longer-running\njobs.\nInstead they can be run \"in parallel\" with the longer-running jobs, which will\nallow them to start and finish quicker.\nThroughput is also improved because overcommitting the resources provides\nopportunities for \"local backfilling\" to occur (see example below).\n\nThe gang scheduling logic works on each partition independently.\nIf a new job has been allocated to resources in a partition that have already\nbeen allocated to an existing job, then the plugin will suspend the new job\nuntil the configured SchedulerTimeslice interval has elapsed.\nThen it will suspend the running job and let the new job make use of the\nresources for a SchedulerTimeslice interval.\nThis will continue until one of the jobs terminates.\n\nNOTE: Heterogeneous jobs are excluded from gang scheduling operations.\nConfiguration\nThere are several important configuration parameters relating to\ngang scheduling:\n\n\nSelectType: The Slurm gang scheduler supports nodes allocated by the\nselect/linear plugin, socket/core/CPU resources allocated by the\nselect/cons_tres plugin.\n\n\nSelectTypeParameters: Since resources will be getting overallocated\nwith jobs (suspended jobs remain in memory), the resource selection plugin\nshould be configured to track the amount of memory used by each job to ensure\nthat memory page swapping does not occur.\nWhen select/linear is chosen, we recommend setting\nSelectTypeParameters=CR_Memory.\nWhen select/cons_tres is chosen, we recommend\nincluding Memory as a resource\n(e.g. SelectTypeParameters=CR_Core_Memory).\n\n\nDefMemPerCPU: Since job requests may not explicitly specify\na memory requirement, we also recommend configuring\nDefMemPerCPU (default memory per allocated CPU) or\nDefMemPerNode (default memory per allocated node).\nIt may also be desirable to configure\nMaxMemPerCPU (maximum memory per allocated CPU) or\nMaxMemPerNode (maximum memory per allocated node) in slurm.conf.\nUsers can use the --mem or --mem-per-cpu option\nat job submission time to specify their memory requirements.\nNote that in order to gang schedule jobs, all jobs must be able to fit into\nmemory at the same time.\n\n\nJobAcctGatherType and JobAcctGatherFrequency:\nIf you wish to enforce memory limits, either that task/cgroup must be\nconfigured to limit each job's memory use or accounting must be enabled\nusing the JobAcctGatherType and JobAcctGatherFrequency\nparameters. If accounting is enabled and a job exceeds its configured\nmemory limits, it will be canceled in order to prevent it from\nadversely affecting other jobs sharing the same resources.\n\n\nPreemptMode: set the GANG option.\nSee the slurm.conf manpage for other options that may be specified to\nenable job preemption in addition to GANG.\nIn order to use gang scheduling, the GANG option must be specified at\nthe cluster level.\n\nNOTE: Gang scheduling is performed independently for each partition, so\nif you only want time-slicing by OverSubscribe, without any preemption,\nthen configuring partitions with overlapping nodes is not recommended.\nOn the other hand, if you want to use PreemptType=preempt/partition_prio\nto allow jobs from higher PriorityTier partitions to Suspend jobs from\nlower PriorityTier partitions, then you will need overlapping partitions,\nand PreemptMode=SUSPEND,GANG to use the Gang scheduler to resume the\nsuspended job(s).\nIn any case, time-slicing won't happen between jobs on different partitions.\n\n\nSchedulerTimeSlice: The default timeslice interval is 30 seconds.\nTo change this duration, set SchedulerTimeSlice to the desired interval\n(in seconds) in slurm.conf. For example, to set the timeslice interval\nto one minute, set SchedulerTimeSlice=60. Short values can increase\nthe overhead of gang scheduling.\n\n\nOverSubscribe: Configure the partition's OverSubscribe setting to\nFORCE for all partitions in which timeslicing is to take place.\nThe FORCE option supports an additional parameter that controls\nhow many jobs can share a compute resource (FORCE[:max_share]). By default the\nmax_share value is 4. To allow up to 6 jobs from this partition to be\nallocated to a common resource, set OverSubscribe=FORCE:6. To only let 2 jobs\ntimeslice on the same resources, set OverSubscribe=FORCE:2.\n\n\nIn order to enable gang scheduling after making the configuration changes\ndescribed above, restart Slurm if it is already running. Any change to the\nplugin settings in Slurm requires a full restart of the daemons. If you\njust change the partition OverSubscribe setting, this can be updated with\nscontrol reconfig.\nTimeslicer Design and Operation\n\n\nWhen enabled, the gang scheduler keeps track of the resources\nallocated to all jobs. For each partition an \"active bitmap\" is maintained that\ntracks all concurrently running jobs in the Slurm cluster. Each time a new\njob is allocated to resources in a partition, the gang scheduler\ncompares these newly allocated resources with the resources already maintained\nin the \"active bitmap\".\nIf these two sets of resources are disjoint then the new job is added to the \"active bitmap\". If these two sets of resources overlap then\nthe new job is suspended. All jobs are tracked in a per-partition job queue\nwithin the gang scheduler logic.\n\nA separate timeslicer thread is spawned by the gang scheduler\non startup. This thread sleeps for the configured SchedulerTimeSlice\ninterval. When it wakes up, it checks each partition for suspended jobs. If\nsuspended jobs are found then the timeslicer thread moves all running\njobs to the end of the job queue. It then reconstructs the \"active bitmap\" for\nthis partition beginning with the suspended job that has waited the longest to\nrun (this will be the first suspended job in the run queue). Each following job\nis then compared with the new \"active bitmap\", and if the job can be run\nconcurrently with the other \"active\" jobs then the job is added. Once this is\ncomplete then the timeslicer thread suspends any currently running jobs\nthat are no longer part of the \"active bitmap\", and resumes jobs that are new\nto the \"active bitmap\".\n\nThis timeslicer thread algorithm for rotating jobs is designed to prevent jobs from starving (remaining in the suspended state indefinitely) and\nto be as fair as possible in the distribution of runtime while still keeping\nall of the resources as busy as possible.\n\nThe gang scheduler suspends jobs via the same internal functions that\nsupport scontrol suspend and scontrol resume.\nA good way to observe the operation of the timeslicer is by running\nsqueue -i<time> in a terminal window where time is set\nequal to SchedulerTimeSlice.\nA Simple Example\n\n\nThe following example is configured with select/linear and OverSubscribe=FORCE.\nThis example takes place on a small cluster of 5 nodes:\n\n[user@n16 load]$ sinfo\nPARTITION AVAIL  TIMELIMIT NODES  STATE NODELIST\nactive*      up   infinite     5   idle n[12-16]\n\nHere are the Scheduler settings (excerpt of output):\n\n[user@n16 load]$ scontrol show config\n...\nPreemptMode             = GANG\n...\nSchedulerTimeSlice      = 30\nSchedulerType           = sched/builtin\n...\n\nThe myload script launches a simple load-generating app that runs\nfor the given number of seconds. Submit myload to run on all nodes:\n\n[user@n16 load]$ sbatch -N5 ./myload 300\nsbatch: Submitted batch job 3\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n    3    active  myload  user     0:05     5 n[12-16]\n\nSubmit it again and watch the gang scheduler suspend it:\n\n[user@n16 load]$ sbatch -N5 ./myload 300\nsbatch: Submitted batch job 4\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n    3    active  myload  user  R  0:13     5 n[12-16]\n    4    active  myload  user  S  0:00     5 n[12-16]\n\nAfter 30 seconds the gang scheduler swaps jobs, and now job 4 is the\nactive one:\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n    4    active  myload  user  R  0:08     5 n[12-16]\n    3    active  myload  user  S  0:41     5 n[12-16]\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n    4    active  myload  user  R  0:21     5 n[12-16]\n    3    active  myload  user  S  0:41     5 n[12-16]\n\nAfter another 30 seconds the gang scheduler sets job 3 running again:\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n    3    active  myload  user  R  0:50     5 n[12-16]\n    4    active  myload  user  S  0:30     5 n[12-16]\n\nA possible side effect of timeslicing: Note that jobs that are\nimmediately suspended may cause their srun commands to produce the\nfollowing output:\n\n[user@n16 load]$ cat slurm-4.out\nsrun: Job step creation temporarily disabled, retrying\nsrun: Job step creation still disabled, retrying\nsrun: Job step creation still disabled, retrying\nsrun: Job step creation still disabled, retrying\nsrun: Job step created\n\nThis occurs because srun is attempting to launch a jobstep in an\nallocation that has been suspended. The srun process will continue in a\nretry loop to launch the jobstep until the allocation has been resumed and the\njobstep can be launched.\n\nWhen the gang scheduler is enabled, this type of output in the user\njobs should be considered benign.\nMore examples\nThe following example shows how the timeslicer algorithm keeps the resources\nbusy. Job 10 runs continually, while jobs 9 and 11 are timesliced:\n\n[user@n16 load]$ sbatch -N3 ./myload 300\nsbatch: Submitted batch job 9\n\n[user@n16 load]$ sbatch -N2 ./myload 300\nsbatch: Submitted batch job 10\n\n[user@n16 load]$ sbatch -N3 ./myload 300\nsbatch: Submitted batch job 11\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n    9    active  myload  user  R  0:11     3 n[12-14]\n   10    active  myload  user  R  0:08     2 n[15-16]\n   11    active  myload  user  S  0:00     3 n[12-14]\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n   10    active  myload  user  R  0:50     2 n[15-16]\n   11    active  myload  user  R  0:12     3 n[12-14]\n    9    active  myload  user  S  0:41     3 n[12-14]\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n   10    active  myload  user  R  1:04     2 n[15-16]\n   11    active  myload  user  R  0:26     3 n[12-14]\n    9    active  myload  user  S  0:41     3 n[12-14]\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n    9    active  myload  user  R  0:46     3 n[12-14]\n   10    active  myload  user  R  1:13     2 n[15-16]\n   11    active  myload  user  S  0:30     3 n[12-14]\n\nThe next example displays \"local backfilling\":\n\n[user@n16 load]$ sbatch -N3 ./myload 300\nsbatch: Submitted batch job 12\n\n[user@n16 load]$ sbatch -N5 ./myload 300\nsbatch: Submitted batch job 13\n\n[user@n16 load]$ sbatch -N2 ./myload 300\nsbatch: Submitted batch job 14\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n   12    active  myload  user  R  0:14     3 n[12-14]\n   14    active  myload  user  R  0:06     2 n[15-16]\n   13    active  myload  user  S  0:00     5 n[12-16]\n\nWithout timeslicing and without the backfill scheduler enabled, job 14 has to\nwait for job 13 to finish.\n\nThis is called \"local\" backfilling because the backfilling only occurs with\njobs close enough in the queue to get allocated by the scheduler as part of\noversubscribing the resources. Recall that the number of jobs that can\novercommit a resource is controlled by the OverSubscribe=FORCE:max_share value,\nso this value effectively controls the scope of \"local backfilling\".\n\nNormal backfill algorithms check all jobs in the wait queue.\nConsumable Resource Examples\n\n\nThe following two examples illustrate the primary difference between\nCR_CPU and CR_Core when consumable resource selection is enabled\n(select/cons_tres).\n\nWhen CR_CPU (or CR_CPU_Memory) is configured then the selector\ntreats the CPUs as simple, interchangeable computing resources\nunless task affinity is enabled. However when task affinity is enabled\nwith CR_CPU or CR_Core (or CR_Core_Memory) is enabled, the\nselector treats the CPUs as individual resources that are specifically\nallocated to jobs.\nThis subtle difference is highlighted when timeslicing is enabled.\n\nIn both examples 6 jobs are submitted. Each job requests 2 CPUs per node, and\nall of the nodes contain two quad-core processors. The timeslicer will\ninitially let the first 4 jobs run and suspend the last 2 jobs.\nThe manner in which these jobs are timesliced depends upon the configured\nSelectTypeParameters.\n\nIn the first example CR_Core_Memory is configured. Note that jobs 46\nand 47 don't ever get suspended. This is because they are not sharing\ntheir cores with any other job.\nJobs 48 and 49 were allocated to the same cores as jobs 44 and 45.\nThe timeslicer recognizes this and timeslices only those jobs:\n\n[user@n16 load]$ sinfo\nPARTITION AVAIL  TIMELIMIT NODES  STATE NODELIST\nactive*      up   infinite     5   idle n[12-16]\n\n[user@n16 load]$ scontrol show config | grep Select\nSelectType              = select/cons_tres\nSelectTypeParameters    = CR_CORE_MEMORY\n\n[user@n16 load]$ sinfo -o \"%20N %5D %5c %5z\"\nNODELIST             NODES CPUS  S:C:T\nn[12-16]             5     8     2:4:1\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 44\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 45\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 46\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 47\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 48\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 49\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n   44    active  myload  user  R  0:09     5 n[12-16]\n   45    active  myload  user  R  0:08     5 n[12-16]\n   46    active  myload  user  R  0:08     5 n[12-16]\n   47    active  myload  user  R  0:07     5 n[12-16]\n   48    active  myload  user  S  0:00     5 n[12-16]\n   49    active  myload  user  S  0:00     5 n[12-16]\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n   46    active  myload  user  R  0:49     5 n[12-16]\n   47    active  myload  user  R  0:48     5 n[12-16]\n   48    active  myload  user  R  0:06     5 n[12-16]\n   49    active  myload  user  R  0:06     5 n[12-16]\n   44    active  myload  user  S  0:44     5 n[12-16]\n   45    active  myload  user  S  0:43     5 n[12-16]\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n   44    active  myload  user  R  1:23     5 n[12-16]\n   45    active  myload  user  R  1:22     5 n[12-16]\n   46    active  myload  user  R  2:22     5 n[12-16]\n   47    active  myload  user  R  2:21     5 n[12-16]\n   48    active  myload  user  S  1:00     5 n[12-16]\n   49    active  myload  user  S  1:00     5 n[12-16]\n\nNote the runtime of all 6 jobs in the output of the last squeue command.\nJobs 46 and 47 have been running continuously, while jobs 44 and 45 are\nsplitting their runtime with jobs 48 and 49.\n\nThe next example has CR_CPU_Memory configured and the same 6 jobs are\nsubmitted. Here the selector and the timeslicer treat the CPUs as countable\nresources which results in all 6 jobs sharing time on the CPUs:\n\n[user@n16 load]$ sinfo\nPARTITION AVAIL  TIMELIMIT NODES  STATE NODELIST\nactive*      up   infinite     5   idle n[12-16]\n\n[user@n16 load]$ scontrol show config | grep Select\nSelectType              = select/cons_tres\nSelectTypeParameters    = CR_CPU_MEMORY\n\n[user@n16 load]$ sinfo -o \"%20N %5D %5c %5z\"\nNODELIST             NODES CPUS  S:C:T\nn[12-16]             5     8     2:4:1\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 51\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 52\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 53\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 54\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 55\n\n[user@n16 load]$ sbatch -n10 -N5 ./myload 300\nsbatch: Submitted batch job 56\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n   51    active  myload  user  R  0:11     5 n[12-16]\n   52    active  myload  user  R  0:11     5 n[12-16]\n   53    active  myload  user  R  0:10     5 n[12-16]\n   54    active  myload  user  R  0:09     5 n[12-16]\n   55    active  myload  user  S  0:00     5 n[12-16]\n   56    active  myload  user  S  0:00     5 n[12-16]\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n   51    active  myload  user  R  1:09     5 n[12-16]\n   52    active  myload  user  R  1:09     5 n[12-16]\n   55    active  myload  user  R  0:23     5 n[12-16]\n   56    active  myload  user  R  0:23     5 n[12-16]\n   53    active  myload  user  S  0:45     5 n[12-16]\n   54    active  myload  user  S  0:44     5 n[12-16]\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n   53    active  myload  user  R  0:55     5 n[12-16]\n   54    active  myload  user  R  0:54     5 n[12-16]\n   55    active  myload  user  R  0:40     5 n[12-16]\n   56    active  myload  user  R  0:40     5 n[12-16]\n   51    active  myload  user  S  1:16     5 n[12-16]\n   52    active  myload  user  S  1:16     5 n[12-16]\n\n[user@n16 load]$ squeue\nJOBID PARTITION    NAME  USER ST  TIME NODES NODELIST\n   51    active  myload  user  R  3:18     5 n[12-16]\n   52    active  myload  user  R  3:18     5 n[12-16]\n   53    active  myload  user  R  3:17     5 n[12-16]\n   54    active  myload  user  R  3:16     5 n[12-16]\n   55    active  myload  user  S  3:00     5 n[12-16]\n   56    active  myload  user  S  3:00     5 n[12-16]\n\nNote that the runtime of all 6 jobs is roughly equal. Jobs 51-54 ran first so\nthey're slightly ahead, but so far all jobs have run for at least 3 minutes.\n\nAt the core level this means that Slurm relies on the Linux kernel to move\njobs around on the cores to maximize performance.\nThis is different than when CR_Core_Memory was configured and the jobs\nwould effectively remain \"pinned\" to their specific cores for the duration of\nthe job.\nNote that CR_Core_Memory supports CPU binding, while\nCR_CPU_Memory does not.\nNote that manually suspending a job (i.e. \"scontrol suspend ...\") releases\nits CPUs for allocation to other jobs.\nResuming a previously suspended job may result in multiple jobs being\nallocated the same CPUs, which could trigger gang scheduling of jobs.\nUse of the scancel command to send SIGSTOP and SIGCONT signals would stop a\njob without releasing its CPUs for allocation to other jobs and would be a\npreferable mechanism in many cases.Last modified 29 January 2024"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        }
    ]
}