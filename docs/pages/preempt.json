{
    "url": "https://slurm.schedmd.com/preempt.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Preemption",
            "content": "\nSlurm supports job preemption, the act of \"stopping\" one or more \"low-priority\"\njobs to let a \"high-priority\" job run.\nJob preemption is implemented as a variation of Slurm's\nGang Scheduling logic.\nWhen a job that can preempt others is allocated resources that are\nalready allocated to one or more jobs that could be preempted by the first job,\nthe preemptable job(s) are preempted.\nBased on the configuration the preempted job(s) can be cancelled, or can be\nrequeued and started using other resources, or suspended and resumed once the\npreemptor job completes, or can even share resources with the preemptor using\nGang Scheduling.\n\nThe PriorityTier of the Partition of the job or its Quality Of Service (QOS)\ncan be used to identify which jobs can preempt or be preempted by other jobs.\nSlurm offers the ability to configure the preemption mechanism used on a per\npartition or per QOS basis.\nFor example, jobs in a low priority queue may get requeued,\nwhile jobs in a medium priority queue may get suspended.\nConfiguration\nThere are several important configuration parameters relating to preemption:\n\n\nSelectType: Slurm job preemption logic supports nodes allocated by the\nselect/linear plugin, socket/core/CPU resources allocated by the\nselect/cons_tres plugin.\n\n\nSelectTypeParameter: Since resources may be getting over-allocated\nwith jobs (suspended jobs remain in memory), the resource selection\nplugin should be configured to track the amount of memory used by each job to\nensure that memory page swapping does not occur.\nWhen select/linear is chosen, we recommend setting\nSelectTypeParameter=CR_Memory.\nWhen select/cons_tres is chosen, we recommend\nincluding Memory as a resource (e.g. SelectTypeParameter=CR_Core_Memory).\n\nNOTE: Unless PreemptMode=SUSPEND,GANG these memory management\nparameters are not critical.\n\n\nDefMemPerCPU: Since job requests may not explicitly specify\na memory requirement, we also recommend configuring\nDefMemPerCPU (default memory per allocated CPU) or\nDefMemPerNode (default memory per allocated node).\nIt may also be desirable to configure\nMaxMemPerCPU (maximum memory per allocated CPU) or\nMaxMemPerNode (maximum memory per allocated node) in slurm.conf.\nUsers can use the --mem or --mem-per-cpu option\nat job submission time to specify their memory requirements.\nNOTE: Unless PreemptMode=SUSPEND,GANG these memory management\nparameters are not critical.\n\n\nGraceTime: Specifies a time period for a job to execute after\nit is selected to be preempted. This option can be specified by partition or\nQOS using the slurm.conf file or database respectively.\nThe GraceTime is specified in\nseconds and the default value is zero, which results in no preemption delay.\nOnce a job has been selected for preemption, its end time is set to the\ncurrent time plus GraceTime. The job is immediately sent SIGCONT and\nSIGTERM signals in order to provide notification of its imminent termination.\nThis is followed by the SIGCONT, SIGTERM and SIGKILL signal sequence upon\nreaching its new end time.\nNOTE: This parameter is not used when PreemptMode=SUSPEND\nis configured or when suspending jobs with scontrol suspend. For\nsetting the preemption grace time in these cases, see\nsuspend_grace_time.\n\n\nJobAcctGatherType and JobAcctGatherFrequency: The \"maximum data segment\nsize\" and \"maximum virtual memory size\" system limits will be configured for\neach job to ensure that the job does not exceed its requested amount of memory.\nIf you wish to enable additional enforcement of memory limits, configure job\naccounting with the JobAcctGatherType and JobAcctGatherFrequency\nparameters. When accounting is enabled and a job exceeds its configured memory\nlimits, it will be canceled in order to prevent it from adversely affecting\nother jobs sharing the same resources.\nNOTE: Unless PreemptMode=SUSPEND,GANG these memory management\nparameters are not critical.\n\n\nPreemptMode: Mechanism used to preempt jobs or enable gang scheduling.\nWhen the PreemptType parameter is set to enable preemption, the\nPreemptMode in the main section of slurm.conf selects the default\nmechanism used to preempt the preemptable jobs for the cluster.\n\nPreemptMode may be specified on a per partition basis to override this\ndefault value if PreemptType=preempt/partition_prio. Alternatively, it\ncan be specified on a per QOS basis if PreemptType=preempt/qos.\nIn either case, a valid default PreemptMode value must be specified for\nthe cluster as a whole when preemption is enabled.\n\nThe GANG option is used to enable gang scheduling independent of whether\npreemption is enabled (i.e. independent of PreemptType).\nIt can be specified in addition to other PreemptMode settings, with the\ntwo options comma separated (e.g. PreemptMode=SUSPEND,GANG).\n\n\nOFF: Is the default value and disables job preemption and gang\nscheduling. It is only compatible with PreemptType=preempt/none.\n\n\nCANCEL: The preempted job will be cancelled.\n\n\nGANG: Enables gang scheduling (time slicing) of jobs in the same\npartition, and allows the resuming of suspended jobs. In order to use gang\nscheduling, the GANG option must be specified at the cluster level.\n\nNOTE: Gang scheduling is performed independently for each partition, so\nif you only want time-slicing by OverSubscribe, without any preemption,\nthen configuring partitions with overlapping nodes is not recommended.\nOn the other hand, if you want to use PreemptType=preempt/partition_prio\nto allow jobs from higher PriorityTier partitions to Suspend jobs from lower\nPriorityTier partitions, then you will need overlapping partitions, and\nPreemptMode=SUSPEND,GANG to use Gang scheduler to resume the suspended\njob(s).\nIn either case, time-slicing won't happen between jobs on different partitions.\n\n\nREQUEUE: Preempts jobs by requeuing them (if possible) or canceling\nthem. For jobs to be requeued they must have the \"--requeue\" sbatch option set\nor the cluster wide JobRequeue parameter in slurm.conf must be set to 1.\n\n\nSUSPEND: The preempted jobs will be suspended, and later the Gang\nscheduler will resume them. Therefore, the SUSPEND preemption mode always\nneeds the GANG option to be specified at the cluster level.\nAlso, because the suspended jobs will still use memory on the allocated nodes,\nSlurm needs to be able to track memory resources to be able to suspend jobs.\n\nNOTE: Because gang scheduling is performed independently for each\npartition, if using PreemptType=preempt/partition_prio then jobs in\nhigher PriorityTier partitions will suspend jobs in lower PriorityTier\npartitions to run on the released resources. Only when the preemptor job ends\nthen the suspended jobs will be resumed by the Gang scheduler.\nIf PreemptType=preempt/qos is configured and if the preempted job(s) and\nthe preemptor job from are on the same partition, then they will share\nresources with the Gang scheduler (time-slicing). If not (i.e. if the\npreemptees and preemptor are on different partitions) then the preempted jobs\nwill remain suspended until the preemptor ends.\n\n\n\n\nPreemptType: Specifies the plugin used to identify which jobs can be\npreempted in order to start a pending job.\n\npreempt/none: Job preemption is disabled (default).\npreempt/partition_prio: Job preemption is based upon partition\nPriorityTier. Jobs in higher PriorityTier partitions may preempt jobs\nfrom lower PriorityTier partitions. This is not compatible with\nPreemptMode=OFF.\npreempt/qos: Job preemption rules are specified by Quality Of\nService (QOS) specifications in the Slurm database. In the case of\nPreemptMode=SUSPEND, a preempting job needs to be submitted to a\npartition with a higher PriorityTier or to the same partition.\nThis option is not compatible with PreemptMode=OFF.\nA configuration of PreemptMode=SUSPEND is only supported by the\nSelectType=select/cons_tres plugin.\nSee the sacctmgr man page to configure the options\nof preempt/qos.\n\n\n\nPreemptExemptTime: Specifies minimum run time of jobs before they are\nconsidered for preemption. This is only honored when the PreemptMode\nis set to REQUEUE or CANCEL. It is specified as a time string:\nA time of -1 disables the option, equivalent to 0. Acceptable time formats\ninclude \"minutes\", \"minutes:seconds\", \"hours:minutes:seconds\", \"days\\-hours\",\n\"days\\-hours:minutes\", and \"days\\-hours:minutes:seconds\".\nPreemptEligibleTime is shown in the output of \"scontrol show job <job id>\"\n\n\nPriorityTier: Configure the partition's PriorityTier setting\nrelative to other partitions to control the preemptive behavior when\nPreemptType=preempt/partition_prio.\nIf two jobs from two\ndifferent partitions are allocated to the same resources, the job in the\npartition with the greater PriorityTier value will preempt the job in the\npartition with the lesser PriorityTier value. If the PriorityTier\nvalues of the two partitions are equal then no preemption will occur. The\ndefault PriorityTier value is 1.\n\nNOTE: In addition to being used for partition based preemption,\nPriorityTier also has an effect on scheduling. The scheduler will\nevaluate jobs in the partition(s) with the highest PriorityTier\nbefore evaluating jobs in other partitions, regardless of which jobs have\nthe highest Priority. The scheduler will consider the job priority when\nevaluating jobs within the partition(s) with the same PriorityTier.\n\n\nOverSubscribe: Configure the partition's OverSubscribe setting to\nFORCE for all partitions in which job preemption using a suspend/resume\nmechanism is used.\nThe FORCE option supports an additional parameter that controls\nhow many jobs can oversubscribe a compute resource (FORCE[:max_share]). By\ndefault the max_share value is 4. In order to preempt jobs (and not gang\nschedule them), always set max_share to 1. To allow up to 2 jobs from this\npartition to be allocated to a common resource (and gang scheduled), set\nOverSubscribe=FORCE:2.\nNOTE: PreemptType=preempt/qos will permit one additional job\nto be run on the partition if started due to job preemption. For example, a\nconfiguration of OverSubscribe=FORCE:1 will only permit one job per\nresource normally, but a second job can be started if done so through\npreemption based upon QOS.\n\n\nExclusiveUser: In partitions with ExclusiveUser=YES, jobs will be\nprevented from preempting or being preempted by any job from any other user.\nThe one exception is that these ExclusiveUser jobs will be able to preempt\n(but not be preempted by) fully \"--exclusive\" jobs from other users.\nThis is for the same reason that \"--exclusive=user\" blocks preemption, but this\npartition-level setting can only be overridden by making a job fully exclusive.\n\n\nMCSParameters: If MCS labels are set on jobs,\npreemption will be restricted to other jobs with the same MCS label. If this\nparameter is configured to use enforced,select, MCS labels will\nbe set by default on jobs, causing this restriction to be universal.\n\n\nTo enable preemption after making the configuration changes described above,\nrestart Slurm if it is already running. Any change to the plugin settings in\nSlurm requires a full restart of the daemons. If you just change the partition\nPriorityTier or OverSubscribe setting, this can be updated with\nscontrol reconfig.\n\nIf a job request restricts Slurm's ability to run jobs from multiple users or\naccounts on a node by using the \"--exclusive=user\" or \"--exclusive=mcs\" job\noptions, the job will be prevented from preempting or being preempted by any job\nthat does not match the user or MCS. The one exception is that these\nexclusive=user jobs will be able to preempt (but not be preempted by)\nfully \"--exclusive\" jobs from other users. If preemption is used, it is\ngenerally advisable to disable the \"--exclusive=user\" and \"--exclusive=mcs\"\njob options by using a job_submit plugin (set the value of \"job_desc.shared\"\nto \"NO_VAL16\").\n\nFor heterogeneous job to be considered for preemption all components\nmust be eligible for preemption. When a heterogeneous job is to be preempted\nthe first identified component of the job with the highest order PreemptMode\n(SUSPEND (highest), REQUEUE, CANCEL (lowest)) will be\nused to set the PreemptMode for all components. The GraceTime and user\nwarning signal for each component of the heterogeneous job remain unique.\n\nBecause licenses are not freed when jobs are suspended, jobs using licenses\nrequested by higher priority jobs will only be prempted when PreemptMode is\neither REQUEUE or CANCEL and\nPreemptParameters=reclaim_licenses is set.\nPreemption Design and Operation\n\n\nThe SelectType plugin will identify resources where a pending job can\nbegin execution. When PreemptMode is configured to CANCEL,\nSUSPEND or REQUEUE, the select plugin will also preempt running\njobs as needed to initiate the pending job. When\nPreemptMode=SUSPEND,GANG the select plugin will initiate the pending\njob and rely upon the gang scheduling logic to perform job suspend and resume,\nas described below.\n\nThe select plugin is passed an ordered list of preemptable jobs to consider for\neach pending job which is a candidate to start.\nThis list is sorted by either:\n\nQOS priority,\nPartition priority and job size (to favor preempting smaller jobs), or\nJob start time (with PreemptParameters=youngest_first).\n\nThe select plugin will determine if the pending job can start without preempting\nany jobs and if so, starts the job using available resources.\nOtherwise, the select plugin will simulate the preemption of each job in the\npriority ordered list and test if the job can be started after each preemption.\nOnce the job can be started, the higher priority jobs in the preemption queue\nwill not be considered, but the jobs to be preempted in the original list may\nbe sub-optimal.\nFor example, to start an 8 node job, the ordered preemption candidates may be\n2 node, 4 node and 8 node.\nPreempting all three jobs would allow the pending job to start, but by reordering\nthe preemption candidates it is possible to start the pending job after\npreempting only one job.\nTo address this issue, the preemption candidates are re-ordered with the final\njob requiring preemption placed first in the list and all of the other jobs\nto be preempted ordered by the number of nodes in their allocation which overlap\nthe resources selected for the pending job.\nIn the example above, the 8 node job would be moved to the first position in\nthe list.\nThe process of simulating the preemption of each job in the priority ordered\nlist will then be repeated for the final decision of which jobs to preempt.\nThis two stage process may preempt jobs which are not strictly in preemption\npriority order, but fewer jobs will be preempted than otherwise required.\nSee the PreemptParameters configuration parameter options of reorder_count\nand strict_order for preemption tuning parameters.\n\nWhen enabled, the gang scheduling logic (which is also supports job\npreemption) keeps track of the resources allocated to all jobs.\nFor each partition an \"active bitmap\" is maintained that tracks all\nconcurrently running jobs in the Slurm cluster.\nEach partition also maintains a job list for that partition, and a list of\n\"shadow\" jobs.\nThe \"shadow\" jobs are high priority job allocations that \"cast shadows\" on the\nactive bitmaps of the low priority jobs.\nJobs caught in these \"shadows\" will be preempted.\n\nEach time a new job is allocated to resources in a partition and begins\nrunning, the gang scheduler adds a \"shadow\" of this job to all lower priority\npartitions.\nThe active bitmap of these lower priority partitions are then rebuilt, with the shadow jobs added first.\nAny existing jobs that were replaced by one or more \"shadow\" jobs are\nsuspended (preempted). Conversely, when a high priority running job completes,\nits \"shadow\" goes away and the active bitmaps of the lower priority\npartitions are rebuilt to see if any suspended jobs can be resumed.\n\nThe gang scheduler plugin is designed to be reactive to the resource\nallocation decisions made by the \"select\" plugins.\nThe \"select\" plugins have been enhanced to recognize when job preemption has\nbeen configured, and to factor in the priority of each partition when selecting resources for a job.\nWhen choosing resources for each job, the selector avoids resources that are\nin use by other jobs (unless sharing has been configured, in which case it\ndoes some load-balancing).\nHowever, when job preemption is enabled, the select plugins may choose\nresources that are already in use by jobs from partitions with a lower\npriority setting, even when sharing is disabled in those partitions.\n\nThis leaves the gang scheduler in charge of controlling which jobs should run\non the over-allocated resources.\nIf PreemptMode=SUSPEND, jobs are suspended using the same internal\nfunctions that support scontrol suspend and scontrol resume.\nA good way to observe the operation of the gang scheduler is by running\nsqueue -i<time> in a terminal window.\nLimitations of Preemption During Backfill Scheduling\n\n\nFor performance reasons, the backfill scheduler reserves whole nodes for jobs,\nnot partial nodes. If during backfill scheduling a job preempts one or more\nother jobs, the whole nodes for those preempted jobs are reserved for the\npreemptor job, even if the preemptor job requested fewer resources than that.\nThese reserved nodes aren't available to other jobs during that backfill\ncycle, even if the other jobs could fit on the nodes. Therefore, jobs may\npreempt more resources during a single backfill iteration than they requested.\nA Simple Example\n\n\nThe following example is configured with select/linear and\nPreemptMode=SUSPEND,GANG.\nThis example takes place on a cluster of 5 nodes:\n\n[user@n16 ~]$ sinfo\nPARTITION AVAIL  TIMELIMIT NODES  STATE NODELIST\nactive*      up   infinite     5   idle n[12-16]\nhipri        up   infinite     5   idle n[12-16]\n\nHere are the Partition settings:\n\n[user@n16 ~]$ grep PartitionName /shared/slurm/slurm.conf\nPartitionName=DEFAULT OverSubscribe=FORCE:1 Nodes=n[12-16]\nPartitionName=active PriorityTier=1 Default=YES\nPartitionName=hipri  PriorityTier=2\n\nThe runit.pl script launches a simple load-generating app that runs\nfor the given number of seconds. Submit 5 single-node runit.pl jobs to\nrun on all nodes:\n\n[user@n16 ~]$ sbatch -N1 ./runit.pl 300\nsbatch: Submitted batch job 485\n[user@n16 ~]$ sbatch -N1 ./runit.pl 300\nsbatch: Submitted batch job 486\n[user@n16 ~]$ sbatch -N1 ./runit.pl 300\nsbatch: Submitted batch job 487\n[user@n16 ~]$ sbatch -N1 ./runit.pl 300\nsbatch: Submitted batch job 488\n[user@n16 ~]$ sbatch -N1 ./runit.pl 300\nsbatch: Submitted batch job 489\n[user@n16 ~]$ squeue -Si\nJOBID PARTITION     NAME   USER  ST   TIME  NODES NODELIST\n  485    active runit.pl   user   R   0:06      1 n12\n  486    active runit.pl   user   R   0:06      1 n13\n  487    active runit.pl   user   R   0:05      1 n14\n  488    active runit.pl   user   R   0:05      1 n15\n  489    active runit.pl   user   R   0:04      1 n16\n\nNow submit a short-running 3-node job to the hipri partition:\n\n[user@n16 ~]$ sbatch -N3 -p hipri ./runit.pl 30\nsbatch: Submitted batch job 490\n[user@n16 ~]$ squeue -Si\nJOBID PARTITION     NAME   USER  ST   TIME  NODES NODELIST\n  485    active runit.pl   user   S   0:27      1 n12\n  486    active runit.pl   user   S   0:27      1 n13\n  487    active runit.pl   user   S   0:26      1 n14\n  488    active runit.pl   user   R   0:29      1 n15\n  489    active runit.pl   user   R   0:28      1 n16\n  490     hipri runit.pl   user   R   0:03      3 n[12-14]\n\nJob 490 in the hipri partition preempted jobs 485, 486, and 487 from\nthe active partition. Jobs 488 and 489 in the active partition\nremained running.\n\nThis state persisted until job 490 completed, at which point the preempted jobs\nwere resumed:\n\n[user@n16 ~]$ squeue\nJOBID PARTITION     NAME   USER  ST   TIME  NODES NODELIST\n  485    active runit.pl   user   R   0:30      1 n12\n  486    active runit.pl   user   R   0:30      1 n13\n  487    active runit.pl   user   R   0:29      1 n14\n  488    active runit.pl   user   R   0:59      1 n15\n  489    active runit.pl   user   R   0:58      1 n16\nAnother Example\n\n\nIn this example we have three different partitions using three different\njob preemption mechanisms.\n\n# Excerpt from slurm.conf\nPartitionName=low Nodes=linux Default=YES OverSubscribe=NO      PriorityTier=10 PreemptMode=requeue\nPartitionName=med Nodes=linux Default=NO  OverSubscribe=FORCE:1 PriorityTier=20 PreemptMode=suspend\nPartitionName=hi  Nodes=linux Default=NO  OverSubscribe=FORCE:1 PriorityTier=30 PreemptMode=off\n\n$ sbatch tmp\nSubmitted batch job 94\n$ sbatch -p med tmp\nSubmitted batch job 95\n$ sbatch -p hi tmp\nSubmitted batch job 96\n$ squeue\n  JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)\n     96        hi      tmp      moe   R       0:04      1 linux\n     94       low      tmp      moe  PD       0:00      1 (Resources)\n     95       med      tmp      moe   S       0:02      1 linux\n(after job 96 completes)\n$ squeue\n  JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)\n     94       low      tmp      moe  PD       0:00      1 (Resources)\n     95       med      tmp      moe   R       0:24      1 linux\nAnother Example\n\n\nIn this example we have one partition on which we want to execute only one\njob per resource (e.g. core) at a time except when a job submitted to the\npartition from a high priority Quality Of Service (QOS) is submitted. In that\ncase, we want that second high priority job to be started and be gang scheduled\nwith the other jobs on overlapping resources.\n\n# Excerpt from slurm.conf\nPreemptMode=Suspend,Gang\nPreemptType=preempt/qos\nPartitionName=normal Nodes=linux Default=NO  OverSubscribe=FORCE:1\nFuture Ideas\n\n\nMore intelligence in the select plugins: This implementation of\npreemption relies on intelligent job placement by the select plugins.\n\nTake the following example:\n\n[user@n8 ~]$ sinfo\nPARTITION AVAIL  TIMELIMIT NODES  STATE NODELIST\nactive*      up   infinite     5   idle n[1-5]\nhipri        up   infinite     5   idle n[1-5]\n[user@n8 ~]$ sbatch -N1 -n2 ./sleepme 60\nsbatch: Submitted batch job 17\n[user@n8 ~]$ sbatch -N1 -n2 ./sleepme 60\nsbatch: Submitted batch job 18\n[user@n8 ~]$ sbatch -N1 -n2 ./sleepme 60\nsbatch: Submitted batch job 19\n[user@n8 ~]$ squeue\n  JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)\n     17    active  sleepme  cholmes   R       0:03      1 n1\n     18    active  sleepme  cholmes   R       0:03      1 n2\n     19    active  sleepme  cholmes   R       0:02      1 n3\n[user@n8 ~]$ sbatch -N3 -n6 -p hipri ./sleepme 20\nsbatch: Submitted batch job 20\n[user@n8 ~]$ squeue -Si\n  JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)\n     17    active  sleepme  cholmes   S       0:16      1 n1\n     18    active  sleepme  cholmes   S       0:16      1 n2\n     19    active  sleepme  cholmes   S       0:15      1 n3\n     20     hipri  sleepme  cholmes   R       0:03      3 n[1-3]\n[user@n8 ~]$ sinfo\nPARTITION AVAIL  TIMELIMIT NODES  STATE NODELIST\nactive*      up   infinite     3  alloc n[1-3]\nactive*      up   infinite     2   idle n[4-5]\nhipri        up   infinite     3  alloc n[1-3]\nhipri        up   infinite     2   idle n[4-5]\n\nIt would be more ideal if the \"hipri\" job were placed on nodes n[3-5], which\nwould allow jobs 17 and 18 to continue running. However, a new \"intelligent\"\nalgorithm would have to include factors such as job size and required nodes in\norder to support ideal placements such as this, which can quickly complicate\nthe design. Any and all help is welcome here!\nLast modified 21 May 2024"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        }
    ]
}