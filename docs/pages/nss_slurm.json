{
    "url": "https://slurm.schedmd.com/nss_slurm.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "nss_slurm",
            "content": "nss_slurm is an optional NSS plugin that can permit passwd, group and\ncloud node host\nresolution for a job on the compute node to be serviced through the local\nslurmstepd process, rather than through some alternate network-based service\nsuch as LDAP, DNS, SSSD, or NSLCD.When enabled on the cluster, for each job, the job's user will have their\nfull struct passwd info \u2014 username, uid, primary gid, gecos info,\nhome directory, and shell \u2014 securely sent as part of each step launch,\nand cached within the slurmstepd process. This info will then be provided to\nany process launched by that step through the\ngetpwuid()/getpwnam()/getpwent() system calls.For group information \u2014 from the\ngetgrgid()/getgrnam()/getgrent() system calls \u2014,\nan abbreviated view of struct group will be provided. Within a given\nprocess, the response will include only those groups that the user belongs to,\nbut with only the user themselves listed as a member. The full list of group\nmembers is not provided.For host information \u2014 from the\ngethostbyname()/gethostbyname system calls \u2014,\nan abbreviated view of struct hostent will be provided. Within a given\nprocess, the response will include only the cloud hosts that belong to\nallocation.All of this information is populated by slurmctld as it is seen on the\nhost running slurmctld.Installation\n\nSource:In your Slurm build directory, navigate to contribs/nss_slurm/\nand run:make && make installThis will install libnss_slurm.so.2 alongside your other Slurm library files\nin your install path.Depending on your Linux distribution, you will likely need to symlink this\nto the directory which includes your other NSS plugins to enable it.\nOn Debian/Ubuntu, /lib/x86_64-linux-gnu is\nrecommended, and for RHEL-based distributions\n/usr/lib64 is recommended. If in doubt,\na command such as\nfind /lib /usr/ -name 'libnss*' should help.\n\n\nSetup\nThe slurmctld must be configured to look up and send the appropriate passwd\nand group details as part of the launch credential. This is handled by setting\nLaunchParameters=enable_nss_slurm in slurm.conf and restarting\nslurmctld.\nOnce enabled, the scontrol getent command\ncan be used on a compute node to print all passwd and group info associated\nwith job steps on that node. As an example:\n\ntim@node0001:~$ scontrol getent node0001\nJobId=1268.Extern:\nUser:\ntim:x:1000:1000:Tim Wickberg:/home/tim:/bin/bash\nGroups:\ntim:x:1000:tim\nprojecta:x:1001:tim\n\nJobId=1268.0:\nUser:\ntim:x:1000:1000:Tim Wickberg:/home/tim:/bin/bash\nGroups:\ntim:x:1000:tim\nprojecta:x:1001:tim\n\nNSS Slurm Configuration\n\n\nnss_slurm has an optional configuration file \u2014\n/etc/nss_slurm.conf. This configuration file is only needed if:\n\nThe node's hostname does not match the NodeName, in which case you must\nexplicitly set the NodeName option.\nThe SlurmdSpoolDir does not match Slurm's default location of\n/var/spool/slurmd, in which case it must be provided as well.\n\nNodeName and SlurmdSpoolDir are the only configuration options supported\nat this time.\nInitial Testing\n\n\nBefore enabling NSS Slurm directly on the node, you should use the\n-s slurm option to getent within a newly launched job step\nto verify that the rest of the setup has been completed successfully. The\n-s option to getent allows it to query a specific database \u2014\neven if it has not been enabled by default through the system's\nnsswitch.conf. Note that nss_slurm only responds to requests from\nprocesses within the job step itself \u2014 you must launch the getent\ncommand within a job step to see any data returned.\nAs an example of a successful query:\n\ntim@blackhole:~$ srun getent -s slurm passwd\ntim:x:1000:1000:Tim Wickberg:/home/tim:/bin/bash\ntim@blackhole:~$ srun getent -s slurm group\ntim:x:1000:tim\nprojecta:x:1001:tim\n\nNSS Configuration\n\n\nEnabling nss_slurm is as simple as adding slurm to the passwd and\ngroup database in /etc/nsswitch.conf. It is recommended that\nslurm is listed first, as the order (from left to right) determines\nthe sequence in which the NSS databases will be queried, and this ensures Slurm\nhandles the request if able before submitting the query to other sources.\nTo enable cloud node name resolution slurm needs to be added to the\nto hosts database in /etc/nsswitch.conf.\nIt is recommended that slurm is listed last.\nOnce enabled, test it by launching getent queries such as:\n\ntim@blackhole:~$ srun getent passwd tim\ntim:x:1000:1000:Tim Wickberg:/home/tim:/bin/bash\ntim@blackhole:~$ srun getent group projecta\nprojecta:x:1001:tim\n\nLimitations\n\n\nnss_slurm will only return results for processes within a given job step.\nIt will not return any results for processes outside of these steps, such as\nsystem monitoring, node health checks, prolog or epilog scripts, and related\nnode system processes.\nnss_slurm is not meant as a full replacement for network directory services\nsuch as LDAP, but as a way to remove load from those systems to improve the\nperformance of large-scale job launches. It accomplishes this by removing\nthe \"thundering-herd\" issue should all tasks of a large job make simultaneous\nlookup requests \u2014 generally for info related to the user themselves,\nwhich is the only information nss_slurm will be able to provide \u2014 and\noverwhelm the underlying directory services.\nnss_slurm is only able to communicate with a single slurmd. If running\nwith --enable-multiple-slurmd, you can specify which slurmd is used with NodeName\nand SlurmdSpoolDir parameters in the nss_slurm.conf file.\nSince the information is gathered from the slurmctld node, there can be\nunexpected consequences if the information differs between the controller and\nthe worker nodes. One possible scenario is if a user's shell is /sbin/nologin\non the slurmctld machine but /bin/bash on the slurmd node. An interactive\nsalloc may fail to launch since it will try to spawn the default shell,\nwhich according to the slurmctld is /sbin/nologin.\nWhen using proctrack/pgid, nss_slurm will rely on the pgid of the process\nto determine if it can respond to that request. The login shell spawned with\nsrun --pty must be run in its own session,\nand therefore its own pgid, so nss_slurm will not respond to requests in an\ninteractive session.\nLast modified 30 Aug 2023\n"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        },
        {
            "title": "Setup",
            "content": "The slurmctld must be configured to look up and send the appropriate passwd\nand group details as part of the launch credential. This is handled by setting\nLaunchParameters=enable_nss_slurm in slurm.conf and restarting\nslurmctld.Once enabled, the scontrol getent command\ncan be used on a compute node to print all passwd and group info associated\nwith job steps on that node. As an example:\ntim@node0001:~$ scontrol getent node0001\nJobId=1268.Extern:\nUser:\ntim:x:1000:1000:Tim Wickberg:/home/tim:/bin/bash\nGroups:\ntim:x:1000:tim\nprojecta:x:1001:tim\n\nJobId=1268.0:\nUser:\ntim:x:1000:1000:Tim Wickberg:/home/tim:/bin/bash\nGroups:\ntim:x:1000:tim\nprojecta:x:1001:tim\nNSS Slurm Configuration\n\nnss_slurm has an optional configuration file \u2014\n/etc/nss_slurm.conf. This configuration file is only needed if:\n\nThe node's hostname does not match the NodeName, in which case you must\nexplicitly set the NodeName option.\nThe SlurmdSpoolDir does not match Slurm's default location of\n/var/spool/slurmd, in which case it must be provided as well.\n\nNodeName and SlurmdSpoolDir are the only configuration options supported\nat this time.\nInitial Testing\n\n\nBefore enabling NSS Slurm directly on the node, you should use the\n-s slurm option to getent within a newly launched job step\nto verify that the rest of the setup has been completed successfully. The\n-s option to getent allows it to query a specific database \u2014\neven if it has not been enabled by default through the system's\nnsswitch.conf. Note that nss_slurm only responds to requests from\nprocesses within the job step itself \u2014 you must launch the getent\ncommand within a job step to see any data returned.\nAs an example of a successful query:\n\ntim@blackhole:~$ srun getent -s slurm passwd\ntim:x:1000:1000:Tim Wickberg:/home/tim:/bin/bash\ntim@blackhole:~$ srun getent -s slurm group\ntim:x:1000:tim\nprojecta:x:1001:tim\n\nNSS Configuration\n\n\nEnabling nss_slurm is as simple as adding slurm to the passwd and\ngroup database in /etc/nsswitch.conf. It is recommended that\nslurm is listed first, as the order (from left to right) determines\nthe sequence in which the NSS databases will be queried, and this ensures Slurm\nhandles the request if able before submitting the query to other sources.\nTo enable cloud node name resolution slurm needs to be added to the\nto hosts database in /etc/nsswitch.conf.\nIt is recommended that slurm is listed last.\nOnce enabled, test it by launching getent queries such as:\n\ntim@blackhole:~$ srun getent passwd tim\ntim:x:1000:1000:Tim Wickberg:/home/tim:/bin/bash\ntim@blackhole:~$ srun getent group projecta\nprojecta:x:1001:tim\n\nLimitations\n\n\nnss_slurm will only return results for processes within a given job step.\nIt will not return any results for processes outside of these steps, such as\nsystem monitoring, node health checks, prolog or epilog scripts, and related\nnode system processes.\nnss_slurm is not meant as a full replacement for network directory services\nsuch as LDAP, but as a way to remove load from those systems to improve the\nperformance of large-scale job launches. It accomplishes this by removing\nthe \"thundering-herd\" issue should all tasks of a large job make simultaneous\nlookup requests \u2014 generally for info related to the user themselves,\nwhich is the only information nss_slurm will be able to provide \u2014 and\noverwhelm the underlying directory services.\nnss_slurm is only able to communicate with a single slurmd. If running\nwith --enable-multiple-slurmd, you can specify which slurmd is used with NodeName\nand SlurmdSpoolDir parameters in the nss_slurm.conf file.\nSince the information is gathered from the slurmctld node, there can be\nunexpected consequences if the information differs between the controller and\nthe worker nodes. One possible scenario is if a user's shell is /sbin/nologin\non the slurmctld machine but /bin/bash on the slurmd node. An interactive\nsalloc may fail to launch since it will try to spawn the default shell,\nwhich according to the slurmctld is /sbin/nologin.\nWhen using proctrack/pgid, nss_slurm will rely on the pgid of the process\nto determine if it can respond to that request. The login shell spawned with\nsrun --pty must be run in its own session,\nand therefore its own pgid, so nss_slurm will not respond to requests in an\ninteractive session.\nLast modified 30 Aug 2023\n"
        },
        {
            "title": "Initial Testing\n\n",
            "content": "Before enabling NSS Slurm directly on the node, you should use the\n-s slurm option to getent within a newly launched job step\nto verify that the rest of the setup has been completed successfully. The\n-s option to getent allows it to query a specific database \u2014\neven if it has not been enabled by default through the system's\nnsswitch.conf. Note that nss_slurm only responds to requests from\nprocesses within the job step itself \u2014 you must launch the getent\ncommand within a job step to see any data returned.As an example of a successful query:\ntim@blackhole:~$ srun getent -s slurm passwd\ntim:x:1000:1000:Tim Wickberg:/home/tim:/bin/bash\ntim@blackhole:~$ srun getent -s slurm group\ntim:x:1000:tim\nprojecta:x:1001:tim\nNSS Configuration\n\nEnabling nss_slurm is as simple as adding slurm to the passwd and\ngroup database in /etc/nsswitch.conf. It is recommended that\nslurm is listed first, as the order (from left to right) determines\nthe sequence in which the NSS databases will be queried, and this ensures Slurm\nhandles the request if able before submitting the query to other sources.To enable cloud node name resolution slurm needs to be added to the\nto hosts database in /etc/nsswitch.conf.\nIt is recommended that slurm is listed last.Once enabled, test it by launching getent queries such as:\ntim@blackhole:~$ srun getent passwd tim\ntim:x:1000:1000:Tim Wickberg:/home/tim:/bin/bash\ntim@blackhole:~$ srun getent group projecta\nprojecta:x:1001:tim\nLimitations\n\nnss_slurm will only return results for processes within a given job step.\nIt will not return any results for processes outside of these steps, such as\nsystem monitoring, node health checks, prolog or epilog scripts, and related\nnode system processes.nss_slurm is not meant as a full replacement for network directory services\nsuch as LDAP, but as a way to remove load from those systems to improve the\nperformance of large-scale job launches. It accomplishes this by removing\nthe \"thundering-herd\" issue should all tasks of a large job make simultaneous\nlookup requests \u2014 generally for info related to the user themselves,\nwhich is the only information nss_slurm will be able to provide \u2014 and\noverwhelm the underlying directory services.nss_slurm is only able to communicate with a single slurmd. If running\nwith --enable-multiple-slurmd, you can specify which slurmd is used with NodeName\nand SlurmdSpoolDir parameters in the nss_slurm.conf file.Since the information is gathered from the slurmctld node, there can be\nunexpected consequences if the information differs between the controller and\nthe worker nodes. One possible scenario is if a user's shell is /sbin/nologin\non the slurmctld machine but /bin/bash on the slurmd node. An interactive\nsalloc may fail to launch since it will try to spawn the default shell,\nwhich according to the slurmctld is /sbin/nologin.When using proctrack/pgid, nss_slurm will rely on the pgid of the process\nto determine if it can respond to that request. The login shell spawned with\nsrun --pty must be run in its own session,\nand therefore its own pgid, so nss_slurm will not respond to requests in an\ninteractive session.Last modified 30 Aug 2023"
        }
    ]
}