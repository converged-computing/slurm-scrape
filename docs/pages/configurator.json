{
    "url": "https://slurm.schedmd.com/configurator.html",
    "sections": [
        {
            "title": "Slurm Version 24.05 Configuration Tool",
            "content": "This form can be used to create a Slurm configuration file with\nyou controlling many of the important configuration parameters.This is the full version of the Slurm configuration tool. This version\nhas all the configuration options to create a Slurm configuration file. There\nis a simplified version of the Slurm configuration tool available at\nconfigurator.easy.html.This tool supports Slurm version 24.05 only.\nConfiguration files for other versions of Slurm should be built\nusing the tool distributed with it in doc/html/configurator.html.\nSome parameters will be set to default values, but you can\nmanually edit the resulting slurm.conf as desired\nfor greater flexibility. See man slurm.conf for more\ndetails about the configuration parameters.Note the while Slurm daemons create log files and other files as needed,\nit treats the lack of parent directories as a fatal error.\nThis prevents the daemons from running if critical file systems are\nnot mounted and will minimize the risk of cold-starting (starting\nwithout preserving jobs).Note that this configuration file must be installed on all nodes\nin your cluster.After you have filled in the fields of interest, use the\n\"Submit\" button on the bottom of the page to build the slurm.conf\nfile. It will appear on your web browser. Save the file in text format\nas slurm.conf for use by Slurm.\n\nFor more information about Slurm, see\nhttps://slurm.schedmd.com/slurm.html\nCluster Name\n ClusterName:\nThe name of your cluster. Using different names for each of your clusters is\nimportant when using a single database to record information from multiple\nSlurm-managed clusters.\n\nControl Machines\nDefine the hostname of the computer on which the Slurm controller and\noptional backup controller will execute.\nHostname values should not be the fully qualified domain\nname (e.g. use tux rather than tux.abc.com).\n\n SlurmctldHost:\nPrimary Controller Hostname\n\n BackupController: Backup\nController Hostname (optional)\n\nCompute Machines\nDefine the machines on which user applications can run.\nYou can also specify addresses of these computers if desired\n(defaults to their hostnames).\nOnly a few of the possible parameters associated with the nodes will\nbe set by this tool, but many others are available.\nExecuting the command slurmd -C on each compute node will print its\nphysical configuration (sockets, cores, real memory size, etc.), which\ncan be used in constructing the slurm.conf file.\nAll of the nodes will be placed into a single partition (or queue)\nwith global access. Many options are available to group nodes into\npartitions with a wide variety of configuration parameters.\nManually edit the slurm.conf produced to exercise these options.\nNode names and addresses may be specified using a numeric range specification.\n\n\n NodeName:\nCompute nodes\n\n NodeAddr: Compute node addresses\n(optional)\n\n PartitionName:\nName of the one partition to be created\n\n MaxTime:\nMaximum time limit of jobs in minutes or INFINITE\n\nThe following parameters describe a node's configuration.\nSet a value for CPUs.\nThe other parameters are optional, but provide more control over scheduled resources:\n\n CPUs: Count of processors\non each compute node.\nIf CPUs is omitted, it will be inferred from:\nSockets, CoresPerSocket, and ThreadsPerCore.\n\n\nSockets:\nNumber of physical processor sockets/chips on the node.\nIf Sockets is omitted, it will be inferred from:\nCPUs, CoresPerSocket, and ThreadsPerCore.\n\n\nCoresPerSocket:\nNumber of cores in a single physical processor socket.\nThe CoresPerSocket value describes physical cores, not\nthe logical number of processors per socket.\n\n\nThreadsPerCore:\nNumber of logical threads in a single physical core.\n\n RealMemory: Amount\nof real memory. This parameter is required when specifying Memory as a\nconsumable resource with the select/cons_tres plug-in. See below\nunder Resource Selection.\n\nSlurm User\nThe Slurm controller (slurmctld) can run without elevated privileges,\nso it is recommended that a user \"slurm\" be created for it. For testing\npurposes any user name can be used.\n\n SlurmUser\n\nSlurm Port Numbers\nThe Slurm controller (slurmctld) requires a unique port for communications\nas do the Slurm compute node daemons (slurmd). If not set, slurm ports\nare set by checking for an entry in /etc/services and if that\nfails by using an interval default set at Slurm build time.\n\n SlurmctldPort\n\n SlurmdPort\n\nState Preservation\nDefine the location of a directory where the slurmctld daemon saves its state.\nThis should be a fully qualified pathname which can be read and written to\nby the Slurm user on both the control machine and backup controller (if configured).\nThe location of a directory where slurmd saves state should also be defined.\nThis must be a unique directory on each compute server (local disk).\nThe use of a highly reliable file system (e.g. RAID) is recommended.\n\n\nStateSaveLocation: Slurmctld state save directory\nMust be writable by all SlurmctldHost nodes\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
        },
        {
            "title": "Cluster Name",
            "content": "ClusterNameControl Machinestuxtux.abc.com\n SlurmctldHost:\nPrimary Controller Hostname\n\n BackupController: Backup\nController Hostname (optional)\n\nCompute Machines\nDefine the machines on which user applications can run.\nYou can also specify addresses of these computers if desired\n(defaults to their hostnames).\nOnly a few of the possible parameters associated with the nodes will\nbe set by this tool, but many others are available.\nExecuting the command slurmd -C on each compute node will print its\nphysical configuration (sockets, cores, real memory size, etc.), which\ncan be used in constructing the slurm.conf file.\nAll of the nodes will be placed into a single partition (or queue)\nwith global access. Many options are available to group nodes into\npartitions with a wide variety of configuration parameters.\nManually edit the slurm.conf produced to exercise these options.\nNode names and addresses may be specified using a numeric range specification.\n\n\n NodeName:\nCompute nodes\n\n NodeAddr: Compute node addresses\n(optional)\n\n PartitionName:\nName of the one partition to be created\n\n MaxTime:\nMaximum time limit of jobs in minutes or INFINITE\n\nThe following parameters describe a node's configuration.\nSet a value for CPUs.\nThe other parameters are optional, but provide more control over scheduled resources:\n\n CPUs: Count of processors\non each compute node.\nIf CPUs is omitted, it will be inferred from:\nSockets, CoresPerSocket, and ThreadsPerCore.\n\n\nSockets:\nNumber of physical processor sockets/chips on the node.\nIf Sockets is omitted, it will be inferred from:\nCPUs, CoresPerSocket, and ThreadsPerCore.\n\n\nCoresPerSocket:\nNumber of cores in a single physical processor socket.\nThe CoresPerSocket value describes physical cores, not\nthe logical number of processors per socket.\n\n\nThreadsPerCore:\nNumber of logical threads in a single physical core.\n\n RealMemory: Amount\nof real memory. This parameter is required when specifying Memory as a\nconsumable resource with the select/cons_tres plug-in. See below\nunder Resource Selection.\n\nSlurm User\nThe Slurm controller (slurmctld) can run without elevated privileges,\nso it is recommended that a user \"slurm\" be created for it. For testing\npurposes any user name can be used.\n\n SlurmUser\n\nSlurm Port Numbers\nThe Slurm controller (slurmctld) requires a unique port for communications\nas do the Slurm compute node daemons (slurmd). If not set, slurm ports\nare set by checking for an entry in /etc/services and if that\nfails by using an interval default set at Slurm build time.\n\n SlurmctldPort\n\n SlurmdPort\n\nState Preservation\nDefine the location of a directory where the slurmctld daemon saves its state.\nThis should be a fully qualified pathname which can be read and written to\nby the Slurm user on both the control machine and backup controller (if configured).\nThe location of a directory where slurmd saves state should also be defined.\nThis must be a unique directory on each compute server (local disk).\nThe use of a highly reliable file system (e.g. RAID) is recommended.\n\n\nStateSaveLocation: Slurmctld state save directory\nMust be writable by all SlurmctldHost nodes\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
        },
        {
            "title": "Compute Machines",
            "content": "slurmd -Cslurm.confslurm.conf\n NodeName:\nCompute nodes\n\n NodeAddr: Compute node addresses\n(optional)\n\n PartitionName:\nName of the one partition to be created\n\n MaxTime:\nMaximum time limit of jobs in minutes or INFINITE\n\nThe following parameters describe a node's configuration.\nSet a value for CPUs.\nThe other parameters are optional, but provide more control over scheduled resources:\n\n CPUs: Count of processors\non each compute node.\nIf CPUs is omitted, it will be inferred from:\nSockets, CoresPerSocket, and ThreadsPerCore.\n\n\nSockets:\nNumber of physical processor sockets/chips on the node.\nIf Sockets is omitted, it will be inferred from:\nCPUs, CoresPerSocket, and ThreadsPerCore.\n\n\nCoresPerSocket:\nNumber of cores in a single physical processor socket.\nThe CoresPerSocket value describes physical cores, not\nthe logical number of processors per socket.\n\n\nThreadsPerCore:\nNumber of logical threads in a single physical core.\n\n RealMemory: Amount\nof real memory. This parameter is required when specifying Memory as a\nconsumable resource with the select/cons_tres plug-in. See below\nunder Resource Selection.\n\nSlurm User\nThe Slurm controller (slurmctld) can run without elevated privileges,\nso it is recommended that a user \"slurm\" be created for it. For testing\npurposes any user name can be used.\n\n SlurmUser\n\nSlurm Port Numbers\nThe Slurm controller (slurmctld) requires a unique port for communications\nas do the Slurm compute node daemons (slurmd). If not set, slurm ports\nare set by checking for an entry in /etc/services and if that\nfails by using an interval default set at Slurm build time.\n\n SlurmctldPort\n\n SlurmdPort\n\nState Preservation\nDefine the location of a directory where the slurmctld daemon saves its state.\nThis should be a fully qualified pathname which can be read and written to\nby the Slurm user on both the control machine and backup controller (if configured).\nThe location of a directory where slurmd saves state should also be defined.\nThis must be a unique directory on each compute server (local disk).\nThe use of a highly reliable file system (e.g. RAID) is recommended.\n\n\nStateSaveLocation: Slurmctld state save directory\nMust be writable by all SlurmctldHost nodes\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
        },
        {
            "title": "Slurm User",
            "content": "\n SlurmUser\n\nSlurm Port Numbers\nThe Slurm controller (slurmctld) requires a unique port for communications\nas do the Slurm compute node daemons (slurmd). If not set, slurm ports\nare set by checking for an entry in /etc/services and if that\nfails by using an interval default set at Slurm build time.\n\n SlurmctldPort\n\n SlurmdPort\n\nState Preservation\nDefine the location of a directory where the slurmctld daemon saves its state.\nThis should be a fully qualified pathname which can be read and written to\nby the Slurm user on both the control machine and backup controller (if configured).\nThe location of a directory where slurmd saves state should also be defined.\nThis must be a unique directory on each compute server (local disk).\nThe use of a highly reliable file system (e.g. RAID) is recommended.\n\n\nStateSaveLocation: Slurmctld state save directory\nMust be writable by all SlurmctldHost nodes\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
        },
        {
            "title": "Slurm Port Numbers",
            "content": "/etc/services\n SlurmctldPort\n\n SlurmdPort\n\nState Preservation\nDefine the location of a directory where the slurmctld daemon saves its state.\nThis should be a fully qualified pathname which can be read and written to\nby the Slurm user on both the control machine and backup controller (if configured).\nThe location of a directory where slurmd saves state should also be defined.\nThis must be a unique directory on each compute server (local disk).\nThe use of a highly reliable file system (e.g. RAID) is recommended.\n\n\nStateSaveLocation: Slurmctld state save directory\nMust be writable by all SlurmctldHost nodes\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
        },
        {
            "title": "State Preservation",
            "content": "\n\nStateSaveLocation: Slurmctld state save directory\nMust be writable by all SlurmctldHost nodes\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
        },
        {
            "title": "Scheduling",
            "content": "SchedulerTypeBackfillBuiltin\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
        },
        {
            "title": "Interconnect",
            "content": "SwitchTypeHPE\n  SlingshotNone\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
        },
        {
            "title": "Default MPI Type",
            "content": "MpiDefaultMPI-PMI2MPI-PMIxNone\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
        },
        {
            "title": "Process Tracking",
            "content": "ProctrackTypeCgroupcgroupcgroup.confLinuxProcPgid\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
        },
        {
            "title": "Resource Selection",
            "content": "SelectTypecons_tres\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
        },
        {
            "title": "Task Launch",
            "content": "TaskPluginNoneAffinityCgroup\nProlog and Epilog\n\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
        },
        {
            "title": "Prolog and Epilog",
            "content": "\nProlog/Epilog: Path that will be executed as root on every\nnode of a user's job before the job's tasks will be initiated there\nand after that job has terminated.\nThese parameters are optional.\n\n  Prolog\n  Epilog\n\n\nSrunProlog/Epilog: Fully qualified path to be executed by srun at\njob step initiation and termination. These parameters may be overridden by\nsrun's --prolog and --epilog options\nThese parameters are optional.\n\n  SrunProlog\n  SrunEpilog\n\n\nTaskProlog/Epilog: Fully qualified path to be executed as the user\nbefore each task begins execution and after each task terminates.\nThese parameters are optional.\n\n  TaskProlog\n  TaskEpilog\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
        },
        {
            "title": "Event Logging",
            "content": "\n SlurmctldDebug\n(default is info)\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n SlurmdDebug\n(default is info)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Completion Logging\nDefine the job completion logging mechanism to be used. Defaults to None.\nSelect one value for JobCompType:\n None:\nNo job completion logging\n Elasticsearch:\nWrite job completion info to an Elasticsearch server\n FileTxt:\nWrite job completion status to a text file\n Kafka:\nWrite job completion info to a Kafka server\n Lua:\nUse a script called jobcomp.lua to log job completion\n Script:\nUse an arbitrary script to log job completion\n MySQL:\nWrite completion status to a MySQL or MariaDB database\n\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
        },
        {
            "title": "Job Completion Logging",
            "content": "JobCompTypeNoneElasticsearchFileTxtKafkaLuaScriptMySQL\n JobCompLoc:\nThis is the location of the text file to be written to (if JobCompType=filetxt),\nor the script to be run (if JobCompType=script), or the URL to the\nElasticsearch server (if JobCompType=elasticsearch), or file containing\nlibrdkafka parameters (if JobCompType=jobcomp/kafka), database name\n(for other values of JobCompType).\nOptions below are for use with a database to specify where the database is running and how to connect to it\n JobCompHost:\nHost the database is running on for Job completion\n JobCompPort:\nPort the database server is listening on for Job completion\n JobCompUser:\nUser we are to use to talk to the database for Job completion\n JobCompParams:\nPass arbitrary text string to Job completion plugin\n JobCompPass:\nPassword we are to use to talk to the database for Job completion\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\n JobAcctGatherFrequency:\npolling interval in seconds. Zero disables periodic sampling.\n\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
        },
        {
            "title": "Job Accounting Gather",
            "content": "JobAcctGatherTypeNonecgroupLinuxJobAcctGatherFrequency\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n AccountingStorageLoc:\nLocation specification or database name.\nThis is the location of the text file to be written to (used by Log only).\nUse a fully qualified pathname. If using a database it is the name of the database you will use or create for the stored data.\nOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
        },
        {
            "title": "Job Accounting Storage",
            "content": "AccountingStorageTypeNoneSlurmDBDAccountingStorageLocOptions below are for use with a database to specify where the database is running and how to connect to it\n AccountingStorageHost:\nHost the database is running on for Job Accounting\n AccountingStoragePort:\nPort the database server is listening on for Job Accounting\n AccountingStorageUser:\nUser we are to use to talk to the database for Job Accounting\n AccountingStoragePass:\nPassword we are to use to talk to the database for Job Accounting.\nIn the case of SlurmDBD, this will be an alternate socket name for use with a Munge\ndaemon providing enterprise-wide authentication (while the default Munge socket\nwould provide cluster-wide authentication only).\n AccountingStoreFlags:\nComma separated list. Options are:\n'job_comment' - store the job comment field in the database;\n'job_env' - store a batch job's env in the database;\n'job_extra' - store a batch job's extra field in the database;\n'job_script' - store the job batch script in the database.\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
        },
        {
            "title": "Process ID Logging",
            "content": "\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\nTimers\nSlurm has a variety of timers to control when to consider a node DOWN,\nwhen to purge job records, how long to give a job to gracefully terminate, etc.\n\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
        },
        {
            "title": "Timers",
            "content": "\n\nSlurmctldTimeout: How many seconds the backup controller waits before\nbecoming the active controller\n\n\nSlurmdTimeout: How many seconds the Slurm controller waits for the slurmd\nto respond to a request before considering the node DOWN\n\n\nInactiveLimit: How many seconds the Slurm controller waits for srun\ncommands to respond before considering the job or job step inactive and\nterminating it. A value of zero indicates unlimited wait\n\n\nMinJobAge: How many seconds the Slurm controller waits after a\njob terminates before purging its record. A record of the job will\npersist in job completion and/or accounting records indefinitely,\nbut will no longer be visible with the squeue command after puring\n\n\nKillWait: How many seconds a job is given to gracefully terminate\nafter reaching its time limit and being sent SIGTERM before sending\na SIGKILLL\n\n\nWaitTime: How many seconds after a job step's first task terminates\nbefore terminating all remaining tasks. A value of zero indicates unlimited wait\n\n\n\n\n\n"
        }
    ]
}