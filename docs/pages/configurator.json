{
    "url": "https://slurm.schedmd.com/configurator.easy.html",
    "sections": [
        {
            "title": "Slurm Version 24.05 Configuration Tool - Easy Version",
            "content": "This form can be used to create a Slurm configuration file with\nyou controlling many of the important configuration parameters.This is a simplified version of the Slurm configuration tool. This version\nhas fewer  options for creating a Slurm configuration file. The full version\nof the Slurm configuration tool is available at\nconfigurator.html.This tool supports Slurm version 24.05 only.\nConfiguration files for other versions of Slurm should be built\nusing the tool distributed with it in doc/html/configurator.html.\nSome parameters will be set to default values, but you can\nmanually edit the resulting slurm.conf as desired\nfor greater flexibility. See man slurm.conf for more\ndetails about the configuration parameters.Note the while Slurm daemons create log files and other files as needed,\nit treats the lack of parent directories as a fatal error.\nThis prevents the daemons from running if critical file systems are\nnot mounted and will minimize the risk of cold-starting (starting\nwithout preserving jobs).Note that this configuration file must be installed on all nodes\nin your cluster.After you have filled in the fields of interest, use the\n\"Submit\" button on the bottom of the page to build the slurm.conf\nfile. It will appear on your web browser. Save the file in text format\nas slurm.conf for use by Slurm.\n\nFor more information about Slurm, see\nhttps://slurm.schedmd.com/slurm.html\nCluster Name\n ClusterName:\nThe name of your cluster. Using different names for each of your clusters is\nimportant when using a single database to record information from multiple\nSlurm-managed clusters.\n\nControl Machines\nDefine the hostname of the computer on which the Slurm controller and\noptional backup controller will execute.\nHostname values should should not be the fully qualified domain\nname (e.g. use tux rather than tux.abc.com).\n\n SlurmctldHost:\nPrimary Controller Hostname\n\nCompute Machines\nDefine the machines on which user applications can run.\nYou can also specify addresses of these computers if desired\n(defaults to their hostnames).\nOnly a few of the possible parameters associated with the nodes will\nbe set by this tool, but many others are available.\nExecuting the command slurmd -C on each compute node will print its\nphysical configuration (sockets, cores, real memory size, etc.), which\ncan be used in constructing the slurm.conf file.\nAll of the nodes will be placed into a single partition (or queue)\nwith global access. Many options are available to group nodes into\npartitions with a wide variety of configuration parameters.\nManually edit the slurm.conf produced to exercise these options.\nNode names and addresses may be specified using a numeric range specification.\n\n\n NodeName:\nCompute nodes\n\n NodeAddr: Compute node addresses\n(optional)\n\n PartitionName:\nName of the one partition to be created\n\n MaxTime:\nMaximum time limit of jobs in minutes or INFINITE\n\nThe following parameters describe a node's configuration.\nSet a value for CPUs.\nThe other parameters are optional, but provide more control over scheduled resources:\n\n CPUs: Count of processors\non each compute node.\nIf CPUs is omitted, it will be inferred from:\nSockets, CoresPerSocket, and ThreadsPerCore.\n\n\nSockets:\nNumber of physical processor sockets/chips on the node.\nIf Sockets is omitted, it will be inferred from:\nCPUs, CoresPerSocket, and ThreadsPerCore.\n\n\nCoresPerSocket:\nNumber of cores in a single physical processor socket.\nThe CoresPerSocket value describes physical cores, not\nthe logical number of processors per socket.\n\n\nThreadsPerCore:\nNumber of logical threads in a single physical core.\n\n RealMemory: Amount\nof real memory. This parameter is required when specifying Memory as a\nconsumable resource with the select/cons_tres plug-in. See below\nunder Resource Selection.\n\nSlurm User\nThe Slurm controller (slurmctld) can run without elevated privileges,\nso it is recommended that a user \"slurm\" be created for it. For testing\npurposes any user name can be used.\n\n SlurmUser\n\nState Preservation\nDefine the location of a directory where the slurmctld daemon saves its state.\nThis should be a fully qualified pathname which can be read and written to\nby the Slurm user on both the control machine and backup controller (if configured).\nThe location of a directory where slurmd saves state should also be defined.\nThis must be a unique directory on each compute server (local disk).\nThe use of a highly reliable file system (e.g. RAID) is recommended.\n\n\nStateSaveLocation: Slurmctld state save directory\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\nhandling required (InfiniBand, Myrinet, Ethernet, etc.)\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
        },
        {
            "title": "Cluster Name",
            "content": "ClusterNameControl Machinestuxtux.abc.com\n SlurmctldHost:\nPrimary Controller Hostname\n\nCompute Machines\nDefine the machines on which user applications can run.\nYou can also specify addresses of these computers if desired\n(defaults to their hostnames).\nOnly a few of the possible parameters associated with the nodes will\nbe set by this tool, but many others are available.\nExecuting the command slurmd -C on each compute node will print its\nphysical configuration (sockets, cores, real memory size, etc.), which\ncan be used in constructing the slurm.conf file.\nAll of the nodes will be placed into a single partition (or queue)\nwith global access. Many options are available to group nodes into\npartitions with a wide variety of configuration parameters.\nManually edit the slurm.conf produced to exercise these options.\nNode names and addresses may be specified using a numeric range specification.\n\n\n NodeName:\nCompute nodes\n\n NodeAddr: Compute node addresses\n(optional)\n\n PartitionName:\nName of the one partition to be created\n\n MaxTime:\nMaximum time limit of jobs in minutes or INFINITE\n\nThe following parameters describe a node's configuration.\nSet a value for CPUs.\nThe other parameters are optional, but provide more control over scheduled resources:\n\n CPUs: Count of processors\non each compute node.\nIf CPUs is omitted, it will be inferred from:\nSockets, CoresPerSocket, and ThreadsPerCore.\n\n\nSockets:\nNumber of physical processor sockets/chips on the node.\nIf Sockets is omitted, it will be inferred from:\nCPUs, CoresPerSocket, and ThreadsPerCore.\n\n\nCoresPerSocket:\nNumber of cores in a single physical processor socket.\nThe CoresPerSocket value describes physical cores, not\nthe logical number of processors per socket.\n\n\nThreadsPerCore:\nNumber of logical threads in a single physical core.\n\n RealMemory: Amount\nof real memory. This parameter is required when specifying Memory as a\nconsumable resource with the select/cons_tres plug-in. See below\nunder Resource Selection.\n\nSlurm User\nThe Slurm controller (slurmctld) can run without elevated privileges,\nso it is recommended that a user \"slurm\" be created for it. For testing\npurposes any user name can be used.\n\n SlurmUser\n\nState Preservation\nDefine the location of a directory where the slurmctld daemon saves its state.\nThis should be a fully qualified pathname which can be read and written to\nby the Slurm user on both the control machine and backup controller (if configured).\nThe location of a directory where slurmd saves state should also be defined.\nThis must be a unique directory on each compute server (local disk).\nThe use of a highly reliable file system (e.g. RAID) is recommended.\n\n\nStateSaveLocation: Slurmctld state save directory\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\nhandling required (InfiniBand, Myrinet, Ethernet, etc.)\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
        },
        {
            "title": "Compute Machines",
            "content": "slurmd -Cslurm.confslurm.conf\n NodeName:\nCompute nodes\n\n NodeAddr: Compute node addresses\n(optional)\n\n PartitionName:\nName of the one partition to be created\n\n MaxTime:\nMaximum time limit of jobs in minutes or INFINITE\n\nThe following parameters describe a node's configuration.\nSet a value for CPUs.\nThe other parameters are optional, but provide more control over scheduled resources:\n\n CPUs: Count of processors\non each compute node.\nIf CPUs is omitted, it will be inferred from:\nSockets, CoresPerSocket, and ThreadsPerCore.\n\n\nSockets:\nNumber of physical processor sockets/chips on the node.\nIf Sockets is omitted, it will be inferred from:\nCPUs, CoresPerSocket, and ThreadsPerCore.\n\n\nCoresPerSocket:\nNumber of cores in a single physical processor socket.\nThe CoresPerSocket value describes physical cores, not\nthe logical number of processors per socket.\n\n\nThreadsPerCore:\nNumber of logical threads in a single physical core.\n\n RealMemory: Amount\nof real memory. This parameter is required when specifying Memory as a\nconsumable resource with the select/cons_tres plug-in. See below\nunder Resource Selection.\n\nSlurm User\nThe Slurm controller (slurmctld) can run without elevated privileges,\nso it is recommended that a user \"slurm\" be created for it. For testing\npurposes any user name can be used.\n\n SlurmUser\n\nState Preservation\nDefine the location of a directory where the slurmctld daemon saves its state.\nThis should be a fully qualified pathname which can be read and written to\nby the Slurm user on both the control machine and backup controller (if configured).\nThe location of a directory where slurmd saves state should also be defined.\nThis must be a unique directory on each compute server (local disk).\nThe use of a highly reliable file system (e.g. RAID) is recommended.\n\n\nStateSaveLocation: Slurmctld state save directory\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\nhandling required (InfiniBand, Myrinet, Ethernet, etc.)\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
        },
        {
            "title": "Slurm User",
            "content": "\n SlurmUser\n\nState Preservation\nDefine the location of a directory where the slurmctld daemon saves its state.\nThis should be a fully qualified pathname which can be read and written to\nby the Slurm user on both the control machine and backup controller (if configured).\nThe location of a directory where slurmd saves state should also be defined.\nThis must be a unique directory on each compute server (local disk).\nThe use of a highly reliable file system (e.g. RAID) is recommended.\n\n\nStateSaveLocation: Slurmctld state save directory\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\nhandling required (InfiniBand, Myrinet, Ethernet, etc.)\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
        },
        {
            "title": "State Preservation",
            "content": "\n\nStateSaveLocation: Slurmctld state save directory\n\n\nSlurmdSpoolDir: Slurmd state save directory\n\nDefine when a non-responding (DOWN) node is returned to service.\nSelect one value for ReturnToService:\n\n0: When explicitly restored to service by an administrator.\n\n1:Upon registration with a valid configuration only if it was set DOWN\ndue to being non-responsive.\n\n2:Upon registration with a valid configuration.\n\nScheduling\nDefine the mechanism to be used for controlling job ordering.\nSelect one value for SchedulerType:\n Backfill:\nFIFO with backfill\n Builtin: First-In\nFirst-Out (FIFO)\n\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\nhandling required (InfiniBand, Myrinet, Ethernet, etc.)\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
        },
        {
            "title": "Scheduling",
            "content": "SchedulerTypeBackfillBuiltin\nInterconnect\nDefine the node interconnect used.\nSelect one value for SwitchType:\n HPE\n  Slingshot: HPE Slingshot proprietary interconnect\n None: No special\nhandling required (InfiniBand, Myrinet, Ethernet, etc.)\n\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
        },
        {
            "title": "Interconnect",
            "content": "SwitchTypeHPE\n  SlingshotNone\nDefault MPI Type\nSpecify the type of MPI to be used by default. Slurm will configure environment\nvariables accordingly. Users can over-ride this specification with an srun option.\nSelect one value for MpiDefault:\n MPI-PMI2\n(For PMI2-supporting MPI implementations)\n MPI-PMIx\n(Exascale PMI implementation)\n None:\n This works for most other MPI types.\n\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
        },
        {
            "title": "Default MPI Type",
            "content": "MpiDefaultMPI-PMI2MPI-PMIxNone\nProcess Tracking\nDefine the algorithm used to identify which processes are associated with a\ngiven job. This is used signal, kill, and account for the processes associated\nwith a job step.\nSelect one value for ProctrackType:\n Cgroup:  Use\nLinux cgroup to create a job container and track processes.\nBuild a cgroup.conf file as well\n LinuxProc: Use\nparent process ID records, processes can escape from Slurm control\n Pgid: Use Unix\nProcess Group ID, processes changing their process group ID can escape from Slurm\ncontrol\n\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
        },
        {
            "title": "Process Tracking",
            "content": "ProctrackTypeCgroupcgroupcgroup.confLinuxProcPgid\nResource Selection\nDefine resource (node) selection algorithm to be used.\nSelect one value for SelectType:\n\ncons_tres: Allocate individual processors, memory, GPUs, and other\ntrackable resources\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
        },
        {
            "title": "Resource Selection",
            "content": "SelectTypecons_tres\n\nLinear: Node-base\nresource allocation, does not manage individual processor allocation\n\nTask Launch\nDefine a task launch plugin. This may be used to\nprovide resource management within a node (e.g. pinning\ntasks to specific processors).\nSelect one value for TaskPlugin:\n None: No task launch actions\n Affinity:\nCPU affinity support\n(see srun man pages for the --cpu-bind, --mem-bind, and -E options)\n Cgroup:\nAllocated resources constraints enforcement using Linux Control Groups\n(see cgroup.conf man page)\n\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
        },
        {
            "title": "Task Launch",
            "content": "TaskPluginNoneAffinityCgroup\nEvent Logging\nSlurmctld and slurmd daemons can each be configured with different\nlevels of logging verbosity from 0 (quiet) to 7 (extremely verbose).\nEach may also be configured to use debug files. Use fully qualified\npathnames for the files.\n\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
        },
        {
            "title": "Event Logging",
            "content": "\n\nSlurmctldLogFile (if empty, log goes to syslog)\n\n\nSlurmdLogFile (if empty, log goes to syslog. String \"%h\" in name gets\nreplaced with hostname)\n\nJob Accounting Gather\nSlurm accounts for resource use per job.  System specifics can be polled\ndetermined by system type\nSelect one value for JobAcctGatherType:\n None: No\njob accounting\n cgroup:\nSpecific Linux cgroup information gathered, use with Linux systems only\n Linux: Specific\nLinux process table information gathered, use with Linux systems only\nJob Accounting Storage\nUsed with the Job Accounting Gather Slurm can store the accounting information in many different fashions.  Fill in your systems choice here\nSelect one value for AccountingStorageType:\n None:\nNo job accounting storage\n SlurmDBD:\nWrite job accounting to Slurm DBD (database daemon) which can securely\nsave the data from many Slurm managed clusters into a common database\n\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
        },
        {
            "title": "Job Accounting Gather",
            "content": "JobAcctGatherTypeNonecgroupLinuxJob Accounting StorageAccountingStorageTypeNoneSlurmDBD\nProcess ID Logging\nDefine the location into which we can record the daemon's process ID.\nThis is used for locate the appropriate daemon for signaling.\nSpecify a specify the fully qualified pathname for the file.\n\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
        },
        {
            "title": "Process ID Logging",
            "content": "\n\nSlurmctldPidFile\n\n\nSlurmdPidFile\n\n\n\n\n\n"
        }
    ]
}