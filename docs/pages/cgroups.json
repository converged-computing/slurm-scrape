{
    "url": "https://slurm.schedmd.com/cgroups.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Control Group in Slurm",
            "content": "Contents\n\n\nControl Group Overview\nSlurm cgroup plugins design\nUse of cgroup in Slurm\nSlurm Cgroup Configuration Overview\nCurrently Available Cgroup Plugins\n\nproctrack/cgroup plugin\ntask/cgroup plugin\njobacct_gather/cgroup plugin\n\n\nUse of cgroup for Resource Specialization\nSlurm cgroup plugins\n\nMain differences between cgroup/v1 and cgroup/v2\nMain differences between controller interfaces\nOther generalities\n\n\nControl Group Overview\n\nControl Group is a mechanism provided by the kernel to organize processes\nhierarchically and distribute system resources along the hierarchy in a\ncontrolled and configurable manner. Slurm can make use of cgroups to constrain\ndifferent resources to jobs, steps and tasks, and to get accounting about these\nresources.A cgroup provides different controllers (formerly \"subsystems\") for different\nresources. Slurm plugins can use several of these controllers, e.g.: memory,\ncpu, devices, freezer, cpuset, cpuacct. Each enabled controller\ngives the ability to constrain resources to a set of processes. If one\ncontroller is not available on the system, then Slurm cannot constrain the\nassociated resources through a cgroup.\"cgroup\" stands for \"control group\" and is never capitalized. The singular\nform is used to designate the whole feature and also as a qualifier as in\n\"cgroup controllers\". When explicitly referring to multiple individual control\ngroups, the plural form \"cgroups\" is used.Slurm supports two cgroup modes, Legacy mode (cgroup v1) and Unified Mode\n(cgroup v2). Hybrid mode where controllers from both version 1 and version 2 are\nmixed in a system is not supported.See the kernel.org documentation for a more comprehensive description of\ncgroup:\n\nKernel's Cgroup v1 documentation\n\n\nKernel's Cgroup v2 documentation \n\nSlurm cgroup plugins design\n\n\ncgroup/v1 plugin documentation (not available yet)\ncgroup/v2 plugin documentation \nUse of cgroup in Slurm  Slurm provides cgroup versions of a number of plugins.\nproctrack/cgroup (for process tracking and management)\ntask/cgroup (for constraining resources at step and task level)\njobacct_gather/cgroup (for gathering statistics)\ncgroups can also be used for resource specialization (constraining daemons to\ncores or memory).Slurm Cgroup Configuration Overview\n\nThere are several sets of configuration options for Slurm cgroups:\nslurm.conf provides options to enable the\ncgroup plugins. Each plugin may be enabled or disabled independently of the\nothers.\n\ncgroup.conf provides general options that are\ncommon to all cgroup plugins, plus additional options that apply only to\nspecific plugins.\n\nSystem-level resource specialization is enabled using node configuration\nparameters.\n\nCurrently Available Cgroup Plugins\n\nproctrack/cgroup plugin\n\nThe proctrack/cgroup plugin is an alternative to other proctrack plugins such\nas proctrack/linux for process tracking and suspend/resume capability.\n\nproctrack/cgroup uses the freezer controller to keep track of all pids of a\njob. It basically stores the pids in a specific hierarchy in the cgroup tree and\ntakes cares of signaling these pids when instructed. For example, if a user\ndecides to cancel a job, Slurm will execute this order internally by calling the\nproctrack plugin and asking it to send a SIGTERM to the job. Since proctrack\nmaintains a hierarchy of all Slurm-related pids in cgroup, it will easily know\nwhich ones will need to be signaled.\n\nProctrack can also respond to queries for getting a list of all the pids of a\njob or a step.\n\nAlternatively, when using proctrack/linux, pids are stored by cgroup in a\nsingle file (cgroup.procs) which is read by the plugin to get all the pids of a\npart of the hierarchy. For example, when using proctrack/cgroup, a single step\nhas its own cgroup.procs file, so getting the pids of the step is instantaneous.\nIn proctrack/linux, we need to read recursively /proc to get all the descendants\nof a parent pid.\nTo enable this plugin, configure the following option in slurm.conf:\nProctrackType=proctrack/cgroup\nThere are no specific options for this plugin in cgroup.conf, but the general\noptions apply. See the cgroup.conf man page for\ndetails.task/cgroup pluginThe task/cgroup plugin allows constraining resources to a job, a step, or a\ntask. This is the only plugin that can ensure that the boundaries of an\nallocation are not violated.\nOnly jobacctgather/linux offers a very simplistic mechanism for\nconstraining memory to a job but it is not reliable (there's a window of time\nwhere jobs can exceed its limits) and only for very rare systems where cgroup is\nnot available.task/cgroup provides the following features:\nConfine jobs and steps to their allocated cpuset.\nConfine jobs and steps to specific memory resources.\nConfine jobs, steps and tasks to their allocated gres, including gpus.\nThe task/cgroup plugin uses the cpuset, memory and devices subsystems.To enable this plugin, add task/cgroup to the TaskPlugin configuration\nparameter in slurm.conf:TaskPlugin=task/cgroupThere are many specific options for this plugin in cgroup.conf. The general\noptions also apply. See the cgroup.conf man page\nfor details.This plugin can be stacked with other task plugins, for example with\ntask/affinity. This will allow it to constrain resources to a job plus\ngetting the advantage of the affinity plugin (order doesn't matter):TaskPlugin=task/cgroup,task/affinityjobacct_gather/cgroup plugin\n\n\nThe jobacct_gather/cgroup plugin is an alternative to the\njobacct_gather/linux plugin for the collection of accounting statistics\nfor jobs, steps and tasks.\n\njobacct_gather/cgroup uses the cpuacct and memory cgroup controllers.\nThe cpu and memory statistics collected by this plugin do not represent the\nsame resources as the cpu and memory statistics collected by the\njobacct_gather/linux. While the cgroup plugin just reads a cgroup.stats\nfile and similar containing the information for the entire subtree of pids, the\nlinux plugin gets information from /proc/pid/stat for every pid and then does\nthe calculations, thus becoming a bit less efficient (thought not noticeable in\nthe practice) than the cgroup one.To enable this plugin, configure the following option in slurm.conf:\nJobacctGatherType=jobacct_gather/cgroup\nThere are no specific options for this plugin in cgroup.conf, but the general\noptions apply. See the cgroup.conf man page for\ndetails.Use of cgroup for Resource Specialization\n\nResource Specialization may be used to reserve a subset of cores or a\nspecific amount of memory on each compute node for exclusive use by the Slurm\ncompute node daemon, slurmd.Slurmstepd is not constrained by this resource specialization, since it is\nconsidered part of the job and its consumption is completely dependent on the\ntypology of the job. For example an MPI job can initialize many ranks with PMI\nand make slurmstepd consume more memory.System-level resource specialization is enabled with special node\nconfiguration parameters. Read slurm.conf and core\nspecialization in core_spec.html for more\ninformation.Slurm cgroup plugins\n\n\nSince 22.05, Slurm supports cgroup/v1 and cgroup/v2. Both plugins have very\ndifferent ways of organizing their hierarchies and respond to different design\nconstraints. The design is the responsibility of the kernel maintainers.\nMain differences between cgroup/v1 and cgroup/v2\n\nThe three main differences between v1 and v2 are:\nUnified mode in v2\nIn cgroup/v1 there's a separate hierarchy for each controller, which\nmeans the job structure must be replicated and managed for every enabled\ncontroller. For example, for the same job, if using\nmemory and freezer controllers, we will need to create the same\nslurm/uid/job_id/step_id/ hierarchy in both controller's directories. For\nexample:\n\n/sys/fs/cgroup/memory/slurm/uid_1000/job_1/step_0/\n/sys/fs/cgroup/freezer/slurm/uid_1000/job_1/step_0/\nIn cgroup/v2 we have a Unified hierarchy, where controllers are\nenabled at the same level and presented to the user as different files.\n/sys/fs/cgroup/system.slice/slurmstepd.scope/job_1/step_0/\n\nTop-down constraint in v2\nResources are distributed top-down and a cgroup can further distribute a\nresource only if the resource has been distributed to it from the parent.\nEnabled controllers are listed in the cgroup.controllers file and\nenabled controllers in a subtree are listed in cgroup.subtree_control.\n\nNo-Internal-Process constraint in v2\nIn cgroup/v1 the hierarchy is free, which means one can create any\ndirectory in the tree and put pids in it. In cgroup/v2 there's a kernel\nrestriction which impedes adding a pid to non-leaf directories.\n\nSystemd dependency on cgroup/v2 - separation of slurmd and stepds\n This is not a kernel limitation but a systemd decision, which imposes an\nimportant restriction on services that decide to use Delegate=yes.\nSystemd, with pid 1, decided to be the complete owner of the cgroup\nhierarchy, /sys/fs/cgroup, trying to impose a single-writer\ndesign. This means that everything related to cgroup must be under control of\nsystemd. If one decides to manually modify the cgroup tree, creating directories\nand moving pids around, it is possible that at some point systemd may decide to\nenable or disable controllers on the entire tree, or move pids around. It's been\nexperienced that a\n\nsystemd reload\n\nor a\n\nsystemd reset-failed\n\nremoved controllers, at any level and directory of the tree, if there was not\nany \"systemd unit\" making use of it and there were not any \"Delegate=Yes\"\nstarted \"systemd unit\" on the system. This is because systemd wants to cleanup\nthe cgroup tree and match it against its internal unit database. In fact,\nlooking at the code of systemd one can see how cgroup directories related to\nunits with \"Delegate=yes\" flag are ignored, while any other cgroup directories\nare modified.  This makes it mandatory to start slurmd and slurmstepd processes\nunder a unit with \"Delegate=yes\". This means we need to start, stop and restart\nslurmd with systemd. If we do that though, since we may have previously modified\nthe tree where slurmd belongs (e.g. adding job directories) systemd will not be\nable to restart slurmd because of the Top-down constraint mentioned\nearlier. It will not be able to put the new slurmd pid into the root cgroup\nwhich is now a non-leaf. This forces us to separate the cgroup hierarchies of\nslurmstepd from the slurmd ones, and since we need to inform systemd about it\nand put slurmstepd into a new unit, we will do a dbus call to systemd to create\na new scope for slurmstepds. See\n\nsystemd ControlGroupInterface for more information.\n\nThe following differences shouldn't affect how other plugins interact with\ncgroup plugins, but instead they only show internal functional differences.\nA controller in cgroup/v2 is enabled by writing to\ncgroup.controllers, while in cgroup/v1 a new mount point must be\nmounted with filesystem type \"-t cgroup\" and corresponding options,\ne.g.\"-o freezer\".\n\nIn cgroup/v2 the freezer controller is inherently present in the\ncgroup.freeze interface. In cgroup/v1 it is a specific and\nseparate controller which needs to be mounted.\n\nThe devices controller does not exist in cgroup/v2, instead a new eBPF\nprogram must be inserted in the kernel.\n\nIn cgroup/v2, memory.stat file has changed and now we do the sum of\nanon+swapcached+anon_thp to match the RSS concept in v1.\n\nIn cgroup/v2, cpu.stat provides metrics in milis while puacct.stat\nin cgroup/v1 provides metrics in USER_HZ.\n\nMain differences between controller interfaces\n\n\n\ncgroup/v1\ncgroup/v2\n\n\nmemory.limit_in_bytes\nmemory.max\n\n\nmemory.soft_limit_in_bytes\nmemory.high\n\n\nmemory.memsw_limit_in_bytes\nmemory.swap.max\n\n\nmemory.swappiness\nnone\n\n\nfreezer.state\ncgroup.freeze\n\n\ncpuset.cpus\ncpuset.cpus.effective and cpuset.cpus\n\n\ncpuset.mems\ncpuset.mems.effective and cpuset.mems\n\n\ncpuacct.stat\ncpu.stat\n\n\ndevice.*\nebpf program\n\nOther generalities\n\n\nWhen using cgroup/v1, some configurations can exclude the swap cgroup\naccounting. This accounting is part of the features provided by the memory\ncontroller.  If this feature is disabled from the kernel or boot parameters,\ntrying to enable swap constraints will produce an error. If this is required,\nadd the following parameters to the kernel command line:\n\ncgroup_enable=memory swapaccount=1\n\nThis can usually be placed in /etc/default/grub inside\nthe GRUB_CMDLINE_LINUX variable. A command such as update-grub\nmust be run after updating the file. This feature can be disabled also at kernel\nconfig with the parameter:\n\nCONFIG_MEMCG_SWAP=\nIn some Linux distributions, it was possible to use the systemd parameter\nJoinControllers, which is actually deprecated. This parameter allowed multiple\ncontrollers to be mounted in a single hierarchy in cgroup/v1, more or\nless trying to emulate the behavior of cgroup/v2 in \"Unified\" mode.\nHowever, Slurm does not work correctly with this configuration, so please make\nsure your system.conf does not use JoinControllers and that all your cgroup\ncontrollers are under separate directories when using\ncgroup/v1 legacy mode.\n\nLast modified 11 October 2024"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        }
    ]
}