{
    "url": "https://slurm.schedmd.com/gres.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Generic Resource (GRES) Scheduling",
            "content": "Contents\nOverview\nConfiguration\nRunning Jobs\nAutoDetect\nGPU Management\nMPS Management\nMIG Management\nSharding\nOverviewSlurm supports the ability to define and schedule arbitrary Generic RESources\n(GRES). Additional built-in features are enabled for specific GRES types,\nincluding Graphics Processing Units (GPUs), CUDA Multi-Process Service (MPS)\ndevices, and Sharding through an extensible plugin mechanism.Configuration\n\nBy default, no GRES are enabled in the cluster's configuration.\nYou must explicitly specify which GRES are to be managed in the\nslurm.conf configuration file. The configuration parameters of\ninterest are GresTypes and Gres.\n\nFor more details, see GresTypes and Gres in the slurm.conf man page.\nNote that the GRES specification for each node works in the same fashion\nas the other resources managed. Nodes which are found to have fewer resources\nthan configured will be placed in a DRAIN state.Snippet from an example slurm.conf file:\n# Configure four GPUs (with MPS), plus bandwidth\nGresTypes=gpu,mps,bandwidth\nNodeName=tux[0-7] Gres=gpu:tesla:2,gpu:kepler:2,mps:400,bandwidth:lustre:no_consume:4G\nEach compute node with generic resources typically contain a gres.conf\nfile describing which resources are available on the node, their count,\nassociated device files and cores which should be used with those resources.There are cases where you may want to define a Generic Resource on a node\nwithout specifying a quantity of that GRES. For example, the filesystem type\nof a node doesn't decrease in value as jobs run on that node.\nYou can use the no_consume flag to allow users to request a GRES\nwithout having a defined count that gets used as it is requested.\nTo view available gres.conf configuration parameters, see the\ngres.conf man page.Running Jobs\n\nJobs will not be allocated any generic resources unless specifically\nrequested at job submit time using the options:\n--gres\nGeneric resources required per node\n--gpus\nGPUs required per job\n--gpus-per-node\nGPUs required per node. Equivalent to the --gres option for GPUs.\n--gpus-per-socket\nGPUs required per socket. Requires the job to specify a task socket.\n--gpus-per-task\nGPUs required per task. Requires the job to specify a task count.\nAll of these options are supported by the salloc, sbatch and\nsrun commands.\nNote that all of the --gpu* options are only supported by Slurm's\nselect/cons_tres plugin.\nJobs requesting these options when the select/cons_tres plugin is not\nconfigured will be rejected.\nThe --gres option requires an argument specifying which generic resources\nare required and how many resources using the form name[:type:count]\nwhile all of the --gpu* options require an argument of the form\n [type]:count.\nThe name is the same name as\nspecified by the GresTypes and Gres configuration parameters.\ntype identifies a specific type of that generic resource (e.g. a\nspecific model of GPU).\ncount specifies how many resources are required and has a default\nvalue of 1. For example:\nsbatch --gres=gpu:kepler:2 ....Requests for typed vs non-typed generic resources must be consistent\nwithin a job. For example, if you request --gres=gpu:2 with\nsbatch, you would not be able to request --gres=gpu:tesla:2\nwith srun to create a job step. The same holds true in reverse,\nif you request a typed GPU to create a job allocation, you should request\na GPU of the same type to create a job step.Several additional resource requirement specifications are available\nspecifically for GPUs and detailed descriptions about these options are\navailable in the man pages for the job submission commands.\nAs for the --gpu* option, these options are only supported by Slurm's\nselect/cons_tres plugin.\n--cpus-per-gpu\nCount of CPUs allocated per GPU.\n--gpu-bind\nDefine how tasks are bound to GPUs.\n--gpu-freq\nSpecify GPU frequency and/or GPU memory frequency.\n--mem-per-gpu\nMemory allocated per GPU.\nJobs will be allocated specific generic resources as needed to satisfy\nthe request. If the job is suspended, those resources do not become available\nfor use by other jobs.Job steps can be allocated generic resources from those allocated to the\njob using the --gres option with the srun command as described\nabove. By default, a job step will be allocated all of the generic resources\nthat have been requested by the job, except those implicitly requested when a\njob is exclusive. If desired, the job step may explicitly specify a\ndifferent generic resource count than the job.\nThis design choice was based upon a scenario where each job executes many\njob steps. If job steps were granted access to all generic resources by\ndefault, some job steps would need to explicitly specify zero generic resource\ncounts, which we considered more confusing. The job step can be allocated\nspecific generic resources and those resources will not be available to other\njob steps. A simple example is shown below.\n#!/bin/bash\n#\n# gres_test.bash\n# Submit as follows:\n# sbatch --gres=gpu:4 -n4 -N1-1 gres_test.bash\n#\nsrun --gres=gpu:2 -n2 --exclusive show_device.sh &\nsrun --gres=gpu:1 -n1 --exclusive show_device.sh &\nsrun --gres=gpu:1 -n1 --exclusive show_device.sh &\nwait\nAutoDetect\n\nIf AutoDetect=nvml, AutoDetect=rsmi, AutoDetect=nrt,\nor AutoDetect=oneapi are set in gres.conf, configuration details\nwill automatically be filled in for any system-detected GPU. This removes the\nneed to explicitly configure GPUs in gres.conf, though the Gres= line in\nslurm.conf is still required in order to tell slurmctld how many GRES to expect.\nNote that AutoDetect=nvml, AutoDetect=rsmi, and\nAutoDetect=oneapi need their corresponding GPU management libraries\ninstalled on the node and found during Slurm configuration in order to work.\nBy default, all system-detected devices are added to the node.\nHowever, if Type and File in gres.conf match a GPU on\nthe system, any other properties explicitly specified (e.g.\nCores or Links) can be double-checked against it.\nIf the system-detected GPU differs from its matching GPU configuration, then the\nGPU is omitted from the node with an error.\nThis allows gres.conf to serve as an optional sanity check and notifies\nadministrators of any unexpected changes in GPU properties.\nIf not all system-detected devices are specified by the slurm.conf\nconfiguration, then the relevant slurmd will be drained. However, it is still\npossible to use a subset of the devices found on the system if they are\nspecified manually (with AutoDetect disabled) in gres.conf.\nExample gres.conf file:\n# Configure four GPUs (with MPS), plus bandwidth\nAutoDetect=nvml\nName=gpu Type=gp100  File=/dev/nvidia0 Cores=0,1\nName=gpu Type=gp100  File=/dev/nvidia1 Cores=0,1\nName=gpu Type=p6000  File=/dev/nvidia2 Cores=2,3\nName=gpu Type=p6000  File=/dev/nvidia3 Cores=2,3\nName=mps Count=200  File=/dev/nvidia0\nName=mps Count=200  File=/dev/nvidia1\nName=mps Count=100  File=/dev/nvidia2\nName=mps Count=100  File=/dev/nvidia3\nName=bandwidth Type=lustre Count=4G Flags=CountOnly\n In this example, since AutoDetect=nvml is specified, Cores\nfor each GPU will be checked against a corresponding GPU found on the system\nmatching the Type and File specified.\nSince Links is not specified, it will be automatically filled in\naccording to what is found on the system.\nIf a matching system GPU is not found, no validation takes place and the GPU is\nassumed to be as the configuration says.\nFor Type to match a system-detected device, it must either exactly\nmatch or be a substring of the GPU name reported by slurmd via the AutoDetect\nmechanism. This GPU name will have all spaces replaced with underscores. To see\nthe GPU name, set SlurmdDebug=debug2 in your slurm.conf and either\nrestart or reconfigure slurmd and check the slurmd log. For example,\nwith AutoDetect=nvml:\ndebug:  gpu/nvml: init: init: GPU NVML plugin loaded\ndebug2: gpu/nvml: _nvml_init: Successfully initialized NVML\ndebug:  gpu/nvml: _get_system_gpu_list_nvml: Systems Graphics Driver Version: 450.36.06\ndebug:  gpu/nvml: _get_system_gpu_list_nvml: NVML Library Version: 11.450.36.06\ndebug2: gpu/nvml: _get_system_gpu_list_nvml: Total CPU count: 6\ndebug2: gpu/nvml: _get_system_gpu_list_nvml: Device count: 1\ndebug2: gpu/nvml: _get_system_gpu_list_nvml: GPU index 0:\ndebug2: gpu/nvml: _get_system_gpu_list_nvml:     Name: geforce_rtx_2060\ndebug2: gpu/nvml: _get_system_gpu_list_nvml:     UUID: GPU-g44ef22a-d954-c552-b5c4-7371354534b2\ndebug2: gpu/nvml: _get_system_gpu_list_nvml:     PCI Domain/Bus/Device: 0:1:0\ndebug2: gpu/nvml: _get_system_gpu_list_nvml:     PCI Bus ID: 00000000:01:00.0\ndebug2: gpu/nvml: _get_system_gpu_list_nvml:     NVLinks: -1\ndebug2: gpu/nvml: _get_system_gpu_list_nvml:     Device File (minor number): /dev/nvidia0\ndebug2: gpu/nvml: _get_system_gpu_list_nvml:     CPU Affinity Range - Machine: 0-5\ndebug2: gpu/nvml: _get_system_gpu_list_nvml:     Core Affinity Range - Abstract: 0-5\nIn this example, the GPU's name is reported as\ngeforce_rtx_2060. So in your slurm.conf and\ngres.conf, the GPU Type can be set to \ngeforce, rtx, \n2060, geforce_rtx_2060, or any other\nsubstring, and slurmd should be able to match it to the system-detected\ndevice geforce_rtx_2060.\n\nGPU Management\n\n\nIn the case of Slurm's GRES plugin for GPUs, the environment variable\nCUDA_VISIBLE_DEVICES\nis set for each job step to determine which GPUs are\navailable for its use on each node. This environment variable is only set\nwhen tasks are launched on a specific compute node (no global environment\nvariable is set for the salloc command and the environment variable set\nfor the sbatch command only reflects the GPUs allocated to that job\non that node, node zero of the allocation).\nCUDA version 3.1 (or higher) uses this environment\nvariable in order to run multiple jobs or job steps on a node with GPUs\nand ensure that the resources assigned to each are unique. In the example\nabove, the allocated node may have four or more graphics devices. In that\ncase, CUDA_VISIBLE_DEVICES\nwill reference unique devices for each file and\nthe output might resemble this:\n\nJobStep=1234.0 CUDA_VISIBLE_DEVICES=0,1\nJobStep=1234.1 CUDA_VISIBLE_DEVICES=2\nJobStep=1234.2 CUDA_VISIBLE_DEVICES=3\n\nNOTE: Be sure to specify the File parameters in the\ngres.conf file and ensure they are in the increasing numeric order.\nThe CUDA_VISIBLE_DEVICES\nenvironment variable will also be set in the job's Prolog and Epilog programs.\nNote that the environment variable set for the job may differ from that set for\nthe Prolog and Epilog if Slurm is configured to constrain the device files\nvisible to a job using Linux cgroup.\nThis is because the Prolog and Epilog programs run outside of any Linux\ncgroup while the job runs inside of the cgroup and may thus have a\ndifferent set of visible devices.\nFor example, if a job is allocated the device \"/dev/nvidia1\", then\nCUDA_VISIBLE_DEVICES will be set to a value of\n\"1\" in the Prolog and Epilog while the job's value of\nCUDA_VISIBLE_DEVICES will be set to a\nvalue of \"0\" (i.e. the first GPU device visible to the job).\nFor more information see the\nProlog and Epilog Guide.\nWhen possible, Slurm automatically determines the GPUs on the system using\nNVML. NVML (which powers the\nnvidia-smi tool) numbers GPUs in order by their\nPCI bus IDs. For this numbering to match the numbering reported by CUDA, the\nCUDA_DEVICE_ORDER environmental variable must\nbe set to CUDA_DEVICE_ORDER=PCI_BUS_ID.\nGPU device files (e.g. /dev/nvidia1) are\nbased on the Linux minor number assignment, while NVML's device numbers are\nassigned via PCI bus ID, from lowest to highest. Mapping between these two is\nnondeterministic and system dependent, and could vary between boots after\nhardware or OS changes. For the most part, this assignment seems fairly stable.\nHowever, an after-bootup check is required to guarantee that a GPU device is\nassigned to a specific device file.\nPlease consult the\n\nNVIDIA CUDA documentation for more information about the\nCUDA_VISIBLE_DEVICES and\nCUDA_DEVICE_ORDER environmental variables.\nMPS Management\n\n\n\nCUDA Multi-Process Service (MPS) provides a mechanism where GPUs can be\nshared by multiple jobs, where each job is allocated some percentage of the\nGPU's resources.\nThe total count of MPS resources available on a node should be configured in\nthe slurm.conf file (e.g. \"NodeName=tux[1-16] Gres=gpu:2,mps:200\").\nSeveral options are available for configuring MPS in the gres.conf file\nas listed below with examples following that:\n\nNo MPS configuration: The count of gres/mps elements defined in the\nslurm.conf will be evenly distributed across all GPUs configured on the\nnode. For example, \"NodeName=tux[1-16] Gres=gpu:2,mps:200\" will configure\na count of 100 gres/mps resources on each of the two GPUs.\nMPS configuration includes only the Name and Count parameters:\nThe count of gres/mps elements will be evenly distributed across all GPUs\nconfigured on the node. This is similar to case 1, but places duplicate\nconfiguration in the gres.conf file.\nMPS configuration includes the Name, File and Count\nparameters: Each File parameter should identify the device file path of a\nGPU and the Count should identify the number of gres/mps resources\navailable for that specific GPU device.\nThis may be useful in a heterogeneous environment.\nFor example, some GPUs on a node may be more powerful than others and thus be\nassociated with a higher gres/mps count.\nAnother use case would be to prevent some GPUs from being used for MPS (i.e.\nthey would have an MPS count of zero).\n\nNote that Type and Cores parameters for gres/mps are ignored.\nThat information is copied from the gres/gpu configuration.\nNote the Count parameter is translated to a percentage, so the value\nwould typically be a multiple of 100.\nNote that if NVIDIA's NVML library is installed, the GPU configuration\n(i.e. Type, File, Cores and Links data) will be\nautomatically gathered from the library and need not be recorded in the\ngres.conf file.\nBy default, job requests for MPS are required to fit on a single gpu on\neach node. This can be overridden with a flag in the slurm.conf\nconfiguration file. See \nOPT_MULTIPLE_SHARING_GRES_PJ.\nNote the same GPU can be allocated either as a GPU type of GRES or as\nan MPS type of GRES, but not both.\nIn other words, once a GPU has been allocated as a gres/gpu resource it will\nnot be available as a gres/mps.\nLikewise, once a GPU has been allocated as a gres/mps resource it will\nnot be available as a gres/gpu.\nHowever the same GPU can be allocated as MPS generic resources to multiple jobs\nbelonging to multiple users, so long as the total count of MPS allocated to\njobs does not exceed the configured count.\nAlso, since shared GRES (MPS) cannot be allocated at the same time as a sharing\nGRES (GPU) this option only allocates all sharing GRES and no underlying shared\nGRES.\nSome example configurations for Slurm's gres.conf file are shown below.\n\n# Example 1 of gres.conf\n# Configure four GPUs (with MPS)\nAutoDetect=nvml\nName=gpu Type=gp100 File=/dev/nvidia0 Cores=0,1\nName=gpu Type=gp100 File=/dev/nvidia1 Cores=0,1\nName=gpu Type=p6000 File=/dev/nvidia2 Cores=2,3\nName=gpu Type=p6000 File=/dev/nvidia3 Cores=2,3\n# Set gres/mps Count value to 100 on each of the 4 available GPUs\nName=mps Count=400\n\n\n\n# Example 2 of gres.conf\n# Configure four different GPU types (with MPS)\nAutoDetect=nvml\nName=gpu Type=gtx1080 File=/dev/nvidia0 Cores=0,1\nName=gpu Type=gtx1070 File=/dev/nvidia1 Cores=0,1\nName=gpu Type=gtx1060 File=/dev/nvidia2 Cores=2,3\nName=gpu Type=gtx1050 File=/dev/nvidia3 Cores=2,3\nName=mps Count=1300   File=/dev/nvidia0\nName=mps Count=1200   File=/dev/nvidia1\nName=mps Count=1100   File=/dev/nvidia2\nName=mps Count=1000   File=/dev/nvidia3\n\nNOTE: gres/mps requires the use of the select/cons_tres\nplugin.\nJob requests for MPS will be processed the same as any other GRES except\nthat the request must be satisfied using only one GPU per node and only one\nGPU per node may be configured for use with MPS.\nFor example, a job request for \"--gres=mps:50\" will not be satisfied by using\n20 percent of one GPU and 30 percent of a second GPU on a single node.\nMultiple jobs from different users can use MPS on a node at the same time.\nNote that GRES types of GPU and MPS can not be requested within\na single job.\nAlso jobs requesting MPS resources can not specify a GPU frequency.\nA prolog program should be used to start and stop MPS servers as needed.\nA sample prolog script to do this is included with the Slurm distribution in\nthe location etc/prolog.example.\nIts mode of operation is if a job is allocated gres/mps resources then the\nProlog will have the CUDA_VISIBLE_DEVICES,\nCUDA_MPS_ACTIVE_THREAD_PERCENTAGE, and\nSLURM_JOB_UID environment variables set.\nThe Prolog should then make sure that an MPS server is started for that GPU\nand user (UID == User ID).\nIt also records the GPU device ID in a local file.\nIf a job is allocated gres/gpu resources then the Prolog will have the\nCUDA_VISIBLE_DEVICES and\nSLURM_JOB_UID environment variables set\n(no CUDA_MPS_ACTIVE_THREAD_PERCENTAGE).\nThe Prolog should then terminate any MPS server associated with that GPU.\nIt may be necessary to modify this script as needed for the local environment.\nFor more information see the\nProlog and Epilog Guide.\nJobs requesting MPS resources will have the\nCUDA_VISIBLE_DEVICES\nand CUDA_DEVICE_ORDER environment variables set.\nThe device ID is relative to those resources under MPS server control and will\nalways have a value of zero in the current implementation (only one GPU will be\nusable in MPS mode per node).\nThe job will also have the\nCUDA_MPS_ACTIVE_THREAD_PERCENTAGE\nenvironment variable set to that job's percentage of MPS resources available on\nthe assigned GPU.\nThe percentage will be calculated based upon the portion of the configured\nCount on the Gres is allocated to a job of step.\nFor example, a job requesting \"--gres=mps:200\" and using\nconfiguration example 2 above would be\nallocated\n15% of the gtx1080 (File=/dev/nvidia0, 200 x 100 / 1300 = 15), or\n16% of the gtx1070 (File=/dev/nvidia0, 200 x 100 / 1200 = 16), or\n18% of the gtx1060 (File=/dev/nvidia0, 200 x 100 / 1100 = 18), or\n20% of the gtx1050 (File=/dev/nvidia0, 200 x 100 / 1000 = 20).\nAn alternate mode of operation would be to permit jobs to be allocated whole\nGPUs then trigger the starting of an MPS server based upon comments in the job.\nFor example, if a job is allocated whole GPUs then search for a comment of\n\"mps-per-gpu\" or \"mps-per-node\" in the job (using the \"scontrol show job\"\ncommand) and use that as a basis for starting one MPS daemon per GPU or across\nall GPUs respectively.\nPlease consult the\n\nNVIDIA Multi-Process Service documentation for more information about MPS.\n\nNote that a vulnerability exists in previous versions of the NVIDIA driver that\nmay affect users when sharing GPUs. More information can be found in\n\nCVE-2018-6260 and in the\n\nSecurity Bulletin: NVIDIA GPU Display Driver - February 2019.\nNVIDIA MPS has a built-in limitation regarding GPU sharing among different\nusers. Only one user on a system may have an active MPS server, and the MPS\ncontrol daemon will queue MPS server activation requests from separate users,\nleading to serialized exclusive access of the GPU between users (see\n\nSection 2.3.1.1 - Limitations in the MPS docs). So different users cannot\ntruly run concurrently on the GPU with MPS; rather, the GPU will be time-sliced\nbetween the users (for a diagram depicting this process, see\n\nSection 3.3 - Provisioning Sequence in the MPS docs).\nMIG Management\n\n\nBeginning in version 21.08, Slurm now supports NVIDIA\nMulti-Instance GPU (MIG) devices. This feature allows some newer NVIDIA\nGPUs (like the A100) to split up a GPU into up to seven separate, isolated GPU\ninstances. Slurm can treat these MIG instances as individual GPUs, complete with\ncgroup isolation and task binding.\nTo configure MIGs in Slurm, specify\nAutoDetect=nvml in gres.conf for the\nnodes with MIGs, and specify Gres\nin slurm.conf as if the MIGs were regular GPUs, like this:\nNodeName=tux[1-16] gres=gpu:2. An optional\nGRES type can be specified to distinguish MIGs of different sizes from each\nother, as well as from other GPUs in the cluster. This type must be a substring\nof the \"MIG Profile\" string as reported by the node in its slurmd log under the\ndebug2 log level. Here is an example slurm.conf\nfor a system with 2 gpus, one of which is partitioned into 2 MIGs where the\n\"MIG Profile\" is nvidia_a100_3g.20gb:\n\nAccountingStorageTRES=gres/gpu,gres/gpu:a100,gres/gpu:a100_3g.20gb\nGresTypes=gpu\nNodeName=tux[1-16] gres=gpu:a100:1,gpu:a100_3g.20gb:2\n\nThe MultipleFiles parameter\nallows you to specify multiple device files for the GPU card.\nThe sanity-check AutoDetect mode is not supported for MIGs.\nSlurm expects MIG devices to already be partitioned, and does not support\ndynamic MIG partitioning.\nFor more information on NVIDIA MIGs (including how to partition them), see\n\nthe MIG user guide.\nSharding\n\n\n\nSharding provides a generic mechanism where GPUs can be\nshared by multiple jobs. While it does permit multiple jobs to run on a given\nGPU it does not fence the processes running on the GPU, it only allows the GPU\nto be shared. Sharding, therefore, works best with homogeneous workflows. It is\nrecommended to limit the number of shards on a node to equal the max possible\njobs that can run simultaneously on the node (i.e. cores).\nThe total count of shards available on a node should be configured in\nthe slurm.conf file (e.g. \"NodeName=tux[1-16] Gres=gpu:2,shard:64\").\nSeveral options are available for configuring shards in the gres.conf file\nas listed below with examples following that:\n\nNo Shard configuration: The count of gres/shard elements defined in the\nslurm.conf will be evenly distributed across all GPUs configured on the\nnode. For example, \"NodeName=tux[1-16] Gres=gpu:2,shard:64\" will configure\na count of 32 gres/shard resources on each of the two GPUs.\nShard configuration includes only the Name and Count parameters:\nThe count of gres/shard elements will be evenly distributed across all GPUs\nconfigured on the node. This is similar to case 1, but places duplicate\nconfiguration in the gres.conf file.\nShard configuration includes the Name, File and Count\nparameters: Each File parameter should identify the device file path of a\nGPU and the Count should identify the number of gres/shard resources\navailable for that specific GPU device.\nThis may be useful in a heterogeneous environment.\nFor example, some GPUs on a node may be more powerful than others and thus be\nassociated with a higher gres/shard count.\nAnother use case would be to prevent some GPUs from being used for sharding (i.e.\nthey would have a shard count of zero).\n\nNote that Type and Cores parameters for gres/shard are ignored.\nThat information is copied from the gres/gpu configuration.\nNote that if NVIDIA's NVML library is installed, the GPU configuration\n(i.e. Type, File, Cores and Links data) will be\nautomatically gathered from the library and need not be recorded in the\ngres.conf file.\nNote the same GPU can be allocated either as a GPU type of GRES or as\na shard type of GRES, but not both.\nIn other words, once a GPU has been allocated as a gres/gpu resource it will\nnot be available as a gres/shard.\nLikewise, once a GPU has been allocated as a gres/shard resource it will\nnot be available as a gres/gpu.\nHowever the same GPU can be allocated as shard generic resources to multiple jobs\nbelonging to multiple users, so long as the total count of SHARD allocated to\njobs does not exceed the configured count.\nBy default, job requests for shards are required to fit on a single gpu on\neach node. This can be overridden with a flag in the slurm.conf\nconfiguration file. See \nOPT_MULTIPLE_SHARING_GRES_PJ.\nIn order for this to be correctly configured, the appropriate nodes need\nto have the shard keyword added as a GRES for the relevant nodes as\nwell as being added to the GresTypes parameter. If you want the shards\nto be tracked in accounting then shard also needs to be added to\nAccountingStorageTRES.\nSee the relevant settings in an example slurm.conf:\n\nAccountingStorageTRES=gres/gpu,gres/shard\nGresTypes=gpu,shard\nNodeName=tux[1-16] Gres=gpu:2,shard:64\n\nSome example configurations for Slurm's gres.conf file are shown below.\n\n# Example 1 of gres.conf\n# Configure four GPUs (with Sharding)\nAutoDetect=nvml\nName=gpu Type=gp100 File=/dev/nvidia0 Cores=0,1\nName=gpu Type=gp100 File=/dev/nvidia1 Cores=0,1\nName=gpu Type=p6000 File=/dev/nvidia2 Cores=2,3\nName=gpu Type=p6000 File=/dev/nvidia3 Cores=2,3\n# Set gres/shard Count value to 8 on each of the 4 available GPUs\nName=shard Count=32\n\n\n\n# Example 2 of gres.conf\n# Configure four different GPU types (with Sharding)\nAutoDetect=nvml\nName=gpu Type=gtx1080 File=/dev/nvidia0 Cores=0,1\nName=gpu Type=gtx1070 File=/dev/nvidia1 Cores=0,1\nName=gpu Type=gtx1060 File=/dev/nvidia2 Cores=2,3\nName=gpu Type=gtx1050 File=/dev/nvidia3 Cores=2,3\nName=shard Count=8    File=/dev/nvidia0\nName=shard Count=8    File=/dev/nvidia1\nName=shard Count=8    File=/dev/nvidia2\nName=shard Count=8    File=/dev/nvidia3\n\nNOTE: gres/shard requires the use of the select/cons_tres\nplugin.\nJob requests for shards can not specify a GPU frequency.\nJobs requesting shards resources will have the\nCUDA_VISIBLE_DEVICES, ROCR_VISIBLE_DEVICES,\nor GPU_DEVICE_ORDINAL environment variable set\nwhich would be the same as if it were a GPU.\n\nSteps with shards haveSLURM_SHARDS_ON_NODE\nset indicating the number of shards allocated.\nLast modified 16 February 2024\n"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        },
        {
            "title": "GPU Management\n\n",
            "content": "In the case of Slurm's GRES plugin for GPUs, the environment variable\nCUDA_VISIBLE_DEVICES\nis set for each job step to determine which GPUs are\navailable for its use on each node. This environment variable is only set\nwhen tasks are launched on a specific compute node (no global environment\nvariable is set for the salloc command and the environment variable set\nfor the sbatch command only reflects the GPUs allocated to that job\non that node, node zero of the allocation).\nCUDA version 3.1 (or higher) uses this environment\nvariable in order to run multiple jobs or job steps on a node with GPUs\nand ensure that the resources assigned to each are unique. In the example\nabove, the allocated node may have four or more graphics devices. In that\ncase, CUDA_VISIBLE_DEVICES\nwill reference unique devices for each file and\nthe output might resemble this:\nJobStep=1234.0 CUDA_VISIBLE_DEVICES=0,1\nJobStep=1234.1 CUDA_VISIBLE_DEVICES=2\nJobStep=1234.2 CUDA_VISIBLE_DEVICES=3\nNOTE: Be sure to specify the File parameters in the\ngres.conf file and ensure they are in the increasing numeric order.The CUDA_VISIBLE_DEVICES\nenvironment variable will also be set in the job's Prolog and Epilog programs.\nNote that the environment variable set for the job may differ from that set for\nthe Prolog and Epilog if Slurm is configured to constrain the device files\nvisible to a job using Linux cgroup.\nThis is because the Prolog and Epilog programs run outside of any Linux\ncgroup while the job runs inside of the cgroup and may thus have a\ndifferent set of visible devices.\nFor example, if a job is allocated the device \"/dev/nvidia1\", then\nCUDA_VISIBLE_DEVICES will be set to a value of\n\"1\" in the Prolog and Epilog while the job's value of\nCUDA_VISIBLE_DEVICES will be set to a\nvalue of \"0\" (i.e. the first GPU device visible to the job).\nFor more information see the\nProlog and Epilog Guide.When possible, Slurm automatically determines the GPUs on the system using\nNVML. NVML (which powers the\nnvidia-smi tool) numbers GPUs in order by their\nPCI bus IDs. For this numbering to match the numbering reported by CUDA, the\nCUDA_DEVICE_ORDER environmental variable must\nbe set to CUDA_DEVICE_ORDER=PCI_BUS_ID.GPU device files (e.g. /dev/nvidia1) are\nbased on the Linux minor number assignment, while NVML's device numbers are\nassigned via PCI bus ID, from lowest to highest. Mapping between these two is\nnondeterministic and system dependent, and could vary between boots after\nhardware or OS changes. For the most part, this assignment seems fairly stable.\nHowever, an after-bootup check is required to guarantee that a GPU device is\nassigned to a specific device file.Please consult the\n\nNVIDIA CUDA documentation for more information about the\nCUDA_VISIBLE_DEVICES and\nCUDA_DEVICE_ORDER environmental variables.MPS Management\n\n\nCUDA Multi-Process Service (MPS) provides a mechanism where GPUs can be\nshared by multiple jobs, where each job is allocated some percentage of the\nGPU's resources.\nThe total count of MPS resources available on a node should be configured in\nthe slurm.conf file (e.g. \"NodeName=tux[1-16] Gres=gpu:2,mps:200\").\nSeveral options are available for configuring MPS in the gres.conf file\nas listed below with examples following that:\nNo MPS configuration: The count of gres/mps elements defined in the\nslurm.conf will be evenly distributed across all GPUs configured on the\nnode. For example, \"NodeName=tux[1-16] Gres=gpu:2,mps:200\" will configure\na count of 100 gres/mps resources on each of the two GPUs.\nMPS configuration includes only the Name and Count parameters:\nThe count of gres/mps elements will be evenly distributed across all GPUs\nconfigured on the node. This is similar to case 1, but places duplicate\nconfiguration in the gres.conf file.\nMPS configuration includes the Name, File and Count\nparameters: Each File parameter should identify the device file path of a\nGPU and the Count should identify the number of gres/mps resources\navailable for that specific GPU device.\nThis may be useful in a heterogeneous environment.\nFor example, some GPUs on a node may be more powerful than others and thus be\nassociated with a higher gres/mps count.\nAnother use case would be to prevent some GPUs from being used for MPS (i.e.\nthey would have an MPS count of zero).\nNote that Type and Cores parameters for gres/mps are ignored.\nThat information is copied from the gres/gpu configuration.Note the Count parameter is translated to a percentage, so the value\nwould typically be a multiple of 100.Note that if NVIDIA's NVML library is installed, the GPU configuration\n(i.e. Type, File, Cores and Links data) will be\nautomatically gathered from the library and need not be recorded in the\ngres.conf file.By default, job requests for MPS are required to fit on a single gpu on\neach node. This can be overridden with a flag in the slurm.conf\nconfiguration file. See \nOPT_MULTIPLE_SHARING_GRES_PJ.Note the same GPU can be allocated either as a GPU type of GRES or as\nan MPS type of GRES, but not both.\nIn other words, once a GPU has been allocated as a gres/gpu resource it will\nnot be available as a gres/mps.\nLikewise, once a GPU has been allocated as a gres/mps resource it will\nnot be available as a gres/gpu.\nHowever the same GPU can be allocated as MPS generic resources to multiple jobs\nbelonging to multiple users, so long as the total count of MPS allocated to\njobs does not exceed the configured count.\nAlso, since shared GRES (MPS) cannot be allocated at the same time as a sharing\nGRES (GPU) this option only allocates all sharing GRES and no underlying shared\nGRES.\nSome example configurations for Slurm's gres.conf file are shown below.\n# Example 1 of gres.conf\n# Configure four GPUs (with MPS)\nAutoDetect=nvml\nName=gpu Type=gp100 File=/dev/nvidia0 Cores=0,1\nName=gpu Type=gp100 File=/dev/nvidia1 Cores=0,1\nName=gpu Type=p6000 File=/dev/nvidia2 Cores=2,3\nName=gpu Type=p6000 File=/dev/nvidia3 Cores=2,3\n# Set gres/mps Count value to 100 on each of the 4 available GPUs\nName=mps Count=400\n\n# Example 2 of gres.conf\n# Configure four different GPU types (with MPS)\nAutoDetect=nvml\nName=gpu Type=gtx1080 File=/dev/nvidia0 Cores=0,1\nName=gpu Type=gtx1070 File=/dev/nvidia1 Cores=0,1\nName=gpu Type=gtx1060 File=/dev/nvidia2 Cores=2,3\nName=gpu Type=gtx1050 File=/dev/nvidia3 Cores=2,3\nName=mps Count=1300   File=/dev/nvidia0\nName=mps Count=1200   File=/dev/nvidia1\nName=mps Count=1100   File=/dev/nvidia2\nName=mps Count=1000   File=/dev/nvidia3\nNOTE: gres/mps requires the use of the select/cons_tres\nplugin.Job requests for MPS will be processed the same as any other GRES except\nthat the request must be satisfied using only one GPU per node and only one\nGPU per node may be configured for use with MPS.\nFor example, a job request for \"--gres=mps:50\" will not be satisfied by using\n20 percent of one GPU and 30 percent of a second GPU on a single node.\nMultiple jobs from different users can use MPS on a node at the same time.\nNote that GRES types of GPU and MPS can not be requested within\na single job.\nAlso jobs requesting MPS resources can not specify a GPU frequency.A prolog program should be used to start and stop MPS servers as needed.\nA sample prolog script to do this is included with the Slurm distribution in\nthe location etc/prolog.example.\nIts mode of operation is if a job is allocated gres/mps resources then the\nProlog will have the CUDA_VISIBLE_DEVICES,\nCUDA_MPS_ACTIVE_THREAD_PERCENTAGE, and\nSLURM_JOB_UID environment variables set.\nThe Prolog should then make sure that an MPS server is started for that GPU\nand user (UID == User ID).\nIt also records the GPU device ID in a local file.\nIf a job is allocated gres/gpu resources then the Prolog will have the\nCUDA_VISIBLE_DEVICES and\nSLURM_JOB_UID environment variables set\n(no CUDA_MPS_ACTIVE_THREAD_PERCENTAGE).\nThe Prolog should then terminate any MPS server associated with that GPU.\nIt may be necessary to modify this script as needed for the local environment.\nFor more information see the\nProlog and Epilog Guide.Jobs requesting MPS resources will have the\nCUDA_VISIBLE_DEVICES\nand CUDA_DEVICE_ORDER environment variables set.\nThe device ID is relative to those resources under MPS server control and will\nalways have a value of zero in the current implementation (only one GPU will be\nusable in MPS mode per node).\nThe job will also have the\nCUDA_MPS_ACTIVE_THREAD_PERCENTAGE\nenvironment variable set to that job's percentage of MPS resources available on\nthe assigned GPU.\nThe percentage will be calculated based upon the portion of the configured\nCount on the Gres is allocated to a job of step.\nFor example, a job requesting \"--gres=mps:200\" and using\nconfiguration example 2 above would be\nallocated\n15% of the gtx1080 (File=/dev/nvidia0, 200 x 100 / 1300 = 15), or\n16% of the gtx1070 (File=/dev/nvidia0, 200 x 100 / 1200 = 16), or\n18% of the gtx1060 (File=/dev/nvidia0, 200 x 100 / 1100 = 18), or\n20% of the gtx1050 (File=/dev/nvidia0, 200 x 100 / 1000 = 20).An alternate mode of operation would be to permit jobs to be allocated whole\nGPUs then trigger the starting of an MPS server based upon comments in the job.\nFor example, if a job is allocated whole GPUs then search for a comment of\n\"mps-per-gpu\" or \"mps-per-node\" in the job (using the \"scontrol show job\"\ncommand) and use that as a basis for starting one MPS daemon per GPU or across\nall GPUs respectively.Please consult the\n\nNVIDIA Multi-Process Service documentation for more information about MPS.\nNote that a vulnerability exists in previous versions of the NVIDIA driver that\nmay affect users when sharing GPUs. More information can be found in\n\nCVE-2018-6260 and in the\n\nSecurity Bulletin: NVIDIA GPU Display Driver - February 2019.NVIDIA MPS has a built-in limitation regarding GPU sharing among different\nusers. Only one user on a system may have an active MPS server, and the MPS\ncontrol daemon will queue MPS server activation requests from separate users,\nleading to serialized exclusive access of the GPU between users (see\n\nSection 2.3.1.1 - Limitations in the MPS docs). So different users cannot\ntruly run concurrently on the GPU with MPS; rather, the GPU will be time-sliced\nbetween the users (for a diagram depicting this process, see\n\nSection 3.3 - Provisioning Sequence in the MPS docs).MIG Management\n\nBeginning in version 21.08, Slurm now supports NVIDIA\nMulti-Instance GPU (MIG) devices. This feature allows some newer NVIDIA\nGPUs (like the A100) to split up a GPU into up to seven separate, isolated GPU\ninstances. Slurm can treat these MIG instances as individual GPUs, complete with\ncgroup isolation and task binding.To configure MIGs in Slurm, specify\nAutoDetect=nvml in gres.conf for the\nnodes with MIGs, and specify Gres\nin slurm.conf as if the MIGs were regular GPUs, like this:\nNodeName=tux[1-16] gres=gpu:2. An optional\nGRES type can be specified to distinguish MIGs of different sizes from each\nother, as well as from other GPUs in the cluster. This type must be a substring\nof the \"MIG Profile\" string as reported by the node in its slurmd log under the\ndebug2 log level. Here is an example slurm.conf\nfor a system with 2 gpus, one of which is partitioned into 2 MIGs where the\n\"MIG Profile\" is nvidia_a100_3g.20gb:\nAccountingStorageTRES=gres/gpu,gres/gpu:a100,gres/gpu:a100_3g.20gb\nGresTypes=gpu\nNodeName=tux[1-16] gres=gpu:a100:1,gpu:a100_3g.20gb:2\nThe MultipleFiles parameter\nallows you to specify multiple device files for the GPU card.The sanity-check AutoDetect mode is not supported for MIGs.\nSlurm expects MIG devices to already be partitioned, and does not support\ndynamic MIG partitioning.For more information on NVIDIA MIGs (including how to partition them), see\n\nthe MIG user guide.Sharding\n\n\nSharding provides a generic mechanism where GPUs can be\nshared by multiple jobs. While it does permit multiple jobs to run on a given\nGPU it does not fence the processes running on the GPU, it only allows the GPU\nto be shared. Sharding, therefore, works best with homogeneous workflows. It is\nrecommended to limit the number of shards on a node to equal the max possible\njobs that can run simultaneously on the node (i.e. cores).\nThe total count of shards available on a node should be configured in\nthe slurm.conf file (e.g. \"NodeName=tux[1-16] Gres=gpu:2,shard:64\").\nSeveral options are available for configuring shards in the gres.conf file\nas listed below with examples following that:\nNo Shard configuration: The count of gres/shard elements defined in the\nslurm.conf will be evenly distributed across all GPUs configured on the\nnode. For example, \"NodeName=tux[1-16] Gres=gpu:2,shard:64\" will configure\na count of 32 gres/shard resources on each of the two GPUs.\nShard configuration includes only the Name and Count parameters:\nThe count of gres/shard elements will be evenly distributed across all GPUs\nconfigured on the node. This is similar to case 1, but places duplicate\nconfiguration in the gres.conf file.\nShard configuration includes the Name, File and Count\nparameters: Each File parameter should identify the device file path of a\nGPU and the Count should identify the number of gres/shard resources\navailable for that specific GPU device.\nThis may be useful in a heterogeneous environment.\nFor example, some GPUs on a node may be more powerful than others and thus be\nassociated with a higher gres/shard count.\nAnother use case would be to prevent some GPUs from being used for sharding (i.e.\nthey would have a shard count of zero).\nNote that Type and Cores parameters for gres/shard are ignored.\nThat information is copied from the gres/gpu configuration.Note that if NVIDIA's NVML library is installed, the GPU configuration\n(i.e. Type, File, Cores and Links data) will be\nautomatically gathered from the library and need not be recorded in the\ngres.conf file.Note the same GPU can be allocated either as a GPU type of GRES or as\na shard type of GRES, but not both.\nIn other words, once a GPU has been allocated as a gres/gpu resource it will\nnot be available as a gres/shard.\nLikewise, once a GPU has been allocated as a gres/shard resource it will\nnot be available as a gres/gpu.\nHowever the same GPU can be allocated as shard generic resources to multiple jobs\nbelonging to multiple users, so long as the total count of SHARD allocated to\njobs does not exceed the configured count.By default, job requests for shards are required to fit on a single gpu on\neach node. This can be overridden with a flag in the slurm.conf\nconfiguration file. See \nOPT_MULTIPLE_SHARING_GRES_PJ.In order for this to be correctly configured, the appropriate nodes need\nto have the shard keyword added as a GRES for the relevant nodes as\nwell as being added to the GresTypes parameter. If you want the shards\nto be tracked in accounting then shard also needs to be added to\nAccountingStorageTRES.\nSee the relevant settings in an example slurm.conf:\n\nAccountingStorageTRES=gres/gpu,gres/shard\nGresTypes=gpu,shard\nNodeName=tux[1-16] Gres=gpu:2,shard:64\n\nSome example configurations for Slurm's gres.conf file are shown below.\n\n# Example 1 of gres.conf\n# Configure four GPUs (with Sharding)\nAutoDetect=nvml\nName=gpu Type=gp100 File=/dev/nvidia0 Cores=0,1\nName=gpu Type=gp100 File=/dev/nvidia1 Cores=0,1\nName=gpu Type=p6000 File=/dev/nvidia2 Cores=2,3\nName=gpu Type=p6000 File=/dev/nvidia3 Cores=2,3\n# Set gres/shard Count value to 8 on each of the 4 available GPUs\nName=shard Count=32\n\n\n\n# Example 2 of gres.conf\n# Configure four different GPU types (with Sharding)\nAutoDetect=nvml\nName=gpu Type=gtx1080 File=/dev/nvidia0 Cores=0,1\nName=gpu Type=gtx1070 File=/dev/nvidia1 Cores=0,1\nName=gpu Type=gtx1060 File=/dev/nvidia2 Cores=2,3\nName=gpu Type=gtx1050 File=/dev/nvidia3 Cores=2,3\nName=shard Count=8    File=/dev/nvidia0\nName=shard Count=8    File=/dev/nvidia1\nName=shard Count=8    File=/dev/nvidia2\nName=shard Count=8    File=/dev/nvidia3\n\nNOTE: gres/shard requires the use of the select/cons_tres\nplugin.\nJob requests for shards can not specify a GPU frequency.\nJobs requesting shards resources will have the\nCUDA_VISIBLE_DEVICES, ROCR_VISIBLE_DEVICES,\nor GPU_DEVICE_ORDINAL environment variable set\nwhich would be the same as if it were a GPU.\n\nSteps with shards haveSLURM_SHARDS_ON_NODE\nset indicating the number of shards allocated.\nLast modified 16 February 2024\n"
        }
    ]
}