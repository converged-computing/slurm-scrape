{
    "url": "https://slurm.schedmd.com/faq.html",
    "sections": [
        {
            "title": "\n\nSlurm Workload Manager\n\n",
            "content": "\n\nSchedMD\n\n"
        },
        {
            "title": "Frequently Asked Questions",
            "content": "For Management\nIs Slurm really free?\nWhy should I use Slurm or other free software?\nWhy should I pay for free software?\nWhat does \"Slurm\" stand for?\nFor Researchers\nHow should I cite work involving Slurm?\nFor UsersDesigning Jobs\nHow can I run multiple jobs from within a single\n  script?\nHow can I run a job within an existing job\n  allocation?\nSlurm documentation refers to CPUs, cores and threads.\n  What exactly is considered a CPU?\nHow do I run specific tasks on certain nodes\n  in my allocation?\nHow can I get the task ID in the output or error file\n  name for a batch job?\nHow does Slurm establish the environment for my\n  job?\nCan the make command utilize the resources\n  allocated to a Slurm job?\nHow can I run an Ansys program with Slurm?\nSubmitting Jobs\nWhy are my srun options ignored?\nWhy does the srun --overcommit option not permit\n  multiple jobs to run on nodes?\nWhy is the srun --u/--unbuffered option adding\n  a carriage return to my output?\nWhat is the difference between the sbatch\n  and srun commands?\nCan tasks be launched with a remote (pseudo)\n  terminal?\nHow can I get shell prompts in interactive mode?\nCan Slurm export an X11 display on an allocated compute node?\nScheduling\nWhy is my job not running?\nWhy is the Slurm backfill scheduler not starting my\n  job?\nKilled Jobs\nWhy is my job killed prematurely?\nWhy is my batch job that launches no job steps being\n  killed?\nWhat does \"srun: Force Terminated job\"\n  indicate?\nWhat does this mean: \"srun: First task exited\n  30s ago\" followed by \"srun Job Failed\"?\nManaging Jobs\nHow can I temporarily prevent a job from running\n  (e.g. place it into a hold state)?\nCan I change my job's size after it has started\n  running?\nWhy does squeue (and \"scontrol show\n  jobid\") sometimes not display a job's estimated start time?\nCan squeue output be color coded?\nWhy is my job/node in a COMPLETING state?\nHow can a job in a complete or failed state be requeued?\nWhy is sview not coloring/highlighting nodes\n  properly?\nWhy is my MPICH2 or MVAPICH2 job not running with\n  Slurm? Why does the DAKOTA program not run with Slurm?\nResource Limits\nWhy are my resource limits not propagated?\nWhy are jobs not getting the appropriate\n  memory limit?\nWhy is my MPI job  failing due to the locked memory\n  (memlock) limit being too low?\nFor AdministratorsTest Environments\nCan multiple Slurm systems be run in\n  parallel for testing purposes?\nCan Slurm emulate a larger cluster?\nCan Slurm emulate nodes with more\n  resources than physically exist on the node?\nBuild and Install\nWhy aren't pam_slurm.so, auth_none.so, or other components in a\n  Slurm RPM?\nHow can I build Slurm with debugging symbols?\nHow can a patch file be generated from a Slurm commit\n  in GitHub?\nHow can I apply a patch to my Slurm source?\nWhy am I being offered an automatic update for Slurm?\nCluster Management\n How should I relocate the primary or backup\n  controller?\nDo I need to maintain synchronized clocks\n  on the cluster?\nHow can I stop Slurm from scheduling jobs?\nHow can I dry up the workload for a maintenance\n  period?\nWhat should I be aware of when upgrading Slurm?\nIs there anything exceptional to be aware of when\n  upgrading my database server?\nWhen adding a new cluster, how can the Slurm cluster\n  configuration be copied from an existing cluster to the new cluster?\nHow could some jobs submitted immediately before the\n  slurmctld daemon crashed be lost?\nIs resource limit propagation\n  useful on a homogeneous cluster?\nWhy are the resource limits set in the database\n  not being enforced?\nCan Slurm be configured to manage licenses?\nHow easy is it to switch from PBS or Torque to Slurm?\nWhat might account for MPI performance being below the\n  expected level?\nHow do I safely remove partitions?\nHow can a routing queue be configured?\nAccounting Database\nWhy should I use the slurmdbd instead of the\n  regular database plugins?\nHow can I rebuild the database hierarchy?\nHow critical is configuring high availability for my\n  database?\nHow can I use double quotes in MySQL queries?\nCompute Nodes (slurmd)\nWhy is a node shown in state DOWN when the node\n  has registered for service?\nWhat happens when a node crashes?\nHow can I control the execution of multiple\n  jobs per node?\nWhy are jobs allocated nodes and then unable to initiate\n  programs on some nodes?\n Why does slurmctld log that some nodes\n  are not responding even if they are not in any partition?\nHow can I easily preserve drained node\n  information between major Slurm updates?\nDoes anyone have an example node health check\n  script for Slurm?\nWhy doesn't the HealthCheckProgram\n  execute on DOWN nodes?\nHow can I prevent the slurmd and\n  slurmstepd daemons from being killed when a node's memory\n  is exhausted?\nI see the host of my calling node as 127.0.1.1\n  instead of the correct IP address.  Why is that?\nHow should I add nodes to Slurm?\nHow should I remove nodes from Slurm?\nWhy is a compute node down with the reason set to\n  \"Node unexpectedly rebooted\"?\nHow do I convert my nodes to Control Group (cgroup)\n  v2?\nCan Slurm be used to run jobs on Amazon's EC2?\nUser Management\nHow can PAM be used to control a user's limits on or\n  access to compute nodes?\nHow can I exclude some users from pam_slurm?\nCan a user's account be changed in the database?\nI had to change a user's UID and now they cannot submit\n  jobs. How do I get the new UID to take effect?\nHow can I get SSSD to work with Slurm?\nJobs\nHow is job suspend/resume useful?\nHow can I suspend, resume, hold or release all\n  of the jobs belonging to a specific user, partition, etc?\nAfter manually setting a job priority value,\n  how can its priority value be returned to being managed by the\n  priority/multifactor plugin?\nCan I update multiple jobs with a single\n  scontrol command?\nHow could I automatically print a job's\n  Slurm job ID to its standard output?\nIs it possible to write to user stdout?\nWhy are user processes and srun\n  running even though the job is supposed to be completed?\nHow can a job which has exited with a specific exit code\n  be requeued?\nWhy is Slurm unable to set the CPU frequency for jobs?\nCan the salloc command be configured to\n  launch a shell on a node in the job's allocation?\nHow can I set up a private /tmp and /dev/shm for\n  jobs on my machine?\nHow do I configure Slurm to work with System V IPC\n  enabled applications?\nGeneral Troubleshooting\nIf a Slurm daemon core dumps, where can I find the\n  core file?\nHow can I get a backtrace from a core file?\nError Messages\n\"Cannot resolve X plugin operations\" on\n  daemon startup\n\"Credential replayed\" in\n  SlurmdLogFile\n\"Invalid job credential\"\n\"Task launch failed on node ... Job credential\n  replayed\"\n\"Unable to accept new connection: Too many open\n  files\"\nSlurmdDebug fails to log job step information\n  at the appropriate level\n\"Batch JobId=# missing from batch node <node>\n  (not found BatchStartTime after startup)\"\nMulti-Instance GPU not working with Slurm and\n  PMIx; GPUs are \"In use by another client\"\n\"srun: error: Unable to accept connection:\n  Resources temporarily unavailable\"\n\"Warning: Note very large processing time\"\n  in SlurmctldLogFile\n\"Duplicate entry\" causes slurmdbd to\n  fail\n\"Unable to find plugin: serializer/json\"\nThird Party Integrations\nCan Slurm be used with Globus?\nHow can TotalView be configured to operate with\n  Slurm?\nFor ManagementIs Slurm really free?\nYes, Slurm is free and open source:\n\nSlurm is free as defined by the\n  Free Software\n  Foundation\nSlurm\u2019s source code and\n  documentation are\n  publicly available under the GNU GPL v2\nSlurm can be \n  downloaded, used, modified, and redistributed at no monetary cost\nWhy should I use Slurm or other free software?\nFree software, as with proprietary software, varies widely in quality, but the\nmechanism itself has proven to be capable of producing high-quality software\nthat is trusted by companies around the world. The Linux kernel is a prominent\nexample, which is often trusted on web servers, infrastructure servers,\nsupercomputers, and mobile devices.Likewise, Slurm has become a trusted tool in the supercomputing world since\nits initial release in 2002 and the founding of SchedMD in 2010 to continue\ndeveloping Slurm. Today, Slurm powers a majority of the\nTOP500 supercomputers. Customers switching\nfrom commercial workload managers to Slurm typically report higher scalability,\nbetter performance and lower costs.Why should I pay for free software?\nFree software does not mean that it is without cost. Software requires\nsignificant time and expertise to write, test, distribute, and maintain. If the\nsoftware is large and complex, like Slurm or the Linux kernel, these costs can\nbecome very substantial.Slurm is often used for highly important tasks at major computing clusters\naround the world. Due to the extensive features available and the complexity of\nthe code required to provide those features, many organizations prefer to have\nexperts available to provide tailored recommendations and troubleshooting\nassistance. While Slurm has a global development community incorporating leading\nedge technology, SchedMD personnel have\ndeveloped most of the code and can provide competitively priced commercial\nsupport and on-site training.What does \"Slurm\" stand for?\nNothing.Originally, \"SLURM\" (completely capitalized) was an acronym for\n\"Simple Linux Utility for Resource Management\". In 2012 the preferred\ncapitalization was changed to Slurm, and the acronym was dropped \u2014 the\ndevelopers preferred to think of Slurm as \"sophisticated\" rather than \"Simple\"\nby this point. And, as Slurm continued to expand it's scheduling capabilities,\nthe \"Resource Management\" label was also viewed as outdated.For ResearchersHow should I cite work involving Slurm?\nWe recommend citing the peer-reviewed paper from JSSPP 2023:\n\nArchitecture of the Slurm Workload Manager.Jette, M.A., Wickberg, T. (2023). Architecture of the Slurm Workload Manager.\nIn: Klus\u00e1\u010dek, D., Corbal\u00e1n, J., Rodrigo, G.P. (eds) Job Scheduling Strategies\nfor Parallel Processing. JSSPP 2023. Lecture Notes in Computer Science,\nvol 14283. Springer, Cham. https://doi.org/10.1007/978-3-031-43943-8_1\nFor UsersDesigning JobsHow can I run multiple jobs from within a\nsingle script?\nA Slurm job is just a resource allocation. You can execute many\njob steps within that allocation, either in parallel or sequentially.\nSome jobs actually launch thousands of job steps this way. The job\nsteps will be allocated nodes that are not already allocated to\nother job steps. This essentially provides a second level of resource\nmanagement within the job for the job steps.How can I run a job within an existing\njob allocation?\nThere is an srun option --jobid that can be used to specify\na job's ID.\nFor a batch job or within an existing resource allocation, the\nenvironment variable SLURM_JOB_ID has already been defined,\nso all job steps will run within that job allocation unless\notherwise specified.\nThe one exception to this is when submitting batch jobs.\nWhen a batch job is submitted from within an existing batch job,\nit is treated as a new job allocation request and will get a\nnew job ID unless explicitly set with the --jobid option.\nIf you specify that a batch job should use an existing allocation,\nthat job allocation will be released upon the termination of\nthat batch job.Slurm documentation refers to CPUs, cores and threads.\nWhat exactly is considered a CPU?\nIf your nodes are configured with hyperthreading, then a CPU is equivalent\nto a hyperthread.\nOtherwise a CPU is equivalent to a core.\nYou can determine if your nodes have more than one thread per core\nusing the command \"scontrol show node\" and looking at the values of\n\"ThreadsPerCore\".Note that even on systems with hyperthreading enabled, the resources will\ngenerally be allocated to jobs at the level of a core (see NOTE below).\nTwo different jobs will not share a core except through the use of a partition\nOverSubscribe configuration parameter.\nFor example, a job requesting resources for three tasks on a node with\nThreadsPerCore=2 will be allocated two full cores.\nNote that Slurm commands contain a multitude of options to control\nresource allocation with respect to base boards, sockets, cores and threads.(NOTE: An exception to this would be if the system administrator\nconfigured SelectTypeParameters=CR_CPU and each node's CPU count without its\nsocket/core/thread specification. In that case, each thread would be\nindependently scheduled as a CPU. This is not a typical configuration.)How do I run specific tasks on certain nodes\nin my allocation?\nOne of the distribution methods for srun '-m\nor --distribution' is 'arbitrary'.  This means you can tell Slurm to\nlayout your tasks in any fashion you want.  For instance if I had an\nallocation of 2 nodes and wanted to run 4 tasks on the first node and\n1 task on the second and my nodes allocated from SLURM_JOB_NODELIST\nwhere tux[0-1] my srun line would look like this:\nsrun -n5 -m arbitrary -w tux[0,0,0,0,1] hostname\nIf I wanted something similar but wanted the third task to be on tux 1\nI could run this:\nsrun -n5 -m arbitrary -w tux[0,0,1,0,0] hostname\nHere is a simple Perl script named arbitrary.pl that can be ran to easily lay\nout tasks on nodes as they are in SLURM_JOB_NODELIST.\n#!/usr/bin/perl\nmy @tasks = split(',', $ARGV[0]);\nmy @nodes = `scontrol show hostnames $SLURM_JOB_NODELIST`;\nmy $node_cnt = $#nodes + 1;\nmy $task_cnt = $#tasks + 1;\n\nif ($node_cnt < $task_cnt) {\n  print STDERR \"ERROR: You only have $node_cnt nodes, but requested layout on $task_cnt nodes.\\n\";\n  $task_cnt = $node_cnt;\n}\n\nmy $cnt = 0;\nmy $layout;\nforeach my $task (@tasks) {\n  my $node = $nodes[$cnt];\n  last if !$node;\n  chomp($node);\n  for(my $i=0; $i < $task; $i++) {\n    $layout .= \",\" if $layout;\n    $layout .= \"$node\";\n  }\n  $cnt++;\n}\nprint $layout;\nWe can now use this script in our srun line in this fashion.\nsrun -m arbitrary -n5 -w `arbitrary.pl 4,1` -l hostname\nThis will layout 4 tasks on the first node in the allocation and 1\ntask on the second node.How can I get the task ID in the output\nor error file name for a batch job?\nIf you want separate output by task, you will need to build a script\ncontaining this specification. For example:\n$ cat test\n#!/bin/sh\necho begin_test\nsrun -o out_%j_%t hostname\n\n$ sbatch -n7 -o out_%j test\nsbatch: Submitted batch job 65541\n\n$ ls -l out*\n-rw-rw-r--  1 jette jette 11 Jun 15 09:15 out_65541\n-rw-rw-r--  1 jette jette  6 Jun 15 09:15 out_65541_0\n-rw-rw-r--  1 jette jette  6 Jun 15 09:15 out_65541_1\n-rw-rw-r--  1 jette jette  6 Jun 15 09:15 out_65541_2\n-rw-rw-r--  1 jette jette  6 Jun 15 09:15 out_65541_3\n-rw-rw-r--  1 jette jette  6 Jun 15 09:15 out_65541_4\n-rw-rw-r--  1 jette jette  6 Jun 15 09:15 out_65541_5\n-rw-rw-r--  1 jette jette  6 Jun 15 09:15 out_65541_6\n\n$ cat out_65541\nbegin_test\n\n$ cat out_65541_2\ntdev2\nHow does Slurm establish the environment\nfor my job?\nSlurm processes are not run under a shell, but directly exec'ed\nby the slurmd daemon (assuming srun is used to launch\nthe processes).\nThe environment variables in effect at the time the srun command\nis executed are propagated to the spawned processes.\nThe ~/.profile and ~/.bashrc scripts are not executed\nas part of the process launch. You can also look at the --export option of\nsrun and sbatch. See man pages for details.Can the make command\nutilize the resources allocated to a Slurm job?\nYes. There is a patch available for GNU make version 3.81\navailable as part of the Slurm distribution in the file\ncontribs/make-3.81.slurm.patch.  For GNU make version 4.0 you\ncan use the patch in the file contribs/make-4.0.slurm.patch.\nThis patch will use Slurm to launch tasks across a job's current resource\nallocation. Depending upon the size of modules to be compiled, this may\nor may not improve performance. If most modules are thousands of lines\nlong, the use of additional resources should more than compensate for the\noverhead of Slurm's task launch. Use with make's -j option within an\nexisting Slurm allocation. Outside of a Slurm allocation, make's behavior\nwill be unchanged.How can I run an Ansys program with Slurm?\nIf you are talking about an interactive run of the Ansys app, then you can use\nthis simple script (it is for Ansys Fluent):\n$ cat ./fluent-srun.sh\n#!/usr/bin/env bash\nHOSTSFILE=.hostlist-job$SLURM_JOB_ID\nif [ \"$SLURM_PROCID\" == \"0\" ]; then\n    srun hostname -f > $HOSTSFILE\n    fluent -t $SLURM_NTASKS -cnf=$HOSTSFILE -ssh 3d\n    rm -f $HOSTSFILE\nfi\nexit 0\nTo run an interactive session, use srun like this:\n$ srun -n <tasks> ./fluent-srun.sh\nSubmitting JobsWhy are my srun options ignored?\nEverything after the command srun is\nexamined to determine if it is a valid option for srun. The first\ntoken that is not a valid option for srun is considered the command\nto execute and everything after that is treated as an option to\nthe command. For example:\nsrun -N2 uptime -pdebug\nsrun processes \"-N2\" as an option to itself. \"uptime\" is the command to\nexecute and \"-pdebug\" is treated as an option to the uptime command. Depending\non the command and options provided, you may get an invalid option message or\nunexpected behavior if the options happen to be valid.Options for srun should appear before the command to be run:\nsrun -N2 -pdebug uptime\nWhy does the srun --overcommit option not permit multiple jobs\nto run on nodes?\nThe --overcommit option is a means of indicating that a job or job step is willing\nto execute more than one task per processor in the job's allocation. For example,\nconsider a cluster of two processor nodes. The srun execute line may be something\nof this sort\nsrun --ntasks=4 --nodes=1 a.out\nThis will result in not one, but two nodes being allocated so that each of the four\ntasks is given its own processor. Note that the srun --nodes option specifies\na minimum node count and optionally a maximum node count. A command line of\nsrun --ntasks=4 --nodes=1-1 a.out\nwould result in the request being rejected. If the --overcommit option\nis added to either command line, then only one node will be allocated for all\nfour tasks to use.More than one job can execute simultaneously on the same compute resource\n(e.g. CPU) through the use of srun's --oversubscribe option in\nconjunction with the OverSubscribe parameter in Slurm's partition\nconfiguration. See the man pages for srun and slurm.conf for more information.Why is the srun --u/--unbuffered option adding\n  a carriage character return to my output?\nThe libc library used by many programs internally buffers output rather than\nwriting it immediately. This is done for performance reasons.\nThe only way to disable this internal buffering is to configure the program to\nwrite to a pseudo terminal (PTY) rather than to a regular file.\nThis configuration causes some implementations of libc to prepend the\ncarriage return character before all line feed characters.\nRemoving the carriage return character would result in desired formatting\nin some instances, while causing bad formatting in other cases.\nIn any case, Slurm is not adding the carriage return character, but displaying\nthe actual program's output.What is the difference between the sbatch\n  and srun commands?\nThe srun command has two different modes of operation. First, if not run within\nan existing job (i.e. not within a Slurm job allocation created by salloc or\nsbatch), then it will create a job allocation and spawn an application.\nIf run within an existing allocation, the srun command only spawns the\napplication.\nFor this question, we will only address the first mode of operation and compare\ncreating a job allocation using the sbatch and srun commands.The srun command is designed for interactive use, with someone monitoring\nthe output.\nThe output of the application is seen as output of the srun command,\ntypically at the user's terminal.\nThe sbatch command is designed to submit a script for later execution and its\noutput is written to a file.\nCommand options used in the job allocation are almost identical.\nThe most noticeable difference in options is that the sbatch command supports\nthe concept of job arrays, while srun does not.\nAnother significant difference is in fault tolerance.\nFailures involving sbatch jobs typically result in the job being requeued\nand executed again, while failures involving srun typically result in an\nerror message being generated with the expectation that the user will respond\nin an appropriate fashion.Can tasks be launched with a remote (pseudo)\nterminal?\nThe best method is to use salloc with\nuse_interactive_step set in the LaunchParameters option in\nslurm.conf. See\ngetting shell prompts in interactive mode.How can I get shell prompts in interactive\nmode?\nStarting in 20.11, the recommended way to get an interactive shell prompt is\nto configure use_interactive_step in slurm.conf:\nLaunchParameters=use_interactive_step\nThis configures salloc to automatically launch an interactive\nshell via srun on a node in the allocation whenever\nsalloc is called without a program to execute.By default, use_interactive_step creates an interactive step on\na node in the allocation and runs the shell in that step. An interactive step\nis to an interactive shell what a batch step is to a batch script - both have\naccess to all resources in the allocation on the node they are running on, but\ndo not \"consume\" them.Note that beginning in 20.11, steps created by srun are now exclusive. This\nmeans that the previously-recommended way to get an interactive shell,\nsrun --pty $SHELL, will no longer work, as the\nshell's step will now consume all resources on the node and cause subsequent\nsrun calls to pend.An alternative but not recommended method is to make use of srun's\n--pty option, (e.g. srun --pty bash -i).\nSrun's --pty option runs task zero in pseudo terminal mode. Bash's\n-i option instructs it to run in interactive mode (with prompts).\nHowever, unlike the batch or interactive steps, this launches a step which\nconsumes all resources in the job. This means that subsequent steps cannot be\nlaunched in the job unless they use the --overlap option. If task plugins\nare configured, the shell is limited to CPUs of the first task. Subsequent\nsteps (which must be launched with --overlap) may be limited to fewer\nresources than expected or may fail to launch tasks altogether if multiple\nnodes were requested.  Therefore, this alternative should rarely be used;\nsalloc should be used instead.\nCan Slurm export an X11 display on an allocated compute node?\nYou can use the X11 builtin feature starting at version 17.11.\nIt is enabled by setting PrologFlags=x11 in slurm.conf.\nOther X11 plugins must be deactivated.\n\nRun it as shown:\n\n$ ssh -X user@login1\n$ srun -n1 --pty --x11 xclock\n\nAn alternative for older versions is to build and install an optional SPANK\nplugin for that functionality. Instructions to build and install the plugin\nfollow. This SPANK plugin will not work if used in combination with native X11\nsupport so you must disable it compiling Slurm with --disable-x11. This\nplugin relies on openssh library and it provides features such as GSSAPI\nsupport. Update the Slurm installation path as needed:\n# It may be obvious, but don't forget the -X on ssh\n$ ssh -X alex@testserver.com\n\n# Get the plugin\n$ mkdir git\n$ cd git\n$ git clone https://github.com/hautreux/slurm-spank-x11.git\n$ cd slurm-spank-x11\n\n# Manually edit the X11_LIBEXEC_PROG macro definition\n$ vi slurm-spank-x11.c\n$ vi slurm-spank-x11-plug.c\n$ grep \"define X11_\" slurm-spank-x11.c\n#define X11_LIBEXEC_PROG \"/opt/slurm/17.02/libexec/slurm-spank-x11\"\n$ grep \"define X11_LIBEXEC_PROG\" slurm-spank-x11-plug.c\n#define X11_LIBEXEC_PROG \"/opt/slurm/17.02/libexec/slurm-spank-x11\"\n\n\n# Compile\n$ gcc -g -o slurm-spank-x11 slurm-spank-x11.c\n$ gcc -g -I/opt/slurm/17.02/include -shared -fPIC -o x11.so slurm-spank-x11-plug.c\n\n# Install\n$ mkdir -p /opt/slurm/17.02/libexec\n$ install -m 755 slurm-spank-x11 /opt/slurm/17.02/libexec\n$ install -m 755 x11.so /opt/slurm/17.02/lib/slurm\n\n# Configure\n$ echo -e \"optional x11.so\" >> /opt/slurm/17.02/etc/plugstack.conf\n$ cd ~/tests\n\n# Run\n$ srun -n1 --pty --x11 xclock\nalex@node1's password:\nSchedulingWhy is my job not running?\nThe answer to this question depends on a lot of factors. The main one is which\nscheduler is used by Slurm. Executing the command\n scontrol show config | grep SchedulerType\n will supply this information. If the scheduler type is builtin, then\njobs will be executed in the order of submission for a given partition. Even if\nresources are available to initiate your job immediately, it will be deferred\nuntil no previously submitted job is pending. If the scheduler type is backfill,\nthen jobs will generally be executed in the order of submission for a given partition\nwith one exception: later submitted jobs will be initiated early if doing so does\nnot delay the expected execution time of an earlier submitted job. In order for\nbackfill scheduling to be effective, users' jobs should specify reasonable time\nlimits. If jobs do not specify time limits, then all jobs will receive the same\ntime limit (that associated with the partition), and the ability to backfill schedule\njobs will be limited. The backfill scheduler does not alter job specifications\nof required or excluded nodes, so jobs which specify nodes will substantially\nreduce the effectiveness of backfill scheduling. See the \nbackfill section for more details. For any scheduler, you can check priorities\nof jobs using the command scontrol show job.\nOther reasons can include waiting for resources, memory, qos, reservations, etc.\nAs a guideline, issue an scontrol show job <jobid>\nand look at the field State and Reason to investigate the cause.\nA full list and explanation of the different Reasons can be found in the\nresource limits page.Why is the Slurm backfill scheduler not starting my job?\n\nThe most common problem is failing to set job time limits. If all jobs have\nthe same time limit (for example the partition's time limit), then backfill\nwill not be effective. Note that partitions can have both default and maximum\ntime limits, which can be helpful in configuring a system for effective\nbackfill scheduling.In addition, there are a multitude of backfill scheduling parameters\nwhich can impact which jobs are considered for backfill scheduling, such\nas the maximum number of jobs tested per user. For more information see\nthe slurm.conf man page and check the configuration of SchedulerParameters\non your system.Killed JobsWhy is my job killed prematurely?\nSlurm has a job purging mechanism to remove inactive jobs (resource allocations)\nbefore reaching its time limit, which could be infinite.\nThis inactivity time limit is configurable by the system administrator.\nYou can check its value with the command\nscontrol show config | grep InactiveLimit\nThe value of InactiveLimit is in seconds.\nA zero value indicates that job purging is disabled.\nA job is considered inactive if it has no active job steps or if the srun\ncommand creating the job is not responding.\nIn the case of a batch job, the srun command terminates after the job script\nis submitted.\nTherefore batch job pre- and post-processing is limited to the InactiveLimit.\nContact your system administrator if you believe the InactiveLimit value\nshould be changed.Why is my batch job that launches no\njob steps being killed?\nSlurm has a configuration parameter InactiveLimit intended\nto kill jobs that do not spawn any job steps for a configurable\nperiod of time. Your system administrator may modify the InactiveLimit\nto satisfy your needs. Alternately, you can just spawn a job step\nat the beginning of your script to execute in the background. It\nwill be purged when your script exits or your job otherwise terminates.\nA line of this sort near the beginning of your script should suffice:\nsrun -N1 -n1 sleep 999999 &What does \"srun: Force Terminated job\"\nindicate?\nThe srun command normally terminates when the standard output and\nerror I/O from the spawned tasks end. This does not necessarily\nhappen at the same time that a job step is terminated. For example,\na file system problem could render a spawned task non-killable\nat the same time that I/O to srun is pending. Alternately a network\nproblem could prevent the I/O from being transmitted to srun.\nIn any event, the srun command is notified when a job step is\nterminated, either upon reaching its time limit or being explicitly\nkilled. If the srun has not already terminated, the message\n\"srun: Force Terminated job\" is printed.\nIf the job step's I/O does not terminate in a timely fashion\nthereafter, pending I/O is abandoned and the srun command\nexits.What does this mean:\n\"srun: First task exited 30s ago\"\nfollowed by \"srun Job Failed\"?\nThe srun command monitors when tasks exit. By default, 30 seconds\nafter the first task exits, the job is killed.\nThis typically indicates some type of job failure and continuing\nto execute a parallel job when one of the tasks has exited is\nnot normally productive. This behavior can be changed using srun's\n--wait=<time> option to either change the timeout\nperiod or disable the timeout altogether. See srun's man page\nfor details.Managing JobsHow can I temporarily prevent a job from running\n(e.g. place it into a hold state)?\nThe easiest way to do this is to change a job's earliest begin time\n(optionally set at job submit time using the --begin option).\nThe example below places a job into hold state (preventing its initiation\nfor 30 days) and later permitting it to start now.\n$ scontrol update JobId=1234 StartTime=now+30days\n... later ...\n$ scontrol update JobId=1234 StartTime=now\nCan I change my job's size after it has started\nrunning?\nSlurm supports the ability to decrease the size of jobs.\nRequesting fewer hardware resources, and changing partition, qos,\nreservation, licenses, etc. is only allowed for pending jobs.Use the scontrol command to change a job's size either by specifying\na new node count (NumNodes=) for the job or identify the specific nodes\n(NodeList=) that you want the job to retain.\nAny job steps running on the nodes which are relinquished by the job will be\nkilled unless initiated with the --no-kill option.\nAfter the job size is changed, some environment variables created by Slurm\ncontaining information about the job's environment will no longer be valid and\nshould either be removed or altered (e.g. SLURM_JOB_NUM_NODES,\nSLURM_JOB_NODELIST and SLURM_NTASKS).\nThe scontrol command will generate a script that can be executed to\nreset local environment variables.\nYou must retain the SLURM_JOB_ID environment variable in order for the\nsrun command to gather information about the job's current state and\nspecify the desired node and/or task count in subsequent srun invocations.\nA new accounting record is generated when a job is resized, showing the job to\nhave been resubmitted and restarted at the new size.\nAn example is shown below.\n#!/bin/bash\nsrun my_big_job\nscontrol update JobId=$SLURM_JOB_ID NumNodes=2\n. slurm_job_${SLURM_JOB_ID}_resize.sh\nsrun -N2 my_small_job\nrm slurm_job_${SLURM_JOB_ID}_resize.*\nWhy does squeue (and \"scontrol show\njobid\") sometimes not display a job's  estimated start time?\nWhen the backfill scheduler is configured, it provides an estimated start time\nfor jobs that are candidates for backfill. Pending jobs with dependencies\nwill not have an estimate as it is difficult to predict what resources will\nbe available when the jobs they are dependent on terminate. Also note that\nthe estimate is better for jobs expected to start soon, as most running jobs\nend before their estimated time. There are other restrictions on backfill that\nmay apply. See the backfill section for more details.\nCan squeue output be color coded?\nThe squeue command output is not color coded, but other tools can be used to\nadd color. One such tool is ColorWrapper\n(https://github.com/rrthomas/cw).\nA sample ColorWrapper configuration file and output are shown below.\npath /bin:/usr/bin:/sbin:/usr/sbin:<env>\nusepty\nbase green+\nmatch red:default (Resources)\nmatch black:default (null)\nmatch black:cyan N/A\nregex cyan:default  PD .*$\nregex red:default ^\\d*\\s*C .*$\nregex red:default ^\\d*\\s*CG .*$\nregex red:default ^\\d*\\s*NF .*$\nregex white:default ^JOBID.*\nWhy is my job/node in a COMPLETING state?\nWhen a job is terminating, both the job and its nodes enter the COMPLETING state.\nAs the Slurm daemon on each node determines that all processes associated with\nthe job have terminated, that node changes state to IDLE or some other appropriate\nstate for use by other jobs.\nWhen every node allocated to a job has determined that all processes associated\nwith it have terminated, the job changes state to COMPLETED or some other\nappropriate state (e.g. FAILED).\nNormally, this happens within a second.\nHowever, if the job has processes that cannot be terminated with a SIGKILL\nsignal, the job and one or more nodes can remain in the COMPLETING state\nfor an extended period of time.\nThis may be indicative of processes hung waiting for a core file\nto complete I/O or operating system failure.\nIf this state persists, the system administrator should check for processes\nassociated with the job that cannot be terminated then use the\nscontrol command to change the node's\nstate to DOWN (e.g. \"scontrol update NodeName=name State=DOWN Reason=hung_completing\"),\nreboot the node, then reset the node's state to IDLE\n(e.g. \"scontrol update NodeName=name State=RESUME\").\nNote that setting the node DOWN will terminate all running or suspended\njobs associated with that node.\nAn alternative is to set the node's state to DRAIN until all jobs\nassociated with it terminate before setting it DOWN and re-booting.Note that Slurm has two configuration parameters that may be used to\nautomate some of this process.\nUnkillableStepProgram specifies a program to execute when\nnon-killable processes are identified.\nUnkillableStepTimeout specifies how long to wait for processes\nto terminate.\nSee the \"man slurm.conf\" for more information about these parameters.How can a job in a complete or failed state be requeued?\n\nSlurm supports requeuing jobs in a done or failed state. Use the\ncommand:scontrol requeue job_idThe job will then be requeued back in the PENDING state and scheduled again.\nSee man(1) scontrol.\nConsider a simple job like this:\n$cat zoppo\n#!/bin/sh\necho \"hello, world\"\nexit 10\n\n$sbatch -o here ./zoppo\nSubmitted batch job 10\n\nThe job finishes in FAILED state because it exits with\na non zero value. We can requeue the job back to\nthe PENDING state and the job will be dispatched again.\n\n$ scontrol requeue 10\n$ squeue\n      JOBID PARTITION  NAME     USER   ST   TIME  NODES NODELIST(REASON)\n      10      mira    zoppo    david  PD   0:00    1   (NonZeroExitCode)\n$ squeue\n    JOBID PARTITION   NAME     USER ST     TIME  NODES NODELIST(REASON)\n      10      mira    zoppo    david  R    0:03    1      alanz1\nSlurm supports requeuing jobs in a hold state with the command:scontrol requeuehold job_idThe job can be in state RUNNING, SUSPENDED, COMPLETED or FAILED\nbefore being requeued.\n$ scontrol requeuehold 10\n$ squeue\n    JOBID PARTITION  NAME     USER ST       TIME  NODES NODELIST(REASON)\n    10      mira    zoppo    david PD       0:00      1 (JobHeldUser)\nWhy is sview not coloring/highlighting nodes\n    properly?\nsview color-coding is affected by the GTK theme. The node status grid\nis made up of button widgets and certain GTK themes don't show the color\nsetting as desired. Changing GTK themes can restore proper color-coding.Why is my MPICH2 or MVAPICH2 job not running with\nSlurm? Why does the DAKOTA program not run with Slurm?\nThe Slurm library used to support MPICH2 or MVAPICH2 references a variety of\nsymbols. If those symbols resolve to functions or variables in your program\nrather than the appropriate library, the application will fail. For example\nDAKOTA, versions 5.1 and\nolder, contains a function named regcomp, which will get used rather\nthan the POSIX regex functions. Rename DAKOTA's function and\nreferences from regcomp to something else to make it work properly.Resource LimitsWhy are my resource limits not propagated?\nWhen the srun command executes, it captures the\nresource limits in effect at submit time on the node where srun executes.\nThese limits are propagated to the allocated nodes before initiating the\nuser's job.\nThe Slurm daemons running on the allocated nodes then try to establish\nidentical resource limits for the job being initiated.\nThere are several possible reasons for not being able to establish those\nresource limits.\nThe hard resource limits applied to Slurm's slurmd daemon are lower\nthan the user's soft resources limits on the submit host. Typically\nthe slurmd daemon is initiated by the init daemon with the operating\nsystem default limits. This may be addressed either through use of the\nulimit command in the /etc/sysconfig/slurm file or enabling\nPAM in Slurm.\nThe user's hard resource limits on the allocated node are lower than\nthe same user's soft hard resource limits on the node from which the\njob was submitted. It is recommended that the system administrator\nestablish uniform hard resource limits for users on all nodes\nwithin a cluster to prevent this from occurring.\nPropagateResourceLimits or PropagateResourceLimitsExcept parameters are\nconfigured in slurm.conf and avoid propagation of specified limits.\nNOTE: This may produce the error message\n\"Can't propagate RLIMIT_...\".\nThe error message is printed only if the user explicitly specifies that\nthe resource limit should be propagated or the srun command is running\nwith verbose logging of actions from the slurmd daemon (e.g. \"srun -d6 ...\").Why are jobs not getting the appropriate\nmemory limit?\nThis is probably a variation on the locked memory limit\nproblem described above.\nUse the same solution for the AS (Address Space), RSS (Resident Set Size),\nor other limits as needed.Why is my MPI job  failing due to the\nlocked memory (memlock) limit being too low?\nBy default, Slurm propagates all of your resource limits at the\ntime of job submission to the spawned tasks.\nThis can be disabled by specifically excluding the propagation of\nspecific limits in the slurm.conf file. For example\nPropagateResourceLimitsExcept=MEMLOCK might be used to\nprevent the propagation of a user's locked memory limit from a\nlogin node to a dedicated node used for his parallel job.\nIf the user's resource limit is not propagated, the limit in\neffect for the slurmd daemon will be used for the spawned job.\nA simple way to control this is to ensure that user root has a\nsufficiently large resource limit and ensuring that slurmd takes\nfull advantage of this limit. For example, you can set user root's\nlocked memory limit ulimit to be unlimited on the compute nodes (see\n\"man limits.conf\") and ensuring that slurmd takes\nfull advantage of this limit (e.g. by adding \"LimitMEMLOCK=infinity\"\nto your systemd's slurmd.service file). It may also be desirable to lock\nthe slurmd daemon's memory to help ensure that it keeps responding if memory\nswapping begins. A sample /etc/sysconfig/slurm which can be read from\nsystemd is shown below.\nRelated information about PAM is also available.\n#\n# Example /etc/sysconfig/slurm\n#\n# Memlocks the slurmd process's memory so that if a node\n# starts swapping, the slurmd will continue to respond\nSLURMD_OPTIONS=\"-M\"\nFor AdministratorsTest EnvironmentsCan multiple Slurm systems be run in\nparallel for testing purposes?\nYes, this is a great way to test new versions of Slurm.\nJust install the test version in a different location with a different\nslurm.conf.\nThe test system's slurm.conf should specify different\npathnames and port numbers to avoid conflicts.\nThe only problem is if more than one version of Slurm is configured\nwith burst_buffer/* plugins or others that may interact with external\nsystem APIs.\nIn that case, there can be conflicting API requests from\nthe different Slurm systems.\nThis can be avoided by configuring the test system with burst_buffer/none.Can Slurm emulate a larger cluster?\nYes, this can be useful for testing purposes.\nIt has also been used to partition \"fat\" nodes into multiple Slurm nodes.\nThere are two ways to do this.\nThe best method for most conditions is to run one slurmd\ndaemon per emulated node in the cluster as follows.\nWhen executing the configure program, use the option\n--enable-multiple-slurmd (or add that option to your ~/.rpmmacros\nfile).\nBuild and install Slurm in the usual manner.\nIn slurm.conf define the desired node names (arbitrary\nnames used only by Slurm) as NodeName along with the actual\naddress of the physical node in NodeHostname. Multiple\nNodeName values can be mapped to a single\nNodeHostname.  Note that each NodeName on a single\nphysical node needs to be configured to use a different port number\n(set Port to a unique value on each line for each node).  You\nwill also want to use the \"%n\" symbol in slurmd related path options in\nslurm.conf (SlurmdLogFile and SlurmdPidFile). \nWhen starting the slurmd daemon, include the NodeName\nof the node that it is supposed to serve on the execute line (e.g.\n\"slurmd -N hostname\").\n This is an example of the slurm.conf file with the  emulated nodes\nand ports configuration. Any valid value for the CPUs, memory or other\nvalid node resources can be specified.\n\nNodeName=dummy26[1-100] NodeHostName=achille Port=[6001-6100] NodeAddr=127.0.0.1 CPUs=4 RealMemory=6000\nPartitionName=mira Default=yes Nodes=dummy26[1-100]\nSee the\nProgrammers Guide\nfor more details about configuring multiple slurmd support.In order to emulate a really large cluster, it can be more\nconvenient to use a single slurmd daemon.\nThat daemon will not be able to launch many tasks, but can\nsuffice for developing or testing scheduling software.\nDo not run job steps with more than a couple of tasks each\nor execute more than a few jobs at any given time.\nDoing so may result in the slurmd daemon exhausting its\nmemory and failing.\nUse this method with caution.\nExecute the configure program with your normal options\nplus --enable-front-end (this will define HAVE_FRONT_END in\nthe resulting config.h file.\nBuild and install Slurm in the usual manner.\nIn slurm.conf define the desired node names (arbitrary\nnames used only by Slurm) as NodeName along with the actual\nname and address of the one physical node in NodeHostName\nand NodeAddr.\nUp to 64k nodes can be configured in this virtual cluster.\nStart your slurmctld and one slurmd daemon.\nIt is advisable to use the \"-c\" option to start the daemons without\ntrying to preserve any state files from previous executions.\nBe sure to use the \"-c\" option when switching from this mode too.\nCreate job allocations as desired, but do not run job steps\nwith more than a couple of tasks.\n\n$ ./configure --enable-debug --enable-front-end --prefix=... --sysconfdir=...\n$ make install\n$ grep NodeHostName slurm.conf\nNodeName=dummy[1-1200] NodeHostName=localhost NodeAddr=127.0.0.1\n$ slurmctld -c\n$ slurmd -c\n$ sinfo\nPARTITION AVAIL  TIMELIMIT NODES  STATE NODELIST\npdebug*      up      30:00  1200   idle dummy[1-1200]\n$ cat tmp\n#!/bin/bash\nsleep 30\n$ srun -N200 -b tmp\nsrun: jobid 65537 submitted\n$ srun -N200 -b tmp\nsrun: jobid 65538 submitted\n$ srun -N800 -b tmp\nsrun: jobid 65539 submitted\n$ squeue\nJOBID PARTITION  NAME   USER  ST  TIME  NODES NODELIST(REASON)\n65537    pdebug   tmp  jette   R  0:03    200 dummy[1-200]\n65538    pdebug   tmp  jette   R  0:03    200 dummy[201-400]\n65539    pdebug   tmp  jette   R  0:02    800 dummy[401-1200]\nCan Slurm emulate nodes with more\nresources than physically exist on the node?\nYes. In the slurm.conf file, configure SlurmdParameters=config_overrides\nand specify\nany desired node resource specifications (CPUs, Sockets,\nCoresPerSocket, ThreadsPerCore, and/or TmpDisk).\nSlurm will use the resource specification for each node that is\ngiven in slurm.conf and will not check these specifications\nagainst those actually found on the node. The system would best be configured\nwith TaskPlugin=task/none, so that launched tasks can run on any\navailable CPU under operating system control.Build and InstallWhy aren't pam_slurm.so, auth_none.so, or other components in a\nSlurm RPM?\nIt is possible that at build time the required dependencies for building the\nlibrary are missing. If you want to build the library then install pam-devel\nand compile again. See the file slurm.spec in the Slurm distribution for a list\nof other options that you can specify at compile time with rpmbuild flags\nand your rpmmacros file.The auth_none plugin is in a separate RPM and not built by default.\nUsing the auth_none plugin means that Slurm communications are not\nauthenticated, so you probably do not want to run in this mode of operation\nexcept for testing purposes. If you want to build the auth_none RPM then\nadd --with auth_none on the rpmbuild command line or add\n%_with_auth_none to your ~/rpmmacros file. See the file slurm.spec\nin the Slurm distribution for a list of other options.How can I build Slurm with debugging symbols?\nWhen configuring, run the configure script with --enable-developer option.\nThat will provide asserts, debug messages and the -Werror flag, that\nwill in turn activate --enable-debug.\nWith the --enable-debug flag, the code will be compiled with\n-ggdb3 and -g -O1 -fno-strict-aliasing flags that will produce\nextra debugging information. Another possible option to use is\n--disable-optimizations that will set -O0.\nSee also auxdir/x_ac_debug.m4 for more details.How can a patch file be generated from a Slurm\ncommit in GitHub?\nFind and open the commit in GitHub then append \".patch\" to the URL and save\nthe resulting file. For an example, see:\n\nhttps://github.com/SchedMD/slurm/commit/91e543d433bed11e0df13ce0499be641774c99a3.patch\nHow can I apply a patch to my Slurm source?\n\nIf you have a patch file that you need to apply to your source, such as a\nsecurity or bug fix patch supplied by SchedMD's support, you can do\nso with the patch command. You would first extract the contents of the\nsource tarball for the version you are using. You can then apply the patch\nto the extracted source. Below is an example of how to do this with the\nsource for Slurm 23.11.1:\n\n$ tar xjvf slurm-23.11.1.tar.bz2 > /dev/null\n$ patch -p1 -d slurm-23.11.1/ < example.patch\npatching file src/slurmctld/step_mgr.c\n\nOnce the patch has been applied to the source code, you can proceed to\nbuild Slurm as you would normally if you build with make. If you use\nrpmbuild to build Slurm, you will have to create a tarball with the\npatched files. The filename of the tarball must match the original filename\nto avoid errors.\n\n$ tar cjvf slurm-23.11.1.tar.bz2 slurm-23.11.1/ > /dev/null\n$ rpmbuild -ta slurm-23.11.1.tar.bz2 > /dev/null\n\nWhy am I being offered an automatic update for Slurm?\n\nEPEL has added Slurm packages to their repository to make them more widely\navailable to the Linux community. However, this packaged version is not\nsupported or maintained by SchedMD, and is not recommend for customers at this\ntime. If you are using the EPEL repo you could be offered an update for Slurm\nthat you may not anticipate. In order to prevent Slurm from being upgraded\nunintentionally, we recommend you modify the EPEL repository configuration file\nto exclude all Slurm packages from automatic updates.\nexclude=slurm*\nCluster ManagementHow should I relocate the primary or\nbackup controller?\nIf the cluster's computers used for the primary or backup controller\nwill be out of service for an extended period of time, it may be desirable\nto relocate them. In order to do so, follow this procedure:\n(Slurm 23.02 and older) Drain the cluster of running jobs\nStop all Slurm daemons\nModify the SlurmctldHost values in the slurm.conf file\nDistribute the updated slurm.conf file to all nodes\nCopy the StateSaveLocation directory to the new host and\nmake sure the permissions allow the SlurmUser to read and write it.\nRestart all Slurm daemons\nStarting with Slurm 23.11, jobs that were started by the old controller will\nreceive the updated controller address and will continue and finish normally.\nOn older versions, jobs started by the old controller will still try to report\nback to the older controller.\nIn both cases, there should be no loss of any pending jobs.\nEnsure that any nodes added to the cluster have a current slurm.conf\nfile installed.CAUTION: If two nodes are simultaneously configured as the primary\ncontroller (two nodes on which SlurmctldHost specify the local host\nand the slurmctld daemon is executing on each), system behavior will be\ndestructive. If a compute node has an incorrect SlurmctldHost parameter,\nthat node may be rendered unusable, but no other harm will result.Do I need to maintain synchronized\nclocks on the cluster?\nIn general, yes. Having inconsistent clocks may cause nodes to\nbe unusable. Slurm log files should contain references to\nexpired credentials. For example:\nerror: Munge decode failed: Expired credential\nENCODED: Wed May 12 12:34:56 2008\nDECODED: Wed May 12 12:01:12 2008\nHow can I stop Slurm from scheduling jobs?\nYou can stop Slurm from scheduling jobs on a per partition basis by setting\nthat partition's state to DOWN. Set its state UP to resume scheduling.\nFor example:\n$ scontrol update PartitionName=foo State=DOWN\n$ scontrol update PartitionName=bar State=UP\nHow can I dry up the workload for a\nmaintenance period?\nCreate a resource reservation as described in Slurm's\nResource Reservation Guide.What should I be aware of when upgrading Slurm?\nRefer to the Upgrade Guide for details.Is there anything exceptional to be aware of when\nupgrading my database server?\nGenerally, no. Special cases are noted in the \nDatabase server section of the Upgrade Guide.When adding a new cluster, how can the Slurm cluster\nconfiguration be copied from an existing cluster to the new cluster?\nAccounts need to be configured for the cluster. An easy way to copy information from\nan existing cluster is to use the sacctmgr command to dump that cluster's information,\nmodify it using some editor, the load the new information using the sacctmgr\ncommand. See the sacctmgr man page for details, including an example.How could some jobs submitted immediately before\nthe slurmctld daemon crashed be lost?\nAny time the slurmctld daemon or hardware fails before state information reaches\ndisk can result in lost state.\nSlurmctld writes state frequently (every five seconds by default), but with\nlarge numbers of jobs, the formatting and writing of records can take seconds\nand recent changes might not be written to disk.\nAnother example is if the state information is written to file, but that\ninformation is cached in memory rather than written to disk when the node fails.\nThe interval between state saves being written to disk can be configured at\nbuild time by defining SAVE_MAX_WAIT to a different value than five.Is resource limit propagation\nuseful on a homogeneous cluster?\nResource limit propagation permits a user to modify resource limits\nand submit a job with those limits.\nBy default, Slurm automatically propagates all resource limits in\neffect at the time of job submission to the tasks spawned as part\nof that job.\nSystem administrators can utilize the PropagateResourceLimits\nand PropagateResourceLimitsExcept configuration parameters to\nchange this behavior.\nUsers can override defaults using the srun --propagate\noption.\nSee \"man slurm.conf\" and \"man srun\" for more information\nabout these options.Why are the resource limits set in the\ndatabase not being enforced?\nIn order to enforce resource limits, set the value of\nAccountingStorageEnforce in each cluster's slurm.conf configuration\nfile appropriately. If AccountingStorageEnforce does not contains\nan option of \"limits\", then resource limits will not be enforced on that cluster.\nSee Resource Limits for more information.Can Slurm be configured to manage licenses?\nSlurm is not currently integrated with FlexLM, but it does provide for the\nallocation of global resources called licenses. Use the Licenses configuration\nparameter in your slurm.conf file (e.g. \"Licenses=foo:10,bar:20\").\nJobs can request licenses and be granted exclusive use of those resources\n(e.g. \"sbatch --licenses=foo:2,bar:1 ...\").\nIt is not currently possible to change the total number of licenses on a system\nwithout restarting the slurmctld daemon, but it is possible to dynamically\nreserve licenses and remove them from being available to jobs on the system\n(e.g. \"scontrol update reservation=licenses_held licenses=foo:5,bar:2\").How easy is it to switch from PBS or Torque to Slurm?\nA lot of users don't even notice the difference.\nSlurm has wrappers available for the mpiexec, pbsnodes, qdel, qhold, qrls,\nqstat, and qsub commands (see contribs/torque in the distribution and the\n\"slurm-torque\" RPM).\nThere is also a wrapper for the showq command at\n\nhttps://github.com/pedmon/slurm_showq.Slurm recognizes and translates the \"#PBS\" options in batch scripts.\nMost, but not all options are supported.Slurm also includes a SPANK plugin that will set all of the PBS environment\nvariables based upon the Slurm environment (e.g. PBS_JOBID, PBS_JOBNAME,\nPBS_WORKDIR, etc.).\nOne environment not set by PBS_ENVIRONMENT, which if set would result in the\nfailure of some MPI implementations.\nThe plugin will be installed in\n<install_directory>/lib/slurm/spank_pbs.so\nSee the SPANK man page for configuration details.What might account for MPI performance being below\nthe expected level?\nStarting the slurmd daemons with limited locked memory can account for this.\nAdding the line \"ulimit -l unlimited\" to the /etc/sysconfig/slurm file can\nfix this.How do I safely remove partitions?\n\nPartitions should be removed using the\n\"scontrol delete PartitionName=<partition>\" command. This is because\nscontrol will prevent any partitions from being removed that are in use.\nPartitions need to be removed from the slurm.conf after being removed using\nscontrol or they will return after a restart.\nAn existing job's partition(s) can be updated with the \"scontrol update\nJobId=<jobid> Partition=<partition(s)>\" command.\nRemoving a partition from the slurm.conf and restarting will cancel any existing\njobs that reference the removed partitions.\nHow can a routing queue be configured?\nA job submit plugin is designed to have access to a job request from a user,\nplus information about all of the available system partitions/queue.\nAn administrator can write a C plugin or LUA script to set an incoming job's\npartition based upon its size, time limit, etc.\nSee the  Job Submit Plugin API\nguide for more information.\nAlso see the available job submit plugins distributed with Slurm for examples\n(look in the \"src/plugins/job_submit\" directory).Accounting DatabaseWhy should I use the slurmdbd instead of the\nregular database plugins?\nWhile the normal storage plugins will work fine without the added\nlayer of the slurmdbd there are some great benefits to using the\nslurmdbd.\nAdded security.  Using the slurmdbd you can have an authenticated\nconnection to the database.\nOffloading processing from the controller. With the slurmdbd there is no\nslowdown to the controller due to a slow or overloaded database.\nKeeping enterprise wide accounting from all Slurm clusters in one database.\nThe slurmdbd is multi-threaded and designed to handle all the\naccounting for the entire enterprise.\nWith the database plugins you can query with sacct accounting stats from\nany node Slurm is installed on. With the slurmdbd you can also query any\ncluster using the slurmdbd from any other cluster's nodes. Other tools like\nsreport are also available.\nHow can I rebuild the database hierarchy?\nIf you see errors of this sort:\nerror: Can't find parent id 3358 for assoc 1504, this should never happen.\nin the slurmctld log file, this is indicative that the database hierarchy\ninformation has been corrupted, typically due to a hardware failure or\nadministrator error in directly modifying the database. In order to rebuild\nthe database information, start the slurmdbd daemon with the \"-R\" option\nfollowed by an optional comma separated list of cluster names to operate on.How critical is configuring high availability for my\ndatabase?\nConsider if you really need a high-availability MySQL setup. A short outage\nof slurmdbd is not a problem, because slurmctld will store all data in memory\nand send it to slurmdbd when it resumes operations. The slurmctld daemon will\nalso cache all user limits and fair share information.\nYou cannot use NDB, since SlurmDBD's MySQL implementation uses keys on BLOB\nvalues (and potentially other features on the incompatibility list).\nYou can set up \"classical\" Linux HA, with heartbeat/corosync to migrate IP\nbetween primary/backup mysql servers and:\n\nConfigure one way replication of mysql, and change primary/backup roles on\nfailure\nUse shared storage for primary/backup mysql servers database, and start\nbackup on primary mysql failure.\n\n\nHow can I use double quotes in MySQL queries?\nExecute:\nSET session sql_mode='ANSI_QUOTES';\nThis will allow double quotes in queries like this:\nshow columns from \"tux_assoc_table\" where Field='is_def';\nCompute Nodes (slurmd)Why is a node shown in state\nDOWN when the node has registered for service?\nThe configuration parameter ReturnToService in slurm.conf\ncontrols how DOWN nodes are handled.\nSet its value to one in order for DOWN nodes to automatically be\nreturned to service once the slurmd daemon registers\nwith a valid node configuration.\nA value of zero is the default and results in a node staying DOWN\nuntil an administrator explicitly returns it to service using\nthe command \"scontrol update NodeName=whatever State=RESUME\".\nSee \"man slurm.conf\" and \"man scontrol\" for more\ndetails.What happens when a node crashes?\nA node is set DOWN when the slurmd daemon on it stops responding\nfor SlurmdTimeout as defined in slurm.conf.\nThe node can also be set DOWN when certain errors occur or the\nnode's configuration is inconsistent with that defined in slurm.conf.\nAny active job on that node will be killed unless it was submitted\nwith the srun option --no-kill.\nAny active job step on that node will be killed.\nSee the slurm.conf and srun man pages for more information.How can I control the execution of multiple\njobs per node?\nThere are two mechanisms to control this.\nIf you want to allocate individual processors on a node to jobs,\nconfigure SelectType=select/cons_tres.\nSee Consumable Resources in Slurm\nfor details about this configuration.\nIf you want to allocate whole nodes to jobs, configure\nconfigure SelectType=select/linear.\nEach partition also has a configuration parameter OverSubscribe\nthat enables more than one job to execute on each node.\nSee man slurm.conf for more information about these\nconfiguration parameters.Why are jobs allocated nodes and then unable\nto initiate programs on some nodes?\nThis typically indicates that the time on some nodes is not consistent\nwith the node on which the slurmctld daemon executes. In order to\ninitiate a job step (or batch job), the slurmctld daemon generates\na credential containing a time stamp. If the slurmd daemon\nreceives a credential containing a time stamp later than the current\ntime or more than a few minutes in the past, it will be rejected.\nIf you check in the SlurmdLogFile on the nodes of interest, you\nwill likely see messages of this sort: \"Invalid job credential from\n<some IP address>: Job credential expired.\" Make the times\nconsistent across all of the nodes and all should be well.Why does slurmctld log that some nodes\nare not responding even if they are not in any partition?\nThe slurmctld daemon periodically pings the slurmd\ndaemon on every configured node, even if not associated with any\npartition. You can control the frequency of this ping with the\nSlurmdTimeout configuration parameter in slurm.conf.How can I easily preserve drained node\ninformation between major Slurm updates?\nMajor Slurm updates generally have changes in the state save files and\ncommunication protocols, so a cold-start (without state) is generally\nrequired. If you have nodes in a DRAIN state and want to preserve that\ninformation, you can easily build a script to preserve that information\nusing the sinfo command. The following command line will report the\nReason field for every node in a DRAIN state and write the output\nin a form that can be executed later to restore state.\nsinfo -t drain -h -o \"scontrol update nodename='%N' state=drain reason='%E'\"\nDoes anyone have an example node\nhealth check script for Slurm?\nProbably the most comprehensive and lightweight health check tool out\nthere is\nNode Health Check.\nIt has integration with Slurm as well as Torque resource managers.Why doesn't the HealthCheckProgram\nexecute on DOWN nodes?\nHierarchical communications are used for sending this message. If there\nare DOWN nodes in the communications hierarchy, messages will need to\nbe re-routed. This limits Slurm's ability to tightly synchronize the\nexecution of the HealthCheckProgram across the cluster, which\ncould adversely impact performance of parallel applications.\nThe use of CRON or node startup scripts may be better suited to ensure\nthat HealthCheckProgram gets executed on nodes that are DOWN\nin Slurm.How can I prevent the slurmd and\nslurmstepd daemons from being killed when a node's memory\nis exhausted?\nYou can set the value in the /proc/self/oom_adj for\nslurmd and slurmstepd by initiating the slurmd\ndaemon with the SLURMD_OOM_ADJ and/or SLURMSTEPD_OOM_ADJ\nenvironment variables set to the desired values.\nA value of -17 typically will disable killing.I see the host of my calling node as 127.0.1.1\n    instead of the correct IP address.  Why is that?\nSome systems by default will put your host in the /etc/hosts file as\nsomething like\n127.0.1.1\tsnowflake.llnl.gov\tsnowflake\nThis will cause srun and Slurm commands to use the 127.0.1.1 address\ninstead of the correct address and prevent communications between nodes.\nThe solution is to either remove this line or configure a different NodeAddr\nthat is known by your other nodes.The CommunicationParameters=NoInAddrAny configuration parameter is subject to\nthis same problem, which can also be addressed by removing the actual node\nname from the \"127.0.1.1\" as well as the \"127.0.0.1\"\naddresses in the /etc/hosts file.  It is ok if they point to\nlocalhost, but not the actual name of the node.How should I add nodes to Slurm?\nThe slurmctld daemon has many bitmaps to track state of nodes and cores in the\ncluster. Adding nodes to a running cluster would require the slurmctld daemon\nto rebuild all of those bitmaps, which the developers feel would be safer to do\nby restarting the daemon. Communications from the slurmd daemons on the compute\nnodes to the slurmctld daemon include a configuration file checksum, so you\nshould maintain the same slurm.conf file on all nodes. The following procedure\nis recommended:\nStop the slurmctld daemon (e.g. systemctl stop slurmctld\n  on the head node)\nUpdate the slurm.conf file on all nodes in the cluster\nRestart the slurmd daemons on all nodes (e.g.\n  systemctl restart slurmd on all nodes)\nRestart the slurmctld daemon (e.g. systemctl start slurmctld\n  on the head node)\nNOTE: Jobs submitted with srun, and that are waiting for an\nallocation, prior to new nodes being added to the slurm.conf can fail if the\njob is allocated one of the new nodes.How should I remove nodes from Slurm?\nTo safely remove a node from a cluster, it's best to drain the node of all jobs.\nThis ensures that job processes aren't running on the node after removal. On\nrestart of the controller, if a node is removed from a running job the\ncontroller will kill the job on any remaining allocated nodes and attempt to\nrequeue the job if possible. The following procedure is recommended:\nDrain node of all jobs (e.g.\n  scontrol update nodename='%N' state=drain reason='removing nodes'\n  )\nStop the slurmctld daemon (e.g. systemctl stop slurmctld\n  on the head node)\nUpdate the slurm.conf file on all nodes in the cluster\nRestart the slurmd daemons on all nodes (e.g.\n  systemctl restart slurmd on all nodes)\nRestart the slurmctld daemon (e.g. systemctl start slurmctld\n  on the head node)\nNOTE: Removing nodes from the cluster may cause some errors in the\nlogs. Verify that any errors in the logs are for nodes that you intended to\nremove.Why is a compute node down with the reason set to\n\"Node unexpectedly rebooted\"?\nThis is indicative of the slurmctld daemon running on the cluster's head node\nas well as the slurmd daemon on the compute node when the compute node reboots.\nIf you want to prevent this condition from setting the node into a DOWN state\nthen configure ReturnToService to 2. See the slurm.conf man page for details.\nOtherwise use scontrol or sview to manually return the node to service.How do I convert my nodes to Control Group (cgroup)\nv2?\nRefer to the cgroup v2 documentation\nfor the conversion procedure.Can Slurm be used to run jobs on\nAmazon's EC2?\nYes, here is a description of Slurm use with\nAmazon's EC2 courtesy of\nAshley Pittman:I do this regularly and have no problem with it, the approach I take is to\nstart as many instances as I want and have a wrapper around\nec2-describe-instances that builds a /etc/hosts file with fixed hostnames\nand the actual IP addresses that have been allocated.  The only other step\nthen is to generate a slurm.conf based on how many node you've chosen to boot\nthat day.  I run this wrapper script on my laptop and it generates the files\nand they rsyncs them to all the instances automatically.One thing I found is that Slurm refuses to start if any nodes specified in\nthe slurm.conf file aren't resolvable, I initially tried to specify cloud[0-15]\nin slurm.conf, but then if I configure less than 16 nodes in /etc/hosts this\ndoesn't work so I dynamically generate the slurm.conf as well as the hosts\nfile.As a comment about EC2 I run just run generic AMIs and have a persistent EBS\nstorage device which I attach to the first instance when I start up.  This\ncontains a /usr/local which has my software like Slurm, pdsh and MPI installed\nwhich I then copy over the /usr/local on the first instance and NFS export to\nall other instances.  This way I have persistent home directories and a very\nsimple first-login script that configures the virtual cluster for me.User ManagementHow can PAM be used to control a user's limits on\nor access to compute nodes?\nTo control a user's limits on a compute node:First, enable Slurm's use of PAM by setting UsePAM=1 in\nslurm.conf.Second, establish PAM configuration file(s) for Slurm in /etc/pam.conf\nor the appropriate files in the /etc/pam.d directory (e.g.\n/etc/pam.d/sshd by adding the line \"account required pam_slurm.so\".\nA basic configuration you might use is:\naccount  required  pam_unix.so\naccount  required  pam_slurm.so\nauth     required  pam_localuser.so\nsession  required  pam_limits.so\nThird, set the desired limits in /etc/security/limits.conf.\nFor example, to set the locked memory limit to unlimited for all users:\n*   hard   memlock   unlimited\n*   soft   memlock   unlimited\nFinally, you need to disable Slurm's forwarding of the limits from the\nsession from which the srun initiating the job ran. By default\nall resource limits are propagated from that session. For example, adding\nthe following line to slurm.conf will prevent the locked memory\nlimit from being propagated:PropagateResourceLimitsExcept=MEMLOCK.To control a user's access to a compute node:The pam_slurm_adopt and pam_slurm modules prevent users from\nlogging into nodes that they have not been allocated (except for user\nroot, which can always login).\nThey are both included with the Slurm distribution.The pam_slurm_adopt module is highly recommended for most installations,\nand is documented in its own guide.pam_slurm is older and less functional.\nThese modules are built by default for RPM packages, but can be disabled using\nthe .rpmmacros option \"%_without_pam 1\" or by entering the command line\noption \"--without pam\" when the configure program is executed.\nTheir source code is in the \"contribs/pam\" and \"contribs/pam_slurm_adopt\"\ndirectories respectively.The use of either pam_slurm_adopt or pam_slurm does not require\nUsePAM being set. The two uses of PAM are independent.How can I exclude some users from pam_slurm?\nCAUTION: Please test this on a test machine/VM before you actually do\nthis on your Slurm computers.Step 1. Make sure pam_listfile.so exists on your system.\nThe following command is an example on Redhat 6:\nls -la /lib64/security/pam_listfile.so\nStep 2. Create user list (e.g. /etc/ssh/allowed_users):\n# /etc/ssh/allowed_users\nroot\nmyadmin\nAnd, change file mode to keep it secret from regular users(Optional):\nchmod 600 /etc/ssh/allowed_users\nNOTE: root is not necessarily listed on the allowed_users, but I\nfeel somewhat safe if it's on the list.Step 3. On /etc/pam.d/sshd, add pam_listfile.so with sufficient flag\nbefore pam_slurm.so (e.g. my /etc/pam.d/sshd looks like this):\n#%PAM-1.0\nauth       required     pam_sepermit.so\nauth       include      password-auth\naccount    sufficient   pam_listfile.so item=user sense=allow file=/etc/ssh/allowed_users onerr=fail\naccount    required     pam_slurm.so\naccount    required     pam_nologin.so\naccount    include      password-auth\npassword   include      password-auth\n# pam_selinux.so close should be the first session rule\nsession    required     pam_selinux.so close\nsession    required     pam_loginuid.so\n# pam_selinux.so open should only be followed by sessions to be executed in the user context\nsession    required     pam_selinux.so open env_params\nsession    optional     pam_keyinit.so force revoke\nsession    include      password-auth\n(Information courtesy of Koji Tanaka, Indiana University)Can a user's account be changed in the database?\nA user's account can not be changed directly. A new association needs to be\ncreated for the user with the new account. Then the association with the old\naccount can be deleted.\n# Assume user \"adam\" is initially in account \"physics\"\nsacctmgr create user name=adam cluster=tux account=physics\nsacctmgr delete user name=adam cluster=tux account=chemistry\nI had to change a user's UID and now they cannot submit\n  jobs. How do I get the new UID to take effect?\nWhen changing UIDs, you will also need to restart the slurmctld for the changes to\ntake effect. Normally, when adding a new user to the system, the UID is filled in\nautomatically and immediately. If the user isn't known on the system yet, there is a\nthread that runs every hour that fills in those UIDs when they become known, but it\ndoesn't recognize UID changes of preexisting users. But you can simply restart the\nslurmctld for those changes to be recognized.How can I get SSSD to work with Slurm?\nSSSD or System Security Services Daemon does not allow enumeration of\ngroup members by default. Note that enabling enumeration in large\nenvironments might not be feasible. However, Slurm does not need enumeration\nexcept for some specific quirky configurations (multiple groups with the same\nGID), so it's probably safe to leave enumeration disabled.\nSSSD is also case sensitive by default for some configurations, which could\npossibly raise other issues. Add the following lines\nto /etc/sssd/sssd.conf on your head node to address these issues:\nenumerate = True\ncase_sensitive = False\nJobsHow is job suspend/resume useful?\nJob suspend/resume is most useful to get particularly large jobs initiated\nin a timely fashion with minimal overhead. Say you want to get a full-system\njob initiated. Normally you would need to either cancel all running jobs\nor wait for them to terminate. Canceling jobs results in the loss of\ntheir work to that point from their beginning.\nWaiting for the jobs to terminate can take hours, depending upon your\nsystem configuration. A more attractive alternative is to suspend the\nrunning jobs, run the full-system job, then resume the suspended jobs.\nThis can easily be accomplished by configuring a special queue for\nfull-system jobs and using a script to control the process.\nThe script would stop the other partitions, suspend running jobs in those\npartitions, and start the full-system partition.\nThe process can be reversed when desired.\nOne can effectively gang schedule (time-slice) multiple jobs\nusing this mechanism, although the algorithms to do so can get quite\ncomplex.\nSuspending and resuming a job makes use of the SIGSTOP and SIGCONT\nsignals respectively, so swap and disk space should be sufficient to\naccommodate all jobs allocated to a node, either running or suspended.How can I suspend, resume, hold or release all\n  of the jobs belonging to a specific user, partition, etc?\nThere isn't any filtering by user, partition, etc. available in the scontrol\ncommand; however the squeue command can be used to perform the filtering and\nbuild a script which you can then execute. For example:\n$ squeue -u adam -h -o \"scontrol hold %i\" >hold_script\nAfter manually setting a job priority\nvalue, how can its priority value be returned to being managed by the\npriority/multifactor plugin?\nHold and then release the job as shown below.\n$ scontrol hold <jobid>\n$ scontrol release <jobid>\nCan I update multiple jobs with a\nsingle scontrol command?\nNo, but you can probably use squeue to build the script taking\nadvantage of its filtering and formatting options. For example:\n$ squeue -tpd -h -o \"scontrol update jobid=%i priority=1000\" >my.script\nHow could I automatically print a job's\nSlurm job ID to its standard output?\nThe configured TaskProlog is the only thing that can write to\nthe job's standard output or set extra environment variables for a job\nor job step. To write to the job's standard output, precede the message\nwith \"print \". To export environment variables, output a line of this\nform \"export name=value\". The example below will print a job's Slurm\njob ID and allocated hosts for a batch job only.\n#!/bin/sh\n#\n# Sample TaskProlog script that will print a batch job's\n# job ID and node list to the job's stdout\n#\n\nif [ X\"$SLURM_STEP_ID\" = \"X\" -a X\"$SLURM_PROCID\" = \"X\"0 ]\nthen\n  echo \"print ==========================================\"\n  echo \"print SLURM_JOB_ID = $SLURM_JOB_ID\"\n  echo \"print SLURM_JOB_NODELIST = $SLURM_JOB_NODELIST\"\n  echo \"print ==========================================\"\nfi\nIs it possible to write to user stdout?\nThe way user I/O is handled by Slurm makes it impossible to write to the\nuser process as an admin after the user process is executed (execve is called).\nThis happens right after the call to\nTaskProlog, which is the last moment we can\nwrite to the stdout of the user process. Slurm assumes that this file\ndescriptor is only owned by the user process while running. The file descriptor\nis opened as specified and passed to the task so it makes use of the file\ndescriptor directly. Slumstepd is able to log error messages to the error file\nby duplicating the standard error of the process.It is possible to write to standard error from SPANK plugins, but this\ncan't be used to append a job summary, since the file descriptors are opened\nwith a close-on-exec flag and are closed by the operating system right after\nthe user process completes. In theory, a central place that could be used to\nprepare some kind of job summary is EpilogSlurmctld. However, using it to\nwrite to a file where user output is stored may be problematic. The script is\nrunning as SlurmUser, so intensive validation of the file name may be required\n(e.g. to prevent users from specifying something like /etc/passwd as the\noutput file). It's also possible that a job could have multiple output files\n(see filename pattern in the srun\nman page).Why are user processes and srun\nrunning even though the job is supposed to be completed?\nSlurm relies upon a configurable process tracking plugin to determine\nwhen all of the processes associated with a job or job step have completed.\nThose plugins relying upon a kernel patch can reliably identify every process.\nThose plugins dependent upon process group IDs or parent process IDs are not\nreliable. See the ProctrackType description in the slurm.conf\nman page for details. We rely upon the cgroup plugin for most systems.How can a job which has exited with a specific exit\n  code be requeued?\nSlurm supports requeue in hold with a SPECIAL_EXIT state using the\ncommand:scontrol requeuehold State=SpecialExit job_idThis is useful when users want to requeue and flag a job which has exited\nwith a specific error case. See man scontrol(1) for more details.\n$ scontrol requeuehold State=SpecialExit 10\n$ squeue\n   JOBID PARTITION  NAME     USER  ST       TIME  NODES NODELIST(REASON)\n    10      mira    zoppo    david SE       0:00      1 (JobHeldUser)\n\nThe job can be later released and run again.\n\nThe requeuing of jobs which exit with a specific exit code can be\nautomated using an EpilogSlurmctld, see man(5) slurm.conf.\nThis is an example of a script which exit code depends on the existence\nof a file.\n\n$ cat exitme\n#!/bin/sh\n#\necho \"hi! `date`\"\nif [ ! -e \"/tmp/myfile\" ]; then\n  echo \"going out with 8\"\n  exit 8\nfi\nrm /tmp/myfile\necho \"going out with 0\"\nexit 0\n\nThis is an example of an EpilogSlurmctld that checks the job exit value\nlooking at the SLURM_JOB_EXIT2 environment variable and requeues a job if\nit exited with value 8. The SLURM_JOB_EXIT2 has the format \"exit:sig\", the first\nnumber is the exit code, typically as set by the exit() function.\nThe second number of the signal that caused the process to terminate if\nit was terminated by a signal.\n\n$ cat slurmctldepilog\n#!/bin/sh\n\nexport PATH=/bin:/home/slurm/linux/bin\nLOG=/home/slurm/linux/log/logslurmepilog\n\necho \"Start `date`\" >> $LOG 2>&1\necho \"Job $SLURM_JOB_ID exitcode $SLURM_JOB_EXIT_CODE2\" >> $LOG 2>&1\nexitcode=`echo $SLURM_JOB_EXIT_CODE2|awk '{split($0, a, \":\"); print a[1]}'` >> $LOG 2>&1\nif [ \"$exitcode\" == \"8\" ]; then\n   echo \"Found REQUEUE_EXIT_CODE: $REQUEUE_EXIT_CODE\" >> $LOG 2>&1\n   scontrol requeuehold state=SpecialExit $SLURM_JOB_ID >> $LOG 2>&1\n   echo $? >> $LOG 2>&1\nelse\n   echo \"Job $SLURM_JOB_ID exit all right\" >> $LOG 2>&1\nfi\necho \"Done `date`\" >> $LOG 2>&1\n\nexit 0\n\nUsing the exitme script as an example, we have it exit with a value of 8 on\nthe first run, then when it gets requeued in hold with SpecialExit state\nwe touch the file /tmp/myfile, then release the job which will finish\nin a COMPLETE state.\nWhy is Slurm unable to set the CPU frequency for\n    jobs?\nFirst check that Slurm is configured to bind jobs to specific CPUs by\nmaking sure that TaskPlugin is configured to either affinity or cgroup.\nNext check that your processor is configured to permit frequency\ncontrol by examining the values in the file\n/sys/devices/system/cpu/cpu0/cpufreq where \"cpu0\" represents a CPU ID 0.\nOf particular interest is the file scaling_available_governors,\nwhich identifies the CPU governors available.\nIf \"userspace\" is not an available CPU governor, this may well be due to the\nintel_pstate driver being installed.\nInformation about disabling the intel_pstate driver is available\nfrom\n\nhttps://bugzilla.kernel.org/show_bug.cgi?id=57141 and\n\nhttp://unix.stackexchange.com/questions/121410/setting-cpu-governor-to-on-demand-or-conservative.Can the salloc command be configured to\nlaunch a shell on a node in the job's allocation?\nYes, just set \"use_interactive_step\" as part of the LaunchParameters\nconfiguration option in slurm.conf.How can I set up a private /tmp and /dev/shm for\n  jobs on my machine?\n\nTmpfs job container plugin can be used by including\nJobContainerType=job_container/tmpfs\nin your slurm.conf file. It additionally requires a\njob_container.conf file to be\nset up which is further described in the man page.\nTmpfs plugin creates a private mount namespace inside of which it mounts a\nprivate /tmp to a location that is configured in job_container.conf. The basepath\nis used to construct the mount path, by creating a job specific directory inside it\nand mounting /tmp to it. Since all the mounts are created inside of a mount\nnamespace which is private, they are only visible inside the job. Hence this\nproves to be a useful solution for jobs that are on shared nodes, since each\njob can only view mounts created in their own mount namespace. A private\n/dev/shm is also mounted to isolate it between different jobs.\nMount namespace construction also happens before job's spank environment is\nset up. Hence all spank related job steps will view only private /tmp the\nplugin creates. The plugin also provides an optional initialization script that\nis invoked before the job's namespace is constructed. This can be useful for\nany site specific customization that may be necessary.\nparallels@linux_vb:~$ echo $SLURM_JOB_ID\n7\nparallels@linux_vb:~$ findmnt -o+PROPAGATION | grep /tmp\n\u2514\u2500/tmp  /dev/sda1[/storage/7/.7] ext4  rw,relatime,errors=remount-ro,data=ordered   private\nIn the example above, BasePath points to /storage and a slurm job with\njob id 7 is set up to mount /tmp on /storage/7/.7. When user from inside a job\ntries to look up mounts, they can see that their /tmp is mounted. However\nthey are prevented from mistakenly accessing the backing directory directly.\nparallels@linux_vb:~$ cd /storage/7/\nbash: cd: /storage/7/: Permission denied\nThey are allowed to access (read/write) /tmp only.\nAdditionally pam_slurm_adopt has also been extended to support this functionality.\nIf a user starts an ssh session which is managed by pam_slurm_adopt, then\nthe user's process joins the namespace that is constructed by tmpfs plugin.\nHence in ssh sessions, user has the same view of /tmp and /dev/shm as\ntheir job. This functionality is enabled by default in pam_slurm_adopt\nbut can be disabled explicitly by appending join_container=false as shown:\naccount\tsufficient  pam_slurm_adopt.so join_container=false\nHow do I configure Slurm to work with System V IPC\n  enabled applications?\nSlurm is generally agnostic to\n\nSystem V IPC (a.k.a. \"sysv ipc\" in the Linux kernel).\nMemory accounting of processes using sysv ipc changes depending on the value\nof \nsysctl kernel.shm_rmid_forced (added in Linux kernel 3.1):\n\nshm_rmid_forced = 1\n\nForces all shared memory usage of processes to be accounted and reported by the\nkernel to Slurm. This breaks the separate namespace of sysv ipc and may cause\nunexpected application issues without careful planning. Processes that share\nthe same sysv ipc namespaces across jobs may end up getting OOM killed when\nanother job ends and their allocation percentage increases.\n\nshm_rmid_forced = 0 (default in most Linux distributions)\n\nSystem V memory usage will not be reported by Slurm for jobs.\nIt is generally suggested to configure the\n\nsysctl kernel.shmmax parameter. The value of kernel.shmmax times the\nmaximum number of job processes should be deducted from each node's\nconfigured RealMemory in your slurm.conf. Most Linux distributions set the\ndefault to what is effectively unlimited, which can cause the OOM killer\nto activate for unrelated new jobs or even for the slurmd process. If any\nprocesses use sysv memory mechanisms, the Linux kernel OOM killer will never\nbe able to free the used memory. A Slurm job epilog script will be needed to\nfree any of the user memory. Setting kernel.shmmax=0 will disable sysv ipc\nmemory allocations but may cause application issues.\n\nGeneral TroubleshootingIf a Slurm daemon core dumps, where can I find the\ncore file?\nIf slurmctld is started with the -D option, then the core file will be\nwritten to the current working directory. If SlurmctldLogFile is an\nabsolute path, the core file will be written to this directory. Otherwise the\ncore file will be written to the StateSaveLocation, or \"/var/tmp/\" as a\nlast resort.\nSlurmUser must have write permission on the directories. If none of the above\ndirectories have write permission for SlurmUser, no core file will be produced.If slurmd is started with the -D option, then the core file will also be\nwritten to the current working directory. If SlurmdLogFile is an\nabsolute path, the core file will be written to the this directory.\nOtherwise the core file will be written to the SlurmdSpoolDir, or\n\"/var/tmp/\" as a last resort.\nIf none of the above directories can be written, no core file will be produced.\nFor slurmstepd, the core file will depend upon when the failure\noccurs. If it is running in a privileged phase, it will be in the same location\nas that described above for the slurmd daemon. If it is running in an\nunprivileged phase, it will be in the spawned job's working directory.Nevertheless, in some operating systems this can vary:\n\nI.e. in RHEL the event\nmay be captured by abrt daemon and generated in the defined abrt configured\ndump location (i.e. /var/spool/abrt).\n\nNormally, distributions need some more tweaking in order to allow the core\nfiles to be generated correctly.slurmstepd uses the setuid() (set user ID) function to escalate\nprivileges. It is possible that in certain systems and for security policies,\nthis causes the core files not to be generated.\nTo allow the generation in such systems you usually must enable the\nsuid_dumpable kernel parameter:The value of 2, \"suidsafe\", makes any binary which normally not be dumped is\ndumped readable by root only.This allows the end user to remove such a dump\nbut not access it directly. For security reasons core dumps in this mode will\nnot overwrite one another or other files. This mode is appropriate when\nadministrators are attempting to debug problems in a normal environment.Then you must also set the core pattern to an absolute pathname:sysctl kernel.core_pattern=/tmp/core.%e.%pWe recommend reading your distribution's documentation about the\nconfiguration of these parameters.It is also usually needed to configure the system core limits, since it can be\nset to 0.\n$ grep core /etc/security/limits.conf\n#        - core - limits the core file size (KB)\n*               hard    core            unlimited\n*               soft    core            unlimited\nIn some systems it is not enough to set a hard limit, you must set also a\nsoft limit.Also, for generating the limits in userspace, the\nPropagateResourceLimits=CORE parameter in slurm.conf could be needed.Be also sure to give SlurmUser the appropriate permissions to write in the\ncore location directories.NOTE: On a diskless node depending on the core_pattern or if\n/var/spool/abrt is pointing to an in-memory filespace like tmpfs, if the job\ncaused an OOM, then the generation of the core may fill up your machine's\nmemory and hang it. It is encouraged then to make coredumps go to a persistent\nstorage. Be careful of multiple nodes writing a core dump to a shared\nfilesystem since it may significantly impact it.\nOther exceptions:On Centos 6, also set \"ProcessUnpackaged = yes\" in the file\n/etc/abrt/abrt-action-save-package-data.conf.\n\nOn RHEL6, also set \"DAEMON_COREFILE_LIMIT=unlimited\" in the file\nrc.d/init.d/functions.\nOn a SELinux enabled system, or on a distribution with similar security\nsystem, get sure it is allowing to dump cores:\n$ getsebool allow_daemons_dump_core\ncoredumpctl can also give valuable information:\n$ coredumpctl info\nHow can I get a backtrace from a core file?\nIf you do have a crash that generates a core file, you will want to get a\nbacktrace of that crash to send to SchedMD for evaluation.\nNOTE: Core files must be analyzed by the same binary that was used\nwhen they were generated. Compile time differences make it almost impossible\nfor SchedMD to use a core file from a different system. You should always\nsend a backtrace rather than a core file when sumitting a support request.\nIn order to generate a backtrace you must use gdb, specify the\npath to the slurm* binary that generated the crash, and specify the\npath to the core file. Below is an example of how to get a backtrace of a\ncore file generated by slurmctld:\n\ngdb -ex 't a a bt full' -batch /path/to/slurmctld <core_file>\n\n\nYou can also use gdb to generate a backtrace without a core file.\nThis can be useful if you are experiencing a crash on startup and aren't\ngetting a core file for some reason. You would want to start the binary\nfrom inside of gdb, wait for it to crash, and generate the backtrace.\nBelow is an example, using slurmctld as the example binary:\n\n(gdb) /path/to/slurmctld\n(gdb) set print pretty\n(gdb) r -d\n(gdb) t a a bt full\n\n\nYou may also need to get a backtrace of a running daemon if it is stuck\nor hung. To do this you would point gdb at the running binary and\nhave it generate the backtrace. Below is an example, again using\nslurmctld as the example:\n\ngdb -ex 't a a bt' -batch -p $(pidof slurmctld)\n\n\nError Messages\n\"Cannot resolve X plugin operations\" on\n  daemon startup\nThis means that symbols expected in the plugin were\nnot found by the daemon. This typically happens when the\nplugin was built or installed improperly or the configuration\nfile is telling the plugin to use an old plugin (say from the\nprevious version of Slurm). Restart the daemon in verbose mode\nfor more information (e.g. \"slurmctld -Dvvvvv\").\n\"Credential replayed\" in\n  SlurmdLogFile\nThis error is indicative of the slurmd daemon not being able\nto respond to job initiation requests from the srun command\nin a timely fashion (a few seconds).\nSrun responds by resending the job initiation request.\nWhen the slurmd daemon finally starts to respond, it\nprocesses both requests.\nThe second request is rejected and the event is logged with\nthe \"credential replayed\" error.\nIf you check the SlurmdLogFile and SlurmctldLogFile,\nyou should see signs of the slurmd daemon's non-responsiveness.\nA variety of factors can be responsible for this problem\nincluding\n\nDiskless nodes encountering network problems\nVery slow Network Information Service (NIS)\nThe Prolog script taking a long time to complete\n\nConfigure MessageTimeout in slurm.conf to a value higher than the\ndefault 10 seconds.\n\"Invalid job credential\"\nThis error is indicative of Slurm's job credential files being inconsistent across\nthe cluster. All nodes in the cluster must have the matching public and private\nkeys as defined by JobCredPrivateKey and JobCredPublicKey in the\nSlurm configuration file slurm.conf.\n\"Task launch failed on node ... Job credential\n  replayed\"\nThis error indicates that a job credential generated by the slurmctld daemon\ncorresponds to a job that the slurmd daemon has already revoked.\nThe slurmctld daemon selects job ID values based upon the configured\nvalue of FirstJobId (the default value is 1) and each job gets\na value one larger than the previous job.\nOn job termination, the slurmctld daemon notifies the slurmd on each\nallocated node that all processes associated with that job should be\nterminated.\nThe slurmd daemon maintains a list of the jobs which have already been\nterminated to avoid replay of task launch requests.\nIf the slurmctld daemon is cold-started (with the \"-c\" option\nor \"/etc/init.d/slurm startclean\"), it starts job ID values\nover based upon FirstJobId.\nIf the slurmd is not also cold-started, it will reject job launch requests\nfor jobs that it considers terminated.\nThis solution to this problem is to cold-start all slurmd daemons whenever\nthe slurmctld daemon is cold-started.\n\"Unable to accept new connection: Too many open\n  files\"\nThe srun command automatically increases its open file limit to\nthe hard limit in order to process all of the standard input and output\nconnections to the launched tasks. It is recommended that you set the\nopen file hard limit to 8192 across the cluster.\nSlurmdDebug fails to log job step information\n  at the appropriate level\nThere are two programs involved here. One is slurmd, which is\na persistent daemon running at the desired debug level. The second\nprogram is slurmstepd, which executes the user job and its\ndebug level is controlled by the user. Submitting the job with\nan option of --debug=# will result in the desired level of\ndetail being logged in the SlurmdLogFile plus the output\nof the program.\n\"Batch JobId=# missing from batch node <node>\n  (not found BatchStartTime after startup)\"\nA shell is launched on node zero of a job's allocation to execute\nthe submitted program. The slurmd daemon executing on each compute\nnode will periodically report to the slurmctld what programs it\nis executing. If a batch program is expected to be running on some\nnode (i.e. node zero of the job's allocation) and is not found, the\nmessage above will be logged and the job canceled. This typically is\nassociated with exhausting memory on the node or some other critical\nfailure that cannot be recovered from.\nMulti-Instance GPU not working with Slurm and PMIx;\n  GPUs are \"In use by another client\"\nPMIx uses the hwloc API for different purposes, including\nOS device features like querying sysfs folders (such as\n/sys/class/net and /sys/class/infiniband) to get the names of\nInfiniband HCAs. With the above mentioned features, hwloc defaults to\nquerying the OpenCL devices, which creates handles on /dev/nvidia* files.\nThese handles are kept by slurmstepd and will result in the following error\ninside a job:\n\n\n$ nvidia-smi mig --id 1 --create-gpu-instance FOO,FOO --default-compute-instance\nUnable to create a GPU instance on GPU 1 using profile FOO: In use by another client\n\n\nIn order to use Multi-Instance GPUs with Slurm and PMIx you can instruct hwloc\nto not query OpenCL devices by setting the\nHWLOC_COMPONENTS=-opencl environment\nvariable for slurmd, i.e. setting this variable in systemd unit file for slurmd.\n\n\"srun: error: Unable to accept connection:\n  Resources temporarily unavailable\"\nThis has been reported on some larger clusters running SUSE Linux when\na user's resource limits are reached. You may need to increase limits\nfor locked memory and stack size to resolve this problem.\n\"Warning: Note very large processing time\"\n  in SlurmctldLogFile\nThis error is indicative of some operation taking an unexpectedly\nlong time to complete, over one second to be specific.\nSetting the value of the SlurmctldDebug configuration parameter\nto debug2 or higher should identify which operation(s) are\nexperiencing long delays.\nThis message typically indicates long delays in file system access\n(writing state information or getting user information).\nAnother possibility is that the node on which the slurmctld\ndaemon executes has exhausted memory and is paging.\nTry running the program top to check for this possibility.\n\"Duplicate entry\" causes slurmdbd to\n  fail\nThis problem has been rarely observed with MySQL, but not MariaDB.\nThe root cause of the failure seems to be reaching the upper limit on the auto increment field.\nUpgrading to MariaDB is recommended.\nIf that is not possible then: backup the database, remove the duplicate record(s),\nand restart the slurmdbd daemon as shown below.\n\n$ slurmdbd -Dvv\n...\nslurmdbd: debug:  Table \"cray_job_table\" has changed.  Updating...\nslurmdbd: error: mysql_query failed: 1062 Duplicate entry '2711-1478734628' for key 'id_job'\n...\n\n$ mysqldump --single-transaction -u<user> -p<user> slurm_acct_db >/tmp/slurm_db_backup.sql\n\n$ mysql\nmysql> use slurm_acct_db;\nmysql> delete from cray_job_table where id_job='2711-1478734628';\nmysql> quit;\nBye\n\nIf necessary, you can edit the database dump and recreate the database as\nshown below.\n\n$ mysql\nmysql> drop database slurm_acct_db;\nmysql> create database slurm_acct_db;\nmysql> quit;\nBye\n\n$ mysql -u<user> -p<user> </tmp/slurm_db_backup.sql\n\n\"Unable to find plugin: serializer/json\"\n  \nSeveral parts of Slurm have swapped to using our centralized serializer\ncode. JSON or YAML plugins are only required if one of the functions that\nrequire it is executed. If one of the functions is executed it will fail to\ncreate the JSON/YAML output and the linker will fail with the following error:\n\n\nslurmctld: fatal: Unable to find plugin: serializer/json\n\n\nIn most cases, these are required for new functionality added after Slurm-20.02.\nHowever, with each release, we have been adding more places that use the\nserializer plugins. Because the list is evolving we do not plan on listing all\nthe commands that require the plugins but will instead provide the error\n(shown above). To correct the issue, please make sure that Slurm is configured,\ncompiled and installed with the relevant JSON or YAML library (or preferably\nboth). Configure can be made to explicitly request these libraries:\n\n\n./configure --with-json=PATH --with-yaml=PATH $@\n\n\nMost distributions include packages to make installation relatively easy.\nPlease make sure to install the 'dev' or 'devel' packages along with the\nlibrary packages. We also provide explicit instructions on how to install from\nsource: libyaml and\nlibjwt.\n\nThird Party Integrations\nCan Slurm be used with Globus?\nYes. Build and install Slurm's Torque/PBS command wrappers along with\nthe Perl APIs from Slurm's contribs directory and configure\nGlobus to use those PBS commands.\nNote there are RPMs available for both of these packages, named\ntorque and perlapi respectively.\n\nHow can TotalView be configured to operate with\n  Slurm?\nThe following lines should also be added to the global .tvdrc file\nfor TotalView to operate with Slurm:\n\n# Enable debug server bulk launch: Checked\ndset -set_as_default TV::bulk_launch_enabled true\n\n# Command:\n# Beginning with TV 7X.1, TV supports Slurm and %J.\n# Specify --mem-per-cpu=0 in case Slurm configured with default memory\n# value and we want TotalView to share the job's memory limit without\n# consuming any of the job's memory so as to block other job steps.\ndset -set_as_default TV::bulk_launch_string {srun --mem-per-cpu=0 -N%N -n%N -w`awk -F. 'BEGIN {ORS=\",\"} {if (NR==%N) ORS=\"\"; print $1}' %t1` -l --input=none %B/tvdsvr%K -callback_host %H -callback_ports %L -set_pws %P -verbosity %V -working_directory %D %F}\n\n# Temp File 1 Prototype:\n# Host Lines:\n# Slurm NodeNames need to be unadorned hostnames. In case %R returns\n# fully qualified hostnames, list the hostnames in %t1 here, and use\n# awk in the launch string above to strip away domain name suffixes.\ndset -set_as_default TV::bulk_launch_tmpfile1_host_lines {%R}\n\n\nLast modified 11 October 2024\n"
        },
        {
            "title": "Navigation",
            "content": "\nSlurm Workload Manager\nVersion 24.05\n\n\nAbout\n\nOverview\nRelease Notes\n\n\n\nUsing\n\nDocumentation\nFAQ\nPublications\n\n\n\nInstalling\n\nDownload\nRelated Software\nInstallation Guide\n\n\n\nGetting Help\n\nMailing Lists\nSupport and Training\nTroubleshooting\n\n\n"
        }
    ]
}